<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/de70bee13400563f.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7da0a892b4ad83db.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7"/><script src="/_next/static/chunks/87c73c54-dd8d81ac9604067c.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/18-2224119117d14cba.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/main-app-57aa1716f0d0f500.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/5462-08221e91030fd747.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/4429-943205658cbafffe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/9976-9250854d58eefaa3.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/1481-25d5bbc4f2d9524a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-6651f8cd8321a0db.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/25-9f305b682cea7558.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7391-f64e18878e224268.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/6373-d56a493968555802.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><meta name="next-size-adjust" content=""/><title>EOEPCA/eoepca | DeepWiki</title><meta name="description" content="This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes "/><meta name="keywords" content="EOEPCA/eoepca,EOEPCA,eoepca,documentation,wiki,codebase,AI documentation,Devin,Overview"/><link rel="canonical" href="https://deepwiki.com/EOEPCA/eoepca"/><meta property="og:title" content="EOEPCA/eoepca | DeepWiki"/><meta property="og:description" content="This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes "/><meta property="og:url" content="https://deepwiki.com/EOEPCA/eoepca"/><meta property="og:site_name" content="DeepWiki"/><meta property="og:image" content="https://deepwiki.com/EOEPCA/eoepca/og-image.png?page=1"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@cognition"/><meta name="twitter:creator" content="@cognition"/><meta name="twitter:title" content="EOEPCA/eoepca | DeepWiki"/><meta name="twitter:description" content="This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes "/><meta name="twitter:image" content="https://deepwiki.com/EOEPCA/eoepca/og-image.png?page=1"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="icon" href="/icon.png?1ee4c6a68a73a205" type="image/png" sizes="48x48"/><link rel="apple-touch-icon" href="/apple-icon.png?a4f658907db0ab87" type="image/png" sizes="180x180"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" noModule=""></script></head><body class="__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased"><div hidden=""><!--$--><!--/$--></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","light",null,["light","dark"],null,true,true)</script><!--$?--><template id="B:0"></template><div class="flex min-h-screen w-full flex-col text-white"><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div></div><!--/$--><script>requestAnimationFrame(function(){$RT=performance.now()});</script><script src="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" id="_R_" async=""></script><div hidden id="S:0"><div class="flex min-h-screen w-full flex-col text-white" id="codebase-wiki-repo-page"><div class="bg-background border-b-border sticky top-0 z-30 border-b border-dashed"><div class="font-geist-mono relative flex h-8 items-center justify-center text-xs font-medium sm:hidden"><div class="powered-by-devin-gradient absolute inset-0 z-[-1] h-8 w-full"></div><button class="flex items-center gap-2"><svg class="size-3 [&amp;_path]:stroke-0 [&amp;_path]:animate-[custom-pulse_1.8s_infinite_var(--delay,0s)]" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="[--delay:0.6s]" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="[--delay:1.2s]" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg>Index your code with Devin</button></div><div class="container-wrapper"><div class="container mx-auto flex w-full flex-row items-center gap-2 py-4 md:py-6"><a class="flex items-center gap-3" href="/"><span class="text-base font-medium leading-none md:text-lg hidden sm:block">DeepWiki</span></a><div class="flex-1"><div class="flex flex-row items-center gap-2"><a class="block text-xs font-medium leading-none text-white sm:hidden md:text-lg" href="/">DeepWiki</a><p class="text-sm font-normal leading-none md:text-lg"><a href="https://github.com/EOEPCA/eoepca" target="_blank" rel="noopener noreferrer" title="Open repository" class="text-muted-foreground hover:text-muted-foreground/80 group inline-flex items-center gap-1 transition-colors">EOEPCA/eoepca<!-- --> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="opacity-0 transition-opacity group-hover:opacity-100"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a></p></div></div><div class="flex items-center gap-4"><button class="group hidden items-center gap-1.5 md:flex"><div class="relative"><span class="text-foreground/70 group-hover:text-foreground text-xs font-light transition-colors">Index your code with</span><div class="bg-foreground/30 absolute bottom-0 left-0 h-[1px] w-0 transition-all duration-300 group-hover:w-full"></div></div><div class="flex items-center gap-1 transition-transform duration-300 group-hover:translate-x-0.5"><svg class="size-4 transform transition-transform duration-700 group-hover:rotate-180 [&amp;_path]:stroke-0" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg><span class="text-sm font-medium">Devin</span></div></button><button aria-label="Edit Wiki" class="flex items-center rounded-md cursor-pointer transition-all border border-border bg-surface hover:border-border-hover hover:bg-component disabled:cursor-default disabled:opacity-50 disabled:hover:border-border disabled:hover:bg-surface gap-2 px-3 py-1.5 text-sm"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 256 256"><path d="M227.32,73.37,182.63,28.69a16,16,0,0,0-22.63,0L36.69,152A15.86,15.86,0,0,0,32,163.31V208a16,16,0,0,0,16,16H216a8,8,0,0,0,0-16H115.32l112-112A16,16,0,0,0,227.32,73.37ZM92.69,208H48V163.31l88-88L180.69,120ZM192,108.69,147.32,64l24-24L216,84.69Z"></path></svg>Edit Wiki</button><button class="flex items-center rounded-md !text-white cursor-pointer transition-all border bg-blue-500 hover:bg-blue-600 border-blue-500 hover:border-blue-600 dark:bg-blue-900 dark:hover:bg-blue-800 dark:border-blue-900 dark:hover:border-blue-800 disabled:cursor-default disabled:opacity-50 disabled:hover:bg-blue-500 disabled:hover:border-blue-500 dark:disabled:hover:bg-blue-900 dark:disabled:hover:border-blue-900 gap-1.5 px-3 py-1.5 text-sm" aria-label="Share" data-state="closed" data-slot="tooltip-trigger"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg><span>Share</span></button><div class="h-8 w-8"></div></div></div></div></div><!--$?--><template id="B:1"></template><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div><!--/$--></div></div><script>$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><div hidden id="S:1"><script type="application/ld+json">{"@context":"https://schema.org","@type":"TechArticle","headline":"Overview","description":"This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes ","image":"https://deepwiki.com/EOEPCA/eoepca/og-image.png","datePublished":"2026-01-13T11:25:08.372991","dateModified":"2026-01-13T11:25:08.372991","author":{"@type":"Organization","name":"DeepWiki","url":"https://deepwiki.com"},"publisher":{"@type":"Organization","name":"DeepWiki","logo":{"@type":"ImageObject","url":"https://deepwiki.com/icon.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://deepwiki.com/EOEPCA/eoepca"}}</script><div class="w-full flex-1"><div class="container-wrapper relative mx-auto h-full px-0"><div class="container relative mx-auto flex h-full w-full flex-col gap-0 max-md:!px-0 md:flex-row md:gap-6 lg:gap-10"><div class="border-r-border hidden max-h-screen border-r border-dashed py-6 pr-4 transition-[border-radius] md:sticky md:left-0 md:top-20 md:block md:h-[calc(100vh-82px)] md:w-64 md:flex-shrink-0 md:overflow-y-auto lg:py-9 xl:w-72"><div class="flex h-full w-full max-w-full flex-shrink-0 flex-col overflow-hidden" style="scrollbar-color:var(--color-border) transparent"><div class="flex-shrink-0 px-2"><div class="text-secondary pb-1 text-xs">Last indexed: <!-- -->13 January 2026<!-- --> (<a href="https://github.com/EOEPCA/eoepca/commits/7aef2f4a" target="_blank" rel="noopener noreferrer" class="underline-offset-2 hover:underline">7aef2f</a>)</div></div><ul class="flex-1 flex-shrink-0 space-y-1 overflow-y-auto py-1" style="scrollbar-width:none"><li style="padding-left:0"><a data-selected="true" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/1-overview">Overview</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/2-getting-started">Getting Started</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/2.1-deployment-guide">Deployment Guide</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/2.2-infrastructure-provisioning">Infrastructure Provisioning</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/2.3-testing-and-validation">Testing and Validation</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/3-system-architecture">System Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/3.1-building-blocks-overview">Building Blocks Overview</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/3.2-gitops-and-flux-cd">GitOps and Flux CD</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/3.3-network-and-ingress">Network and Ingress</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/4-user-management-and-identity">User Management and Identity</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/4.1-identity-service-(keycloak)">Identity Service (Keycloak)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/4.2-login-service-(gluu)">Login Service (Gluu)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/4.3-policy-enforcement-(peppdp)">Policy Enforcement (PEP/PDP)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/4.4-uma-authentication-flow">UMA Authentication Flow</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/5-resource-management">Resource Management</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/5.1-data-access-services">Data Access Services</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/5.2-resource-catalogue">Resource Catalogue</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/5.3-workspace-api">Workspace API</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/5.4-data-registration-and-harvesting">Data Registration and Harvesting</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/5.5-multi-tenant-workspaces">Multi-Tenant Workspaces</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/6-processing-and-chaining">Processing and Chaining</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/6.1-ades-(application-deployment-and-execution-service)">ADES (Application Deployment and Execution Service)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/6.2-application-hub-(jupyterhub)">Application Hub (JupyterHub)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/6.3-processor-development-environment-(pde)">Processor Development Environment (PDE)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/6.4-resource-guards-and-access-control">Resource Guards and Access Control</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/6.5-cwl-application-packages">CWL Application Packages</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/7-storage-and-persistence">Storage and Persistence</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/7.1-s3-storage-architecture">S3 Storage Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/7.2-database-systems">Database Systems</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/7.3-nfs-and-persistent-volumes">NFS and Persistent Volumes</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/8-infrastructure">Infrastructure</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/8.1-kubernetes-cluster-setup">Kubernetes Cluster Setup</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/8.2-terraform-infrastructure-as-code">Terraform Infrastructure as Code</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/8.3-network-architecture">Network Architecture</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/9-development-and-testing">Development and Testing</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/9.1-democlient-library">DemoClient Library</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/9.2-acceptance-testing-framework">Acceptance Testing Framework</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/9.3-local-development-with-minikube">Local Development with Minikube</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/10-configuration-and-secrets-management">Configuration and Secrets Management</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/10.1-sealedsecrets">SealedSecrets</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/10.2-oidc-client-configuration">OIDC Client Configuration</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/11-operations-and-maintenance">Operations and Maintenance</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/11.1-policy-management-tools">Policy Management Tools</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/EOEPCA/eoepca/11.2-monitoring-and-troubleshooting">Monitoring and Troubleshooting</a></li></ul></div></div><div class="flex h-full flex-1 flex-col overflow-hidden"><div class="bg-background border-b-border sticky top-0 z-10 border-b border-dashed md:hidden"><div class="flex cursor-pointer items-center gap-2 p-3"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="transition-transform"><path d="M184.49,136.49l-80,80a12,12,0,0,1-17-17L159,128,87.51,56.49a12,12,0,1,1,17-17l80,80A12,12,0,0,1,184.49,136.49Z"></path></svg><span class="truncate text-base font-normal">Menu</span></div></div><div class="relative flex-1 overflow-y-auto px-3 pt-3 md:rounded-md md:px-0 md:pt-0 [&amp;_::selection]:bg-purple-500/40" style="scrollbar-color:var(--color-night) transparent"><div class="pb-30 mx-auto max-w-2xl md:pb-40 md:pt-6 lg:pt-8"><div class="prose prose-invert dark:prose-invert prose-headings:text-inherit prose-p:text-inherit max-w-none"><div><div class="prose-custom prose-custom-md prose-custom-gray !max-w-none text-neutral-300 [overflow-wrap:anywhere]"><h1 id="overview" class="group" data-header="true">Overview<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h1>
<details>
<summary>Relevant source files</summary>
<ul>
<li><a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span></a></li>
<li><a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/minikube/README.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>minikube/README.md</span></a></li>
<li><a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/release-notes/release-0.3.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>release-notes/release-0.3.md</span></a></li>
<li><a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/system/clusters/README.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>system/clusters/README.md</span></a></li>
</ul>
</details>
<h2 id="purpose-and-scope" class="group" data-header="true">Purpose and Scope<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes the repository&#x27;s purpose, structure, major building blocks, and deployment model. For detailed deployment instructions, see <a href="/EOEPCA/eoepca/2-getting-started" class="text-neutral-300 hover:text-neutral-200 hover:underline">Getting Started</a>. For architectural details of individual subsystems, see <a href="/EOEPCA/eoepca/3-system-architecture" class="text-neutral-300 hover:text-neutral-200 hover:underline">System Architecture</a>.</p>
<h2 id="about-eoepca" class="group" data-header="true">About EOEPCA<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The EOEPCA project establishes a consensus of best practices for Earth Observation (EO) Exploitation Platforms based on open standards. The goal is to enable interoperability between distributed EO platforms, creating an open network of resources where users can efficiently access and collaborate on geospatial analysis tasks.</p>
<p>This repository at <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">https://github.com/EOEPCA/eoepca</code> represents the <strong>system integration</strong> of building blocks that comprise the Common Architecture reference implementation. It is designed for deployment to cloud infrastructure orchestrated by Kubernetes and includes automation for provisioning, deploying, and testing the complete EOEPCA system.</p>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L58-L70" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">58-70</span></a></p>
<h2 id="repository-structure" class="group" data-header="true">Repository Structure<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The repository is organized into several key directories that separate concerns for infrastructure provisioning, deployment specifications, and testing:</p>





































<table><thead><tr><th>Directory</th><th>Purpose</th></tr></thead><tbody><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">system/</code></td><td>GitOps deployment specifications using Flux CD, organized by building block</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">system/clusters/</code></td><td>Cluster-specific deployment configurations (e.g., <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">develop</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">creodias</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">minikube</code>)</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">creodias/</code></td><td>Infrastructure provisioning for CREODIAS OpenStack using Terraform</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">kubernetes/</code></td><td>Kubernetes cluster setup using Rancher Kubernetes Engine (RKE)</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">minikube/</code></td><td>Local Kubernetes cluster setup for development using Minikube or k3s</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">test/acceptance/</code></td><td>Robot Framework acceptance test suite</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">release-notes/</code></td><td>Historical release documentation</td></tr></tbody></table>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">system/</code> directory contains subdirectories for each building block domain: <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">user-management/</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">resource-management/</code>, and <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">processing-and-chaining/</code>. Each contains HelmRelease resources and Kustomization manifests that define the deployment.</p>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L88-L96" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">88-96</span></a> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/system/clusters/README.md#L1-L95" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>system/clusters/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-95</span></a> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/minikube/README.md#L1-L53" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>minikube/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-53</span></a></p>
<h2 id="major-building-blocks" class="group" data-header="true">Major Building Blocks<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>EOEPCA consists of three major building block domains, each containing multiple services deployed as Kubernetes resources:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<h3 id="user-management-building-blocks" class="group" data-header="true">User Management Building Blocks<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The user management domain provides authentication and authorization services:</p>
<ul>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">identity-service</code></strong>: Keycloak-based identity provider with PostgreSQL backend, providing OAuth2/OIDC authentication</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">login-service</code></strong>: Gluu-based login service (deprecated, replaced by Keycloak solution)</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">pdp-engine</code></strong>: Policy Decision Point for evaluating access control policies</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">resource-guard</code></strong>: Policy Enforcement Point that intercepts requests and enforces UMA-based authorization</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">user-profile</code></strong>: SCIM-compliant user profile management</li>
</ul>
<p>For detailed information, see <a href="/EOEPCA/eoepca/4-user-management-and-identity" class="text-neutral-300 hover:text-neutral-200 hover:underline">User Management and Identity</a>.</p>
<h3 id="resource-management-building-blocks" class="group" data-header="true">Resource Management Building Blocks<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The resource management domain handles data cataloging, access, and workspace provisioning:</p>
<ul>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">resource-catalogue</code></strong>: OGC CSW/OpenSearch interface built on <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">pycsw</code>, storing ISO 19115 metadata in PostgreSQL</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">data-access</code></strong>: OGC WMS/WCS/WMTS services using EOX View Server with rendering and caching capabilities</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">workspace-api</code></strong>: Multi-tenant workspace provisioning, creating per-user namespaces with isolated resources</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">registration-api</code></strong>: API for registering datasets into the catalogue</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">harvester</code></strong>: OpenSearch-based harvester for ingesting external data sources</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">bucket-operator</code></strong>: MinIO bucket lifecycle management</li>
</ul>
<p>For detailed information, see <a href="/EOEPCA/eoepca/5-resource-management" class="text-neutral-300 hover:text-neutral-200 hover:underline">Resource Management</a>.</p>
<h3 id="processing-chaining-building-blocks" class="group" data-header="true">Processing &amp; Chaining Building Blocks<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The processing domain provides application deployment and execution capabilities:</p>
<ul>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">ades</code></strong>: Application Deployment and Execution Service based on ZOO-Project DRU, implementing OGC API Processes</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">application-hub</code></strong>: JupyterHub deployment for interactive development environments</li>
<li><strong>Calrissian</strong>: CWL workflow executor that creates Kubernetes pods for processing tasks</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">pde-container</code></strong>: Processor Development Environment with integrated tools</li>
</ul>
<p>For detailed information, see <a href="/EOEPCA/eoepca/6-processing-and-chaining" class="text-neutral-300 hover:text-neutral-200 hover:underline">Processing and Chaining</a>.</p>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L128-L160" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">128-160</span></a> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/release-notes/release-0.3.md#L1-L319" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>release-notes/release-0.3.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-319</span></a></p>
<h2 id="gitops-deployment-model" class="group" data-header="true">GitOps Deployment Model<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>EOEPCA uses Flux CD to implement a GitOps continuous delivery model. The deployment process reconciles the cluster state with declarative specifications stored in this Git repository:</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>The deployment is bootstrapped using the <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">deployCluster.sh</code> script, which executes <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">flux bootstrap github</code> to install Flux controllers and configure synchronization with the Git repository. Flux monitors HelmRelease and Kustomization resources in <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">system/clusters/${TARGET}/</code> and automatically reconciles the cluster state.</p>
<p><strong>Key Flux Resources:</strong></p>
<ul>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">GitRepository</code></strong>: Defines the Git source at <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">https://github.com/EOEPCA/eoepca</code> with polling interval</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">Kustomization</code></strong>: References directories like <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">./user-management</code> to apply manifests</li>
<li><strong><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">HelmRelease</code></strong>: Defines Helm chart deployments with values overrides</li>
</ul>
<p>The <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">TARGET</code> variable determines which cluster configuration to deploy (e.g., <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">develop</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">creodias</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">minikube</code>). Each target has cluster-specific values including:</p>
<ul>
<li>Public IP address for ingress (e.g., <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">185.52.193.87</code> for develop cluster)</li>
<li>DNS domain using <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">nip.io</code> pattern (e.g., <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">185-52-193-87.nip.io</code>)</li>
<li>TLS certificate configuration</li>
<li>Resource limits and replicas</li>
</ul>
<p>For more details, see <a href="/EOEPCA/eoepca/3.2-gitops-and-flux-cd" class="text-neutral-300 hover:text-neutral-200 hover:underline">GitOps and Flux CD</a>.</p>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/system/clusters/README.md#L1-L95" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>system/clusters/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-95</span></a> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L79-L112" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">79-112</span></a></p>
<h2 id="multi-tenant-architecture" class="group" data-header="true">Multi-Tenant Architecture<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>EOEPCA implements multi-tenancy through dynamic workspace provisioning. When a user requests a workspace via the <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">workspace-api</code>, the system:</p>
<ol>
<li>Creates a dedicated Kubernetes namespace (e.g., <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">eric-workspace</code>)</li>
<li>Provisions an S3 bucket via <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">bucket-operator</code> and MinIO API</li>
<li>Instantiates template HelmReleases for user-specific services:
<ul>
<li><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">template-hr-resource-catalogue</code>: Private catalogue instance</li>
<li><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">template-hr-data-access</code>: Private data access service</li>
<li><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">template-hr-resource-guard</code>: Private PEP for access control</li>
</ul>
</li>
<li>Configures ownership policies based on user identity token</li>
</ol>
<p>This architecture ensures isolation while enabling federated search across workspaces through the global <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">resource-catalogue</code> service.</p>
<p>For more details, see <a href="/EOEPCA/eoepca/5.5-multi-tenant-workspaces" class="text-neutral-300 hover:text-neutral-200 hover:underline">Multi-Tenant Workspaces</a>.</p>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L58-L70" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">58-70</span></a></p>
<h2 id="infrastructure-and-technology-stack" class="group" data-header="true">Infrastructure and Technology Stack<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<h3 id="kubernetes-platforms" class="group" data-header="true">Kubernetes Platforms<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>EOEPCA supports multiple Kubernetes distributions:</p>
<ul>
<li><strong>Production</strong>: Rancher Kubernetes Engine (RKE) on CREODIAS OpenStack</li>
<li><strong>Development</strong>: Minikube with Docker or native drivers</li>
<li><strong>Lightweight</strong>: k3s as an alternative to Minikube</li>
</ul>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L88-L96" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">88-96</span></a> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/minikube/README.md#L1-L53" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>minikube/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-53</span></a></p>
<h3 id="storage-architecture" class="group" data-header="true">Storage Architecture<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>Multiple storage tiers serve different purposes:</p>



































<table><thead><tr><th>Storage Type</th><th>Implementation</th><th>Purpose</th></tr></thead><tbody><tr><td>Object Storage (Input)</td><td>CloudFerro <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">eodata</code> S3</td><td>Source EO datasets (read-only)</td></tr><tr><td>Object Storage (Output)</td><td>MinIO</td><td>Per-workspace output buckets</td></tr><tr><td>Persistent Volumes</td><td>NFS with <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">managed-nfs</code> StorageClass</td><td>Service state and metadata</td></tr><tr><td>Databases</td><td>PostgreSQL</td><td>Resource Catalogue metadata, Identity Service</td></tr><tr><td>Caching</td><td>Redis</td><td>Data Access caching, Harvester queues</td></tr></tbody></table>
<p>For more details, see <a href="/EOEPCA/eoepca/7-storage-and-persistence" class="text-neutral-300 hover:text-neutral-200 hover:underline">Storage and Persistence</a>.</p>
<h3 id="implemented-ogc-standards" class="group" data-header="true">Implemented OGC Standards<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>EOEPCA implements the following OGC standards:</p>
<ul>
<li><strong>OGC API</strong>: Processes, Coverages, Maps, Records, Features, Tiles</li>
<li><strong>OGC Web Services</strong>: WPS 2.0, WMS 1.1-1.3, WMTS 1.0, WCS 2.0, CSW 2.0.2/3.0.0</li>
<li><strong>OpenSearch</strong>: Geo, Time, and EO Extensions</li>
<li><strong>Authentication</strong>: OAuth2/OIDC, User-Managed Access (UMA) 2.0</li>
</ul>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L162-L170" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">162-170</span></a> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/release-notes/release-0.3.md#L49-L81" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>release-notes/release-0.3.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">49-81</span></a></p>
<h2 id="naming-and-dns-conventions" class="group" data-header="true">Naming and DNS Conventions<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>To simplify development and testing without requiring public DNS configuration, EOEPCA uses the <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">nip.io</code> dynamic DNS service. Service hostnames embed the cluster&#x27;s public IP address:</p>
<p><strong>Format:</strong> <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">&lt;service-name&gt;.&lt;public-ip-with-dashes&gt;.nip.io</code></p>
<p><strong>Examples:</strong></p>
<ul>
<li>Minikube: <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">workspace.192-168-49-2.nip.io</code></li>
<li>CREODIAS develop: <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">workspace.185-52-193-87.nip.io</code></li>
</ul>
<p>Kubernetes Ingress resources use name-based routing with these hostnames. The public IP is baked into deployment configurations and must be updated when reusing configurations for different environments.</p>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L100-L112" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">100-112</span></a></p>
<h2 id="next-steps" class="group" data-header="true">Next Steps<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>To deploy EOEPCA:</p>
<ol>
<li><strong>Cloud Deployment</strong>: Provision infrastructure with Terraform on OpenStack (see <a href="/EOEPCA/eoepca/2.2-infrastructure-provisioning" class="text-neutral-300 hover:text-neutral-200 hover:underline">Infrastructure Provisioning</a>), then set up RKE Kubernetes cluster (see <a href="/EOEPCA/eoepca/8.1-kubernetes-cluster-setup" class="text-neutral-300 hover:text-neutral-200 hover:underline">Kubernetes Cluster Setup</a>)</li>
<li><strong>Local Development</strong>: Set up Minikube or k3s (see <a href="/EOEPCA/eoepca/9.3-local-development-with-minikube" class="text-neutral-300 hover:text-neutral-200 hover:underline">Local Development with Minikube</a>)</li>
<li><strong>Deploy System</strong>: Use Flux CD for GitOps deployment (see <a href="/EOEPCA/eoepca/2.1-deployment-guide" class="text-neutral-300 hover:text-neutral-200 hover:underline">Deployment Guide</a>)</li>
<li><strong>Validate</strong>: Run acceptance tests (see <a href="/EOEPCA/eoepca/2.3-testing-and-validation" class="text-neutral-300 hover:text-neutral-200 hover:underline">Testing and Validation</a>)</li>
</ol>
<p>For a detailed walkthrough, consult the <a href="https://deployment-guide.docs.eoepca.org/" class="text-neutral-300 hover:text-neutral-200 hover:underline">EOEPCA Deployment Guide</a>.</p>
<p><strong>Sources:</strong> <a href="https://github.com/EOEPCA/eoepca/blob/7aef2f4a/README.md#L72-L99" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">72-99</span></a></p></div></div></div></div></div></div><div class="hidden overflow-hidden transition-[border-radius] xl:sticky xl:right-0 xl:top-20 xl:block xl:h-[calc(100vh-82px)] xl:w-64 xl:flex-shrink-0 2xl:w-72" style="scrollbar-width:none"><div class="flex max-h-full w-full flex-shrink-0 flex-col py-6 pt-0 text-sm lg:pb-4 lg:pt-8 xl:w-64 2xl:w-72" style="scrollbar-color:var(--color-night) transparent"><div><div class="relative mx-4 my-4 rounded-md border border-neutral-200 bg-neutral-100 p-3 text-sm text-neutral-600 dark:border-neutral-800 dark:bg-neutral-900 dark:text-neutral-400"><button class="absolute right-2 top-2 rounded-sm p-1 opacity-70 transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-neutral-400 focus:ring-offset-2"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M205.66,194.34a8,8,0,0,1-11.32,11.32L128,139.31,61.66,205.66a8,8,0,0,1-11.32-11.32L116.69,128,50.34,61.66A8,8,0,0,1,61.66,50.34L128,116.69l66.34-66.35a8,8,0,0,1,11.32,11.32L139.31,128Z"></path></svg><span class="sr-only">Dismiss</span></button><p class="text-sm font-medium">Refresh this wiki</p><p class="mt-2 text-sm font-light text-neutral-500 dark:text-neutral-400">This wiki was recently refreshed. Please wait<!-- --> <!-- -->7<!-- --> day<!-- -->s<!-- --> to refresh again.</p></div></div><h3 class="px-4 pb-5 text-lg font-medium leading-none">On this page</h3><ul style="scrollbar-width:none" class="min-h-0 flex-1 space-y-3 overflow-y-auto p-4 pt-0"><li class=""><a href="#overview" class="hover:text-primary pr-1 transition-all text-primary font-medium">Overview</a></li><li class="ml-3"><a href="#purpose-and-scope" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Purpose and Scope</a></li><li class="ml-3"><a href="#about-eoepca" class="hover:text-primary pr-1 font-normal transition-all text-secondary">About EOEPCA</a></li><li class="ml-3"><a href="#repository-structure" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Repository Structure</a></li><li class="ml-3"><a href="#major-building-blocks" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Major Building Blocks</a></li><li class="ml-6"><a href="#user-management-building-blocks" class="hover:text-primary pr-1 font-normal transition-all text-secondary">User Management Building Blocks</a></li><li class="ml-6"><a href="#resource-management-building-blocks" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Resource Management Building Blocks</a></li><li class="ml-6"><a href="#processing-chaining-building-blocks" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Processing &amp; Chaining Building Blocks</a></li><li class="ml-3"><a href="#gitops-deployment-model" class="hover:text-primary pr-1 font-normal transition-all text-secondary">GitOps Deployment Model</a></li><li class="ml-3"><a href="#multi-tenant-architecture" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Multi-Tenant Architecture</a></li><li class="ml-3"><a href="#infrastructure-and-technology-stack" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Infrastructure and Technology Stack</a></li><li class="ml-6"><a href="#kubernetes-platforms" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Kubernetes Platforms</a></li><li class="ml-6"><a href="#storage-architecture" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Storage Architecture</a></li><li class="ml-6"><a href="#implemented-ogc-standards" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Implemented OGC Standards</a></li><li class="ml-3"><a href="#naming-and-dns-conventions" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Naming and DNS Conventions</a></li><li class="ml-3"><a href="#next-steps" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Next Steps</a></li></ul></div></div><div class="pointer-events-none fixed bottom-2 left-2 right-2 mt-2 md:bottom-4 md:left-0 md:right-0"><div class="z-10 mx-auto max-w-3xl"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div></div></div></div><!--$--><!--/$--></div><script>$RC("B:1","S:1")</script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n"])</script><script>self.__next_f.push([1,"2:I[49138,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7177\",\"static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"RootProvider\"]\n"])</script><script>self.__next_f.push([1,"3:I[85341,[],\"\"]\n4:I[90025,[],\"\"]\n7:I[41012,[],\"ClientPageRoot\"]\n"])</script><script>self.__next_f.push([1,"8:I[57456,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4129\",\"static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"2545\",\"static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8461\",\"static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7198\",\"static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"5462\",\"static/chunks/5462-08221e91030fd747.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4429\",\"static/chunks/4429-943205658cbafffe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9976\",\"static/chunks/9976-9250854d58eefaa3.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1481\",\"static/chunks/1481-25d5bbc4f2d9524a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"3285\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-6651f8cd8321a0db.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"b:I[15104,[],\"OutletBoundary\"]\nd:I[94777,[],\"AsyncMetadataOutlet\"]\nf:I[15104,[],\"ViewportBoundary\"]\n11:I[15104,[],\"MetadataBoundary\"]\n12:\"$Sreact.suspense\"\n14:I[34431,[],\"\"]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/de70bee13400563f.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"style\"]\n:HL[\"/_next/static/css/7da0a892b4ad83db.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"SzzXrDjxzhDKxxD_1GShd\",\"p\":\"\",\"c\":[\"\",\"EOEPCA\",\"eoepca\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"org\",\"EOEPCA\",\"d\"],{\"children\":[[\"repo\",\"eoepca\",\"d\"],{\"children\":[[\"wikiRoutes\",\"\",\"oc\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/de70bee13400563f.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7da0a892b4ad83db.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"className\":\"__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}],{\"children\":[[\"org\",\"EOEPCA\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"repo\",\"eoepca\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L5\"]}],{\"children\":[[\"wikiRoutes\",\"\",\"oc\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L6\"]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L7\",null,{\"Component\":\"$8\",\"searchParams\":{},\"params\":{\"org\":\"EOEPCA\",\"repo\":\"eoepca\"},\"promises\":[\"$@9\",\"$@a\"]}],null,[\"$\",\"$Lb\",null,{\"children\":[\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L11\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$12\",null,{\"fallback\":null,\"children\":\"$L13\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"9:{}\na:\"$0:f:0:1:2:children:2:children:2:children:2:children:1:props:children:0:props:params\"\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nc:null\n"])</script><script>self.__next_f.push([1,"15:I[13550,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7391\",\"static/chunks/7391-f64e18878e224268.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6373\",\"static/chunks/6373-d56a493968555802.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6375\",\"static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9437\",\"static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"HeaderWrapperWithSuspense\"]\n"])</script><script>self.__next_f.push([1,"16:I[82188,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7391\",\"static/chunks/7391-f64e18878e224268.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6373\",\"static/chunks/6373-d56a493968555802.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6375\",\"static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9437\",\"static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"WikiContextProvider\"]\n"])</script><script>self.__next_f.push([1,"17:T30ef,"])</script><script>self.__next_f.push([1,"# Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [minikube/README.md](minikube/README.md)\n- [release-notes/release-0.3.md](release-notes/release-0.3.md)\n- [system/clusters/README.md](system/clusters/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes the repository's purpose, structure, major building blocks, and deployment model. For detailed deployment instructions, see [Getting Started](#2). For architectural details of individual subsystems, see [System Architecture](#3).\n\n## About EOEPCA\n\nThe EOEPCA project establishes a consensus of best practices for Earth Observation (EO) Exploitation Platforms based on open standards. The goal is to enable interoperability between distributed EO platforms, creating an open network of resources where users can efficiently access and collaborate on geospatial analysis tasks.\n\nThis repository at `https://github.com/EOEPCA/eoepca` represents the **system integration** of building blocks that comprise the Common Architecture reference implementation. It is designed for deployment to cloud infrastructure orchestrated by Kubernetes and includes automation for provisioning, deploying, and testing the complete EOEPCA system.\n\n**Sources:** [README.md:58-70]()\n\n## Repository Structure\n\nThe repository is organized into several key directories that separate concerns for infrastructure provisioning, deployment specifications, and testing:\n\n| Directory | Purpose |\n|-----------|---------|\n| `system/` | GitOps deployment specifications using Flux CD, organized by building block |\n| `system/clusters/` | Cluster-specific deployment configurations (e.g., `develop`, `creodias`, `minikube`) |\n| `creodias/` | Infrastructure provisioning for CREODIAS OpenStack using Terraform |\n| `kubernetes/` | Kubernetes cluster setup using Rancher Kubernetes Engine (RKE) |\n| `minikube/` | Local Kubernetes cluster setup for development using Minikube or k3s |\n| `test/acceptance/` | Robot Framework acceptance test suite |\n| `release-notes/` | Historical release documentation |\n\nThe `system/` directory contains subdirectories for each building block domain: `user-management/`, `resource-management/`, and `processing-and-chaining/`. Each contains HelmRelease resources and Kustomization manifests that define the deployment.\n\n**Sources:** [README.md:88-96](), [system/clusters/README.md:1-95](), [minikube/README.md:1-53]()\n\n## Major Building Blocks\n\nEOEPCA consists of three major building block domains, each containing multiple services deployed as Kubernetes resources:\n\n```mermaid\ngraph TB\n    subgraph \"User Management (um namespace)\"\n        LoginService[\"login-service\u003cbr/\u003e(Helm Chart)\"]\n        IdentityService[\"identity-service\u003cbr/\u003e(Keycloak)\"]\n        PDPEngine[\"pdp-engine\u003cbr/\u003e(Policy Decision Point)\"]\n        UserProfile[\"user-profile\"]\n        ResourceGuard[\"resource-guard\u003cbr/\u003e(Policy Enforcement Point)\"]\n    end\n    \n    subgraph \"Resource Management (rm namespace)\"\n        WorkspaceAPI[\"workspace-api\"]\n        ResourceCatalogue[\"resource-catalogue\u003cbr/\u003e(pycsw)\"]\n        DataAccess[\"data-access\u003cbr/\u003e(VS/View Server)\"]\n        RegistrationAPI[\"registration-api\"]\n        BucketOperator[\"bucket-operator\"]\n        Harvester[\"harvester\"]\n    end\n    \n    subgraph \"Processing \u0026 Chaining (proc namespace)\"\n        ADES[\"ades\u003cbr/\u003e(ZOO-Project DRU)\"]\n        AppHub[\"application-hub\u003cbr/\u003e(JupyterHub)\"]\n        PDE[\"pde-container\"]\n        Calrissian[\"Calrissian\u003cbr/\u003e(CWL Executor)\"]\n    end\n    \n    subgraph \"Infrastructure\"\n        Ingress[\"ingress-nginx\"]\n        CertManager[\"cert-manager\"]\n        SealedSecrets[\"sealed-secrets-controller\"]\n        FluxCD[\"flux-system\"]\n    end\n    \n    LoginService --\u003e IdentityService\n    ResourceGuard --\u003e PDPEngine\n    ResourceGuard --\u003e IdentityService\n    \n    WorkspaceAPI --\u003e ResourceCatalogue\n    WorkspaceAPI --\u003e BucketOperator\n    DataAccess --\u003e ResourceCatalogue\n    Harvester --\u003e RegistrationAPI\n    \n    ADES --\u003e Calrissian\n    ADES --\u003e ResourceCatalogue\n    AppHub --\u003e ADES\n    PDE --\u003e ADES\n    \n    FluxCD -.-\u003e|\"Deploys\"| LoginService\n    FluxCD -.-\u003e|\"Deploys\"| WorkspaceAPI\n    FluxCD -.-\u003e|\"Deploys\"| ADES\n    Ingress -.-\u003e|\"Routes\"| ResourceGuard\n```\n\n### User Management Building Blocks\n\nThe user management domain provides authentication and authorization services:\n\n- **`identity-service`**: Keycloak-based identity provider with PostgreSQL backend, providing OAuth2/OIDC authentication\n- **`login-service`**: Gluu-based login service (deprecated, replaced by Keycloak solution)\n- **`pdp-engine`**: Policy Decision Point for evaluating access control policies\n- **`resource-guard`**: Policy Enforcement Point that intercepts requests and enforces UMA-based authorization\n- **`user-profile`**: SCIM-compliant user profile management\n\nFor detailed information, see [User Management and Identity](#4).\n\n### Resource Management Building Blocks\n\nThe resource management domain handles data cataloging, access, and workspace provisioning:\n\n- **`resource-catalogue`**: OGC CSW/OpenSearch interface built on `pycsw`, storing ISO 19115 metadata in PostgreSQL\n- **`data-access`**: OGC WMS/WCS/WMTS services using EOX View Server with rendering and caching capabilities\n- **`workspace-api`**: Multi-tenant workspace provisioning, creating per-user namespaces with isolated resources\n- **`registration-api`**: API for registering datasets into the catalogue\n- **`harvester`**: OpenSearch-based harvester for ingesting external data sources\n- **`bucket-operator`**: MinIO bucket lifecycle management\n\nFor detailed information, see [Resource Management](#5).\n\n### Processing \u0026 Chaining Building Blocks\n\nThe processing domain provides application deployment and execution capabilities:\n\n- **`ades`**: Application Deployment and Execution Service based on ZOO-Project DRU, implementing OGC API Processes\n- **`application-hub`**: JupyterHub deployment for interactive development environments\n- **Calrissian**: CWL workflow executor that creates Kubernetes pods for processing tasks\n- **`pde-container`**: Processor Development Environment with integrated tools\n\nFor detailed information, see [Processing and Chaining](#6).\n\n**Sources:** [README.md:128-160](), [release-notes/release-0.3.md:1-319]()\n\n## GitOps Deployment Model\n\nEOEPCA uses Flux CD to implement a GitOps continuous delivery model. The deployment process reconciles the cluster state with declarative specifications stored in this Git repository:\n\n```mermaid\ngraph LR\n    GitRepo[\"GitHub\u003cbr/\u003eEOEPCA/eoepca\u003cbr/\u003e(branch: develop)\"]\n    \n    subgraph \"Flux System (flux-system namespace)\"\n        SourceController[\"source-controller\"]\n        KustomizeController[\"kustomize-controller\"]\n        HelmController[\"helm-controller\"]\n    end\n    \n    subgraph \"Cluster Deployment Specifications\"\n        SystemKust[\"system/clusters/{TARGET}/system/\"]\n        UMKust[\"user-management/\"]\n        RMKust[\"resource-management/\"]\n        ProcKust[\"processing-and-chaining/\"]\n    end\n    \n    subgraph \"Kubernetes Resources\"\n        UMNamespace[\"um namespace\"]\n        RMNamespace[\"rm namespace\"]\n        ProcNamespace[\"proc namespace\"]\n    end\n    \n    GitRepo --\u003e|\"Poll every 1m\"| SourceController\n    SourceController --\u003e KustomizeController\n    SourceController --\u003e HelmController\n    \n    KustomizeController --\u003e|\"Apply\"| SystemKust\n    SystemKust --\u003e UMKust\n    SystemKust --\u003e RMKust\n    SystemKust --\u003e ProcKust\n    \n    HelmController --\u003e|\"Install/Upgrade\"| UMKust\n    HelmController --\u003e|\"Install/Upgrade\"| RMKust\n    HelmController --\u003e|\"Install/Upgrade\"| ProcKust\n    \n    UMKust --\u003e UMNamespace\n    RMKust --\u003e RMNamespace\n    ProcKust --\u003e ProcNamespace\n```\n\nThe deployment is bootstrapped using the `deployCluster.sh` script, which executes `flux bootstrap github` to install Flux controllers and configure synchronization with the Git repository. Flux monitors HelmRelease and Kustomization resources in `system/clusters/${TARGET}/` and automatically reconciles the cluster state.\n\n**Key Flux Resources:**\n\n- **`GitRepository`**: Defines the Git source at `https://github.com/EOEPCA/eoepca` with polling interval\n- **`Kustomization`**: References directories like `./user-management` to apply manifests\n- **`HelmRelease`**: Defines Helm chart deployments with values overrides\n\nThe `TARGET` variable determines which cluster configuration to deploy (e.g., `develop`, `creodias`, `minikube`). Each target has cluster-specific values including:\n- Public IP address for ingress (e.g., `185.52.193.87` for develop cluster)\n- DNS domain using `nip.io` pattern (e.g., `185-52-193-87.nip.io`)\n- TLS certificate configuration\n- Resource limits and replicas\n\nFor more details, see [GitOps and Flux CD](#3.2).\n\n**Sources:** [system/clusters/README.md:1-95](), [README.md:79-112]()\n\n## Multi-Tenant Architecture\n\nEOEPCA implements multi-tenancy through dynamic workspace provisioning. When a user requests a workspace via the `workspace-api`, the system:\n\n1. Creates a dedicated Kubernetes namespace (e.g., `eric-workspace`)\n2. Provisions an S3 bucket via `bucket-operator` and MinIO API\n3. Instantiates template HelmReleases for user-specific services:\n   - `template-hr-resource-catalogue`: Private catalogue instance\n   - `template-hr-data-access`: Private data access service\n   - `template-hr-resource-guard`: Private PEP for access control\n4. Configures ownership policies based on user identity token\n\nThis architecture ensures isolation while enabling federated search across workspaces through the global `resource-catalogue` service.\n\nFor more details, see [Multi-Tenant Workspaces](#5.5).\n\n**Sources:** [README.md:58-70]()\n\n## Infrastructure and Technology Stack\n\n### Kubernetes Platforms\n\nEOEPCA supports multiple Kubernetes distributions:\n\n- **Production**: Rancher Kubernetes Engine (RKE) on CREODIAS OpenStack\n- **Development**: Minikube with Docker or native drivers\n- **Lightweight**: k3s as an alternative to Minikube\n\n**Sources:** [README.md:88-96](), [minikube/README.md:1-53]()\n\n### Storage Architecture\n\nMultiple storage tiers serve different purposes:\n\n| Storage Type | Implementation | Purpose |\n|--------------|----------------|---------|\n| Object Storage (Input) | CloudFerro `eodata` S3 | Source EO datasets (read-only) |\n| Object Storage (Output) | MinIO | Per-workspace output buckets |\n| Persistent Volumes | NFS with `managed-nfs` StorageClass | Service state and metadata |\n| Databases | PostgreSQL | Resource Catalogue metadata, Identity Service |\n| Caching | Redis | Data Access caching, Harvester queues |\n\nFor more details, see [Storage and Persistence](#7).\n\n### Implemented OGC Standards\n\nEOEPCA implements the following OGC standards:\n\n- **OGC API**: Processes, Coverages, Maps, Records, Features, Tiles\n- **OGC Web Services**: WPS 2.0, WMS 1.1-1.3, WMTS 1.0, WCS 2.0, CSW 2.0.2/3.0.0\n- **OpenSearch**: Geo, Time, and EO Extensions\n- **Authentication**: OAuth2/OIDC, User-Managed Access (UMA) 2.0\n\n**Sources:** [README.md:162-170](), [release-notes/release-0.3.md:49-81]()\n\n## Naming and DNS Conventions\n\nTo simplify development and testing without requiring public DNS configuration, EOEPCA uses the `nip.io` dynamic DNS service. Service hostnames embed the cluster's public IP address:\n\n**Format:** `\u003cservice-name\u003e.\u003cpublic-ip-with-dashes\u003e.nip.io`\n\n**Examples:**\n- Minikube: `workspace.192-168-49-2.nip.io`\n- CREODIAS develop: `workspace.185-52-193-87.nip.io`\n\nKubernetes Ingress resources use name-based routing with these hostnames. The public IP is baked into deployment configurations and must be updated when reusing configurations for different environments.\n\n**Sources:** [README.md:100-112]()\n\n## Next Steps\n\nTo deploy EOEPCA:\n1. **Cloud Deployment**: Provision infrastructure with Terraform on OpenStack (see [Infrastructure Provisioning](#2.2)), then set up RKE Kubernetes cluster (see [Kubernetes Cluster Setup](#8.1))\n2. **Local Development**: Set up Minikube or k3s (see [Local Development with Minikube](#9.3))\n3. **Deploy System**: Use Flux CD for GitOps deployment (see [Deployment Guide](#2.1))\n4. **Validate**: Run acceptance tests (see [Testing and Validation](#2.3))\n\nFor a detailed walkthrough, consult the [EOEPCA Deployment Guide](https://deployment-guide.docs.eoepca.org/).\n\n**Sources:** [README.md:72-99]()"])</script><script>self.__next_f.push([1,"18:T4d81,"])</script><script>self.__next_f.push([1,"# Getting Started\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [minikube/README.md](minikube/README.md)\n- [system/clusters/README.md](system/clusters/README.md)\n\n\u003c/details\u003e\n\n\n\nThis page provides an overview of the steps required to deploy an EOEPCA system. It covers the essential prerequisites, deployment paths, and initial setup procedures to get a working EOEPCA platform running in either a cloud or local development environment.\n\nFor detailed deployment instructions, see [Deployment Guide](#2.1). For infrastructure provisioning specifics, see [Infrastructure Provisioning](#2.2). For testing procedures after deployment, see [Testing and Validation](#2.3).\n\n## Overview\n\nThe EOEPCA system is designed for deployment to Kubernetes clusters using a GitOps approach with Flux CD. The platform supports two primary deployment scenarios:\n\n- **Cloud Deployment**: Full production-scale deployment on cloud infrastructure (OpenStack/CREODIAS)\n- **Local Development**: Lightweight deployment on local Kubernetes (Minikube or k3s)\n\nBoth deployment paths follow a similar workflow but differ in infrastructure provisioning and resource requirements.\n\n## Deployment Architecture\n\nThe following diagram illustrates the high-level deployment workflow:\n\n```mermaid\nflowchart TD\n    Start[\"Fork Repository\u003cbr/\u003egithub.com/EOEPCA/eoepca\"] --\u003e Choice{\"Deployment\u003cbr/\u003eTarget?\"}\n    \n    Choice --\u003e|Cloud| CloudPath[\"Cloud Deployment Path\"]\n    Choice --\u003e|Local| LocalPath[\"Local Development Path\"]\n    \n    CloudPath --\u003e Infra[\"Provision Infrastructure\u003cbr/\u003eTerraform + OpenStack\"]\n    Infra --\u003e RKE[\"Setup Kubernetes Cluster\u003cbr/\u003eRKE\"]\n    \n    LocalPath --\u003e MinikubeChoice{\"K8s\u003cbr/\u003ePlatform?\"}\n    MinikubeChoice --\u003e|Minikube| SetupMinikube[\"setup-minikube.sh\"]\n    MinikubeChoice --\u003e|k3s| SetupK3s[\"setup-k3s.sh\"]\n    \n    RKE --\u003e FluxBootstrap[\"Bootstrap Flux CD\u003cbr/\u003edeployCluster.sh\"]\n    SetupMinikube --\u003e FluxBootstrap\n    SetupK3s --\u003e FluxBootstrap\n    \n    FluxBootstrap --\u003e GitOps[\"GitOps Reconciliation\u003cbr/\u003eFlux syncs from GitHub\"]\n    GitOps --\u003e Deploy[\"Deploy EOEPCA Components\u003cbr/\u003eHelmReleases applied\"]\n    Deploy --\u003e Verify[\"Verify Deployment\u003cbr/\u003eRun Acceptance Tests\"]\n    \n    Verify --\u003e Ready[\"EOEPCA System Ready\"]\n```\n\nSources: [README.md:73-96](), [system/clusters/README.md:1-78](), [minikube/README.md:1-52]()\n\n## Prerequisites\n\n### Required Tools\n\nThe following tools must be installed on your local system:\n\n| Tool | Purpose | Installation |\n|------|---------|--------------|\n| `git` | Repository cloning and version control | System package manager |\n| `kubectl` | Kubernetes cluster administration | [bin/install-kubectl.sh]() or [kubectl docs](https://kubernetes.io/docs/tasks/tools/) |\n| `flux` | GitOps continuous delivery | `curl -s https://toolkit.fluxcd.io/install.sh \\| sudo bash` |\n| `helm` | Kubernetes package management (optional) | [helm.sh](https://helm.sh/docs/intro/install/) |\n\nFor cloud deployment, additional tools are required:\n- `terraform` - Infrastructure provisioning\n- `rke` - Rancher Kubernetes Engine setup\n\n### GitHub Configuration\n\nEOEPCA uses a GitOps deployment model where Flux CD continuously synchronizes cluster state with a Git repository. You must configure GitHub credentials:\n\n1. **Fork the repository**: Navigate to `https://github.com/EOEPCA/eoepca` and fork it to your GitHub account\n2. **Create Personal Access Token**: Generate a token at `https://github.com/settings/tokens` with all `repo` scopes selected\n3. **Set environment variables**:\n\n```bash\nexport GITHUB_USER=\u003cyour-username\u003e\nexport GITHUB_TOKEN=\u003cyour-token\u003e\n```\n\n**Important**: Forking (rather than cloning) is required because Flux CD needs write access to commit deployment state back to the repository.\n\nSources: [README.md:79-86](), [system/clusters/README.md:31-43]()\n\n### Resource Requirements\n\nMinimum system requirements vary by deployment type:\n\n| Deployment Type | CPU | Memory | Storage | Notes |\n|----------------|-----|--------|---------|-------|\n| **Cloud (Multi-node)** | 16+ cores | 64+ GB | 500+ GB | Distributed across multiple VMs |\n| **Local (Single-node)** | 8 cores | 32 GB | 100 GB | Single minikube/k3s instance |\n| **Minimal Development** | 4 cores | 16 GB | 50 GB | Subset of components only |\n\n**Note**: As of release v1.4, deploying the full EOEPCA system to a single minikube node is challenging due to expanded component count. The full system is better suited to multi-node cloud deployments.\n\nSources: [README.md:96]()\n\n## Repository Structure\n\nUnderstanding the repository layout is essential for configuration and deployment:\n\n```mermaid\ngraph TB\n    Root[\"/\"]\n    \n    Root --\u003e System[\"system/\"]\n    Root --\u003e Creodias[\"creodias/\"]\n    Root --\u003e Kubernetes[\"kubernetes/\"]\n    Root --\u003e Minikube[\"minikube/\"]\n    Root --\u003e Test[\"test/\"]\n    Root --\u003e Bin[\"bin/\"]\n    \n    System --\u003e Clusters[\"clusters/\"]\n    Clusters --\u003e Develop[\"develop/\u003cbr/\u003eProduction cluster config\"]\n    Clusters --\u003e MinikubeCfg[\"minikube/\u003cbr/\u003eLocal cluster config\"]\n    \n    System --\u003e UserMgmt[\"user-management/\u003cbr/\u003eIdentity, Login, PEP/PDP\"]\n    System --\u003e ResourceMgmt[\"resource-management/\u003cbr/\u003eWorkspace, Catalogue, Data Access\"]\n    System --\u003e Processing[\"processing-and-chaining/\u003cbr/\u003eADES, Application Hub\"]\n    \n    Creodias --\u003e Terraform[\"terraform/\u003cbr/\u003eOpenStack provisioning\"]\n    Kubernetes --\u003e RKEConfig[\"rke-cluster.yml\u003cbr/\u003eRKE configuration\"]\n    \n    Minikube --\u003e SetupScripts[\"setup-minikube.sh\u003cbr/\u003esetup-k3s.sh\"]\n    Test --\u003e Acceptance[\"acceptance/\u003cbr/\u003eRobot Framework tests\"]\n    Bin --\u003e Helpers[\"install-kubectl.sh\u003cbr/\u003eUtility scripts\"]\n    \n    style System fill:#e8f5e9\n    style Creodias fill:#e1f5ff\n    style Minikube fill:#e1f5ff\n    style Test fill:#fff4e1\n```\n\n**Key Directories:**\n- `system/clusters/{target}/` - Flux CD deployment configurations per cluster\n- `system/user-management/` - Identity and access management components\n- `system/resource-management/` - Data cataloging and workspace services\n- `system/processing-and-chaining/` - Application execution services\n- `creodias/` - Cloud infrastructure provisioning (Terraform)\n- `kubernetes/` - RKE cluster setup\n- `minikube/` - Local Kubernetes setup scripts\n- `test/acceptance/` - Acceptance test suite\n\nSources: [README.md:88-94](), [system/clusters/README.md:47-49]()\n\n## Quick Start: Local Development\n\nFor developers who want to quickly spin up a local EOEPCA environment:\n\n### Step 1: Clone the Repository\n\n```bash\ngit clone git@github.com:\u003cyour-fork\u003e/eoepca.git\ncd eoepca\ngit checkout v1.4  # or 'develop' for latest\n```\n\n### Step 2: Setup Local Kubernetes\n\nChoose either Minikube or k3s:\n\n**Option A - Minikube (recommended for most users)**:\n```bash\nminikube/setup-minikube.sh\n```\n\n**Option B - k3s (faster, more lightweight)**:\n```bash\nminikube/setup-k3s.sh\n```\n\nThe [setup-minikube.sh:1-30]() script downloads and configures minikube with the Docker driver by default. For VM deployments with limited resources, you can use the `native` driver: `minikube/setup-minikube.sh native`.\n\nThe [setup-k3s.sh:1-48]() script installs k3s with Docker runtime (required for the ADES Argo component).\n\n### Step 3: Install Flux CD\n\n```bash\ncurl -s https://toolkit.fluxcd.io/install.sh | sudo bash\nflux check --pre\n```\n\n### Step 4: Configure Deployment Target\n\nThe default minikube configuration is in `system/clusters/minikube/`. You must update hostnames to match your minikube IP:\n\n```bash\nMINIKUBE_IP=$(minikube ip)\n# Update all occurrences of hostnames in system/clusters/minikube/\n# Pattern: \u003cservice\u003e.\u003cip\u003e.nip.io\n```\n\nFor example, if `minikube ip` returns `192.168.49.2`, services will be accessible at:\n- `workspace.192-168-49-2.nip.io`\n- `resource-catalogue.192-168-49-2.nip.io`\n- `ades.192-168-49-2.nip.io`\n\n### Step 5: Deploy EOEPCA System\n\n```bash\nexport GITHUB_USER=\u003cyour-username\u003e\nexport GITHUB_TOKEN=\u003cyour-token\u003e\nexport TARGET=minikube\nexport BRANCH=v1.4\n\n./system/clusters/deployCluster.sh\n```\n\nThe [system/clusters/deployCluster.sh:1-58]() script performs the following:\n1. Runs `flux bootstrap github` to install Flux controllers\n2. Configures Flux to watch your GitHub repository\n3. Applies Kustomizations for system components\n4. Flux begins reconciling HelmReleases\n\n### Step 6: Monitor Deployment\n\n```bash\n# Watch Flux reconciliation\nflux get kustomizations --watch\n\n# Check HelmRelease status\nflux get helmreleases -A\n\n# Verify pods are running\nkubectl get pods -A\n```\n\nFlux polls the GitHub repository every 1 minute and applies changes automatically.\n\nSources: [README.md:81-86](), [minikube/README.md:1-52](), [system/clusters/README.md:9-78]()\n\n## Quick Start: Cloud Deployment\n\nFor production or development deployments on cloud infrastructure:\n\n### Step 1: Provision Infrastructure\n\nNavigate to the CREODIAS directory and provision VMs using Terraform:\n\n```bash\ncd creodias/\nterraform init\nterraform plan\nterraform apply\n```\n\nThis creates:\n- Multiple VMs on OpenStack\n- Network configuration and security groups\n- Load balancer for ingress\n- NFS storage server\n\nFor detailed infrastructure provisioning, see [Infrastructure Provisioning](#2.2).\n\n### Step 2: Setup Kubernetes Cluster\n\nUse RKE to bootstrap the Kubernetes cluster on provisioned VMs:\n\n```bash\ncd ../kubernetes/\n# Edit rke-cluster.yml with VM IP addresses\nrke up --config rke-cluster.yml\n```\n\nThis generates a `kubeconfig` file at `kube_config_rke-cluster.yml`.\n\n### Step 3: Configure Deployment\n\nCreate a cluster-specific configuration directory (or use `system/clusters/develop/` as template):\n\n```bash\ncp -r system/clusters/develop system/clusters/my-cluster\ncd system/clusters/my-cluster\n# Update all IP addresses and hostnames for your environment\n```\n\n**Critical**: Search and replace the load balancer IP throughout the configuration. For example, if your load balancer IP is `203.0.113.45`, update all instances of service hostnames to use `\u003cservice\u003e.203-0-113-45.nip.io`.\n\n### Step 4: Bootstrap Flux\n\n```bash\nexport KUBECONFIG=kubernetes/kube_config_rke-cluster.yml\nexport GITHUB_USER=\u003cyour-username\u003e\nexport GITHUB_TOKEN=\u003cyour-token\u003e\nexport TARGET=my-cluster\nexport BRANCH=develop\n\n./system/clusters/deployCluster.sh\n```\n\n### Step 5: Verify Deployment\n\n```bash\nexport KUBECONFIG=kubernetes/kube_config_rke-cluster.yml\nflux get kustomizations -A\nkubectl get pods -A\n```\n\nSources: [README.md:88-94](), [system/clusters/README.md:45-78]()\n\n## Hostname Configuration and DNS\n\nEOEPCA uses dynamic DNS via [nip.io](https://nip.io/) to avoid requiring public DNS configuration during development and testing. This service embeds the IP address directly in the hostname.\n\n### Hostname Pattern\n\nAll services follow this pattern:\n```\n\u003cservice-name\u003e.\u003cip-with-dashes\u003e.nip.io\n```\n\nExamples:\n- `workspace-api.192-168-49-2.nip.io` (minikube with IP 192.168.49.2)\n- `ades.185-52-193-87.nip.io` (cloud with IP 185.52.193.87)\n\n**Important**: Use dashes (not dots) to delimit the IP address, as this works better with LetsEncrypt rate limits.\n\n### Determining Your Public IP\n\n| Deployment | Command | Example Result |\n|------------|---------|----------------|\n| Minikube | `minikube ip` | `192.168.49.2` |\n| k3s | `kubectl get nodes -o wide` | Check EXTERNAL-IP column |\n| Cloud | Check load balancer IP | Terraform output or OpenStack console |\n\n### Updating Configuration\n\nThe public IP is embedded throughout the deployment configuration, particularly in Kubernetes Ingress resources. When setting up your deployment:\n\n1. Identify your public IP\n2. Search all files under `system/clusters/{target}/` for IP addresses\n3. Replace with your IP (using dashes)\n4. Commit changes to your fork\n\nExample search/replace patterns to look for:\n- `185.52.193.87`  `\u003cyour-ip\u003e`\n- `185-52-193-87.nip.io`  `\u003cyour-ip-with-dashes\u003e.nip.io`\n\nSources: [README.md:100-111]()\n\n## Deployment Approaches\n\nEOEPCA offers two approaches for system deployment:\n\n### 1. GitOps with Flux CD (Recommended for Production)\n\nFlux CD continuously monitors your Git repository and automatically reconciles cluster state:\n\n**Advantages:**\n- Declarative infrastructure as code\n- Automatic synchronization (every 1 minute)\n- Version-controlled deployment history\n- Easy rollback and disaster recovery\n- Multi-cluster management\n\n**Key Concepts:**\n- `GitRepository` resources define the source Git repos\n- `Kustomization` resources apply Kubernetes manifests\n- `HelmRelease` resources manage Helm chart deployments\n- Flux controllers run in `flux-system` namespace\n\n**Workflow:**\n1. Edit configuration in `system/clusters/{target}/`\n2. Commit and push to GitHub\n3. Flux detects changes within 1 minute\n4. Flux applies changes to cluster\n5. Verify via `flux get kustomizations`\n\n### 2. Manual Deployment with Helm (Recommended for Learning)\n\nThe [Deployment Guide](https://deployment-guide.docs.eoepca.org/) provides scripts that deploy components directly using `helm`:\n\n**Advantages:**\n- More transparent for learning\n- Step-by-step component deployment\n- Easier to understand dependencies\n- Suitable for development/testing\n\n**Workflow:**\n1. Follow Deployment Guide instructions\n2. Run provided shell scripts\n3. Scripts execute `helm install` commands\n4. Components deployed in sequence\n\n**Note**: The Deployment Guide approach uses Minikube out-of-the-box and provides detailed explanations of each component's configuration.\n\nSources: [README.md:93-98](), [system/clusters/README.md:1-6](), [system/clusters/README.md:79-86]()\n\n## Deployment Flow Detail\n\nThe following diagram shows the technical flow when using GitOps deployment:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant GitHub[\"GitHub Repository\u003cbr/\u003eEOEPCA/eoepca\"]\n    participant Flux[\"Flux Controllers\u003cbr/\u003eflux-system namespace\"]\n    participant K8s[\"Kubernetes API\"]\n    participant Helm[\"Helm Controller\"]\n    participant Services[\"EOEPCA Services\"]\n    \n    User-\u003e\u003eUser: \"git clone\u003cbr/\u003eEdit system/clusters/{target}/\"\n    User-\u003e\u003eGitHub: \"git push\u003cbr/\u003eCommit configuration changes\"\n    \n    Note over Flux: \"Polling interval: 1 minute\"\n    \n    Flux-\u003e\u003eGitHub: \"Poll for changes\u003cbr/\u003esource-controller\"\n    GitHub--\u003e\u003eFlux: \"Return updated manifests\"\n    \n    Flux-\u003e\u003eFlux: \"Detect HelmRelease updates\u003cbr/\u003ekustomize-controller\"\n    \n    Flux-\u003e\u003eHelm: \"Apply HelmRelease\u003cbr/\u003euser-management/identity-service\"\n    Helm-\u003e\u003eK8s: \"Create/Update resources\u003cbr/\u003eDeployment, Service, Ingress\"\n    K8s-\u003e\u003eServices: \"Schedule pods\"\n    Services--\u003e\u003eFlux: \"Reconciliation status\"\n    \n    Flux-\u003e\u003eHelm: \"Apply HelmRelease\u003cbr/\u003eresource-management/workspace-api\"\n    Helm-\u003e\u003eK8s: \"Create/Update resources\"\n    K8s-\u003e\u003eServices: \"Schedule pods\"\n    \n    Flux-\u003e\u003eHelm: \"Apply HelmRelease\u003cbr/\u003eprocessing-and-chaining/ades\"\n    Helm-\u003e\u003eK8s: \"Create/Update resources\"\n    K8s-\u003e\u003eServices: \"Schedule pods\"\n    \n    Flux-\u003e\u003eGitHub: \"Commit reconciliation state\"\n    GitHub--\u003e\u003eFlux: \"State persisted\"\n    \n    User-\u003e\u003eK8s: \"kubectl get pods -A\u003cbr/\u003eVerify deployment\"\n    K8s--\u003e\u003eUser: \"Pod status\"\n    \n    User-\u003e\u003eFlux: \"flux get helmreleases\"\n    Flux--\u003e\u003eUser: \"HelmRelease status\"\n```\n\n**Key Flux Components:**\n- `source-controller` - Fetches artifacts from Git/Helm repositories\n- `kustomize-controller` - Applies Kustomization resources\n- `helm-controller` - Manages HelmRelease lifecycles\n- `notification-controller` - Sends deployment notifications\n\n**Configuration Layers:**\n1. **System Infrastructure** (`system/clusters/{target}/system/`) - Flux itself, ingress-nginx, cert-manager\n2. **User Management** (`user-management/`) - identity-service, login-service, pdp-engine\n3. **Resource Management** (`resource-management/`) - workspace-api, resource-catalogue, data-access\n4. **Processing** (`processing-and-chaining/`) - ades, application-hub\n\nSources: [system/clusters/README.md:79-95]()\n\n## Version Selection\n\nEOEPCA releases are tagged in Git. When deploying, choose the appropriate version:\n\n| Branch/Tag | Description | Recommended For |\n|------------|-------------|-----------------|\n| `v1.4` | Latest stable release (Feb 2024) | Production, learning |\n| `develop` | Latest development | Testing new features |\n| `v1.3` | Previous stable (Sep 2023) | Compatibility requirements |\n\nTo deploy a specific version:\n\n```bash\ngit checkout v1.4  # or other tag/branch\nexport BRANCH=v1.4\n./system/clusters/deployCluster.sh\n```\n\n**Important**: Ensure your `BRANCH` environment variable matches your Git checkout when running `deployCluster.sh`.\n\nSources: [README.md:77-86](), [README.md:173-189]()\n\n## Post-Deployment Verification\n\nAfter deployment, verify the system is operational:\n\n### 1. Check Flux Status\n\n```bash\n# All Kustomizations should show \"Applied revision\"\nflux get kustomizations -A\n\n# All HelmReleases should show \"Release reconciliation succeeded\"\nflux get helmreleases -A\n```\n\n### 2. Verify Pods\n\n```bash\n# Check all namespaces\nkubectl get pods -A\n\n# Should see pods in:\n# - flux-system (Flux controllers)\n# - um (User Management)\n# - rm (Resource Management)  \n# - proc (Processing \u0026 Chaining)\n```\n\n### 3. Test Service Access\n\nAccess services via browser using nip.io hostnames:\n- Identity Service: `https://identity-service.\u003cip\u003e.nip.io`\n- Workspace API: `https://workspace-api.\u003cip\u003e.nip.io`\n- ADES: `https://ades.\u003cip\u003e.nip.io`\n\n### 4. Run Acceptance Tests\n\nExecute the automated test suite:\n\n```bash\ncd test/acceptance\n# See Testing and Validation page for details\n```\n\nFor detailed testing procedures, see [Testing and Validation](#2.3).\n\nSources: [README.md:94]()\n\n## Common Issues and Troubleshooting\n\n### Issue: Flux fails to connect to GitHub\n\n**Symptom**: `flux check --pre` reports authentication errors\n\n**Solution**:\n1. Verify `GITHUB_TOKEN` has correct permissions (all `repo` scopes)\n2. Check token hasn't expired\n3. Ensure repository is forked (not cloned directly)\n\n### Issue: Ingress not accessible\n\n**Symptom**: Services return connection timeout or DNS errors\n\n**Solution**:\n1. Verify IP address in hostnames matches actual cluster IP\n2. For minikube: Run `minikube tunnel` in separate terminal\n3. Check ingress-nginx pods are running: `kubectl get pods -n ingress-nginx`\n4. Verify nip.io resolves correctly: `nslookup workspace.\u003cip\u003e.nip.io`\n\n### Issue: HelmRelease stuck in \"Installing\" state\n\n**Symptom**: `flux get helmreleases` shows releases not progressing\n\n**Solution**:\n1. Check pod logs: `kubectl logs -n \u003cnamespace\u003e \u003cpod\u003e`\n2. Verify Helm chart version incremented (for GitRepository sources)\n3. Check resource quotas and node capacity\n4. Suspend and resume release: `flux suspend hr \u003cname\u003e -n \u003cnamespace\u003e` then `flux resume hr \u003cname\u003e -n \u003cnamespace\u003e`\n\n### Issue: Insufficient resources\n\n**Symptom**: Pods stuck in \"Pending\" state\n\n**Solution**:\n1. Check node resources: `kubectl describe nodes`\n2. For minikube: Increase resources with `minikube config set cpus 8` and `minikube config set memory 32768`\n3. Consider deploying subset of components for testing\n4. Scale down replica counts in HelmRelease values\n\nSources: [system/clusters/README.md:17-21](), [minikube/README.md:25-35]()\n\n## Next Steps\n\nAfter completing the initial deployment:\n\n1. **Configure Identity Management** - Set up users, OIDC clients, and access policies. See [User Management and Identity](#4)\n2. **Provision Workspaces** - Create user-specific workspaces with isolated storage. See [Workspace API](#5.3)\n3. **Deploy Applications** - Package and deploy CWL applications to ADES. See [ADES](#6.1)\n4. **Ingest Data** - Configure data harvesting and registration. See [Data Registration and Harvesting](#5.4)\n\nFor detailed deployment procedures including component-by-component configuration, see [Deployment Guide](#2.1).\n\nFor infrastructure provisioning details including Terraform modules and OpenStack setup, see [Infrastructure Provisioning](#2.2).\n\nSources: [README.md:43-54](), [README.md:128-160]()"])</script><script>self.__next_f.push([1,"19:T5bc3,"])</script><script>self.__next_f.push([1,"# Deployment Guide\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [minikube/README.md](minikube/README.md)\n- [system/clusters/README.md](system/clusters/README.md)\n- [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml](system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system/flux-system-patch.yaml](system/clusters/creodias/system/flux-system/flux-system-patch.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-components.yaml](system/clusters/creodias/system/flux-system/gotk-components.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-sync.yaml](system/clusters/creodias/system/flux-system/gotk-sync.yaml)\n- [system/clusters/creodias/system/flux-system/kustomization.yaml](system/clusters/creodias/system/flux-system/kustomization.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis guide provides detailed instructions for deploying the EOEPCA system using a GitOps approach with Flux CD. It covers the deployment process, prerequisites, configuration steps, and synchronization mechanisms. This document focuses specifically on the deployment mechanics and Flux CD setup.\n\nFor infrastructure provisioning (OpenStack, Terraform, Kubernetes cluster setup), see [Infrastructure Provisioning](#2.2). For post-deployment validation, see [Testing and Validation](#2.3). For architectural details about the GitOps model, see [GitOps and Flux CD](#3.2).\n\n**Sources:** [README.md:72-98](), [system/clusters/README.md:1-95]()\n\n---\n\n## Prerequisites\n\nBefore deploying the EOEPCA system, ensure the following prerequisites are met:\n\n### Required Tools\n\n| Tool | Purpose | Installation |\n|------|---------|--------------|\n| `kubectl` | Kubernetes cluster administration | See [Kubernetes docs](https://kubernetes.io/docs/tasks/tools/install-kubectl/) or use `bin/install-kubectl.sh` |\n| `flux` | GitOps continuous delivery | Installed via `curl -s https://toolkit.fluxcd.io/install.sh \\| sudo bash` |\n| `git` | Version control and repository management | Standard package manager installation |\n\n### Kubernetes Cluster\n\nA running Kubernetes cluster is required. Supported options include:\n\n- **Cloud Deployment**: Rancher Kubernetes Engine (RKE) on OpenStack - see [Infrastructure Provisioning](#2.2)\n- **Local Development**: Minikube or k3s - see [minikube/README.md:1-53]()\n\n### GitHub Credentials\n\nFlux requires GitHub access with a Personal Access Token:\n\n```bash\nexport GITHUB_USER=\u003cyour-username\u003e\nexport GITHUB_TOKEN=\u003cyour-token\u003e\n```\n\nThe token must include all `repo` scopes. Create tokens at: https://github.com/settings/tokens\n\n**Sources:** [system/clusters/README.md:10-44](), [minikube/README.md:1-53]()\n\n---\n\n## Deployment Workflow\n\nThe following diagram illustrates the complete deployment workflow from repository fork to synchronized cluster:\n\n```mermaid\nflowchart TD\n    Fork[\"Fork Repository\u003cbr/\u003egithub.com/EOEPCA/eoepca\"]\n    Clone[\"Clone Forked Repository\u003cbr/\u003egit clone git@github.com:user/eoepca.git\"]\n    Checkout[\"Checkout Version\u003cbr/\u003egit checkout v1.4\"]\n    ConfigTarget[\"Configure Deployment Target\u003cbr/\u003esystem/clusters/TARGET/\"]\n    InstallFlux[\"Install Flux CLI\u003cbr/\u003e/usr/local/bin/flux\"]\n    CheckPre[\"Verify Prerequisites\u003cbr/\u003eflux check --pre\"]\n    SetEnv[\"Set Environment Variables\u003cbr/\u003eGITHUB_USER, GITHUB_TOKEN\u003cbr/\u003eBRANCH, TARGET\"]\n    Bootstrap[\"Bootstrap Flux\u003cbr/\u003e./system/clusters/deployCluster.sh\"]\n    CreateGitRepo[\"Flux Creates GitRepository\u003cbr/\u003eflux-system\"]\n    CreateKustomization[\"Flux Creates Kustomization\u003cbr/\u003eflux-system\"]\n    Sync[\"Flux Synchronizes\u003cbr/\u003eInterval: 1m\"]\n    Deploy[\"Deploy Building Blocks\u003cbr/\u003eum, rm, proc namespaces\"]\n    \n    Fork --\u003e Clone\n    Clone --\u003e Checkout\n    Checkout --\u003e ConfigTarget\n    ConfigTarget --\u003e InstallFlux\n    InstallFlux --\u003e CheckPre\n    CheckPre --\u003e SetEnv\n    SetEnv --\u003e Bootstrap\n    Bootstrap --\u003e CreateGitRepo\n    Bootstrap --\u003e CreateKustomization\n    CreateGitRepo --\u003e Sync\n    CreateKustomization --\u003e Sync\n    Sync --\u003e Deploy\n```\n\n**Deployment Workflow Phases**\n\n1. **Repository Preparation**: Fork and clone the EOEPCA repository to enable GitOps write access\n2. **Configuration**: Customize deployment target configuration files for your environment\n3. **Flux Setup**: Install Flux CLI and verify cluster compatibility\n4. **Bootstrap**: Initialize Flux in the cluster, creating `GitRepository` and `Kustomization` resources\n5. **Synchronization**: Flux continuously reconciles cluster state with Git repository\n6. **Deployment**: Building blocks are deployed to respective namespaces\n\n**Sources:** [README.md:79-93](), [system/clusters/README.md:51-78]()\n\n---\n\n## Step 1: Fork and Clone Repository\n\nFork the EOEPCA repository to your GitHub account to support GitOps write access. Flux requires write permissions to commit deployment state.\n\n```bash\n# Clone your forked repository\ngit clone git@github.com:\u003cyour-username\u003e/eoepca.git\ncd eoepca\n\n# Checkout specific release version\ngit checkout v1.4\n```\n\nThe `develop` branch can be used for the latest development version instead of a tagged release.\n\n**Sources:** [README.md:79-86]()\n\n---\n\n## Step 2: Install Flux CD\n\nInstall the Flux CLI to `/usr/local/bin/flux`:\n\n```bash\ncurl -s https://toolkit.fluxcd.io/install.sh | sudo bash\n```\n\nVerify installation and prerequisites:\n\n```bash\nflux check --pre\n```\n\nThis validates that the Kubernetes cluster meets Flux requirements (Kubernetes version, RBAC enabled, etc.).\n\n**Important**: The `flux` CLI relies on the `KUBECONFIG` environment variable (defaults to `~/.kube/config`). Unlike `kubectl`, Flux does not support `KUBECONFIG` as a colon-delimited path list. Set it to a single file path.\n\n**Sources:** [system/clusters/README.md:9-29]()\n\n---\n\n## Step 3: Configure Deployment Target\n\nThe repository contains multiple deployment target configurations under `system/clusters/`:\n\n| Target | Purpose | Path |\n|--------|---------|------|\n| `creodias` | Production deployment to CREODIAS OpenStack | `system/clusters/creodias/` |\n| `develop` | Development deployment | `system/clusters/develop/` |\n| `minikube` | Local development deployment | `system/clusters/minikube/` |\n\n### Customize for Your Environment\n\nEach deployment target contains environment-specific configuration, particularly:\n\n1. **Public IP Address**: Replace references to the default public IP throughout configuration files\n2. **DNS Names**: Update `nip.io` hostname patterns with your cluster's public IP\n3. **Ingress Hostnames**: Update Kubernetes Ingress resources with your domain\n\nFor example, the CREODIAS deployment uses IP `185.52.193.87`. Search and replace:\n- `185.52.193.87`  your public IP (with dots)\n- `185-52-193-87`  your public IP (with dashes for nip.io)\n\n### Dynamic DNS with nip.io\n\nEOEPCA uses [nip.io](https://nip.io/) for dynamic DNS, embedding IP addresses in hostnames:\n\n```\n\u003cservice-name\u003e.\u003cpublic-ip-with-dashes\u003e.nip.io\n```\n\nExample: `workspace-api.192-168-49-2.nip.io` resolves to `192.168.49.2`\n\nThis approach avoids the need for DNS server configuration during development/testing.\n\n**Sources:** [README.md:100-112](), [system/clusters/README.md:45-50]()\n\n---\n\n## Step 4: Bootstrap Flux in Cluster\n\nThe following diagram shows the Flux resource structure created during bootstrap:\n\n```mermaid\ngraph TB\n    subgraph \"flux-system Namespace\"\n        GitRepo[\"GitRepository\u003cbr/\u003eflux-system\u003cbr/\u003eSource: github.com/EOEPCA/eoepca\u003cbr/\u003eBranch: develop\u003cbr/\u003eInterval: 1m\"]\n        \n        KustomMain[\"Kustomization\u003cbr/\u003eflux-system\u003cbr/\u003ePath: ./system/clusters/creodias/system\u003cbr/\u003eInterval: 10m\"]\n        \n        KustomUM[\"Kustomization\u003cbr/\u003euser-management\u003cbr/\u003ePath: ./system/clusters/creodias/user-management\u003cbr/\u003eInterval: 1m\"]\n        \n        KustomRM[\"Kustomization\u003cbr/\u003eresource-management\u003cbr/\u003ePath: ./system/clusters/creodias/resource-management\u003cbr/\u003eInterval: 1m\"]\n        \n        KustomProc[\"Kustomization\u003cbr/\u003eprocessing-and-chaining\u003cbr/\u003ePath: ./system/clusters/creodias/processing-and-chaining\u003cbr/\u003eInterval: 1m\"]\n        \n        Controllers[\"Flux Controllers\u003cbr/\u003esource-controller\u003cbr/\u003ekustomize-controller\u003cbr/\u003ehelm-controller\u003cbr/\u003enotification-controller\"]\n    end\n    \n    GitRepo --\u003e|\"sourceRef\"| KustomMain\n    GitRepo --\u003e|\"sourceRef\"| KustomUM\n    GitRepo --\u003e|\"sourceRef\"| KustomRM\n    GitRepo --\u003e|\"sourceRef\"| KustomProc\n    \n    Controllers --\u003e|\"Reconcile\"| GitRepo\n    Controllers --\u003e|\"Reconcile\"| KustomMain\n    Controllers --\u003e|\"Reconcile\"| KustomUM\n    Controllers --\u003e|\"Reconcile\"| KustomRM\n    Controllers --\u003e|\"Reconcile\"| KustomProc\n```\n\n### Execute Bootstrap Script\n\nRun the deployment script with appropriate environment variables:\n\n```bash\nexport GITHUB_USER=\u003cyour-username\u003e\nexport GITHUB_TOKEN=\u003cyour-token\u003e\nexport BRANCH=v1.4              # or develop\nexport TARGET=minikube          # or creodias, develop\n\n./system/clusters/deployCluster.sh\n```\n\n### Script Environment Variables\n\nThe `deployCluster.sh` script accepts:\n\n- **BRANCH**: Git branch to synchronize (defaults to current working branch)\n- **TARGET**: Deployment target directory name under `system/clusters/` (defaults to `minikube`)\n\n### Bootstrap Process\n\nThe script executes `flux bootstrap` which:\n\n1. Installs Flux controllers in the `flux-system` namespace\n2. Creates a `GitRepository` resource pointing to your repository\n3. Creates a root `Kustomization` resource for the system path\n4. Configures write access via SSH deploy key\n5. Commits Flux manifests back to your repository\n\nFor personal GitHub accounts, modify the bootstrap command:\n\n```bash\nflux bootstrap github \\\n  --owner=$GITHUB_USER \\\n  --repository=eoepca \\\n  --branch=\"${BRANCH}\" \\\n  --path=\"system/clusters/${TARGET}/system\" \\\n  --personal\n```\n\n**Sources:** [system/clusters/README.md:51-78](), [system/clusters/creodias/system/flux-system/gotk-sync.yaml:1-28]()\n\n---\n\n## Flux Resource Structure\n\nThe following diagram maps Flux CustomResourceDefinitions to their manifest files:\n\n```mermaid\ngraph LR\n    subgraph \"Flux CRDs\"\n        GitRepoCRD[\"CustomResourceDefinition\u003cbr/\u003egitrepositories.source.toolkit.fluxcd.io\"]\n        KustomizationCRD[\"CustomResourceDefinition\u003cbr/\u003ekustomizations.kustomize.toolkit.fluxcd.io\"]\n        HelmReleaseCRD[\"CustomResourceDefinition\u003cbr/\u003ehelmreleases.helm.toolkit.fluxcd.io\"]\n        HelmRepoCRD[\"CustomResourceDefinition\u003cbr/\u003ehelmrepositories.source.toolkit.fluxcd.io\"]\n    end\n    \n    subgraph \"System Manifests\"\n        GotKComponents[\"gotk-components.yaml\u003cbr/\u003eLines 1-172217\"]\n        GotKSync[\"gotk-sync.yaml\u003cbr/\u003eGitRepository + Kustomization\"]\n        FluxPatch[\"flux-system-patch.yaml\u003cbr/\u003eResource limits\"]\n    end\n    \n    subgraph \"Kustomization Structure\"\n        MainKust[\"system/flux-system/\u003cbr/\u003ekustomization.yaml\"]\n        UMSync[\"flux-system-extended/\u003cbr/\u003euser-management-sync.yaml\"]\n        RMSync[\"flux-system-extended/\u003cbr/\u003eresource-management-sync.yaml\"]\n        ProcSync[\"flux-system-extended/\u003cbr/\u003eprocessing-and-chaining-sync.yaml\"]\n    end\n    \n    GitRepoCRD -.-\u003e|\"Defines\"| GotKSync\n    KustomizationCRD -.-\u003e|\"Defines\"| GotKSync\n    KustomizationCRD -.-\u003e|\"Defines\"| UMSync\n    KustomizationCRD -.-\u003e|\"Defines\"| RMSync\n    KustomizationCRD -.-\u003e|\"Defines\"| ProcSync\n    \n    MainKust --\u003e|\"Resources\"| GotKComponents\n    MainKust --\u003e|\"Resources\"| GotKSync\n    MainKust --\u003e|\"Patches\"| FluxPatch\n```\n\n### Core Flux Components\n\nThe `gotk-components.yaml` manifest defines all Flux CRDs and controllers:\n\n- **source-controller**: Fetches artifacts from Git/Helm/S3 sources\n- **kustomize-controller**: Applies Kustomize overlays\n- **helm-controller**: Manages HelmRelease resources\n- **notification-controller**: Sends alerts and handles webhooks\n\n**Sources:** [system/clusters/creodias/system/flux-system/gotk-components.yaml:1-100](), [system/clusters/creodias/system/flux-system/kustomization.yaml:1-8]()\n\n---\n\n## GitRepository Configuration\n\nThe `GitRepository` resource defines the source of truth:\n\n```yaml\n# From: system/clusters/creodias/system/flux-system/gotk-sync.yaml\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  ref:\n    branch: develop\n  secretRef:\n    name: flux-system\n  url: ssh://git@github.com/EOEPCA/eoepca\n```\n\n**Key Fields:**\n- `interval`: How often to poll the Git repository (1 minute)\n- `ref.branch`: Branch to track (`develop` or `v1.4`)\n- `secretRef`: SSH credentials for repository access\n- `url`: Repository SSH URL\n\n**Sources:** [system/clusters/creodias/system/flux-system/gotk-sync.yaml:3-14]()\n\n---\n\n## Kustomization Resources\n\nThe root `Kustomization` resource triggers deployment:\n\n```yaml\n# From: system/clusters/creodias/system/flux-system/gotk-sync.yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  path: ./system/clusters/creodias/system\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n```\n\n**Key Fields:**\n- `interval`: Reconciliation frequency (10 minutes, overridden to 1m by patch)\n- `path`: Directory path within repository to apply\n- `prune`: Delete resources removed from Git\n- `sourceRef`: References the `flux-system` GitRepository\n\n### Building Block Kustomizations\n\nAdditional `Kustomization` resources deploy each building block domain:\n\n| Kustomization | Path | Namespace | File |\n|---------------|------|-----------|------|\n| `user-management` | `./system/clusters/creodias/user-management` | `flux-system` | [user-management-sync.yaml:1-13]() |\n| `resource-management` | `./system/clusters/creodias/resource-management` | `flux-system` | [resource-management-sync.yaml:1-13]() |\n| `processing-and-chaining` | `./system/clusters/creodias/processing-and-chaining` | `flux-system` | [processing-and-chaining-sync.yaml:1-13]() |\n\n**Sources:** [system/clusters/creodias/system/flux-system/gotk-sync.yaml:16-28](), [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml:1-13](), [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml:1-13](), [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml:1-13]()\n\n---\n\n## Resource Patches\n\nThe `flux-system-patch.yaml` file customizes Flux controller resources:\n\n### Memory Limits\n\nIncreases memory allocation for source and kustomize controllers:\n\n```yaml\n# From: system/clusters/creodias/system/flux-system/flux-system-patch.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: source-controller\n  namespace: flux-system\nspec:\n  template:\n    spec:\n      containers:\n        - name: manager\n          resources:\n            limits:\n              memory: 2Gi\n            requests:\n              memory: 500Mi\n```\n\nSimilar patches apply to `kustomize-controller`. These increased limits accommodate the large EOEPCA deployment manifests.\n\n### Reconciliation Interval\n\nReduces the root `Kustomization` interval to 1 minute for faster synchronization:\n\n```yaml\n# From: system/clusters/creodias/system/flux-system/flux-system-patch.yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m0s\n```\n\n**Sources:** [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:1-40]()\n\n---\n\n## Deployment Targets Comparison\n\nThe following table compares configuration differences across deployment targets:\n\n| Aspect | minikube | creodias | develop |\n|--------|----------|----------|---------|\n| Infrastructure | Local VM | OpenStack VMs | OpenStack VMs |\n| Public IP | `minikube ip` output | Fixed floating IP | Fixed floating IP |\n| DNS Pattern | `\u003cservice\u003e.192-168-49-2.nip.io` | `\u003cservice\u003e.185-52-193-87.nip.io` | `\u003cservice\u003e.develop.eoepca.org` |\n| Storage Backend | MinIO (local) | CloudFerro S3 + MinIO | CloudFerro S3 + MinIO |\n| TLS Certificates | Self-signed or LetsEncrypt | LetsEncrypt | LetsEncrypt |\n| Resource Limits | Reduced for single-node | Full production specs | Full production specs |\n| Building Blocks | Selective deployment | Full system | Full system |\n\n### Minikube Deployment Considerations\n\nThe v1.4 release expanded system components to a level where full deployment in Minikube requires significant resources (8 CPU, 32GB RAM). Consider:\n\n- Deploying selective building blocks\n- Using the [Deployment Guide](https://deployment-guide.docs.eoepca.org/) Helm scripts for component-by-component deployment\n- Upgrading to k3s for more lightweight execution\n\n**Sources:** [README.md:88-98](), [minikube/README.md:26-49]()\n\n---\n\n## GitOps Synchronization\n\nFlux continuously synchronizes the cluster state with the Git repository:\n\n```mermaid\nsequenceDiagram\n    participant Git as GitHub Repository\u003cbr/\u003eEOEPCA/eoepca\n    participant SC as source-controller\n    participant KC as kustomize-controller\n    participant HC as helm-controller\n    participant K8s as Kubernetes API\n    \n    Note over SC: Poll every 1m\n    SC-\u003e\u003eGit: Fetch GitRepository\u003cbr/\u003eflux-system\n    Git--\u003e\u003eSC: Return commit SHA\n    \n    alt New Commit Detected\n        SC-\u003e\u003eSC: Download artifact\n        SC-\u003e\u003eK8s: Update GitRepository\u003cbr/\u003estatus.artifact\n        \n        Note over KC: Reconcile every 1m\n        KC-\u003e\u003eK8s: Get Kustomization\u003cbr/\u003eflux-system\n        K8s--\u003e\u003eKC: Return spec\n        KC-\u003e\u003eSC: Get artifact from\u003cbr/\u003eGitRepository\n        SC--\u003e\u003eKC: Return manifests\n        KC-\u003e\u003eKC: Build Kustomization\n        KC-\u003e\u003eK8s: Apply resources\n        \n        Note over HC: Process HelmReleases\n        HC-\u003e\u003eK8s: Watch HelmRelease\u003cbr/\u003eresources\n        K8s--\u003e\u003eHC: HelmRelease objects\n        HC-\u003e\u003eHC: Render Helm charts\n        HC-\u003e\u003eK8s: Apply rendered\u003cbr/\u003emanifests\n    end\n```\n\n### Synchronization Intervals\n\n| Controller | Default Interval | Configurable Via |\n|------------|------------------|------------------|\n| `source-controller` | 1m | `GitRepository.spec.interval` |\n| `kustomize-controller` | 1m (patched) | `Kustomization.spec.interval` |\n| `helm-controller` | Varies per HelmRelease | `HelmRelease.spec.interval` |\n\n### Triggering Manual Reconciliation\n\nForce immediate reconciliation without waiting for the interval:\n\n```bash\n# Reconcile specific Kustomization\nflux reconcile kustomization flux-system\n\n# Reconcile GitRepository source\nflux reconcile source git flux-system\n\n# Reconcile HelmRelease\nflux reconcile helmrelease \u003crelease-name\u003e -n \u003cnamespace\u003e\n```\n\n**Sources:** [system/clusters/README.md:79-86]()\n\n---\n\n## Chart Version Requirements\n\nWhen using a `GitRepository` as a Helm chart source, chart version numbers must be incremented for Flux to detect updates. Flux tracks the `Chart.yaml` version field to determine if reconciliation is needed.\n\nThis is enforced for charts stored in Git rather than Helm repositories. If modifying chart values without changing logic, increment the patch version (e.g., `1.0.0`  `1.0.1`).\n\n**Sources:** [system/clusters/README.md:84-86]()\n\n---\n\n## Undeploying Flux\n\nTo remove Flux from the cluster:\n\n```bash\n./system/clusters/undeployCluster.sh\n```\n\n**Important**: This removes Flux controllers but **does not** uninstall EOEPCA building blocks deployed by Flux. Those must be manually deleted or the cluster destroyed.\n\nTo fully clean up:\n\n```bash\n# Delete building block namespaces\nkubectl delete namespace um rm proc\n\n# Delete Flux namespace\nkubectl delete namespace flux-system\n```\n\n**Sources:** [system/clusters/README.md:88-95]()\n\n---\n\n## Verification Steps\n\nAfter bootstrap completes, verify the deployment:\n\n### Check Flux Controllers\n\n```bash\n# Verify all Flux controllers are running\nkubectl get pods -n flux-system\n\n# Expected output:\n# NAME                                       READY   STATUS\n# helm-controller-...                        1/1     Running\n# kustomize-controller-...                   1/1     Running\n# notification-controller-...                1/1     Running\n# source-controller-...                      1/1     Running\n```\n\n### Check GitRepository Status\n\n```bash\nflux get sources git\n\n# Expected output shows artifact fetched successfully\n# NAME         READY  MESSAGE\n# flux-system  True   Fetched revision: develop/abc123...\n```\n\n### Check Kustomization Status\n\n```bash\nflux get kustomizations\n\n# Expected output shows all Kustomizations applied\n# NAME                      READY  MESSAGE\n# flux-system               True   Applied revision: develop/abc123\n# user-management           True   Applied revision: develop/abc123\n# resource-management       True   Applied revision: develop/abc123\n# processing-and-chaining   True   Applied revision: develop/abc123\n```\n\n### Check Deployed Resources\n\n```bash\n# List HelmReleases across all namespaces\nflux get helmreleases --all-namespaces\n\n# Check building block namespaces\nkubectl get pods -n um    # User Management\nkubectl get pods -n rm    # Resource Management\nkubectl get pods -n proc  # Processing \u0026 Chaining\n```\n\nFor comprehensive testing and validation, see [Testing and Validation](#2.3).\n\n**Sources:** [system/clusters/README.md:25-29]()\n\n---\n\n## Troubleshooting Common Issues\n\n### Flux Not Reconciling\n\n**Symptoms**: `GitRepository` or `Kustomization` stuck in failed state\n\n**Resolution**:\n```bash\n# Check Flux controller logs\nkubectl logs -n flux-system deploy/source-controller\nkubectl logs -n flux-system deploy/kustomize-controller\n\n# Suspend and resume to reset\nflux suspend kustomization flux-system\nflux resume kustomization flux-system\n```\n\n### KUBECONFIG Path Issues\n\n**Symptoms**: Flux commands fail with \"unable to read kubeconfig\"\n\n**Resolution**: Set `KUBECONFIG` to a single file path, not a colon-delimited list:\n```bash\nexport KUBECONFIG=~/.kube/config\n# NOT: export KUBECONFIG=~/.kube/config:~/.kube/other-config\n```\n\n### Authentication Failures\n\n**Symptoms**: `GitRepository` fails with \"authentication required\"\n\n**Resolution**: Verify GitHub credentials and regenerate deploy key:\n```bash\nflux create source git flux-system \\\n  --url=ssh://git@github.com/$GITHUB_USER/eoepca \\\n  --branch=develop \\\n  --secret-ref=flux-system\n```\n\n### Resource Exhaustion\n\n**Symptoms**: Pods stuck in `Pending` or `OOMKilled` state\n\n**Resolution**: Increase controller resource limits in `flux-system-patch.yaml` or upgrade cluster resources.\n\n**Sources:** [system/clusters/README.md:17-21]()\n\n---\n\n## Alternative Deployment Approach\n\nThe [Deployment Guide](https://deployment-guide.docs.eoepca.org/) provides an alternative approach using direct Helm commands instead of Flux GitOps. This approach:\n\n- Provides more detailed component-level documentation\n- Uses shell scripts for step-by-step deployment\n- Assumes Minikube by default\n- Better suited for learning and development\n\nUse the Flux GitOps approach (documented here) for:\n- Production deployments\n- Continuous delivery requirements\n- Multiple cluster management\n- Automated reconciliation\n\nUse the Deployment Guide approach for:\n- Initial learning and exploration\n- Development and testing\n- Single-component deployment\n- Troubleshooting individual services\n\n**Sources:** [README.md:93-99](), [system/clusters/README.md:3-6]()"])</script><script>self.__next_f.push([1,"1a:T56fa,"])</script><script>self.__next_f.push([1,"# Infrastructure Provisioning\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [bin/install-kubeseal.sh](bin/install-kubeseal.sh)\n- [bin/install-rke.sh](bin/install-rke.sh)\n- [creodias/.gitignore](creodias/.gitignore)\n- [creodias/.terraform/modules/modules.json](creodias/.terraform/modules/modules.json)\n- [creodias/README.md](creodias/README.md)\n- [creodias/deployCREODIAS.sh](creodias/deployCREODIAS.sh)\n- [creodias/eoepca.tf](creodias/eoepca.tf)\n- [creodias/eoepca.tfvars](creodias/eoepca.tfvars)\n- [creodias/modules/compute/main.tf](creodias/modules/compute/main.tf)\n- [creodias/modules/compute/nfs-setup.sh](creodias/modules/compute/nfs-setup.sh)\n- [creodias/modules/compute/nfs.tf](creodias/modules/compute/nfs.tf)\n- [creodias/modules/compute/outputs.tf](creodias/modules/compute/outputs.tf)\n- [creodias/modules/compute/variables.tf](creodias/modules/compute/variables.tf)\n- [creodias/modules/loadbalancer/main.tf](creodias/modules/loadbalancer/main.tf)\n- [creodias/terraform.tfstate](creodias/terraform.tfstate)\n- [creodias/terraform.tfstate.backup](creodias/terraform.tfstate.backup)\n- [creodias/variables.tf](creodias/variables.tf)\n- [kubernetes/cluster.7z](kubernetes/cluster.7z)\n- [kubernetes/create-cluster-config.sh](kubernetes/create-cluster-config.sh)\n- [minikube/README.md](minikube/README.md)\n- [system/clusters/README.md](system/clusters/README.md)\n- [travis/acceptanceTest.sh](travis/acceptanceTest.sh)\n- [travis/setupMinikube.sh](travis/setupMinikube.sh)\n- [travis/setupRobot.sh](travis/setupRobot.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains how to provision the underlying infrastructure for EOEPCA deployments. It covers three deployment scenarios:\n\n1. **Cloud deployment** using Terraform to provision VMs on OpenStack (CREODIAS), followed by Kubernetes cluster setup with RKE\n2. **Local development** using Minikube for single-node Kubernetes clusters\n3. **Lightweight alternative** using k3s for resource-constrained environments\n\nFor GitOps-based system deployment once infrastructure is ready, see [Deployment Guide](#2.1). For detailed Kubernetes cluster configuration specifics, see [Kubernetes Cluster Setup](#8.1). For network architecture details, see [Network Architecture](#8.3).\n\n**Sources:** [README.md:72-96](), [creodias/README.md:1-128]()\n\n---\n\n## Infrastructure Deployment Paths\n\nThe EOEPCA system supports three infrastructure provisioning approaches, depending on the target environment:\n\n| Deployment Type | Infrastructure Tool | Kubernetes Tool | Use Case |\n|----------------|--------------------|--------------------|----------|\n| Cloud (OpenStack) | Terraform | RKE (Rancher Kubernetes Engine) | Production deployments on CREODIAS |\n| Local Development | Direct installation | Minikube | Developer workstations, testing |\n| Lightweight | Direct installation | k3s | VMs, resource-constrained environments |\n\n```mermaid\ngraph TB\n    subgraph \"Cloud Deployment Path\"\n        TF[\"Terraform\u003cbr/\u003e(creodias/*.tf)\"]\n        OS[\"OpenStack API\u003cbr/\u003eCREODIAS\"]\n        RKE[\"RKE\u003cbr/\u003e(rke up)\"]\n        K8sCloud[\"Kubernetes Cluster\"]\n        \n        TF --\u003e|\"openstack provider\u003cbr/\u003eprovision VMs\"| OS\n        OS --\u003e|\"bastion, master,\u003cbr/\u003eworker VMs\"| RKE\n        RKE --\u003e|\"cluster.yml config\"| K8sCloud\n    end\n    \n    subgraph \"Local Development Path\"\n        Minikube[\"minikube start\u003cbr/\u003e(setup-minikube.sh)\"]\n        K8sLocal[\"Kubernetes Cluster\u003cbr/\u003e(single node)\"]\n        \n        Minikube --\u003e|\"docker driver\"| K8sLocal\n    end\n    \n    subgraph \"Lightweight Path\"\n        K3s[\"k3s install\u003cbr/\u003e(setup-k3s.sh)\"]\n        K8sLite[\"Kubernetes Cluster\u003cbr/\u003e(lightweight)\"]\n        \n        K3s --\u003e|\"docker runtime\"| K8sLite\n    end\n    \n    K8sCloud --\u003e Flux[\"Flux CD\u003cbr/\u003eGitOps Deployment\"]\n    K8sLocal --\u003e Flux\n    K8sLite --\u003e Flux\n```\n\n**Sources:** [README.md:88-94](), [creodias/README.md:9-11](), [minikube/README.md:3-52]()\n\n---\n\n## OpenStack Provisioning with Terraform\n\n### Terraform Module Structure\n\nThe Terraform infrastructure code is organized into modules under `creodias/modules/`:\n\n```mermaid\ngraph LR\n    Main[\"eoepca.tf\u003cbr/\u003eRoot Module\"]\n    \n    Network[\"network module\u003cbr/\u003emodules/network/\"]\n    IPs[\"ips module\u003cbr/\u003emodules/ips/\"]\n    Compute[\"compute module\u003cbr/\u003emodules/compute/\"]\n    LB[\"loadbalancer module\u003cbr/\u003emodules/loadbalancer/\"]\n    \n    Main --\u003e|\"network_name\u003cbr/\u003esubnet_cidr\"| Network\n    Main --\u003e|\"floatingip_pool\"| IPs\n    Main --\u003e|\"number_of_k8s_masters\u003cbr/\u003enumber_of_k8s_nodes\"| Compute\n    Main --\u003e|\"k8s_node_ips\"| LB\n    \n    Network --\u003e|\"network_id\u003cbr/\u003erouter_id\"| Compute\n    IPs --\u003e|\"bastion_fips\u003cbr/\u003ek8s_master_fips\"| Compute\n    Compute --\u003e|\"k8s_secgroup_id\"| LB\n```\n\nThe root module `eoepca.tf` orchestrates infrastructure provisioning by calling specialized modules for networking, IP allocation, compute resources, and load balancing.\n\n**Sources:** [creodias/eoepca.tf:1-146](), [creodias/.terraform/modules/modules.json:1]()\n\n### Prerequisites and Configuration\n\nBefore running Terraform, several prerequisites must be satisfied:\n\n#### OpenStack Client Setup\n\nThe OpenStack provider requires the `openstackclient` Python package and a `clouds.yaml` configuration file:\n\n```yaml\nclouds:\n  eoepca:\n    auth:\n      auth_url: https://cf2.cloudferro.com:5000/v3\n      username: \"user@example.com\"\n      project_name: \"eoepca\"\n      project_id: d86660d4a1a443579c71096771a8104c\n      user_domain_name: \"cloud_xxxxx\"\n      password: \"xxxxxxxxxx\"\n    region_name: \"RegionOne\"\n    interface: \"public\"\n    identity_api_version: 3\n```\n\nThis file must be placed in `./clouds.yaml`, `~/.config/openstack/clouds.yaml`, or `/etc/openstack/clouds.yaml`.\n\n**Sources:** [creodias/README.md:18-52]()\n\n#### Deployment Configuration\n\nThe file `creodias/eoepca.tfvars` defines deployment parameters:\n\n| Variable | Description | Example Value |\n|----------|-------------|---------------|\n| `cluster_name` | Unique identifier for the cluster | `\"develop\"` |\n| `number_of_bastions` | Number of bastion hosts (0 or 1) | `1` |\n| `number_of_k8s_masters_no_floating_ip` | Master nodes without public IPs | `1` |\n| `number_of_k8s_nodes_no_floating_ip` | Worker nodes without public IPs | `6` |\n| `flavor_bastion` | OpenStack flavor ID for bastion | `\"14\"` (eo1.xsmall) |\n| `flavor_k8s_master` | OpenStack flavor ID for masters | `\"20\"` (eo2.large) |\n| `flavor_k8s_node` | OpenStack flavor ID for workers | `\"21\"` (eo2.xlarge) |\n| `nfs_disk_size` | NFS volume size in GB | `1024` |\n| `subnet_cidr` | Private network CIDR | `\"192.168.123.0/24\"` |\n\n**Sources:** [creodias/eoepca.tfvars:1-57](), [creodias/variables.tf:1-233]()\n\n### Deployment Execution\n\nThe deployment script `deployCREODIAS.sh` orchestrates the Terraform provisioning process:\n\n```bash\n#!/usr/bin/env bash\nexport OS_CLOUD=eoepca\n./creodias/deployCREODIAS.sh\n```\n\nThe script performs a two-phase deployment:\n\n1. **Keypair Creation** - Provisions `openstack_compute_keypair_v2.k8s` first to ensure SSH access\n2. **Full Deployment** - Provisions all remaining resources\n\n**Sources:** [creodias/deployCREODIAS.sh:1-52](), [creodias/README.md:60-73]()\n\n---\n\n## Provisioned Infrastructure\n\n### Resource Topology\n\nThe Terraform deployment creates the following infrastructure components:\n\n```mermaid\ngraph TB\n    subgraph \"Public Network\u003cbr/\u003e(external3)\"\n        BastionFIP[\"Bastion Floating IP\u003cbr/\u003e185.52.192.185\"]\n        LBFIP[\"LoadBalancer Floating IP\u003cbr/\u003e185.52.192.231\"]\n    end\n    \n    subgraph \"Private Network\u003cbr/\u003e(192.168.123.0/24)\"\n        Router[\"OpenStack Router\u003cbr/\u003erouter_id: 3e69d186...\"]\n        \n        Bastion[\"Bastion VM\u003cbr/\u003edevelop-bastion-1\u003cbr/\u003e192.168.123.24\u003cbr/\u003eflavor: eo1.xsmall\"]\n        \n        Master[\"Master VM\u003cbr/\u003edevelop-k8s-master-nf-1\u003cbr/\u003e192.168.123.15\u003cbr/\u003eflavor: eo2.large\"]\n        \n        Worker1[\"Worker VM 1\u003cbr/\u003e192.168.123.16\u003cbr/\u003eflavor: eo2.xlarge\"]\n        Worker2[\"Worker VM 2\u003cbr/\u003e192.168.123.6\u003cbr/\u003eflavor: eo2.xlarge\"]\n        WorkerN[\"Worker VM 6\u003cbr/\u003e192.168.123.22\u003cbr/\u003eflavor: eo2.xlarge\"]\n        \n        NFS[\"NFS Server\u003cbr/\u003edevelop-nfs\u003cbr/\u003e192.168.123.14\u003cbr/\u003eflavor: eo2.large\"]\n        NFSVol[\"Block Volume\u003cbr/\u003edevelop-nfs-expansion\u003cbr/\u003e1024GB SSD\u003cbr/\u003e/dev/sdb\"]\n        \n        LB[\"Load Balancer\u003cbr/\u003edevelop-lb\u003cbr/\u003evip_port_id\"]\n    end\n    \n    BastionFIP -.-\u003e|floating_ip_associate| Bastion\n    LBFIP -.-\u003e|floating_ip_associate| LB\n    \n    Router --\u003e Bastion\n    Router --\u003e Master\n    Router --\u003e Worker1\n    Router --\u003e Worker2\n    Router --\u003e WorkerN\n    Router --\u003e NFS\n    Router --\u003e LB\n    \n    NFSVol --\u003e|volume_attach /dev/sdb| NFS\n    \n    LB --\u003e|pool members\u003cbr/\u003eport 31080/31443| Worker1\n    LB --\u003e|pool members\u003cbr/\u003eport 31080/31443| Worker2\n    LB --\u003e|pool members\u003cbr/\u003eport 31080/31443| WorkerN\n```\n\n**Sources:** [creodias/terraform.tfstate:6-146](), [creodias/eoepca.tf:95-146]()\n\n### Compute Instance Details\n\nThe `openstack_compute_instance_v2` resources are created with specific metadata for Kubernetes roles:\n\n| Instance Type | Resource Name | Kubespray Groups | Floating IP |\n|---------------|---------------|------------------|-------------|\n| Bastion | `openstack_compute_instance_v2.bastion` | `bastion` | Yes |\n| Master | `openstack_compute_instance_v2.k8s_master_no_floating_ip` | `etcd,kube-master,k8s-cluster,vault,no-floating` | No |\n| Worker | `openstack_compute_instance_v2.k8s_node_no_floating_ip` | `kube-node,k8s-cluster,no-floating` | No |\n| NFS | `openstack_compute_instance_v2.eoepca_nfs` | `nfs,no-floating` | No |\n\n**Sources:** [creodias/modules/compute/main.tf:116-174](), [creodias/modules/compute/main.tf:384-428](), [creodias/modules/compute/nfs.tf:1-49]()\n\n### Security Groups\n\nThe deployment creates multiple `openstack_networking_secgroup_v2` resources:\n\n| Security Group | Resource Name | Purpose | Key Rules |\n|----------------|---------------|---------|-----------|\n| `develop-bastion` | `openstack_networking_secgroup_v2.bastion` | SSH access to bastion | Ingress TCP 22 from `bastion_allowed_remote_ips` |\n| `develop-k8s-master` | `openstack_networking_secgroup_v2.k8s_master` | Kubernetes API access | Ingress TCP 6443 from `master_allowed_remote_ips` |\n| `develop-k8s` | `openstack_networking_secgroup_v2.k8s` | Internal cluster traffic | Ingress from same group, SSH from `k8s_allowed_remote_ips` |\n| `develop-k8s-worker` | `openstack_networking_secgroup_v2.worker` | NodePort services | Ingress TCP 30000-32767 from 0.0.0.0/0 |\n| `develop-lb` | `openstack_networking_secgroup_v2.lb` | Load balancer traffic | Ingress TCP 80/443 from 0.0.0.0/0 |\n\n**Sources:** [creodias/modules/compute/main.tf:14-96](), [creodias/modules/loadbalancer/main.tf:2-36]()\n\n### Load Balancer Configuration\n\nThe `openstack_lb_loadbalancer_v2.k8s` resource creates an Octavia load balancer with the following structure:\n\n```mermaid\ngraph TB\n    Client[\"External Client\"]\n    FIP[\"Floating IP\u003cbr/\u003e185.52.192.231\"]\n    \n    subgraph \"Load Balancer (develop-lb)\"\n        ListenerHTTPS[\"openstack_lb_listener_v2.https\u003cbr/\u003eport 443\u003cbr/\u003eprotocol HTTPS\"]\n        ListenerHTTP[\"openstack_lb_listener_v2.http\u003cbr/\u003eport 80\u003cbr/\u003eprotocol HTTP\"]\n        \n        PoolHTTPS[\"openstack_lb_pool_v2.https\u003cbr/\u003elb_method: ROUND_ROBIN\u003cbr/\u003epersistence: SOURCE_IP\"]\n        PoolHTTP[\"openstack_lb_pool_v2.http\u003cbr/\u003elb_method: ROUND_ROBIN\u003cbr/\u003epersistence: SOURCE_IP\"]\n        \n        ListenerHTTPS --\u003e PoolHTTPS\n        ListenerHTTP --\u003e PoolHTTP\n    end\n    \n    subgraph \"Backend Members\"\n        Member1[\"Worker 1\u003cbr/\u003e192.168.123.16:31443\u003cbr/\u003e192.168.123.16:31080\"]\n        Member2[\"Worker 2\u003cbr/\u003e192.168.123.6:31443\u003cbr/\u003e192.168.123.6:31080\"]\n        MemberN[\"Worker 6\u003cbr/\u003e192.168.123.22:31443\u003cbr/\u003e192.168.123.22:31080\"]\n    end\n    \n    Client --\u003e|HTTPS/HTTP| FIP\n    FIP --\u003e ListenerHTTPS\n    FIP --\u003e ListenerHTTP\n    \n    PoolHTTPS --\u003e Member1\n    PoolHTTPS --\u003e Member2\n    PoolHTTPS --\u003e MemberN\n    \n    PoolHTTP --\u003e Member1\n    PoolHTTP --\u003e Member2\n    PoolHTTP --\u003e MemberN\n```\n\nThe load balancer forwards traffic from public ports (80/443) to Kubernetes ingress controller NodePorts (31080/31443) on all worker nodes.\n\n**Sources:** [creodias/modules/loadbalancer/main.tf:38-183](), [creodias/README.md:90-92]()\n\n### NFS Server Provisioning\n\nThe NFS server is provisioned with an attached block volume and automated configuration:\n\n#### Volume Attachment\n\n```mermaid\ngraph LR\n    NFSInstance[\"openstack_compute_instance_v2.eoepca_nfs\u003cbr/\u003edevelop-nfs\u003cbr/\u003eimage: Ubuntu 18.04 LTS\u003cbr/\u003eflavor: eo2.large\"]\n    \n    NFSVolume[\"openstack_blockstorage_volume_v2.nfs_expansion\u003cbr/\u003edevelop-nfs-expansion\u003cbr/\u003esize: 1024 GB\u003cbr/\u003evolume_type: SSD\"]\n    \n    Attach[\"openstack_compute_volume_attach_v2.volume_attachment_nfs\u003cbr/\u003edevice: /dev/sdb\"]\n    \n    NFSInstance --\u003e Attach\n    NFSVolume --\u003e Attach\n```\n\n**Sources:** [creodias/modules/compute/nfs.tf:1-70]()\n\n#### Automated NFS Setup\n\nThe NFS configuration is automated via a provisioner that executes `nfs-setup.sh`:\n\n```bash\nprovisioner \"remote-exec\" {\n  inline = [\n    \"sudo chmod +x /tmp/nfs-setup.sh\",\n    \"echo /tmp/nfs-setup.sh | at now + 2 minute\",\n  ]\n}\n```\n\nThe `nfs-setup.sh` script performs:\n1. Installs `nfs-kernel-server` package\n2. Creates partition on `/dev/sdb` using `fdisk`\n3. Formats partition as ext4\n4. Mounts partition to `/data`\n5. Creates export directories: `/data/userman`, `/data/proc`, `/data/resman`, `/data/dynamic`\n6. Configures NFS exports in `/etc/exports` with options: `rw,no_root_squash,no_subtree_check`\n7. Restarts NFS service\n\n**Sources:** [creodias/modules/compute/nfs.tf:26-44](), [creodias/modules/compute/nfs-setup.sh:1-44]()\n\n---\n\n## Kubernetes Cluster Setup with RKE\n\n### Cluster Configuration Generation\n\nAfter Terraform provisioning, the RKE cluster configuration is generated using `create-cluster-config.sh`:\n\n```bash\n#!/usr/bin/env bash\ncd kubernetes\n./create-cluster-config.sh develop cluster.yml\n```\n\nThe script queries Terraform state to extract node IPs and generate the `cluster.yml` configuration:\n\n```mermaid\ngraph LR\n    TFState[\"terraform.tfstate\u003cbr/\u003eterraform output -json\"]\n    Script[\"create-cluster-config.sh\"]\n    \n    subgraph \"Extracted Data\"\n        Masters[\"k8s_master_ips.value[]\u003cbr/\u003emaster_nodes()\"]\n        Workers[\"k8s_node_ips.value[]\u003cbr/\u003eworker_nodes()\"]\n        Bastion[\"bastion_fips.value[]\u003cbr/\u003ebastion_host()\"]\n    end\n    \n    ClusterYML[\"cluster.yml\u003cbr/\u003eRKE configuration\"]\n    \n    TFState --\u003e|jq queries| Script\n    Script --\u003e Masters\n    Script --\u003e Workers\n    Script --\u003e Bastion\n    Masters --\u003e ClusterYML\n    Workers --\u003e ClusterYML\n    Bastion --\u003e ClusterYML\n```\n\n**Sources:** [kubernetes/create-cluster-config.sh:1-117]()\n\n### RKE Cluster Configuration Structure\n\nThe generated `cluster.yml` defines node roles and cluster settings:\n\n```yaml\ncluster_name: develop\nkubernetes_version: \"v1.22.11-rancher1-1\"\nnodes:\n  - address: 192.168.123.15\n    user: eouser\n    role:\n      - controlplane\n      - etcd\n  - address: 192.168.123.16\n    user: eouser\n    role:\n      - worker\n  # ... additional worker nodes\n\ningress:\n  provider: none\n\nbastion_host:\n  address: 185.52.192.185\n  user: eouser\n```\n\nKey configuration elements:\n- **kubernetes_version**: Set to `v1.22.11-rancher1-1`\n- **ingress.provider**: Set to `none` (EOEPCA deploys nginx-ingress separately)\n- **bastion_host**: Enables SSH tunneling through bastion for node access\n\n**Sources:** [kubernetes/create-cluster-config.sh:92-109]()\n\n### RKE Installation and Execution\n\nThe `rke` binary is installed via helper script:\n\n```bash\nbin/install-rke.sh\n# Downloads rke_linux-amd64 to ~/.local/bin/rke\n```\n\nCluster provisioning is executed with:\n\n```bash\nrke up --config cluster.yml\n```\n\nThis command:\n1. Establishes SSH connections to nodes via bastion\n2. Deploys Kubernetes system components\n3. Configures etcd cluster on master nodes\n4. Initializes worker nodes\n5. Generates `kube_config_cluster.yml` for kubectl access\n\n**Sources:** [bin/install-rke.sh:1-14](), [README.md:91]()\n\n---\n\n## Local Development with Minikube\n\n### Minikube Installation\n\nMinikube provides single-node Kubernetes clusters for local development:\n\n```bash\nminikube/setup-minikube.sh\n# Downloads and installs minikube to /usr/local/bin/\n```\n\nThe script supports two driver modes:\n\n| Driver | Usage | Description |\n|--------|-------|-------------|\n| `docker` (default) | `setup-minikube.sh` | Runs Kubernetes in Docker container |\n| `none` (native) | `setup-minikube.sh native` | Runs Kubernetes directly on host VM |\n\nThe `docker` driver is preferred and recommended by the Minikube team.\n\n**Sources:** [minikube/README.md:20-35](), [travis/setupMinikube.sh:1-51]()\n\n### Minikube Startup Configuration\n\nThe setup script starts Minikube with specific configuration:\n\n```bash\nexport CHANGE_MINIKUBE_NONE_USER=true\nminikube delete --purge --all\nminikube start --vm-driver=none \\\n  --bootstrapper=kubeadm \\\n  --kubernetes-version=${K8S_VER} \\\n  --extra-config=apiserver.authorization-mode=RBAC\n\nminikube addons enable ingress\nminikube update-context\n```\n\nKey parameters:\n- **vm-driver**: Uses `none` to run directly on host (in Travis CI context)\n- **bootstrapper**: Uses `kubeadm` for cluster initialization\n- **authorization-mode**: Enables RBAC for security\n- **ingress addon**: Deploys nginx-ingress controller\n\n**Sources:** [travis/setupMinikube.sh:18-36]()\n\n### Kubectl Configuration\n\nAfter Minikube starts, kubectl access is configured:\n\n```bash\nsudo chmod o+r ${HOME}/.kube/config\nsudo chmod -R o+r ${HOME}/.minikube/\nsudo chown -R $USER $HOME/.kube $HOME/.minikube\n```\n\nThe `kube_config` is typically located at `~/.kube/config` and provides cluster access credentials.\n\n**Sources:** [travis/setupMinikube.sh:34-36]()\n\n---\n\n## Lightweight Alternative: k3s\n\n### k3s Installation\n\nk3s offers a lightweight Kubernetes distribution suitable for resource-constrained environments:\n\n```bash\nminikube/setup-k3s.sh\n# Installs k3s with docker runtime\n```\n\n### k3s vs Minikube Comparison\n\n| Feature | k3s | Minikube |\n|---------|-----|----------|\n| Installation time | Faster | Slower |\n| Resource usage | Lower | Higher |\n| Default runtime | containerd | docker/containerd |\n| Production readiness | Production-grade | Development-focused |\n| Relationship to RKE | Sibling product from Rancher | Independent project |\n\nk3s is particularly advantageous when:\n- Running in VMs with limited resources\n- Faster setup times are required\n- Alignment with Rancher ecosystem is desired\n\n**Sources:** [minikube/README.md:39-48](), [README.md:96]()\n\n### k3s Docker Runtime Configuration\n\nThe setup script explicitly selects the `docker` container runtime:\n\n```bash\n# setup-k3s.sh selects docker runtime\ncurl -sfL https://get.k3s.io | sh -s - --docker\n```\n\nThis is required for ADES building block compatibility, which depends on Docker for CWL workflow execution with Calrissian.\n\n**Sources:** [minikube/README.md:48]()\n\n---\n\n## Bastion Access Patterns\n\n### SSH Jump Host Access\n\nThe bastion host provides the sole SSH entry point to the private cluster network:\n\n```bash\n# Single SSH session to a node\nssh -J eouser@185.52.192.185 eouser@192.168.123.15\n```\n\nWhere:\n- `185.52.192.185` is the bastion floating IP (from `bastion_fips` Terraform output)\n- `192.168.123.15` is a cluster node IP (from `k8s_master_ips` or `k8s_node_ips`)\n\n**Sources:** [creodias/README.md:102-106]()\n\n### VPN via sshuttle\n\nFor persistent cluster access, `sshuttle` creates a VPN tunnel through the bastion:\n\n```bash\n# Manual invocation\nsshuttle -r eouser@185.52.192.185 192.168.123.0/24\n\n# Using helper script (extracts IPs from Terraform state)\nbin/bastion-vpn.sh\n\n# Daemon mode (background)\nbin/bastion-vpn.sh -D\n\n# Terminate VPN\nkillall sshuttle\n```\n\nThis approach:\n- Routes all traffic to `192.168.123.0/24` through the bastion\n- Enables direct `kubectl` access to cluster API server\n- Persists across multiple terminal sessions (in daemon mode)\n\n**Sources:** [creodias/README.md:108-120]()\n\n---\n\n## Infrastructure Outputs\n\n### Terraform Output Variables\n\nAfter successful deployment, Terraform exposes infrastructure details via output variables:\n\n```bash\nterraform output -json\n```\n\n| Output Variable | Type | Description | Example Value |\n|-----------------|------|-------------|---------------|\n| `bastion_fips` | list(string) | Bastion floating IPs | `[\"185.52.192.185\"]` |\n| `loadbalancer_fips` | list(string) | Load balancer floating IPs | `[\"185.52.192.231\"]` |\n| `k8s_master_ips` | list(string) | Master node private IPs | `[\"192.168.123.15\"]` |\n| `k8s_node_ips` | list(string) | Worker node private IPs | `[\"192.168.123.16\", \"192.168.123.6\", ...]` |\n| `nfs_ip_address` | string | NFS server private IP | `\"192.168.123.14\"` |\n| `subnet_cidr` | string | Private network CIDR | `\"192.168.123.0/24\"` |\n| `router_id` | string | OpenStack router UUID | `\"3e69d186-59dd-4dcc-998c-db78c72d366f\"` |\n\nThese outputs are consumed by:\n- `create-cluster-config.sh` for RKE configuration generation\n- `bastion-vpn.sh` for VPN tunnel setup\n- Subsequent deployment scripts for service configuration\n\n**Sources:** [creodias/eoepca.tf:95-146](), [creodias/terraform.tfstate:6-94](), [kubernetes/create-cluster-config.sh:20-80]()\n\n---\n\n## Summary\n\nInfrastructure provisioning for EOEPCA follows three distinct paths depending on deployment context:\n\n1. **Production (OpenStack + RKE)**:\n   - Terraform provisions VMs, networking, and load balancers on CREODIAS OpenStack\n   - RKE deploys multi-node Kubernetes cluster with etcd, control plane, and worker nodes\n   - Bastion host provides secure SSH access via jump host or VPN tunnel\n   - NFS server provides shared persistence with automated partition setup\n\n2. **Local Development (Minikube)**:\n   - Single-node Kubernetes cluster running in Docker container or natively\n   - Simplified setup for component development and testing\n   - Ingress addon pre-configured for local service exposure\n\n3. **Lightweight (k3s)**:\n   - Minimal Kubernetes distribution for resource-constrained environments\n   - Faster installation and lower overhead than Minikube\n   - Docker runtime compatibility for ADES/Calrissian workflows\n\nAll paths converge to a functional Kubernetes cluster ready for GitOps deployment via Flux CD.\n\n**Sources:** [README.md:72-96](), [creodias/README.md:76-97](), [minikube/README.md:1-52]()"])</script><script>self.__next_f.push([1,"1b:T8200,"])</script><script>self.__next_f.push([1,"# Testing and Validation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot](test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot)\n- [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot](test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot)\n- [test/acceptance/01__UserManagement/03__PDP_Engine/getOwnership.py](test/acceptance/01__UserManagement/03__PDP_Engine/getOwnership.py)\n- [test/acceptance/01__UserManagement/03__PDP_Engine/tkn.sh](test/acceptance/01__UserManagement/03__PDP_Engine/tkn.sh)\n- [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot](test/acceptance/02__Processing/01__ADES/01__API_PROC.robot)\n- [test/acceptance/02__Processing/01__ADES/02__WPS.robot](test/acceptance/02__Processing/01__ADES/02__WPS.robot)\n- [test/acceptance/__init__.robot](test/acceptance/__init__.robot)\n- [test/client/.gitignore](test/client/.gitignore)\n- [test/client/DemoClient.py](test/client/DemoClient.py)\n- [test/client/debug/jwt-output-by-pep.json](test/client/debug/jwt-output-by-pep.json)\n- [test/client/main.py](test/client/main.py)\n- [test/client/requirements.txt](test/client/requirements.txt)\n- [test/client/setup.sh](test/client/setup.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the testing and validation framework for the EOEPCA platform. It covers the automated acceptance tests built with Robot Framework, the `DemoClient` Python library for programmatic interaction with EOEPCA services, and manual testing procedures. The test framework validates authentication flows, resource protection policies, processing capabilities, and workspace provisioning across the entire platform.\n\nFor information about local development environments, see [Local Development with Minikube](#9.3). For deployment procedures that should precede testing, see [Deployment Guide](#2.1).\n\n**Sources:** [test/acceptance/__init__.robot:1-22](), [test/client/main.py:1-10]()\n\n---\n\n## Test Framework Architecture\n\nThe EOEPCA test framework consists of three primary components:\n\n| Component | Technology | Purpose |\n|-----------|-----------|---------|\n| **DemoClient Library** | Python 3 | Reusable client library providing methods for authentication, resource registration, and service interaction |\n| **Acceptance Tests** | Robot Framework | Automated test suites covering User Management, Processing, and Resource Management |\n| **Manual Test Client** | Python Script | Interactive test script demonstrating end-to-end workflows |\n\n```mermaid\ngraph TB\n    subgraph \"Test Framework Components\"\n        ManualClient[\"main.py\u003cbr/\u003eManual Test Script\"]\n        DemoClient[\"DemoClient.py\u003cbr/\u003ePython Library\u003cbr/\u003e@library decorator\"]\n        RobotTests[\"Robot Framework Tests\u003cbr/\u003e*.robot files\"]\n    end\n    \n    subgraph \"Test Organization\"\n        InitRobot[\"__init__.robot\u003cbr/\u003eGlobal Variables\"]\n        UMTests[\"01__UserManagement/\u003cbr/\u003eLogin, PDP, UserProfile\"]\n        ProcTests[\"02__Processing/\u003cbr/\u003eADES API, WPS\"]\n        RMTests[\"03__ResourceManagement/\u003cbr/\u003eCatalogue, Workspace\"]\n    end\n    \n    subgraph \"Test Targets\"\n        IdentityService[\"Identity Service\u003cbr/\u003ehttps://auth.domain\"]\n        LoginService[\"Login Service\u003cbr/\u003eKeycloak/Gluu\"]\n        PDPEngine[\"PDP Engine\u003cbr/\u003e/pdp/policy/validate\"]\n        ADES[\"ADES Service\u003cbr/\u003e/{user}/wps3, /{user}/zoo\"]\n        WorkspaceAPI[\"Workspace API\u003cbr/\u003e/workspaces/{prefix}-{user}\"]\n        ResourceCatalogue[\"Resource Catalogue\u003cbr/\u003epycsw CSW/OpenSearch\"]\n        PEPResources[\"PEP Resource APIs\u003cbr/\u003ehttp://*-pepapi.domain\"]\n    end\n    \n    ManualClient --\u003e|\"import\"| DemoClient\n    RobotTests --\u003e|\"Library\"| DemoClient\n    RobotTests --\u003e|\"import\"| InitRobot\n    \n    InitRobot --\u003e UMTests\n    InitRobot --\u003e ProcTests\n    InitRobot --\u003e RMTests\n    \n    DemoClient --\u003e|\"authenticate\"| IdentityService\n    DemoClient --\u003e|\"register resources\"| PEPResources\n    DemoClient --\u003e|\"validate policies\"| PDPEngine\n    DemoClient --\u003e|\"execute processes\"| ADES\n    DemoClient --\u003e|\"provision workspaces\"| WorkspaceAPI\n    \n    UMTests --\u003e|\"test\"| LoginService\n    UMTests --\u003e|\"test\"| PDPEngine\n    ProcTests --\u003e|\"test\"| ADES\n    RMTests --\u003e|\"test\"| WorkspaceAPI\n    RMTests --\u003e|\"test\"| ResourceCatalogue\n```\n\n**Test Framework Data Flow**\n\n```mermaid\nsequenceDiagram\n    participant TestRunner as \"Robot Framework\u003cbr/\u003eor main.py\"\n    participant DemoClient as \"DemoClient\u003cbr/\u003eClass Instance\"\n    participant StateFile as \"state.json\u003cbr/\u003ePersistent State\"\n    participant Services as \"EOEPCA Services\"\n    \n    TestRunner-\u003e\u003eDemoClient: Initialize(base_url)\n    DemoClient-\u003e\u003eStateFile: load_state()\n    StateFile--\u003e\u003eDemoClient: client_id, client_secret, resources\n    \n    TestRunner-\u003e\u003eDemoClient: get_id_token(username, password)\n    DemoClient-\u003e\u003eServices: POST /token (password grant)\n    Services--\u003e\u003eDemoClient: id_token\n    \n    TestRunner-\u003e\u003eDemoClient: register_protected_resource(...)\n    DemoClient-\u003e\u003eServices: POST /resources\n    Services--\u003e\u003eDemoClient: resource_id\n    DemoClient-\u003e\u003eStateFile: save resource_id\n    \n    TestRunner-\u003e\u003eDemoClient: uma_http_request(method, url, ...)\n    DemoClient-\u003e\u003eServices: Request without token\n    Services--\u003e\u003eDemoClient: 401 + WWW-Authenticate: ticket\n    DemoClient-\u003e\u003eServices: POST /token (UMA ticket grant)\n    Services--\u003e\u003eDemoClient: access_token (RPT)\n    DemoClient-\u003e\u003eServices: Retry request with RPT\n    Services--\u003e\u003eDemoClient: 200 OK\n    \n    TestRunner-\u003e\u003eDemoClient: save_state()\n    DemoClient-\u003e\u003eStateFile: Write state.json\n```\n\n**Sources:** [test/client/DemoClient.py:14-33](), [test/client/main.py:7-13](), [test/acceptance/__init__.robot:1-22]()\n\n---\n\n## DemoClient Library\n\nThe `DemoClient` class provides a Python interface to all EOEPCA services with built-in UMA authentication handling. It is implemented as both a standalone Python library and a Robot Framework library using the `@library` decorator.\n\n### Class Structure\n\n```mermaid\ngraph LR\n    DemoClient[\"DemoClient Class\u003cbr/\u003e@library decorator\u003cbr/\u003eROBOT_LIBRARY_SCOPE='GLOBAL'\"]\n    \n    subgraph \"Authentication Methods\"\n        GetTokenEndpoint[\"get_token_endpoint()\u003cbr/\u003eUMA2 discovery\"]\n        RegisterClient[\"register_client()\u003cbr/\u003eEOEPCA_Scim\"]\n        GetIDToken[\"get_id_token()\u003cbr/\u003e@keyword\"]\n        GetAccessToken[\"get_access_token_from_ticket()\u003cbr/\u003eUMA ticket exchange\"]\n    end\n    \n    subgraph \"Resource Management\"\n        RegisterResource[\"register_protected_resource()\u003cbr/\u003e@keyword\"]\n        GetResourceByName[\"get_resource_by_name()\u003cbr/\u003e@keyword\"]\n        GetResourceByURI[\"get_resource_by_uri()\u003cbr/\u003e@keyword\"]\n        CleanResources[\"clean_owner_resources()\u003cbr/\u003e@keyword\"]\n    end\n    \n    subgraph \"Service Interaction\"\n        UMARequest[\"uma_http_request()\u003cbr/\u003eHandles 401/ticket flow\"]\n        ProcListProcesses[\"proc_list_processes()\u003cbr/\u003e@keyword\"]\n        ProcDeployApp[\"proc_deploy_application()\u003cbr/\u003e@keyword\"]\n        ProcExecuteApp[\"proc_execute_application()\u003cbr/\u003e@keyword\"]\n        WSAPICreate[\"wsapi_create()\u003cbr/\u003e@keyword\"]\n    end\n    \n    subgraph \"State Management\"\n        LoadState[\"load_state()\u003cbr/\u003estate.json\"]\n        SaveState[\"save_state()\u003cbr/\u003e@keyword\"]\n    end\n    \n    DemoClient --\u003e GetTokenEndpoint\n    DemoClient --\u003e RegisterClient\n    DemoClient --\u003e GetIDToken\n    DemoClient --\u003e GetAccessToken\n    DemoClient --\u003e RegisterResource\n    DemoClient --\u003e GetResourceByName\n    DemoClient --\u003e UMARequest\n    DemoClient --\u003e ProcListProcesses\n    DemoClient --\u003e LoadState\n    DemoClient --\u003e SaveState\n```\n\n### Key Methods\n\n| Method | Parameters | Purpose | Robot Keyword |\n|--------|-----------|---------|---------------|\n| `get_id_token` | username, password | Obtain user ID token via password grant |  |\n| `register_protected_resource` | resource_api_url, uri, id_token, name, scopes | Register resource with PEP |  |\n| `uma_http_request` | method, url, headers, id_token, access_token | Execute HTTP request with UMA flow |  |\n| `proc_list_processes` | service_base_url, id_token, access_token | List deployed processes in ADES |  |\n| `proc_deploy_application` | service_base_url, app_deploy_body_filename, id_token | Deploy CWL application |  |\n| `proc_execute_application` | service_base_url, app_name, app_execute_body_filename, id_token | Execute deployed process |  |\n| `proc_poll_job_completion` | service_base_url, job_location, interval, id_token | Poll job status until completion |  |\n| `wsapi_create` | service_base_url, name, owner, id_token | Create user workspace |  |\n| `update_policy` | pdp_base_url, policy_cfg, resource_id, id_token | Modify PDP policy |  |\n\n**Sources:** [test/client/DemoClient.py:14-683]()\n\n### UMA Authentication Flow Implementation\n\nThe `uma_http_request` method implements the complete UMA 2.0 authentication flow:\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e InitRequest\n    InitRequest: uma_http_request(method, url, ...)\n    \n    InitRequest --\u003e CheckAccessToken: count \u003c 2\n    CheckAccessToken --\u003e WithAccessToken: access_token != None\n    CheckAccessToken --\u003e WithoutAccessToken: access_token == None\n    \n    WithAccessToken --\u003e SendRequest: Set Authorization header\n    WithoutAccessToken --\u003e SendRequest: No Authorization header\n    \n    SendRequest --\u003e CheckResponse: self.session.request()\n    \n    CheckResponse --\u003e Success: r.ok == True\n    CheckResponse --\u003e UMAFlow: r.status_code == 401\n    CheckResponse --\u003e Error: other status\n    \n    UMAFlow --\u003e ExtractTicket: Parse WWW-Authenticate header\n    ExtractTicket --\u003e ExchangeTicket: get_access_token_from_ticket()\n    ExchangeTicket --\u003e Retry: access_token obtained\n    \n    Retry --\u003e CheckAccessToken: repeat = True, count++\n    \n    Success --\u003e [*]: Return response, access_token\n    Error --\u003e [*]: Return response, access_token\n```\n\n**Implementation Details:**\n\n- **Line 239-291**: The `uma_http_request` method implements a retry loop (max 2 iterations)\n- **Line 257**: Sets `X-User-Id` header with the ID token for user identification\n- **Line 260**: Uses existing access token (RPT) if available\n- **Line 270-286**: Handles 401 response by extracting ticket from `WWW-Authenticate` header\n- **Line 283**: Calls `get_access_token_from_ticket` to exchange ticket for RPT\n- **Line 284**: Sets `repeat = True` to retry the original request with new token\n\n**Sources:** [test/client/DemoClient.py:239-291](), [test/client/DemoClient.py:185-212]()\n\n### State Persistence\n\nThe `DemoClient` maintains persistent state across test runs in `state.json`:\n\n```json\n{\n  \"client_id\": \"963e4c2a-9924-4c4d-b999-bebe79c96a5e\",\n  \"client_secret\": \"...\",\n  \"resources\": {\n    \"http://ades-pepapi.example.org\": {\n      \"/eric\": \"6c58a5e5-95b7-44ca-ab49-b1192e0db198\"\n    },\n    \"http://workspace-api-pepapi.example.org\": {\n      \"/workspaces/develop-user-eric\": \"7d69b6f6-a6c8-55db-bc5a-c2203f1ec209\"\n    }\n  }\n}\n```\n\n- **Lines 35-46**: `load_state()` reads state.json on initialization\n- **Lines 48-55**: `save_state()` writes state.json with sorted keys for readability\n- **Lines 92-111**: Client registration skipped if `client_id` already in state\n- **Lines 149-183**: Resource registration skipped if resource already tracked in state\n\n**Sources:** [test/client/DemoClient.py:35-55](), [test/client/DemoClient.py:92-183](), [test/client/.gitignore:4]()\n\n---\n\n## Robot Framework Acceptance Tests\n\nThe acceptance test suite is organized by building block and uses Robot Framework's keyword-driven approach.\n\n### Test Suite Organization\n\n```\ntest/acceptance/\n __init__.robot                    # Global setup and variables\n 01__UserManagement/\n    02__UserProfile/\n       LoginServiceInteraction.robot\n    03__PDP_Engine/\n        PDP_Engine.robot\n        tkn.sh\n        getOwnership.py\n 02__Processing/\n     01__ADES/\n         01__API_PROC.robot\n         02__WPS.robot\n         data/\n             app-deploy-body-cwl.json\n             app-execute-body.json\n```\n\n**Sources:** [test/acceptance/__init__.robot:1-22]()\n\n### Global Test Configuration\n\nThe `__init__.robot` file establishes global variables for all test suites:\n\n| Variable | Example Value | Purpose |\n|----------|---------------|---------|\n| `${UM_BASE_URL}` | `https://auth.${PUBLIC_HOSTNAME}` | Identity Service base URL |\n| `${ADES_BASE_URL}` | `http://ades.${PUBLIC_HOSTNAME}` | ADES service endpoint |\n| `${ADES_RESOURCES_API_URL}` | `http://ades-pepapi.${PUBLIC_HOSTNAME}` | ADES PEP resource API |\n| `${DUMMY_SERVICE_RESOURCES_API_URL}` | `http://dummy-service-pepapi.${PUBLIC_HOSTNAME}` | Test service PEP API |\n| `${CATALOGUE_BASE_URL}` | `https://resource-catalogue.${PUBLIC_HOSTNAME}` | Resource Catalogue endpoint |\n| `${USER_A_NAME}` | `eric` | Test user A username |\n| `${USER_A_PASSWORD}` | `defaultPWD` | Test user A password |\n| `${USER_PREFIX}` | `develop-user` | Workspace name prefix |\n\nThe `PUBLIC_HOSTNAME` variable must be set as an environment variable or passed via command line when running tests.\n\n**Sources:** [test/acceptance/__init__.robot:6-22]()\n\n### PDP Engine Tests\n\nThe PDP Engine test suite validates policy enforcement and authorization decisions.\n\n**Test Case Structure:**\n\n```mermaid\ngraph TD\n    TestCase[\"PDP Test Case\"]\n    \n    GetIDToken[\"Get ID Token\u003cbr/\u003eGet Id Token keyword\"]\n    GetResourceID[\"Get Resource ID\u003cbr/\u003eGet Resource By URI keyword\"]\n    ValidateAccess[\"Validate Access\u003cbr/\u003ePDP Validate Resource Access\"]\n    \n    subgraph \"Validation Request\"\n        BuildRequest[\"Build JSON Request\u003cbr/\u003eAccessSubject, Action, Resource\"]\n        CallPDP[\"GET /pdp/policy/validate\"]\n        ParseResponse[\"Parse Response.Decision\"]\n    end\n    \n    CheckDecision[\"Check Decision\u003cbr/\u003ePermit or Deny\"]\n    \n    TestCase --\u003e GetIDToken\n    GetIDToken --\u003e GetResourceID\n    GetResourceID --\u003e ValidateAccess\n    ValidateAccess --\u003e BuildRequest\n    BuildRequest --\u003e CallPDP\n    CallPDP --\u003e ParseResponse\n    ParseResponse --\u003e CheckDecision\n```\n\n**Example Test Cases:**\n\n- **Lines 13-16**: Validates user `eric` can access `/ericspace` (Permit expected)\n- **Lines 18-21**: Validates user `bob` cannot access `/ericspace` (Deny expected)\n- **Lines 23-26**: Validates user `bob` can access `/bobspace` (Permit expected)\n- **Lines 28-46**: Tests different HTTP methods (GET, POST, PUT, DELETE, HEAD)\n- **Lines 63-71**: Tests unauthorized policy changes (401 expected)\n- **Lines 73-82**: Tests authorized policy changes by resource owner\n\n**PDP Validation Request Format:**\n\n```json\n{\n  \"Request\": {\n    \"AccessSubject\": [{\n      \"Attribute\": [{\n        \"AttributeId\": \"user_name\",\n        \"Value\": \"eric\",\n        \"DataType\": \"string\"\n      }]\n    }],\n    \"Action\": [{\n      \"Attribute\": [{\n        \"AttributeId\": \"action-id\",\n        \"Value\": \"get\"\n      }]\n    }],\n    \"Resource\": [{\n      \"Attribute\": [{\n        \"AttributeId\": \"resource-id\",\n        \"Value\": \"6c58a5e5-95b7-44ca-ab49-b1192e0db198\"\n      }]\n    }]\n  }\n}\n```\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:1-123]()\n\n### ADES API Processes Tests\n\nThe ADES acceptance tests validate the OGC API Processes interface for application deployment and execution.\n\n**Test Flow:**\n\n```mermaid\nsequenceDiagram\n    participant Test as \"Robot Framework Test\"\n    participant DemoClient as \"DemoClient Library\"\n    participant ADES as \"ADES Service\u003cbr/\u003e/{user}/wps3\"\n    \n    Note over Test: Suite Setup\n    Test-\u003e\u003eDemoClient: Get ID Token (eric, defaultPWD)\n    DemoClient--\u003e\u003eTest: id_token\n    \n    Note over Test: Test Case: Deploy Application\n    Test-\u003e\u003eDemoClient: Proc Deploy App(url, app-deploy-body-cwl.json)\n    DemoClient-\u003e\u003eADES: POST /processes\n    ADES--\u003e\u003eDemoClient: 201 Created\n    DemoClient--\u003e\u003eTest: Response\n    Test-\u003e\u003eTest: Sleep 5 seconds\n    \n    Note over Test: Test Case: Get Application Details\n    Test-\u003e\u003eDemoClient: Proc App Details(snuggs-0_3_0)\n    DemoClient-\u003e\u003eADES: GET /processes/snuggs-0_3_0\n    ADES--\u003e\u003eDemoClient: 200 OK\n    \n    Note over Test: Test Case: Execute Application\n    Test-\u003e\u003eDemoClient: Proc Execute App(snuggs-0_3_0, execute-body.json)\n    DemoClient-\u003e\u003eADES: POST /processes/snuggs-0_3_0/execution\n    ADES--\u003e\u003eDemoClient: 201 Created + Location header\n    DemoClient--\u003e\u003eTest: job_location\n    \n    Test-\u003e\u003eDemoClient: Proc Poll Job Completion(job_location, interval=30)\n    loop Until status == 'successful'\n        DemoClient-\u003e\u003eADES: GET /eric/wps3/jobs/{jobId}\n        ADES--\u003e\u003eDemoClient: status: running/successful\n        DemoClient-\u003e\u003eDemoClient: Sleep 30 seconds\n    end\n    DemoClient--\u003e\u003eTest: Final status\n    \n    Note over Test: Test Case: Undeploy Application\n    Test-\u003e\u003eDemoClient: Proc Undeploy App(snuggs-0_3_0)\n    DemoClient-\u003e\u003eADES: DELETE /processes/snuggs-0_3_0\n    ADES--\u003e\u003eDemoClient: 200 OK\n```\n\n**Key Test Cases:**\n\n| Test Case | Lines | Verification |\n|-----------|-------|-------------|\n| Initial Process List | 23-24 | Records baseline deployed processes |\n| Deploy Application | 26-29 | Deploys CWL from `app-deploy-body-cwl.json`, expects 201 |\n| Get Application Details | 31-32 | Retrieves process description, expects 200 |\n| Execute Application | 34-35 | Submits job with inputs, expects 201 + Location header |\n| Undeploy Application | 37-41 | Removes process, expects 200, verifies removal |\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot:1-124]()\n\n### ADES WPS Tests\n\nThe WPS test suite validates the legacy OGC WPS 1.0.0 interface.\n\n**Test Coverage:**\n\n- **Lines 24-25**: Initial process list via GetCapabilities\n- **Lines 27-28**: Unauthorized access without authentication (expects 401)\n- **Lines 50-55**: `WPS Get Capabilities` with UMA authentication\n- **Lines 57-61**: `WPS Get Capabilities` without authentication\n- **Lines 63-73**: XML parsing to extract process identifiers\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/02__WPS.robot:1-74]()\n\n### Login Service Interaction Tests\n\nThese tests validate the browser-based login flow using Selenium WebDriver.\n\n**Browser Configuration:**\n\n```python\nchrome_options = ChromeOptions()\nchrome_options.add_argument(\"headless\")\nchrome_options.add_argument(\"disable-gpu\")\nchrome_options.add_argument(\"disable-dev-shm-usage\")\nchrome_options.add_argument(\"no-sandbox\")\nchrome_options.add_argument(\"ignore-certificate-errors\")\n```\n\n**Test Scenarios:**\n\n1. **User Profile Login Flow** (Lines 20-37):\n   - Navigate to User Profile UI\n   - Redirect to Login Service (oxAuth)\n   - Authenticate with credentials\n   - Authorization consent (if required)\n   - Redirect back to User Profile\n   - Logout\n\n2. **User Creation via Gluu Admin UI** (Lines 39-59):\n   - Login to Gluu admin interface\n   - Navigate to Users section\n   - Create user A with username, email, password\n   - Create user B with username, email, password\n\n**Sources:** [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:1-128]()\n\n---\n\n## Manual Testing with Test Client\n\nThe `main.py` script provides an interactive demonstration of the complete EOEPCA workflow.\n\n### Execution Flow\n\n```mermaid\ngraph TB\n    Start[\"main.py execution\"]\n    \n    subgraph \"User Management Setup\"\n        TokenEndpoint[\"Get token endpoint\u003cbr/\u003e/.well-known/uma2-configuration\"]\n        RegisterClient[\"Register client\u003cbr/\u003eEOEPCA_Scim.registerClient()\"]\n        GetIDToken[\"Get user ID token\u003cbr/\u003eeric / defaultPWD\"]\n        RegisterADES[\"Register ADES resource\u003cbr/\u003e/eric as protected\"]\n        RegisterWS[\"Register workspace resource\u003cbr/\u003e/workspaces/develop-user-eric\"]\n    end\n    \n    subgraph \"Dummy Service Test\"\n        CallDummy[\"Call Dummy Service\u003cbr/\u003e/ericspace with trace_flow=True\"]\n    end\n    \n    subgraph \"Workspace API Test\"\n        CreateWorkspace[\"Create workspace\u003cbr/\u003ePOST /workspaces\"]\n        GetDetails[\"Get workspace details\u003cbr/\u003eGET /workspaces/develop-user-eric\"]\n        GetDetailsBob[\"Get details as bob\u003cbr/\u003eExpects 403\"]\n    end\n    \n    subgraph \"Processing Test\"\n        ListProc[\"List processes\u003cbr/\u003eGET /eric/wps3/processes\"]\n        DeployApp[\"Deploy s-expression-0_0_2\u003cbr/\u003ePOST /eric/wps3/processes\"]\n        GetAppDetails[\"Get application details\u003cbr/\u003eSleep 5 seconds\"]\n        ExecuteApp[\"Execute application\u003cbr/\u003ePOST .../execution\"]\n        PollJob[\"Poll job completion\u003cbr/\u003einterval=30 seconds\"]\n        GetResult[\"Get job result\u003cbr/\u003eSTAC catalog URI\"]\n        UndeployApp[\"Undeploy application\u003cbr/\u003eDELETE /eric/wps3/processes/...\"]\n    end\n    \n    subgraph \"WPS Test\"\n        WPSGetCap[\"WPS GetCapabilities\u003cbr/\u003eGET /eric/zoo?service=WPS\"]\n    end\n    \n    SaveState[\"Save state to state.json\"]\n    \n    Start --\u003e TokenEndpoint\n    TokenEndpoint --\u003e RegisterClient\n    RegisterClient --\u003e GetIDToken\n    GetIDToken --\u003e RegisterADES\n    RegisterADES --\u003e RegisterWS\n    \n    RegisterWS --\u003e CallDummy\n    \n    CallDummy --\u003e CreateWorkspace\n    CreateWorkspace --\u003e GetDetails\n    GetDetails --\u003e GetDetailsBob\n    \n    GetDetailsBob --\u003e ListProc\n    ListProc --\u003e DeployApp\n    DeployApp --\u003e GetAppDetails\n    GetAppDetails --\u003e ExecuteApp\n    ExecuteApp --\u003e PollJob\n    PollJob --\u003e GetResult\n    GetResult --\u003e UndeployApp\n    \n    UndeployApp --\u003e WPSGetCap\n    WPSGetCap --\u003e SaveState\n```\n\n### Configuration Variables\n\nThe test client configuration is at the top of `main.py`:\n\n| Variable | Line | Default Value | Purpose |\n|----------|------|---------------|---------|\n| `USER_NAME` | 9 | `\"eric\"` | Test user username |\n| `USER_PASSWORD` | 10 | `\"defaultPWD\"` | Test user password |\n| `domain` | 12 | `\"develop.eoepca.org\"` | Deployment domain |\n| `ades_url` | 18 | `\"https://ades.\" + domain` | ADES public endpoint |\n| `ades_user_prefix` | 20 | `\"/\" + ades_user` | ADES workspace path |\n| `wsapi_url` | 24 | `\"https://workspace-api.\" + domain` | Workspace API endpoint |\n| `wsapi_prefix` | 27 | `\"develop-user\"` | Workspace name prefix |\n\n**Sources:** [test/client/main.py:7-33]()\n\n### Key Test Sections\n\n**1. User Management Setup (Lines 35-65):**\n- Discovers token endpoint via UMA2 configuration\n- Registers OAuth2 client with required scopes\n- Obtains user ID token via password grant\n- Registers ADES and Workspace API resources as protected\n\n**2. Workspace API Tests (Lines 100-128):**\n- Creates workspace with `POST /workspaces`\n- Retrieves workspace details with user's token (expects 200)\n- Attempts to retrieve workspace details as different user (expects 403)\n\n**3. Processing Tests (Lines 132-207):**\n- Lists deployed processes\n- Deploys CWL application from `app-deploy-body-cwl-S3.json`\n- Executes application with inputs from `app-execute-body.json`\n- Polls job status with 30-second interval until completion\n- Retrieves STAC catalog URI from results\n- Undeploys application\n\n**4. State Persistence (Line 221):**\n- Saves client credentials and resource IDs to `state.json`\n\n**Sources:** [test/client/main.py:35-224]()\n\n---\n\n## Test Environment Setup\n\n### Prerequisites\n\n- Python 3.7 or later\n- Virtual environment tools (`python3-venv`)\n- Chrome/Chromium browser (for Selenium tests)\n- ChromeDriver matching browser version\n- Access to deployed EOEPCA instance\n\n### Installation\n\nThe `setup.sh` script automates the test environment setup:\n\n```bash\n#!/usr/bin/env bash\npython3 -m venv venv\nsource venv/bin/activate\npython -m pip install -U pip\npip install -U wheel\npip install -U -r requirements.txt\n```\n\n**Dependencies installed:**\n\n| Package | Version | Purpose |\n|---------|---------|---------|\n| `eoepca-scim` | 2.8.1 | SCIM client for user/client management |\n| `pycrypto` | 2.6.1 | Cryptographic operations |\n| `pyjwkest` | 1.4.2 | JWT/JWK handling |\n| `PyJWT` | 2.6.0 | JWT encoding/decoding |\n| `requests` | 2.26.0 | HTTP client library |\n| `robotframework` | 4.1 | Test automation framework |\n| `WellKnownHandler` | 0.2.0 | OpenID Connect discovery |\n\n**Sources:** [test/client/setup.sh:1-14](), [test/client/requirements.txt:1-8]()\n\n### Manual Setup Steps\n\n```bash\n# Navigate to test client directory\ncd test/client\n\n# Run setup script\n./setup.sh\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Verify installation\npython -c \"import DemoClient; print('DemoClient loaded successfully')\"\n```\n\n**Sources:** [test/client/setup.sh:1-14]()\n\n---\n\n## Test Execution\n\n### Running the Manual Test Client\n\n```bash\ncd test/client\nsource venv/bin/activate\n\n# Edit domain configuration in main.py if needed\n# vi main.py  # Change line 12: domain = \"your-deployment.domain\"\n\n# Execute the test client\npython main.py\n```\n\n**Expected Output:**\n\n```\n### TEST CLIENT ###\n\n### TOKEN ENDPOINT ###\ntoken_endpoint: https://auth.develop.eoepca.org/oxauth/restv1/token\n\n### REGISTER CLIENT ###\nclient_id: 963e4c2a-9924-4c4d-b999-bebe79c96a5e\n\n### USER ID TOKEN ###\neyJhbGciOiJSUzI1NiIsImtpZCI6IlJTQTEi...\n\n### REGISTER USER'S ADES BASE RESOURCE PATH ###\nresource_id: 6c58a5e5-95b7-44ca-ab49-b1192e0db198 @http://ades-pepapi.develop.eoepca.org = /eric\n\n### Workspace: Create ###\n[UMA] Attempting to use existing access token\n[UMA] Successfully accessed resource\nDETAILS = {\n  \"preferred_name\": \"eric\",\n  \"workspace_id\": \"develop-user-eric\",\n  ...\n}\n\n### API Proc Deploy Application ###\n[Deploy Response] = 201 (Created)\n...\n```\n\nThe script uses `trace_flow = True` for certain operations to display detailed UMA flow logging.\n\n**Sources:** [test/client/main.py:7-224]()\n\n### Running Robot Framework Tests\n\n#### Single Test Suite\n\n```bash\ncd test/acceptance\n\n# Set public hostname\nexport PUBLIC_HOSTNAME=develop.eoepca.org\n\n# Run PDP Engine tests\nrobot 01__UserManagement/03__PDP_Engine/PDP_Engine.robot\n\n# Run ADES API Processes tests\nrobot 02__Processing/01__ADES/01__API_PROC.robot\n```\n\n#### All Tests\n\n```bash\ncd test/acceptance\nexport PUBLIC_HOSTNAME=develop.eoepca.org\n\n# Run all acceptance tests\nrobot .\n```\n\n#### With Custom Variables\n\n```bash\nrobot --variable PUBLIC_HOSTNAME:my-deployment.org \\\n      --variable USER_A_NAME:testuser \\\n      --variable USER_A_PASSWORD:testpass \\\n      01__UserManagement/03__PDP_Engine/PDP_Engine.robot\n```\n\n**Sources:** [test/acceptance/__init__.robot:6-22]()\n\n### Test Output\n\nRobot Framework generates detailed reports:\n\n```\ntest/acceptance/\n log.html          # Detailed execution log\n output.xml        # Machine-readable results\n report.html       # Summary report\n```\n\n**Report Contents:**\n\n- **Test Statistics**: Pass/fail counts by suite and tag\n- **Execution Timeline**: Duration of each test case\n- **Keyword Details**: Step-by-step execution with arguments\n- **Screenshots**: For Selenium-based tests (if enabled)\n- **Error Messages**: Stack traces and failure reasons\n\n**Sources:** Robot Framework generates these files automatically based on test execution.\n\n---\n\n## Test Coverage\n\n### User Management Tests\n\n| Test Category | Test Cases | Validates |\n|---------------|-----------|-----------|\n| **PDP Policy Validation** | 13 test cases | Policy enforcement, access control decisions, unauthorized policy changes |\n| **Login Service** | 2 test cases | Browser-based login flow, user creation in Gluu |\n\n**PDP Test Scenarios:**\n\n- Resource ownership enforcement (eric owns /ericspace, bob owns /bobspace)\n- HTTP method-specific policies (GET, POST, PUT, DELETE, HEAD)\n- Invalid resource/action handling\n- Policy modification authorization\n- Policy modification by resource owner\n- Multi-user policy configuration (OR rules)\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:1-123](), [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:1-128]()\n\n### Processing Tests\n\n| Test Category | Test Cases | Validates |\n|---------------|-----------|-----------|\n| **ADES API Processes** | 5 test cases | Application deployment, execution, lifecycle management |\n| **ADES WPS** | 2 test cases | Legacy WPS interface, unauthorized access handling |\n\n**ADES API Test Scenarios:**\n\n- Process listing (baseline state)\n- CWL application deployment from file\n- Application description retrieval\n- Asynchronous job execution with polling\n- Job result retrieval (STAC catalog)\n- Application undeployment\n\n**ADES WPS Test Scenarios:**\n\n- GetCapabilities with authentication\n- GetCapabilities without authentication (401 expected)\n- XML response parsing\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot:1-124](), [test/acceptance/02__Processing/01__ADES/02__WPS.robot:1-74]()\n\n### Integration Test Coverage\n\nThe manual test client (`main.py`) provides end-to-end integration testing:\n\n```mermaid\ngraph LR\n    subgraph \"Components Validated\"\n        UM[\"User Management\u003cbr/\u003eIdentity, PEP, PDP\"]\n        RM[\"Resource Management\u003cbr/\u003eWorkspace API\"]\n        Proc[\"Processing\u003cbr/\u003eADES API + WPS\"]\n    end\n    \n    subgraph \"Workflows Tested\"\n        Auth[\"Authentication Flow\u003cbr/\u003eUMA ticket exchange\"]\n        ResourceReg[\"Resource Registration\u003cbr/\u003eProtected paths\"]\n        Workspace[\"Workspace Provisioning\u003cbr/\u003eMulti-tenant isolation\"]\n        AppDeploy[\"Application Deployment\u003cbr/\u003eCWL packages\"]\n        JobExec[\"Job Execution\u003cbr/\u003eStage-in/out\"]\n    end\n    \n    UM --\u003e|\"validates\"| Auth\n    UM --\u003e|\"validates\"| ResourceReg\n    RM --\u003e|\"validates\"| Workspace\n    Proc --\u003e|\"validates\"| AppDeploy\n    Proc --\u003e|\"validates\"| JobExec\n    \n    Auth --\u003e ResourceReg\n    ResourceReg --\u003e Workspace\n    Workspace --\u003e AppDeploy\n    AppDeploy --\u003e JobExec\n```\n\n**Sources:** [test/client/main.py:7-224]()\n\n---\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Symptom | Resolution |\n|-------|---------|------------|\n| **401 Unauthorized** | Tests fail with \"401 (Unauthorized)\" | Verify user credentials in test configuration. Check Identity Service is running. Ensure PEP/PDP are deployed. |\n| **403 Forbidden** | Tests fail with \"403 (Forbidden)\" | Verify resource is registered with correct owner. Check PDP policies with `dump-policy.sh`. Reset policy with `reset_resource_policy` keyword. |\n| **404 Not Found** | Service endpoints return 404 | Verify `PUBLIC_HOSTNAME` environment variable. Check Ingress routes are configured. Confirm services are deployed in correct namespaces. |\n| **Connection Refused** | Cannot connect to services | Verify services are running: `kubectl get pods -A`. Check ingress controller: `kubectl get ingress -A`. |\n| **Expired Token** | \"Token is expired\" errors | Tokens have limited lifetime. Re-run `get_id_token` to obtain fresh token. |\n| **Missing state.json** | \"client_id not in state\" | Delete `state.json` if corrupted. Client will re-register on next run. |\n\n### Debugging UMA Flow\n\nEnable detailed UMA flow tracing:\n\n```python\n# In main.py or test script\ndemo.trace_flow = True\nresponse, access_token = demo.uma_http_request(\"GET\", url, ...)\ndemo.trace_flow = False\n```\n\n**Trace Output:**\n\n```\n[UMA] No existing access token - making a naive attempt\n[Request] GET =\u003e https://ades.develop.eoepca.org/eric/wps3/processes\n[UMA] Received a 401 (Unauthorized) response to access attempt\n[UMA] Got ticket from response. Using ID Token + ticket to request an RPT\n[UMA] Calling token endpoint with ID Token + ticket: POST =\u003e https://auth.develop.eoepca.org/oxauth/restv1/token\n[UMA] Successfully exchanged ticket for RPT\n[Request] GET =\u003e https://ades.develop.eoepca.org/eric/wps3/processes\n[UMA] Successfully accessed resource\n```\n\n**Sources:** [test/client/DemoClient.py:57-68](), [test/client/main.py:94-96]()\n\n### Verifying PDP Policies\n\nUse the DemoClient to query current policies:\n\n```python\nfrom DemoClient import DemoClient\n\ndemo = DemoClient(\"https://auth.develop.eoepca.org\")\neric_id_token = demo.get_id_token(\"eric\", \"defaultPWD\")\nresource_id = demo.get_resource_by_name(\n    \"http://ades-pepapi.develop.eoepca.org\",\n    \"ADES Service for user eric\",\n    eric_id_token\n)\n\n# Get policies for resource\n# Use dump-policy.sh script or PDP API directly\n```\n\n**Sources:** [test/client/DemoClient.py:577-591]()\n\n### Cleaning Test Resources\n\nAfter testing, clean up registered resources:\n\n```python\n# Clean resources tracked in state.json\ndemo.clean_state_resources(\n    \"http://ades-pepapi.develop.eoepca.org\",\n    eric_id_token\n)\n\n# Clean all resources owned by user\ndemo.clean_owner_resources(\n    \"http://ades-pepapi.develop.eoepca.org\",\n    eric_id_token,\n    \"ADES Service for user eric\"\n)\n```\n\n**Sources:** [test/client/DemoClient.py:608-631]()\n\n---\n\n## Summary\n\nThe EOEPCA test framework provides comprehensive validation through:\n\n1. **DemoClient Library**: Reusable Python/Robot Framework library with UMA authentication\n2. **Robot Framework Tests**: Automated acceptance tests for all building blocks\n3. **Manual Test Client**: Interactive demonstration of end-to-end workflows\n\nKey testing capabilities:\n- UMA 2.0 authentication flow validation\n- PEP/PDP policy enforcement verification\n- ADES application deployment and execution\n- Workspace provisioning and isolation\n- Multi-user authorization scenarios\n\nFor detailed information about specific components tested, refer to:\n- User Management: [User Management and Identity](#4)\n- Processing: [Processing and Chaining](#6)\n- Resource Management: [Resource Management](#5)\n\n**Sources:** [test/client/DemoClient.py:1-683](), [test/client/main.py:1-224](), [test/acceptance/__init__.robot:1-22]()"])</script><script>self.__next_f.push([1,"1c:T4ec9,"])</script><script>self.__next_f.push([1,"# System Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [minikube/README.md](minikube/README.md)\n- [release-notes/release-0.3.md](release-notes/release-0.3.md)\n- [system/clusters/README.md](system/clusters/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a high-level architectural overview of the EOEPCA system, describing its major components, their interactions, and the organizational principles that govern the system design. The architecture is presented from multiple perspectives: logical building blocks, deployment structure, and runtime interactions.\n\nFor detailed deployment procedures, see [Deployment Guide](#2.1). For specific building block implementations, refer to [Building Blocks Overview](#3.1), [User Management and Identity](#4), [Resource Management](#5), and [Processing and Chaining](#6). Infrastructure provisioning details are covered in [Infrastructure Provisioning](#2.2) and [Terraform Infrastructure as Code](#8.2).\n\n## Architectural Principles\n\nThe EOEPCA system is designed according to the following core principles:\n\n### Standards-Based Architecture\n\nThe platform implements Open Geospatial Consortium (OGC) standards for interoperability:\n- **OGC API Processes** - Application execution interfaces\n- **OGC CSW 3.0** - Catalogue services\n- **OGC WMS/WMTS/WCS** - Data visualization and access\n- **OpenSearch** - Data discovery with EO, Geo, and Time extensions\n- **STAC** - Metadata for stage-in/stage-out workflows\n\nSources: [README.md:163-170]()\n\n### GitOps-Driven Deployment\n\nAll system components are deployed and managed using Flux CD, implementing a GitOps approach where the Git repository serves as the single source of truth for cluster state. This ensures version-controlled, auditable, and reproducible deployments.\n\nSources: [system/clusters/README.md:1-95](), [README.md:92-93]()\n\n### Multi-Tenant Isolation\n\nUser workspaces are provisioned as isolated Kubernetes namespaces with dedicated S3 buckets, catalogues, and policy enforcement points, enabling secure multi-tenancy while maintaining resource sharing capabilities.\n\nSources: Diagram 4 from high-level overview\n\n### Microservices on Kubernetes\n\nThe system is composed of loosely-coupled microservices deployed as containers orchestrated by Kubernetes, supporting both cloud (OpenStack via RKE) and local (Minikube/k3s) deployments.\n\nSources: [README.md:70](), [minikube/README.md:1-52]()\n\n## System Decomposition\n\n### Building Block Domains\n\nThe EOEPCA system is organized into three primary functional domains, each deployed in a dedicated Kubernetes namespace:\n\n```mermaid\ngraph TB\n    subgraph \"um namespace\"\n        UM[\"User Management\u003cbr/\u003eBuilding Block\"]\n        LoginSvc[\"login-service\"]\n        IdentitySvc[\"identity-service\"]\n        PDPEngine[\"pdp-engine\"]\n        UserProfile[\"user-profile\"]\n        UM --\u003e LoginSvc\n        UM --\u003e IdentitySvc\n        UM --\u003e PDPEngine\n        UM --\u003e UserProfile\n    end\n    \n    subgraph \"rm namespace\"\n        RM[\"Resource Management\u003cbr/\u003eBuilding Block\"]\n        WorkspaceAPI[\"workspace-api\"]\n        ResourceCat[\"resource-catalogue\"]\n        DataAccess[\"data-access\"]\n        Registrar[\"registrar\"]\n        Harvester[\"harvester\"]\n        BucketOp[\"bucket-operator\"]\n        RM --\u003e WorkspaceAPI\n        RM --\u003e ResourceCat\n        RM --\u003e DataAccess\n        RM --\u003e Registrar\n        RM --\u003e Harvester\n        RM --\u003e BucketOp\n    end\n    \n    subgraph \"proc namespace\"\n        PC[\"Processing \u0026 Chaining\u003cbr/\u003eBuilding Block\"]\n        ADES[\"ades\"]\n        AppHub[\"application-hub\"]\n        PDEHub[\"pde-hub\"]\n        PC --\u003e ADES\n        PC --\u003e AppHub\n        PC --\u003e PDEHub\n    end\n    \n    subgraph \"system namespace\"\n        SYS[\"System Infrastructure\"]\n        IngressNginx[\"ingress-nginx\"]\n        CertManager[\"cert-manager\"]\n        SealedSecrets[\"sealed-secrets-controller\"]\n        FluxSystem[\"flux-system\"]\n        SYS --\u003e IngressNginx\n        SYS --\u003e CertManager\n        SYS --\u003e SealedSecrets\n        SYS --\u003e FluxSystem\n    end\n    \n    SYS -.-\u003e|\"provides\"| UM\n    SYS -.-\u003e|\"provides\"| RM\n    SYS -.-\u003e|\"provides\"| PC\n    UM -.-\u003e|\"protects\"| RM\n    UM -.-\u003e|\"protects\"| PC\n    RM -.-\u003e|\"data for\"| PC\n```\n\n**Domain Architecture by Namespace**\n\n| Namespace | Domain | Primary Services | Configuration Path |\n|-----------|--------|-----------------|-------------------|\n| `um` | User Management | `login-service`, `identity-service`, `pdp-engine`, `user-profile` | `system/clusters/{target}/user-management/` |\n| `rm` | Resource Management | `workspace-api`, `resource-catalogue`, `data-access`, `registrar`, `harvester` | `system/clusters/{target}/resource-management/` |\n| `proc` | Processing \u0026 Chaining | `ades`, `application-hub`, `pde-hub` | `system/clusters/{target}/processing-and-chaining/` |\n| `flux-system` | GitOps Control Plane | `source-controller`, `kustomize-controller`, `helm-controller` | `system/clusters/{target}/system/flux-system/` |\n| `ingress-nginx` | External Access | `ingress-nginx-controller` | `system/clusters/{target}/system/ingress-nginx/` |\n| `cert-manager` | TLS Certificates | `cert-manager`, `cert-manager-webhook` | `system/clusters/{target}/system/cert-manager/` |\n\nSources: [README.md:128-160](), [release-notes/release-0.3.md:97-319](), Diagram 1 and Diagram 5 from high-level overview\n\n### Directory Structure and GitOps Mapping\n\nThe repository structure directly maps to the deployment architecture:\n\n```mermaid\ngraph LR\n    subgraph \"Git Repository Structure\"\n        Root[\"eoepca/\"]\n        System[\"system/\"]\n        Clusters[\"clusters/\"]\n        Target[\"develop/\u003cbr/\u003eminikube/\u003cbr/\u003ecreodias/\"]\n        SystemDir[\"system/\"]\n        UM[\"user-management/\"]\n        RM[\"resource-management/\"]\n        Proc[\"processing-and-chaining/\"]\n        \n        Root --\u003e System\n        System --\u003e Clusters\n        Clusters --\u003e Target\n        Target --\u003e SystemDir\n        Target --\u003e UM\n        Target --\u003e RM\n        Target --\u003e Proc\n    end\n    \n    subgraph \"Flux Resources\"\n        GitRepo[\"GitRepository\u003cbr/\u003eeoepca-system\"]\n        Kustomization[\"Kustomization\u003cbr/\u003eeoepca-system\"]\n        HelmReleases[\"HelmRelease\u003cbr/\u003eresources\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        Namespaces[\"Namespaces\u003cbr/\u003eum, rm, proc\"]\n        Deployments[\"Deployments\u003cbr/\u003ePods, Services\"]\n    end\n    \n    SystemDir --\u003e GitRepo\n    UM --\u003e HelmReleases\n    RM --\u003e HelmReleases\n    Proc --\u003e HelmReleases\n    GitRepo --\u003e Kustomization\n    Kustomization --\u003e HelmReleases\n    HelmReleases --\u003e Namespaces\n    Namespaces --\u003e Deployments\n```\n\n**Key Directory Paths:**\n\n- `system/clusters/{target}/` - Deployment configuration for a specific cluster\n- `system/clusters/{target}/system/` - Flux bootstrap and system infrastructure\n- `system/clusters/{target}/user-management/` - HelmRelease manifests for Identity/Access services\n- `system/clusters/{target}/resource-management/` - HelmRelease manifests for Catalogue/Data services\n- `system/clusters/{target}/processing-and-chaining/` - HelmRelease manifests for ADES/Application Hub\n\nSources: [system/clusters/README.md:45-50](), [system/clusters/README.md:81-85]()\n\n## Component Interaction Architecture\n\n### Authentication and Authorization Flow\n\nThe system implements User-Managed Access (UMA) 2.0 for fine-grained authorization:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Ingress as \"ingress-nginx\"\n    participant PEP as \"resource-guard\u003cbr/\u003e(PEP)\"\n    participant PDP as \"pdp-engine\"\n    participant Keycloak as \"identity-service\u003cbr/\u003e(Keycloak)\"\n    participant Backend as \"workspace-api /\u003cbr/\u003eades / etc\"\n    \n    User-\u003e\u003eIngress: \"GET /workspaces\"\n    Ingress-\u003e\u003ePEP: \"Forward request\"\n    \n    alt \"No Bearer Token\"\n        PEP-\u003e\u003eKeycloak: \"POST /uma-configuration\"\n        Keycloak--\u003e\u003ePEP: \"UMA ticket\"\n        PEP--\u003e\u003eUser: \"401 + WWW-Authenticate:\u003cbr/\u003eticket={ticket}\"\n        User-\u003e\u003eKeycloak: \"POST /token\u003cbr/\u003e(username, password, ticket)\"\n        Keycloak--\u003e\u003eUser: \"RPT (access_token)\"\n        User-\u003e\u003eIngress: \"GET /workspaces\u003cbr/\u003eAuthorization: Bearer {RPT}\"\n        Ingress-\u003e\u003ePEP: \"Forward with token\"\n    end\n    \n    PEP-\u003e\u003ePDP: \"POST /authz\u003cbr/\u003e{token, resource, action}\"\n    PDP-\u003e\u003eKeycloak: \"Introspect token\"\n    Keycloak--\u003e\u003ePDP: \"Token claims\"\n    PDP-\u003e\u003ePDP: \"Evaluate policies\"\n    PDP--\u003e\u003ePEP: \"Decision: Permit/Deny\"\n    \n    alt \"Permit\"\n        PEP-\u003e\u003eBackend: \"Forward request\u003cbr/\u003e+ user headers\"\n        Backend--\u003e\u003ePEP: \"Response\"\n        PEP--\u003e\u003eUser: \"200 OK\"\n    else \"Deny\"\n        PEP--\u003e\u003eUser: \"403 Forbidden\"\n    end\n```\n\n**Key Components:**\n\n- `resource-guard` - PEP implementation protecting HTTP endpoints\n- `pdp-engine` - Policy Decision Point evaluating access policies\n- `identity-service` - Keycloak-based IdP providing UMA and OIDC\n- `login-service` - Legacy Gluu-based authentication (deprecated in v1.4)\n\nSources: [README.md:150-159](), Diagram 2 from high-level overview, [release-notes/release-0.3.md:17-29]()\n\n### Data Access and Processing Pipeline\n\n```mermaid\ngraph TB\n    subgraph \"External Data Sources\"\n        S2[\"Sentinel-2\u003cbr/\u003eCreoDIAS eodata\"]\n        STAC[\"STAC Catalogs\"]\n    end\n    \n    subgraph \"Data Ingestion\"\n        Harvester[\"harvester\u003cbr/\u003eOpenSearch polling\"]\n        Queue[\"Redis\u003cbr/\u003eregister_queue\"]\n        Registrar[\"registrar\u003cbr/\u003emetadata ingestion\"]\n    end\n    \n    subgraph \"Cataloging\"\n        PyCSW[\"resource-catalogue\u003cbr/\u003epycsw\"]\n        PGSQL[(\"PostgreSQL\u003cbr/\u003ecsw.records\")]\n    end\n    \n    subgraph \"Access Services\"\n        VS[\"data-access\u003cbr/\u003eViewServer\"]\n        Renderer[\"renderer\u003cbr/\u003e4 replicas\"]\n        Cache[\"cache-seeder\"]\n    end\n    \n    subgraph \"Processing\"\n        User[\"User\"]\n        ADES[\"ades\u003cbr/\u003eZOO-Project\"]\n        Calrissian[\"Calrissian\u003cbr/\u003eCWL executor\"]\n        K8sJobs[\"Kubernetes Jobs\u003cbr/\u003eprocessing pods\"]\n    end\n    \n    subgraph \"Storage\"\n        S3In[\"S3 Input\u003cbr/\u003eeodata bucket\"]\n        S3Out[\"S3 Output\u003cbr/\u003eworkspace bucket\"]\n    end\n    \n    S2 --\u003e Harvester\n    STAC --\u003e Harvester\n    Harvester --\u003e|\"LPUSH\"| Queue\n    Queue --\u003e|\"RPOP\"| Registrar\n    Registrar --\u003e|\"INSERT\"| PGSQL\n    PGSQL --\u003e PyCSW\n    \n    PyCSW --\u003e|\"OpenSearch\"| User\n    PyCSW --\u003e|\"CSW GetRecords\"| VS\n    PyCSW --\u003e|\"CSW GetRecords\"| ADES\n    \n    VS --\u003e Renderer\n    Renderer --\u003e|\"stage-in\"| S3In\n    Renderer --\u003e Cache\n    \n    User --\u003e|\"POST /processes/{id}/execution\"| ADES\n    ADES --\u003e|\"Query inputs\"| PyCSW\n    ADES --\u003e|\"Submit CWL\"| Calrissian\n    Calrissian --\u003e|\"Create\"| K8sJobs\n    K8sJobs --\u003e|\"Read data\"| S3In\n    K8sJobs --\u003e|\"Write results\"| S3Out\n    ADES --\u003e|\"Generate STAC\"| S3Out\n```\n\n**Key Integration Points:**\n\n- `harvester` pulls metadata from OpenSearch endpoints and enqueues to Redis\n- `registrar` consumes queue and writes ISO 19115 records to PostgreSQL\n- `pycsw` provides OGC CSW and OpenSearch interfaces over PostgreSQL\n- `ades` orchestrates CWL workflows using Calrissian on Kubernetes\n- Stage-in/out uses S3 for both input data (`eodata`) and output workspaces\n\nSources: [README.md:132-149](), [release-notes/release-0.3.md:31-80](), Diagram 3 from high-level overview\n\n## Workspace Multi-Tenancy Architecture\n\nEach user workspace is provisioned as an isolated environment with dedicated resources:\n\n```mermaid\ngraph TB\n    subgraph \"Global Services (rm namespace)\"\n        GlobalWS[\"workspace-api\u003cbr/\u003eHelmRelease\"]\n        MinioAPI[\"bucket-operator\u003cbr/\u003eMinIO S3 API\"]\n        TemplateRepo[\"HelmRepository\u003cbr/\u003eeoepca-helm-charts\"]\n    end\n    \n    subgraph \"User: eric\"\n        NS1[\"Namespace\u003cbr/\u003ews-eric\"]\n        Guard1[\"resource-guard\u003cbr/\u003eowner=eric\"]\n        RC1[\"resource-catalogue\u003cbr/\u003eeric's metadata\"]\n        DA1[\"data-access\u003cbr/\u003eeric's WMS\"]\n        Bucket1[\"S3 Bucket\u003cbr/\u003ews-eric\"]\n        PVC1[\"PersistentVolumeClaim\u003cbr/\u003emanaged-nfs\"]\n    end\n    \n    subgraph \"User: alice\"\n        NS2[\"Namespace\u003cbr/\u003ews-alice\"]\n        Guard2[\"resource-guard\u003cbr/\u003eowner=alice\"]\n        RC2[\"resource-catalogue\u003cbr/\u003ealice's metadata\"]\n        DA2[\"data-access\u003cbr/\u003ealice's WMS\"]\n        Bucket2[\"S3 Bucket\u003cbr/\u003ews-alice\"]\n        PVC2[\"PersistentVolumeClaim\u003cbr/\u003emanaged-nfs\"]\n    end\n    \n    subgraph \"Template System\"\n        Templates[\"HelmRelease Templates\u003cbr/\u003e- template-hr-data-access\u003cbr/\u003e- template-hr-resource-catalogue\u003cbr/\u003e- template-hr-resource-guard\"]\n    end\n    \n    GlobalWS --\u003e|\"1. Create namespace\"| NS1\n    GlobalWS --\u003e|\"2. Provision bucket\"| MinioAPI\n    MinioAPI --\u003e Bucket1\n    GlobalWS --\u003e|\"3. Instantiate from\"| Templates\n    Templates --\u003e|\"values:\u003cbr/\u003eworkspace_name=ws-eric\u003cbr/\u003edefault_owner=eric\"| Guard1\n    Templates --\u003e RC1\n    Templates --\u003e DA1\n    \n    Guard1 --\u003e Bucket1\n    DA1 --\u003e Bucket1\n    RC1 --\u003e PVC1\n    \n    GlobalWS -.-\u003e|\"alice requests\"| NS2\n    NS2 --\u003e Guard2\n    NS2 --\u003e RC2\n    NS2 --\u003e DA2\n    Guard2 --\u003e Bucket2\n```\n\n**Workspace Provisioning Sequence:**\n\n1. User requests workspace via `workspace-api`\n2. API creates Kubernetes namespace `ws-{username}`\n3. `bucket-operator` provisions S3 bucket `ws-{username}`\n4. API instantiates HelmRelease templates with user-specific values\n5. Templates deploy `resource-guard`, `resource-catalogue`, `data-access` in namespace\n6. PEP (`resource-guard`) enforces ownership policies based on `default_owner`\n\nSources: [README.md:148](), Diagram 4 from high-level overview\n\n## Deployment Architecture\n\n### Flux CD GitOps Workflow\n\n```mermaid\ngraph TB\n    subgraph \"Git Repository\"\n        Repo[\"github.com/EOEPCA/eoepca\u003cbr/\u003ebranch: develop\"]\n        SystemPath[\"system/clusters/develop/\"]\n    end\n    \n    subgraph \"Flux Controllers (flux-system namespace)\"\n        SourceCtrl[\"source-controller\u003cbr/\u003epolls Git every 1m\"]\n        KustomizeCtrl[\"kustomize-controller\u003cbr/\u003ereconciles Kustomizations\"]\n        HelmCtrl[\"helm-controller\u003cbr/\u003ereconciles HelmReleases\"]\n        NotifyCtrl[\"notification-controller\u003cbr/\u003ealerts on changes\"]\n    end\n    \n    subgraph \"Flux Resources\"\n        GitRepoRes[\"GitRepository\u003cbr/\u003eeoepca-system\"]\n        KustomizationRes[\"Kustomization\u003cbr/\u003eeoepca-system\"]\n        HRs[\"HelmRelease\u003cbr/\u003eum-login-service\u003cbr/\u003erm-workspace-api\u003cbr/\u003eproc-ades\u003cbr/\u003e...\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        Pods[\"Deployments\u003cbr/\u003ePods\u003cbr/\u003eServices\u003cbr/\u003eIngresses\"]\n    end\n    \n    Repo --\u003e|\"Git pull\"| SourceCtrl\n    SourceCtrl --\u003e GitRepoRes\n    GitRepoRes --\u003e KustomizeCtrl\n    KustomizeCtrl --\u003e KustomizationRes\n    KustomizationRes --\u003e HelmCtrl\n    HelmCtrl --\u003e HRs\n    HRs --\u003e|\"helm install/upgrade\"| Pods\n    \n    SystemPath -.-\u003e|\"contains\"| GitRepoRes\n    SystemPath -.-\u003e|\"contains\"| KustomizationRes\n    SystemPath -.-\u003e|\"contains\"| HRs\n```\n\n**GitOps Deployment Flow:**\n\n1. Developer commits changes to `system/clusters/{target}/` in Git\n2. `source-controller` detects changes via polling (interval: 1m)\n3. `kustomize-controller` reconciles `Kustomization` resources\n4. `helm-controller` reconciles `HelmRelease` resources\n5. Helm charts are installed/upgraded in target namespaces\n6. Kubernetes applies Deployment/StatefulSet/Job resources\n\n**Flux Bootstrap Command:**\n\nThe deployment is initialized using `deployCluster.sh` which executes:\n\n```bash\nflux bootstrap github \\\n  --owner=EOEPCA \\\n  --repository=eoepca \\\n  --branch=develop \\\n  --path=system/clusters/develop/system \\\n  --personal=false\n```\n\nSources: [system/clusters/README.md:51-77](), Diagram 5 from high-level overview\n\n### Infrastructure Layers\n\n```mermaid\ngraph TB\n    subgraph \"Physical Infrastructure\"\n        CREODIAS[\"OpenStack\u003cbr/\u003eCREODIAS Cloud\"]\n        TF[\"Terraform\u003cbr/\u003eIaC provisioning\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        RKE[\"Rancher Kubernetes Engine\u003cbr/\u003eor Minikube/k3s\"]\n        Masters[\"Control Plane Nodes\"]\n        Workers[\"Worker Nodes\"]\n    end\n    \n    subgraph \"Platform Services\"\n        Ingress[\"ingress-nginx\u003cbr/\u003eExternal routing\"]\n        CertMgr[\"cert-manager\u003cbr/\u003eTLS automation\"]\n        Sealed[\"sealed-secrets-controller\u003cbr/\u003eEncrypted secrets\"]\n        StorageClass[\"Storage Classes\u003cbr/\u003emanaged-nfs, ebs\"]\n    end\n    \n    subgraph \"EOEPCA Building Blocks\"\n        UM[\"User Management\u003cbr/\u003eum namespace\"]\n        RM[\"Resource Management\u003cbr/\u003erm namespace\"]\n        Proc[\"Processing \u0026 Chaining\u003cbr/\u003eproc namespace\"]\n        Workspaces[\"User Workspaces\u003cbr/\u003ews-* namespaces\"]\n    end\n    \n    TF --\u003e|\"provision VMs\"| CREODIAS\n    CREODIAS --\u003e RKE\n    RKE --\u003e Masters\n    RKE --\u003e Workers\n    \n    Masters --\u003e Ingress\n    Masters --\u003e CertMgr\n    Masters --\u003e Sealed\n    Masters --\u003e StorageClass\n    \n    Ingress --\u003e UM\n    Ingress --\u003e RM\n    Ingress --\u003e Proc\n    Ingress --\u003e Workspaces\n    \n    StorageClass --\u003e RM\n    StorageClass --\u003e Proc\n    StorageClass --\u003e Workspaces\n```\n\n**Deployment Targets:**\n\n| Target | Infrastructure | Kubernetes | Configuration Path |\n|--------|---------------|------------|-------------------|\n| Cloud (Production) | OpenStack CREODIAS | RKE 1.24+ | `system/clusters/creodias/` |\n| Development | OpenStack CREODIAS | RKE 1.24+ | `system/clusters/develop/` |\n| Local Testing | Local machine | Minikube 1.25+ | `system/clusters/minikube/` |\n| Local Testing (Alternative) | Local machine | k3s 1.24+ | `system/clusters/minikube/` |\n\n**DNS and Ingress Pattern:**\n\nServices are exposed using `nip.io` dynamic DNS:\n- Pattern: `{service}.{ip-with-dashes}.nip.io`\n- Example: `workspace-api.192-168-49-2.nip.io` (Minikube)\n- Example: `workspace-api.185-52-193-87.nip.io` (CREODIAS)\n\nThis avoids the need for public DNS configuration during development/testing.\n\nSources: [README.md:88-96](), [README.md:100-112](), [minikube/README.md:1-52](), Diagram 5 from high-level overview\n\n## Key Configuration Conventions\n\n### HelmRelease Naming Convention\n\nHelmReleases follow a consistent naming pattern that maps to service names:\n\n| HelmRelease Name | Namespace | Service Name | Configuration File |\n|-----------------|-----------|--------------|-------------------|\n| `um-login-service` | `um` | `login-service` | `user-management/um-login-service.yaml` |\n| `um-identity-service` | `um` | `identity-service` | `user-management/um-identity-service.yaml` |\n| `um-pdp-engine` | `um` | `pdp-engine` | `user-management/um-pdp-engine.yaml` |\n| `rm-workspace-api` | `rm` | `workspace-api` | `resource-management/rm-workspace-api.yaml` |\n| `rm-resource-catalogue` | `rm` | `resource-catalogue` | `resource-management/rm-resource-catalogue.yaml` |\n| `rm-data-access` | `rm` | `data-access` | `resource-management/rm-data-access.yaml` |\n| `proc-ades` | `proc` | `ades` | `processing-and-chaining/proc-ades.yaml` |\n| `proc-application-hub` | `proc` | `application-hub` | `processing-and-chaining/proc-application-hub.yaml` |\n\n### Secret Management\n\nSensitive configuration is managed using SealedSecrets:\n- Encrypted secrets are stored in Git as `SealedSecret` resources\n- `sealed-secrets-controller` decrypts them into native `Secret` resources\n- Pattern: `{service}-sealedsecret.yaml` files in component directories\n- Created using: `kubeseal --format yaml \u003c secret.yaml \u003e sealedsecret.yaml`\n\nFor details on secret management, see [SealedSecrets](#10.1).\n\nSources: [system/clusters/README.md:46-50](), [README.md:100-112]()\n\n## Summary\n\nThe EOEPCA system architecture implements a modular, standards-based platform for Earth Observation data exploitation. The architecture is characterized by:\n\n1. **Clear Domain Separation** - Three building blocks (User Management, Resource Management, Processing \u0026 Chaining) deployed in isolated namespaces\n2. **GitOps-Driven Operations** - Flux CD continuously reconciles cluster state with Git repository\n3. **Multi-Tenant Isolation** - Workspace API provisions per-user namespaces with dedicated resources\n4. **Standards Compliance** - OGC APIs for interoperability across data access, cataloging, and processing\n5. **Infrastructure Flexibility** - Supports cloud (OpenStack/RKE) and local (Minikube/k3s) deployments\n\nFor specific component details, refer to the child pages under [Building Blocks Overview](#3.1) and the domain-specific sections ([User Management](#4), [Resource Management](#5), [Processing and Chaining](#6)).\n\nSources: [README.md:58-68](), [README.md:128-160](), [system/clusters/README.md:1-95]()"])</script><script>self.__next_f.push([1,"1d:T6f59,"])</script><script>self.__next_f.push([1,"# Building Blocks Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [release-notes/release-0.3.md](release-notes/release-0.3.md)\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a detailed technical explanation of the three primary building blocks that comprise the EOEPCA platform architecture, their constituent components, and their integration patterns. The building blocks are:\n\n1. **User Management** - Authentication, authorization, and identity services\n2. **Resource Management** - Data cataloging, access services, and workspace provisioning\n3. **Processing \u0026 Chaining** - Application deployment, execution, and development environments\n\nFor information about the GitOps deployment model and infrastructure provisioning, see [GitOps and Flux CD](#3.2). For details on specific building block implementations, see [User Management and Identity](#4), [Resource Management](#5), and [Processing and Chaining](#6).\n\n---\n\n## Building Block Architecture\n\nThe EOEPCA system is organized into three loosely-coupled but highly-integrated building blocks, each deployed to dedicated Kubernetes namespaces and managed through GitOps.\n\n### High-Level Building Block Structure\n\n```mermaid\ngraph TB\n    subgraph \"User Management (um namespace)\"\n        UM[User Management\u003cbr/\u003eBuilding Block]\n        LoginSvc[\"login-service\u003cbr/\u003e(Gluu)\"]\n        IdentitySvc[\"identity-service\u003cbr/\u003e(Keycloak)\"]\n        PDPEngine[\"pdp-engine\"]\n        UserProfile[\"user-profile\"]\n        PEP[\"Policy Enforcement Points\u003cbr/\u003e(resource-guard instances)\"]\n    end\n    \n    subgraph \"Resource Management (rm namespace)\"\n        RM[Resource Management\u003cbr/\u003eBuilding Block]\n        WorkspaceAPI[\"workspace-api\"]\n        ResourceCat[\"resource-catalogue\u003cbr/\u003e(pycsw)\"]\n        DataAccess[\"data-access\u003cbr/\u003e(OGC WMS/WCS/WMTS)\"]\n        RegistrationAPI[\"registration-api\"]\n        Harvester[\"harvester\u003cbr/\u003e(OpenSearch)\"]\n        BucketOp[\"bucket-operator\u003cbr/\u003e(MinIO)\"]\n    end\n    \n    subgraph \"Processing \u0026 Chaining (proc namespace)\"\n        PC[Processing \u0026 Chaining\u003cbr/\u003eBuilding Block]\n        ADES[\"ades\u003cbr/\u003e(OGC API Processes)\"]\n        AppHub[\"application-hub\u003cbr/\u003e(JupyterHub)\"]\n        PDE[\"pde-hub\u003cbr/\u003e(Development)\"]\n        Calrissian[\"Calrissian\u003cbr/\u003e(CWL Executor)\"]\n    end\n    \n    subgraph \"Storage Layer\"\n        S3[\"S3 Storage\"]\n        PostgreSQL[\"PostgreSQL\"]\n        Redis[\"Redis\"]\n        NFS[\"NFS Storage\"]\n    end\n    \n    subgraph \"External Services\"\n        ExtData[\"External Data Sources\u003cbr/\u003e(CreoDIAS, Sentinel Hubs)\"]\n        Users[\"End Users\"]\n    end\n    \n    Users --\u003e|\"Authenticate\"| LoginSvc\n    LoginSvc --\u003e IdentitySvc\n    Users --\u003e|\"Protected Requests\"| PEP\n    PEP --\u003e|\"Policy Check\"| PDPEngine\n    PDPEngine --\u003e IdentitySvc\n    \n    PEP -.-\u003e|\"Protect\"| WorkspaceAPI\n    PEP -.-\u003e|\"Protect\"| ResourceCat\n    PEP -.-\u003e|\"Protect\"| DataAccess\n    PEP -.-\u003e|\"Protect\"| ADES\n    \n    ExtData --\u003e|\"Harvest\"| Harvester\n    Harvester --\u003e|\"Queue\"| Redis\n    Redis --\u003e|\"Register\"| RegistrationAPI\n    RegistrationAPI --\u003e|\"Write Metadata\"| ResourceCat\n    \n    WorkspaceAPI --\u003e|\"Provision Bucket\"| BucketOp\n    BucketOp --\u003e|\"Create\"| S3\n    WorkspaceAPI --\u003e|\"Deploy Templates\"| ResourceCat\n    WorkspaceAPI --\u003e|\"Deploy Templates\"| DataAccess\n    \n    ADES --\u003e|\"Query Data\"| ResourceCat\n    ADES --\u003e|\"Execute CWL\"| Calrissian\n    ADES --\u003e|\"Stage-in/out\"| S3\n    \n    DataAccess --\u003e|\"Read Data\"| S3\n    DataAccess --\u003e|\"Query Metadata\"| ResourceCat\n    ResourceCat --\u003e PostgreSQL\n    DataAccess --\u003e Redis\n    \n    AppHub --\u003e|\"Develop Apps\"| ADES\n    PDE --\u003e|\"Build Packages\"| ADES\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml](), [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](), [release-notes/release-0.3.md:97-264]()\n\n---\n\n## Building Block 1: User Management\n\nThe User Management building block provides authentication, authorization, and identity services across the entire platform. It implements a UMA 2.0 (User-Managed Access) authorization pattern with fine-grained policy-based access control.\n\n### User Management Components\n\n| Component | Service Name | Purpose | Key Technologies |\n|-----------|-------------|---------|------------------|\n| Login Service | `login-service` | Primary authentication provider | Gluu Server (OpenDJ, oxAuth, oxTrust, oxPassport) |\n| Identity Service | `identity-service` | Identity provider and token issuer | Keycloak, PostgreSQL |\n| Policy Decision Point | `pdp-engine` | Central policy evaluation engine | MongoDB |\n| User Profile | `user-profile` | User attribute management | SCIM 2.0 |\n| Policy Enforcement Points | `resource-guard` instances | Service-level authorization enforcement | Multiple instances per protected service |\n\n### User Management Component Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Login Service\"\n        Gluu[\"Gluu Server\"]\n        OpenDJ[\"OpenDJ\u003cbr/\u003e(LDAP)\"]\n        oxAuth[\"oxAuth\u003cbr/\u003e(OAuth/OIDC)\"]\n        oxTrust[\"oxTrust\u003cbr/\u003e(Admin UI)\"]\n        oxPassport[\"oxPassport\u003cbr/\u003e(Social Login)\"]\n    end\n    \n    subgraph \"Identity Service\"\n        Keycloak[\"Keycloak Server\"]\n        KeycloakDB[\"PostgreSQL\u003cbr/\u003e(Identity DB)\"]\n        KeycloakAPI[\"Keycloak API\"]\n    end\n    \n    subgraph \"Policy Decision Point\"\n        PDPEngine[\"pdp-engine\"]\n        PDPMongo[\"MongoDB\u003cbr/\u003e(Policy DB)\"]\n    end\n    \n    subgraph \"Policy Enforcement\"\n        PEP1[\"resource-guard\u003cbr/\u003e(ADES PEP)\"]\n        PEP2[\"resource-guard\u003cbr/\u003e(Workspace PEP)\"]\n        PEP3[\"resource-guard\u003cbr/\u003e(Data Access PEP)\"]\n    end\n    \n    UserProfile[\"user-profile\u003cbr/\u003e(SCIM API)\"]\n    \n    Gluu --\u003e OpenDJ\n    Gluu --\u003e oxAuth\n    Gluu --\u003e oxTrust\n    Gluu --\u003e oxPassport\n    \n    oxAuth --\u003e|\"OIDC Flow\"| Keycloak\n    Keycloak --\u003e KeycloakDB\n    Keycloak --\u003e KeycloakAPI\n    \n    PEP1 --\u003e|\"Policy Query\"| PDPEngine\n    PEP2 --\u003e|\"Policy Query\"| PDPEngine\n    PEP3 --\u003e|\"Policy Query\"| PDPEngine\n    PDPEngine --\u003e PDPMongo\n    PDPEngine --\u003e|\"Token Validation\"| Keycloak\n    \n    UserProfile --\u003e|\"SCIM\"| Keycloak\n```\n\n**Sources:** [release-notes/release-0.3.md:102-218]()\n\n### Key Integration Points\n\nThe User Management building block provides:\n\n- **OIDC Identity Provider**: Keycloak serves as the primary identity provider at `identity-service` endpoint\n- **UMA Authorization Server**: Issues UMA tickets and RPT tokens for resource access\n- **Policy Management**: Central policy repository in `pdp-engine` accessed by all PEPs\n- **Resource Protection**: Each protected service deploys its own `resource-guard` (PEP) instance\n\nFor details on the UMA authentication flow, see [UMA Authentication Flow](#4.4). For PEP/PDP implementation details, see [Policy Enforcement (PEP/PDP)](#4.3).\n\n---\n\n## Building Block 2: Resource Management\n\nThe Resource Management building block handles data cataloging, visualization, access services, and user workspace provisioning. It manages both global platform resources and per-user isolated workspaces.\n\n### Resource Management Components\n\n| Component | Service Name | Purpose | Key Technologies |\n|-----------|-------------|---------|------------------|\n| Data Access | `data-access` | OGC-compliant data visualization and access | EOX ViewServer, OGC WMS/WCS/WMTS |\n| Resource Catalogue | `resource-catalogue` | Metadata catalog and discovery | pycsw, OGC CSW, OpenSearch |\n| Workspace API | `workspace-api` | Multi-tenant workspace orchestration | Kubernetes operator pattern |\n| Registration API | `registration-api` | Data and metadata registration endpoint | Redis queue consumer |\n| Harvester | `harvester` | External data source ingestion | OpenSearch clients |\n| Bucket Operator | `bucket-operator` (via MinIO) | S3 bucket provisioning | MinIO API |\n\n### Resource Management Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Global Services (rm namespace)\"\n        WorkspaceAPI[\"workspace-api\"]\n        GlobalRC[\"resource-catalogue\u003cbr/\u003e(pycsw)\"]\n        GlobalDA[\"data-access\"]\n        RegistrationAPI[\"registration-api\"]\n        Harvester[\"harvester\"]\n        MinioAPI[\"minio-bucket-api\"]\n    end\n    \n    subgraph \"Data Access Components\"\n        Renderer[\"renderer\u003cbr/\u003e(4 replicas)\"]\n        Registrar[\"registrar\"]\n        Client[\"client\"]\n        Cache[\"cache/seeder\"]\n        DARedis[\"data-access-redis-master\"]\n    end\n    \n    subgraph \"Catalogue Components\"\n        PyCSW[\"pycsw\"]\n        RCDB[\"resource-catalogue-db\u003cbr/\u003e(PostgreSQL)\"]\n    end\n    \n    subgraph \"Storage\"\n        S3Global[\"S3 Buckets\"]\n        NFSStorage[\"NFS PersistentVolumes\"]\n    end\n    \n    subgraph \"External Sources\"\n        CreoDIAS[\"CreoDIAS OpenSearch\"]\n        Sentinel[\"Sentinel Hubs\"]\n    end\n    \n    GlobalDA --\u003e Renderer\n    GlobalDA --\u003e Registrar\n    GlobalDA --\u003e Client\n    GlobalDA --\u003e Cache\n    GlobalDA --\u003e DARedis\n    \n    GlobalRC --\u003e PyCSW\n    PyCSW --\u003e RCDB\n    \n    Harvester --\u003e|\"Poll\"| CreoDIAS\n    Harvester --\u003e|\"Poll\"| Sentinel\n    Harvester --\u003e|\"Enqueue\"| DARedis\n    DARedis --\u003e|\"Consume\"| RegistrationAPI\n    RegistrationAPI --\u003e|\"Write\"| RCDB\n    \n    Renderer --\u003e|\"Read\"| S3Global\n    Registrar --\u003e|\"Register\"| RCDB\n    \n    WorkspaceAPI --\u003e|\"Create Bucket\"| MinioAPI\n    MinioAPI --\u003e S3Global\n    WorkspaceAPI --\u003e|\"Provision PVCs\"| NFSStorage\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:1-1144](), [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:1-82](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:1-50](), [system/clusters/creodias/resource-management/hr-registration-api.yaml:1-37]()\n\n### Data Access Service Details\n\nThe Data Access service is deployed as `data-access` in the `rm` namespace and provides OGC-compliant data visualization. Configuration is defined in [system/clusters/creodias/resource-management/hr-data-access.yaml:1-1144]().\n\n**Key Configuration:**\n- **Renderer**: 4 replicas for parallel processing ([hr-data-access.yaml:865-877]())\n- **Ingress**: Exposed at `data-access.develop.eoepca.org` ([hr-data-access.yaml:42-47]())\n- **S3 Data Source**: CloudFerro eodata at `http://data.cloudferro.com` ([hr-data-access.yaml:49-57]())\n- **Cache Storage**: S3 cache bucket at `https://cf2.cloudferro.com:8080` ([hr-data-access.yaml:58-64]())\n\n**Supported Collections:**\n- Sentinel-2 L1C and L2A with multiple band configurations ([hr-data-access.yaml:218-252]())\n- Landsat-8 L1TP and L1GT ([hr-data-access.yaml:253-278]())\n- Sentinel-1 GRD and SLC ([hr-data-access.yaml:279-286]())\n- Sentinel-3 OL_2_LFR ([hr-data-access.yaml:287-290]())\n\n### Resource Catalogue Details\n\nThe Resource Catalogue is deployed as `resource-catalogue` using pycsw and provides OGC CSW 3.0/2.0.2 interfaces. Configuration is in [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:1-82]().\n\n**Key Configuration:**\n- **Database**: PostgreSQL with 5Gi volume and tuned performance parameters ([hr-resource-catalogue.yaml:19-31]())\n- **Server URL**: `https://resource-catalogue.develop.eoepca.org/` ([hr-resource-catalogue.yaml:45]())\n- **Transactions**: Enabled with allowed IPs set to \"*\" ([hr-resource-catalogue.yaml:47-48]())\n- **INSPIRE**: Enabled with multi-language support ([hr-resource-catalogue.yaml:72-81]())\n\n### Workspace API Details\n\nThe Workspace API orchestrates multi-tenant workspace provisioning. Configuration is in [system/clusters/creodias/resource-management/hr-workspace-api.yaml:1-50]().\n\n**Key Configuration:**\n- **Namespace Prefix**: `develop-user` for workspace namespaces ([hr-workspace-api.yaml:35]())\n- **S3 Endpoint**: MinIO at `https://minio.develop.eoepca.org` ([hr-workspace-api.yaml:39]())\n- **Harbor Registry**: `https://harbor.develop.eoepca.org` ([hr-workspace-api.yaml:41]())\n- **Bucket API**: `http://minio-bucket-api:8080/bucket` ([hr-workspace-api.yaml:47]())\n- **PEP Integration**: `http://workspace-api-pep:5576/resources` ([hr-workspace-api.yaml:48]())\n- **Auto-Protection**: Enabled for automatic resource registration ([hr-workspace-api.yaml:49]())\n\nThe Workspace API uses HelmRelease templates to provision per-user instances of services. See [Multi-Tenant Workspaces](#5.5) for details.\n\n### Registration Pipeline\n\nThe registration pipeline handles ingestion of metadata from external sources:\n\n```mermaid\nsequenceDiagram\n    participant Harvester as harvester\n    participant Redis as data-access-redis-master\n    participant RegAPI as registration-api\n    participant Registrar as registrar\n    participant PyCSW as resource-catalogue-db\n    \n    Harvester-\u003e\u003eHarvester: Poll OpenSearch endpoints\n    Harvester-\u003e\u003eRedis: Enqueue items to register_queue\n    RegAPI-\u003e\u003eRedis: Consume from register_queue\n    RegAPI-\u003e\u003eRegistrar: Route to backend\n    \n    alt Collection Registration\n        Registrar-\u003e\u003ePyCSW: Write to CollectionBackend\n    else Item Registration\n        Registrar-\u003e\u003ePyCSW: Write to ItemBackend\n    else ADES Registration\n        Registrar-\u003e\u003ePyCSW: Write to ADESBackend\n    else Application Registration\n        Registrar-\u003e\u003ePyCSW: Write to CWLBackend\n    end\n    \n    PyCSW-\u003e\u003ePyCSW: Store ISO 19115 metadata\n```\n\n**Registrar Routes** (configured in [hr-data-access.yaml:894-947]()):\n- `collections`: STAC Collection route to `CollectionBackend`\n- `ades`: JSON route for ADES service registration to `ADESBackend`\n- `application`: JSON route for CWL application registration to `CWLBackend`\n- `catalogue`: JSON route for catalogue federation to `CatalogueBackend`\n- `json`/`xml`: Generic registration routes to `JSONBackend`/`XMLBackend`\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:878-947](), [system/clusters/creodias/resource-management/hr-registration-api.yaml:34-37]()\n\n---\n\n## Building Block 3: Processing \u0026 Chaining\n\nThe Processing \u0026 Chaining building block provides application deployment, workflow execution, and development environments for Earth Observation processing.\n\n### Processing \u0026 Chaining Components\n\n| Component | Service Name | Purpose | Key Technologies |\n|-----------|-------------|---------|------------------|\n| ADES | `ades` | Application deployment and execution service | OGC API Processes, ZOO-Project |\n| Application Hub | `application-hub` | Interactive development environment | JupyterHub, OAuth2 |\n| PDE Hub | `pde-hub` | Processor development tools | Theia IDE, Jenkins, Docker-in-Docker |\n| CWL Executor | Calrissian (invoked by ADES) | Kubernetes-native workflow execution | Calrissian, CWL |\n\n### ADES Integration Architecture\n\n```mermaid\ngraph TB\n    subgraph \"ADES Service\"\n        ADESCore[\"ades\u003cbr/\u003e(ZOO-Project)\"]\n        WPSAPI[\"OGC WPS 2.0 API\"]\n        ProcAPI[\"OGC API Processes\"]\n    end\n    \n    subgraph \"Workflow Execution\"\n        Calrissian[\"Calrissian\u003cbr/\u003e(CWL Executor)\"]\n        K8sPods[\"Processing Pods\u003cbr/\u003e(Dynamic)\"]\n    end\n    \n    subgraph \"Stage-in/Stage-out\"\n        StageIn[\"stars-t2:0.6.17.0\u003cbr/\u003e(Stage-in)\"]\n        StageOut[\"stars-t2:0.6.17.0\u003cbr/\u003e(Stage-out)\"]\n    end\n    \n    subgraph \"Data Discovery\"\n        ResourceCat[\"resource-catalogue\"]\n        OpenSearch[\"OpenSearch API\"]\n    end\n    \n    subgraph \"Storage\"\n        S3Input[\"S3 Input Data\u003cbr/\u003e(eodata)\"]\n        S3Output[\"S3 Output Data\u003cbr/\u003e(Workspace Bucket)\"]\n    end\n    \n    User[\"User\"] --\u003e|\"Deploy Process\"| ADESCore\n    User --\u003e|\"Execute Job\"| ADESCore\n    ADESCore --\u003e WPSAPI\n    ADESCore --\u003e ProcAPI\n    \n    ADESCore --\u003e|\"Query Inputs\"| ResourceCat\n    ResourceCat --\u003e OpenSearch\n    \n    ADESCore --\u003e|\"Execute CWL\"| Calrissian\n    Calrissian --\u003e|\"Create\"| K8sPods\n    \n    ADESCore --\u003e|\"Invoke\"| StageIn\n    StageIn --\u003e|\"Fetch\"| S3Input\n    \n    K8sPods --\u003e|\"Process\"| S3Input\n    K8sPods --\u003e|\"Write\"| S3Output\n    \n    ADESCore --\u003e|\"Invoke\"| StageOut\n    StageOut --\u003e|\"Write STAC\"| S3Output\n```\n\n**Sources:** [release-notes/release-0.3.md:220-249]()\n\n### ADES Capabilities\n\nThe ADES provides the following capabilities (from [release-notes/release-0.3.md:30-41]()):\n\n1. **OGC Interfaces**: Implements OGC WPS 2.0 and OGC API Processes\n2. **Process Management**:\n   - List available processes\n   - Deploy process (Docker container with CWL application package)\n   - Execute process (create job)\n   - Get job status\n   - Undeploy process\n3. **Data Integration**:\n   - Stage-in via OpenSearch catalogue reference\n   - Stage-out to S3 bucket with STAC manifests\n4. **Workflow Execution**: Calrissian CWL engine with native Kubernetes integration\n5. **User Context**: Dedicated user 'context' within ADES service\n\n### Application Development Workflow\n\n```mermaid\ngraph LR\n    subgraph \"Development (PDE)\"\n        Developer[\"Developer\"]\n        JupyterLab[\"JupyterLab\"]\n        TheiaIDE[\"Theia IDE\"]\n        Jenkins[\"Jenkins CI\"]\n        DockerBuild[\"Docker Build\"]\n    end\n    \n    subgraph \"Testing (Application Hub)\"\n        AppHub[\"application-hub\u003cbr/\u003e(JupyterHub)\"]\n        TestEnv[\"Test Environment\"]\n    end\n    \n    subgraph \"Deployment (ADES)\"\n        DeployProc[\"Deploy Process\"]\n        CWLPackage[\"CWL Package\"]\n        DockerImage[\"Docker Image\"]\n    end\n    \n    subgraph \"Execution\"\n        ExecuteJob[\"Execute Job\"]\n        Calrissian[\"Calrissian\"]\n        Results[\"Results to S3\"]\n    end\n    \n    Developer --\u003e JupyterLab\n    Developer --\u003e TheiaIDE\n    JupyterLab --\u003e Jenkins\n    TheiaIDE --\u003e Jenkins\n    Jenkins --\u003e DockerBuild\n    DockerBuild --\u003e DockerImage\n    \n    DockerImage --\u003e AppHub\n    AppHub --\u003e TestEnv\n    TestEnv --\u003e CWLPackage\n    \n    CWLPackage --\u003e DeployProc\n    DockerImage --\u003e DeployProc\n    DeployProc --\u003e ExecuteJob\n    ExecuteJob --\u003e Calrissian\n    Calrissian --\u003e Results\n```\n\n**Sources:** [release-notes/release-0.3.md:42-49](), [release-notes/release-0.3.md:252-263]()\n\n---\n\n## Building Block Integration Patterns\n\nThe three building blocks integrate through well-defined interfaces and shared infrastructure components.\n\n### Cross-Building-Block Integration\n\n```mermaid\ngraph TB\n    subgraph \"User Management\"\n        Auth[\"Authentication\u003cbr/\u003e(login-service)\"]\n        Authz[\"Authorization\u003cbr/\u003e(pdp-engine)\"]\n        PEPs[\"Policy Enforcement\u003cbr/\u003e(resource-guard)\"]\n    end\n    \n    subgraph \"Resource Management\"\n        WS[\"Workspace API\"]\n        Cat[\"Resource Catalogue\"]\n        DA[\"Data Access\"]\n    end\n    \n    subgraph \"Processing \u0026 Chaining\"\n        ADES[\"ADES\"]\n        AppH[\"Application Hub\"]\n    end\n    \n    User[\"User\"]\n    \n    User --\u003e|\"1. Authenticate\"| Auth\n    Auth --\u003e|\"2. Issue ID Token\"| User\n    User --\u003e|\"3. Request (with Token)\"| PEPs\n    PEPs --\u003e|\"4. Validate Policy\"| Authz\n    PEPs --\u003e|\"5. Forward if Authorized\"| WS\n    PEPs --\u003e|\"5. Forward if Authorized\"| Cat\n    PEPs --\u003e|\"5. Forward if Authorized\"| DA\n    PEPs --\u003e|\"5. Forward if Authorized\"| ADES\n    \n    WS --\u003e|\"Provision Resources\"| Cat\n    WS --\u003e|\"Provision Resources\"| DA\n    ADES --\u003e|\"Discover Data\"| Cat\n    DA --\u003e|\"Serve Data\"| ADES\n    AppH --\u003e|\"Deploy Applications\"| ADES\n    \n    ADES -.-\u003e|\"Protected by\"| PEPs\n    WS -.-\u003e|\"Protected by\"| PEPs\n    Cat -.-\u003e|\"Protected by\"| PEPs\n    DA -.-\u003e|\"Protected by\"| PEPs\n```\n\n### Key Integration Mechanisms\n\n**Authentication Integration:**\n- All services validate tokens against `identity-service` (Keycloak)\n- UMA flow issues RPT tokens for resource access\n- See [UMA Authentication Flow](#4.4) for sequence details\n\n**Authorization Integration:**\n- Each protected service deploys a `resource-guard` (PEP) sidecar\n- PEPs query `pdp-engine` for policy decisions\n- Resources auto-registered with protection policies (when `autoProtectionEnabled: \"True\"` in [hr-workspace-api.yaml:49]())\n\n**Data Integration:**\n- ADES queries `resource-catalogue` via OpenSearch API\n- Data Access reads from same S3 buckets that ADES writes to\n- Workspace API provisions isolated catalogue and data access instances per user\n\n**Metadata Integration:**\n- All components register metadata to `resource-catalogue` via `registration-api`\n- Registrar routes metadata to appropriate backends (Item, Collection, ADES, CWL)\n- pycsw stores metadata in PostgreSQL using ISO 19115 format\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:44-49](), [system/clusters/creodias/resource-management/hr-data-access.yaml:878-947]()\n\n---\n\n## Workspace Template System\n\nThe Resource Management building block implements a template-based system for provisioning per-user workspace instances. This enables multi-tenancy with isolated resources.\n\n### Template Instantiation\n\n```mermaid\ngraph TB\n    subgraph \"Global Service (rm namespace)\"\n        WSAPI[\"workspace-api\"]\n        Templates[\"workspace-charts\u003cbr/\u003eConfigMap\"]\n    end\n    \n    subgraph \"Template Resources\"\n        TempDA[\"template-hr-data-access.yaml\"]\n        TempRC[\"template-hr-resource-catalogue.yaml\"]\n        TempRG[\"template-hr-resource-guard.yaml\"]\n    end\n    \n    subgraph \"User Workspace (user-workspace namespace)\"\n        UserDA[\"data-access\u003cbr/\u003e(user instance)\"]\n        UserRC[\"resource-catalogue\u003cbr/\u003e(user instance)\"]\n        UserRG[\"resource-guard\u003cbr/\u003e(user instance)\"]\n        UserBucket[\"S3 Bucket\u003cbr/\u003e(user-workspace)\"]\n        UserPVC[\"PersistentVolumeClaim\u003cbr/\u003e(NFS)\"]\n    end\n    \n    User[\"User Request\"] --\u003e|\"POST /workspaces\"| WSAPI\n    WSAPI --\u003e Templates\n    Templates --\u003e TempDA\n    Templates --\u003e TempRC\n    Templates --\u003e TempRG\n    \n    TempDA --\u003e|\"Substitute {{ workspace_name }}\"| UserDA\n    TempDA --\u003e|\"Substitute {{ bucket }}\"| UserDA\n    TempDA --\u003e|\"Substitute {{ access_key_id }}\"| UserDA\n    \n    TempRC --\u003e|\"Substitute {{ workspace_name }}\"| UserRC\n    TempRG --\u003e|\"Substitute {{ default_owner }}\"| UserRG\n    \n    WSAPI --\u003e|\"Create Bucket\"| UserBucket\n    WSAPI --\u003e|\"Provision Storage\"| UserPVC\n    \n    UserDA --\u003e UserBucket\n    UserDA --\u003e UserPVC\n    UserRC --\u003e UserPVC\n```\n\n**Template Variables** (used in [template-hr-data-access.yaml]() and [template-hr-resource-catalogue.yaml]()):\n- `{{ workspace_name }}`: User workspace identifier\n- `{{ bucket }}`: S3 bucket name\n- `{{ access_key_id }}`: S3 access key\n- `{{ secret_access_key }}`: S3 secret key\n- `{{ default_owner }}`: User identity for policy enforcement\n\n**Template Configuration Examples:**\n\nFrom [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:28-44]():\n```yaml\ningress:\n  tls:\n    - hosts:\n        - data-access.{{ workspace_name }}.develop.eoepca.org\nstorage:\n  data:\n    data:\n      type: \"S3\"\n      endpoint_url: https://minio.develop.eoepca.org\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      bucket: {{ bucket }}\n```\n\nFrom [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:17-33]():\n```yaml\nglobal:\n  namespace: \"{{ workspace_name }}\"\npycsw:\n  config:\n    server:\n      url: \"https://resource-catalogue.{{ workspace_name }}.develop.eoepca.org\"\n    metadata:\n      identification_title: Resource Catalogue - {{ workspace_name }}\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:1-269](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:1-68](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:46]()\n\n---\n\n## Storage Architecture Integration\n\nAll three building blocks integrate with a common storage layer:\n\n| Storage Type | Purpose | Used By | Configuration |\n|-------------|---------|---------|---------------|\n| S3 (CloudFerro) | Input EO data | Data Access (read), ADES (stage-in) | `http://data.cloudferro.com` |\n| S3 (MinIO) | User workspaces, outputs | Workspace API, ADES (stage-out), Data Access (cache) | `https://minio.develop.eoepca.org` |\n| PostgreSQL | Catalogue metadata | Resource Catalogue, Identity Service | Per-service databases |\n| Redis | Registration queues, caching | Data Access, Registration API | Per-service instances |\n| NFS | Persistent volumes | Data Access DB, Resource Catalogue DB | `managed-nfs-storage` StorageClass |\n\n**Data Access Storage** ([hr-data-access.yaml:49-64]()):\n- Input data from CloudFerro: `http://data.cloudferro.com` (read-only)\n- Cache bucket: `https://cf2.cloudferro.com:8080/cache-bucket` (read-write)\n\n**Workspace Storage** ([hr-workspace-api.yaml:39](), [template-hr-data-access.yaml:39-44]()):\n- Per-user buckets provisioned via MinIO API\n- Workspace-specific NFS PVCs with `managed-nfs-storage` StorageClass ([template-hr-data-access.yaml:220-224]())\n\n**ADES Storage:**\n- Stage-in from CloudFerro eodata or STAC catalogs\n- Stage-out to workspace S3 buckets\n- STAC manifests describe inputs and outputs\n\nFor detailed storage architecture, see [Storage and Persistence](#7).\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:38-39](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-228]()\n\n---\n\n## Deployment and Namespace Organization\n\nThe building blocks are deployed to separate Kubernetes namespaces, each managed by GitOps Kustomization resources:\n\n| Building Block | Namespace | GitOps Kustomization | HelmReleases |\n|---------------|-----------|---------------------|-------------|\n| User Management | `um` | `./user-management` | `login-service`, `identity-service`, `pdp-engine`, `user-profile` |\n| Resource Management | `rm` | `./resource-management` | `data-access`, `resource-catalogue`, `workspace-api`, `registration-api` |\n| Processing \u0026 Chaining | `proc` | `./processing-and-chaining` | `ades`, `application-hub`, `pde-hub` |\n| System Infrastructure | various | `./system` | `ingress-nginx`, `cert-manager`, `sealed-secrets` |\n\nEach HelmRelease references charts from the `eoepca` HelmRepository in the `common` namespace. For example:\n\nFrom [system/clusters/creodias/resource-management/hr-workspace-api.yaml:8-15]():\n```yaml\nchart:\n  spec:\n    chart: rm-workspace-api\n    version: \"1.4.0\"\n    sourceRef:\n      kind: HelmRepository\n      name: eoepca\n      namespace: common\n```\n\nFor complete GitOps deployment details, see [GitOps and Flux CD](#3.2).\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:8-15](), [system/clusters/creodias/resource-management/hr-data-access.yaml:8-15](), [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:8-15]()\n\n---\n\n## Summary of Building Block Interactions\n\nThe three building blocks work together to provide a complete Earth Observation platform:\n\n1. **User Management** provides identity and authorization services that protect all other services through PEP instances\n2. **Resource Management** provisions workspaces, catalogs data, and provides visualization services\n3. **Processing \u0026 Chaining** executes user applications on data discovered through Resource Management and protected by User Management\n\nAll building blocks:\n- Share common storage infrastructure (S3, PostgreSQL, Redis, NFS)\n- Are deployed via GitOps using Flux CD HelmReleases\n- Follow OGC standards for interoperability (CSW, WMS, WCS, WMTS, WPS, API Processes, OpenSearch)\n- Support multi-tenancy through workspace isolation\n- Integrate through well-defined APIs and message queues\n\nFor specific implementation details, see the dedicated pages for each building block: [User Management and Identity](#4), [Resource Management](#5), and [Processing and Chaining](#6)."])</script><script>self.__next_f.push([1,"1e:T6601,"])</script><script>self.__next_f.push([1,"# GitOps and Flux CD\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [minikube/README.md](minikube/README.md)\n- [system/clusters/README.md](system/clusters/README.md)\n- [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml](system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system/flux-system-patch.yaml](system/clusters/creodias/system/flux-system/flux-system-patch.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-components.yaml](system/clusters/creodias/system/flux-system/gotk-components.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-sync.yaml](system/clusters/creodias/system/flux-system/gotk-sync.yaml)\n- [system/clusters/creodias/system/flux-system/kustomization.yaml](system/clusters/creodias/system/flux-system/kustomization.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains the GitOps deployment model used by EOEPCA and the Flux CD continuous delivery system that implements it. It covers the Flux CD architecture, Custom Resource Definitions (CRDs), repository structure, bootstrap process, and synchronization mechanisms. For initial deployment instructions, see [Deployment Guide](#2.1). For infrastructure provisioning before Flux deployment, see [Infrastructure Provisioning](#2.2). For monitoring and troubleshooting Flux operations, see [Monitoring and Troubleshooting](#11.2).\n\n## GitOps Fundamentals\n\nEOEPCA adopts a GitOps approach where the Git repository at `https://github.com/EOEPCA/eoepca` serves as the single source of truth for the cluster's desired state. All system configuration, Kubernetes manifests, and Helm chart values are stored in Git. Flux CD continuously monitors this repository and automatically reconciles the cluster state to match the declared configuration.\n\nThis approach provides:\n- **Version Control**: All configuration changes are tracked in Git with full audit history\n- **Declarative Infrastructure**: Cluster state is declared as code rather than imperatively applied\n- **Automated Reconciliation**: Flux automatically applies changes and self-heals drift\n- **Rollback Capability**: Any commit in Git history represents a deployable state\n- **Multi-Cluster Support**: Different directories represent different cluster configurations\n\nSources: [README.md:79-112](), [system/clusters/README.md:1-6]()\n\n## Flux CD Architecture\n\n### Core Components\n\nFlux CD v0.24.0 is deployed to the `flux-system` namespace and consists of four controller deployments that work together to implement GitOps synchronization.\n\n**Flux CD Component Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"flux-system namespace\"\n        SC[source-controller\u003cbr/\u003eDeployment]\n        KC[kustomize-controller\u003cbr/\u003eDeployment]\n        HC[helm-controller\u003cbr/\u003eDeployment]\n        NC[notification-controller\u003cbr/\u003eDeployment]\n    end\n    \n    subgraph \"Custom Resources\"\n        GR[GitRepository\u003cbr/\u003esource.toolkit.fluxcd.io/v1beta2]\n        Bucket[Bucket\u003cbr/\u003esource.toolkit.fluxcd.io/v1beta1]\n        HR[HelmRelease\u003cbr/\u003ehelm.toolkit.fluxcd.io/v2beta1]\n        HRepo[HelmRepository\u003cbr/\u003esource.toolkit.fluxcd.io/v1beta1]\n        Kust[Kustomization\u003cbr/\u003ekustomize.toolkit.fluxcd.io/v1beta2]\n        Alert[Alert\u003cbr/\u003enotification.toolkit.fluxcd.io/v1beta1]\n    end\n    \n    subgraph \"External\"\n        Git[GitHub Repository\u003cbr/\u003eEOEPCA/eoepca]\n        HelmCharts[Helm Chart Repos]\n    end\n    \n    SC --\u003e|Fetches| Git\n    SC --\u003e|Fetches| HelmCharts\n    SC --\u003e|Creates Artifact| GR\n    SC --\u003e|Creates Artifact| Bucket\n    SC --\u003e|Creates Artifact| HRepo\n    \n    KC --\u003e|Watches| Kust\n    KC --\u003e|Uses Artifact from| GR\n    KC --\u003e|Applies to Cluster| K8sAPI[Kubernetes API]\n    \n    HC --\u003e|Watches| HR\n    HC --\u003e|Uses Artifact from| HRepo\n    HC --\u003e|Installs Charts| K8sAPI\n    \n    NC --\u003e|Watches| Alert\n    NC --\u003e|Notifies on Events| External[External Systems]\n    \n    GR -.-\u003e|Referenced by| Kust\n    HRepo -.-\u003e|Referenced by| HR\n```\n\nSources: [system/clusters/creodias/system/flux-system/gotk-components.yaml:1-12](), [system/clusters/README.md:3]()\n\n### Controller Responsibilities\n\n| Controller | Purpose | Key CRDs Managed |\n|------------|---------|------------------|\n| **source-controller** | Fetches artifacts from Git repositories, Helm repositories, and S3 buckets. Exposes artifacts via HTTP to other controllers. | `GitRepository`, `HelmRepository`, `HelmChart`, `Bucket` |\n| **kustomize-controller** | Applies Kustomize overlays to the cluster. Watches `Kustomization` resources and fetches source artifacts from source-controller. | `Kustomization` |\n| **helm-controller** | Performs Helm releases. Watches `HelmRelease` resources and installs/upgrades Helm charts using artifacts from source-controller. | `HelmRelease` |\n| **notification-controller** | Handles events and notifications. Can send alerts to external systems and manage Git commit status updates. | `Alert`, `Provider`, `Receiver` |\n\nSources: [system/clusters/creodias/system/flux-system/gotk-components.yaml:2-4]()\n\n### Custom Resource Definitions\n\nFlux installs multiple CRDs that define the declarative API for GitOps operations. The primary CRDs are:\n\n**GitRepository CRD** (`source.toolkit.fluxcd.io/v1beta2`): Defines a Git repository source. Includes fields for `url`, `ref` (branch/tag/commit), `interval` for polling, and `secretRef` for authentication.\n\n**Kustomization CRD** (`kustomize.toolkit.fluxcd.io/v1beta2`): Defines a set of Kubernetes resources to apply from a source. Includes `path` within the repository, `prune` flag for garbage collection, and `sourceRef` pointing to a GitRepository.\n\n**HelmRelease CRD** (`helm.toolkit.fluxcd.io/v2beta1`): Defines a Helm release. Includes `chart` specification, `values` overrides, and `interval` for reconciliation.\n\n**HelmRepository CRD** (`source.toolkit.fluxcd.io/v1beta1`): Defines a Helm chart repository source.\n\nSources: [system/clusters/creodias/system/flux-system/gotk-components.yaml:14-482](), [system/clusters/creodias/system/flux-system/gotk-components.yaml:484-3774]()\n\n### Resource Limits and Tuning\n\nThe EOEPCA deployment patches the default Flux controller resource limits to handle the complexity of the system. The `source-controller` and `kustomize-controller` are configured with increased memory limits.\n\n```yaml\n# source-controller resource patch\nresources:\n  limits:\n    memory: 2Gi\n  requests:\n    memory: 500Mi\n\n# kustomize-controller resource patch  \nresources:\n  limits:\n    memory: 2Gi\n  requests:\n    memory: 500Mi\n```\n\nSources: [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:1-31]()\n\n## Repository Structure\n\nThe EOEPCA repository is organized to support multiple cluster deployments with a clear separation between system infrastructure, building blocks, and cluster-specific configuration.\n\n**GitOps Repository Layout**\n\n```mermaid\ngraph TB\n    Root[\"/eoepca repository\"]\n    \n    Root --\u003e System[\"system/clusters/\"]\n    \n    System --\u003e CreoDIAS[\"creodias/\"]\n    System --\u003e Minikube[\"minikube/\"]\n    System --\u003e Develop[\"develop/\"]\n    \n    CreoDIAS --\u003e CSys[\"system/\"]\n    CreoDIAS --\u003e CUM[\"user-management/\"]\n    CreoDIAS --\u003e CRM[\"resource-management/\"]\n    CreoDIAS --\u003e CProc[\"processing-and-chaining/\"]\n    \n    CSys --\u003e FluxSys[\"flux-system/\"]\n    CSys --\u003e FluxExt[\"flux-system-extended/\"]\n    CSys --\u003e Infrastructure[\"infrastructure/\"]\n    \n    FluxSys --\u003e GotKComp[\"gotk-components.yaml\"]\n    FluxSys --\u003e GotKSync[\"gotk-sync.yaml\"]\n    FluxSys --\u003e Kust1[\"kustomization.yaml\"]\n    FluxSys --\u003e Patch[\"flux-system-patch.yaml\"]\n    \n    FluxExt --\u003e UMSync[\"user-management-sync.yaml\"]\n    FluxExt --\u003e RMSync[\"resource-management-sync.yaml\"]\n    FluxExt --\u003e ProcSync[\"processing-and-chaining-sync.yaml\"]\n    \n    CUM --\u003e UMHelmReleases[\"HelmRelease manifests\u003cbr/\u003elogin-service\u003cbr/\u003eidentity-service\u003cbr/\u003epdp-engine\"]\n    CRM --\u003e RMHelmReleases[\"HelmRelease manifests\u003cbr/\u003eworkspace-api\u003cbr/\u003eresource-catalogue\u003cbr/\u003edata-access\"]\n    CProc --\u003e ProcHelmReleases[\"HelmRelease manifests\u003cbr/\u003eades\u003cbr/\u003eapplication-hub\"]\n```\n\nSources: [system/clusters/creodias/system/flux-system/kustomization.yaml:1-8](), [system/clusters/README.md:45-50]()\n\n### Cluster Directory Structure\n\nEach cluster deployment is isolated in its own directory under `system/clusters/\u003ccluster-name\u003e/`. The `creodias` directory represents the main development cluster:\n\n- **`system/flux-system/`**: Contains the core Flux CD installation manifests generated during bootstrap\n- **`system/flux-system-extended/`**: Contains Kustomization resources that reference building block directories\n- **`system/infrastructure/`**: Infrastructure components like ingress-nginx, cert-manager, sealed-secrets\n- **`user-management/`**: HelmRelease manifests for identity and access management components\n- **`resource-management/`**: HelmRelease manifests for data access and cataloging components  \n- **`processing-and-chaining/`**: HelmRelease manifests for ADES and Application Hub components\n\nThis separation allows independent versioning and reconciliation of different subsystems while maintaining a single GitRepository source.\n\nSources: [system/clusters/README.md:45-77](), [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml:1-13]()\n\n## Bootstrap Process\n\nFlux bootstrap initializes the GitOps continuous delivery system in a target Kubernetes cluster. The process establishes the connection between the cluster and the Git repository, installs Flux components, and begins synchronization.\n\n**Flux Bootstrap Sequence**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant FluxCLI as flux CLI\n    participant K8s as Kubernetes Cluster\n    participant Git as GitHub Repository\u003cbr/\u003eEOEPCA/eoepca\n    \n    User-\u003e\u003eFluxCLI: flux bootstrap github\u003cbr/\u003e--owner=EOEPCA\u003cbr/\u003e--repository=eoepca\u003cbr/\u003e--branch=develop\u003cbr/\u003e--path=system/clusters/creodias/system\n    \n    FluxCLI-\u003e\u003eGit: Check repository access\n    Git--\u003e\u003eFluxCLI: Repository accessible\n    \n    FluxCLI-\u003e\u003eK8s: Create flux-system namespace\n    FluxCLI-\u003e\u003eK8s: Install Flux CRDs\u003cbr/\u003e(GitRepository, Kustomization, etc.)\n    FluxCLI-\u003e\u003eK8s: Deploy source-controller\n    FluxCLI-\u003e\u003eK8s: Deploy kustomize-controller\n    FluxCLI-\u003e\u003eK8s: Deploy helm-controller\n    FluxCLI-\u003e\u003eK8s: Deploy notification-controller\n    \n    FluxCLI-\u003e\u003eGit: Commit flux-system manifests\u003cbr/\u003eto system/clusters/creodias/system/flux-system/\n    Git--\u003e\u003eFluxCLI: Commit successful\n    \n    FluxCLI-\u003e\u003eK8s: Create GitRepository/flux-system\u003cbr/\u003epointing to EOEPCA/eoepca\n    FluxCLI-\u003e\u003eK8s: Create Kustomization/flux-system\u003cbr/\u003epath: system/clusters/creodias/system\n    FluxCLI-\u003e\u003eK8s: Create Secret/flux-system\u003cbr/\u003ewith Git credentials\n    \n    K8s-\u003e\u003eK8s: source-controller starts\u003cbr/\u003epolling Git repository every 1m\n    K8s-\u003e\u003eGit: Fetch system/clusters/creodias/system\n    Git--\u003e\u003eK8s: Return manifests\n    \n    K8s-\u003e\u003eK8s: kustomize-controller applies\u003cbr/\u003emanifests to cluster\n    K8s-\u003e\u003eK8s: Flux becomes self-managing\n```\n\nSources: [system/clusters/README.md:9-77](), [system/clusters/creodias/system/flux-system/gotk-sync.yaml:1-28]()\n\n### Bootstrap Script\n\nThe EOEPCA repository includes a wrapper script `deployCluster.sh` that simplifies the bootstrap process:\n\n```bash\n# Environment variables control bootstrap behavior\nBRANCH=${BRANCH:-develop}        # Git branch to synchronize\nTARGET=${TARGET:-minikube}       # Cluster configuration directory\n\n./system/clusters/deployCluster.sh\n```\n\nThe script performs validation, sets up GitHub credentials from `GITHUB_USER` and `GITHUB_TOKEN` environment variables, and executes the appropriate `flux bootstrap` command with organization-specific parameters.\n\nSources: [system/clusters/README.md:56-77](), [README.md:79-86]()\n\n### GitRepository Resource\n\nThe bootstrap process creates a `GitRepository` resource in the `flux-system` namespace that defines the source of truth:\n\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  ref:\n    branch: develop\n  secretRef:\n    name: flux-system\n  url: ssh://git@github.com/EOEPCA/eoepca\n```\n\nThe `interval: 1m0s` field configures source-controller to poll the repository every minute. The `secretRef` points to a Secret containing the SSH deploy key for Git authentication.\n\nSources: [system/clusters/creodias/system/flux-system/gotk-sync.yaml:3-14]()\n\n### Root Kustomization Resource\n\nA root `Kustomization` resource is created to apply manifests from the cluster-specific path:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  path: ./system/clusters/creodias/system\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n```\n\nThe `prune: true` setting enables garbage collection - resources removed from Git are automatically deleted from the cluster. The `interval: 10m0s` is patched from the default to 1m0s in the EOEPCA configuration for faster reconciliation.\n\nSources: [system/clusters/creodias/system/flux-system/gotk-sync.yaml:16-28](), [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:33-40]()\n\n## Synchronization and Reconciliation\n\nFlux continuously monitors the Git repository and reconciles the cluster state to match the declared configuration through a multi-stage reconciliation loop.\n\n**Flux Reconciliation Loop**\n\n```mermaid\ngraph LR\n    subgraph \"source-controller cycle\"\n        Poll[\"Poll GitRepository\u003cbr/\u003eevery interval\"]\n        Fetch[\"Fetch from Git\u003cbr/\u003essh://git@github.com/EOEPCA/eoepca\"]\n        Artifact[\"Create Artifact\u003cbr/\u003ewith revision SHA\"]\n        Store[\"Store in internal\u003cbr/\u003eartifact server\"]\n    end\n    \n    subgraph \"kustomize-controller cycle\"\n        Watch[\"Watch Kustomization\u003cbr/\u003eresources\"]\n        GetArtifact[\"Fetch Artifact\u003cbr/\u003efrom source-controller\"]\n        Build[\"Build Kustomize\u003cbr/\u003eoverlay\"]\n        Validate[\"Validate manifests\u003cbr/\u003eclient-side\"]\n        Apply[\"Apply to Kubernetes\u003cbr/\u003eserver-side apply\"]\n        Health[\"Wait for health\u003cbr/\u003echeck readiness\"]\n    end\n    \n    subgraph \"helm-controller cycle\"\n        WatchHR[\"Watch HelmRelease\u003cbr/\u003eresources\"]\n        GetChart[\"Fetch HelmChart\u003cbr/\u003eArtifact\"]\n        Render[\"Render chart with\u003cbr/\u003evalues\"]\n        Install[\"Helm install/upgrade\u003cbr/\u003ewith atomic flag\"]\n        Test[\"Run Helm tests\u003cbr/\u003eif configured\"]\n    end\n    \n    Poll --\u003e Fetch\n    Fetch --\u003e Artifact\n    Artifact --\u003e Store\n    Store --\u003e Poll\n    \n    Watch --\u003e GetArtifact\n    GetArtifact --\u003e Build\n    Build --\u003e Validate\n    Validate --\u003e Apply\n    Apply --\u003e Health\n    Health --\u003e Watch\n    \n    WatchHR --\u003e GetChart\n    GetChart --\u003e Render\n    Render --\u003e Install\n    Install --\u003e Test\n    Test --\u003e WatchHR\n    \n    Store -.-\u003e|Artifact ready| GetArtifact\n    Store -.-\u003e|Artifact ready| GetChart\n```\n\nSources: [system/clusters/README.md:79-86](), [system/clusters/creodias/system/flux-system/gotk-sync.yaml:9]()\n\n### Reconciliation Intervals\n\nDifferent resources have different reconciliation intervals optimized for their update frequency:\n\n| Resource Type | Interval | Purpose |\n|--------------|----------|---------|\n| GitRepository/flux-system | 1m | Fast detection of Git commits |\n| Kustomization/flux-system | 1m | Quick application of system changes |\n| Kustomization/user-management | 1m | Building block reconciliation |\n| Kustomization/resource-management | 1m | Building block reconciliation |\n| Kustomization/processing-and-chaining | 1m | Building block reconciliation |\n\nThese fast intervals (1 minute) ensure rapid propagation of configuration changes from Git to the cluster. In production environments with stable configurations, longer intervals (e.g., 10m or 1h) can reduce API server load.\n\nSources: [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:33-40](), [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml:7]()\n\n### Drift Detection and Self-Healing\n\nFlux implements continuous drift detection. If a resource in the cluster is manually modified (e.g., via `kubectl edit`), the kustomize-controller detects the difference during the next reconciliation cycle and reverts the change to match the Git state. This ensures the cluster state always converges to the declared configuration.\n\nThe `prune: true` setting enables deletion of resources that are removed from Git, providing complete lifecycle management.\n\nSources: [system/clusters/creodias/system/flux-system/gotk-sync.yaml:24]()\n\n## Building Block Deployment\n\nEOEPCA organizes building blocks into separate Kustomization resources for modular deployment and independent lifecycle management. The `flux-system-extended` directory contains sync manifests for each building block subsystem.\n\n### User Management Sync\n\nThe user-management Kustomization deploys identity and access management components:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: user-management\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  path: ./system/clusters/creodias/user-management\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  validation: client\n```\n\nThis references HelmRelease manifests in `system/clusters/creodias/user-management/` for components like `login-service`, `identity-service`, `pdp-engine`, and `user-profile`. Each HelmRelease specifies a Helm chart source and values overrides.\n\nSources: [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml:1-13]()\n\n### Resource Management Sync\n\nThe resource-management Kustomization deploys data access and cataloging services:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: resource-management\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  path: ./system/clusters/creodias/resource-management\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  validation: client\n```\n\nThis references HelmRelease manifests for `workspace-api`, `resource-catalogue`, `data-access`, `registration-api`, `harvester`, and related services in the `rm` namespace.\n\nSources: [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml:1-13]()\n\n### Processing and Chaining Sync\n\nThe processing-and-chaining Kustomization deploys application execution services:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: processing-and-chaining\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  path: ./system/clusters/creodias/processing-and-chaining\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  validation: client\n```\n\nThis references HelmRelease manifests for `ades`, `application-hub`, and `pde-hub` in the `proc` namespace.\n\nSources: [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml:1-13]()\n\n### Dependency Management\n\nWhile not explicitly configured in the current deployment, Flux Kustomization resources support `dependsOn` fields to enforce deployment ordering. For example, infrastructure components (ingress-nginx, cert-manager) should be deployed before application services:\n\n```yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1beta2\nkind: Kustomization\nmetadata:\n  name: user-management\nspec:\n  dependsOn:\n    - name: infrastructure\n```\n\nThis ensures that required infrastructure is ready before dependent services are deployed.\n\nSources: [system/clusters/creodias/system/flux-system-extended/]()\n\n## HelmRelease Pattern\n\nEOEPCA building blocks are packaged as Helm charts and deployed via Flux `HelmRelease` resources. This pattern provides:\n\n- **Declarative Helm Releases**: Helm releases are declared as Kubernetes resources\n- **Automated Upgrades**: Flux monitors chart versions and performs automated upgrades\n- **Values Management**: Chart values are version-controlled alongside the HelmRelease\n- **Rollback Support**: Failed upgrades can be automatically rolled back\n- **Health Checks**: Flux waits for deployed resources to become ready before marking success\n\nA typical HelmRelease structure references a chart from a GitRepository or HelmRepository source and provides values overrides:\n\n```yaml\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: workspace-api\n  namespace: rm\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: ./charts/workspace-api\n      sourceRef:\n        kind: GitRepository\n        name: rm-workspace-api\n        namespace: flux-system\n      interval: 1m\n  values:\n    # Values overrides specific to this deployment\n    ingress:\n      enabled: true\n      hosts:\n        - host: workspace-api.185-52-193-87.nip.io\n```\n\nWhen the chart version increments in the GitRepository, Flux detects the change and performs a Helm upgrade operation.\n\nSources: [system/clusters/README.md:82-86]()\n\n## Multiple Cluster Management\n\nThe repository structure supports managing multiple independent clusters by isolating each cluster's configuration in separate directories under `system/clusters/`. Each cluster directory represents a complete, independent deployment.\n\n**Multi-Cluster Configuration Structure**\n\n```mermaid\ngraph TB\n    Repo[EOEPCA/eoepca Repository]\n    \n    Repo --\u003e ClustersDir[\"system/clusters/\"]\n    \n    ClustersDir --\u003e C1[\"creodias/\u003cbr/\u003eProduction Cluster\"]\n    ClustersDir --\u003e C2[\"develop/\u003cbr/\u003eDevelopment Cluster\"]\n    ClustersDir --\u003e C3[\"minikube/\u003cbr/\u003eLocal Testing\"]\n    ClustersDir --\u003e C4[\"demo/\u003cbr/\u003eDemo Environment\"]\n    \n    C1 --\u003e C1Config[\"system/flux-system/gotk-sync.yaml\u003cbr/\u003epath: system/clusters/creodias/system\"]\n    C2 --\u003e C2Config[\"system/flux-system/gotk-sync.yaml\u003cbr/\u003epath: system/clusters/develop/system\"]\n    C3 --\u003e C3Config[\"system/flux-system/gotk-sync.yaml\u003cbr/\u003epath: system/clusters/minikube/system\"]\n    \n    C1 --\u003e C1IP[\"Public IP: 185.52.193.87\u003cbr/\u003eIngress hosts: *.185-52-193-87.nip.io\"]\n    C2 --\u003e C2IP[\"Different IP configuration\u003cbr/\u003edevelop.eoepca.org domain\"]\n    C3 --\u003e C3IP[\"Minikube IP: 192.168.49.2\u003cbr/\u003e*.192-168-49-2.nip.io\"]\n    \n    C1Config -.-\u003e C1Deployments[\"user-management/\u003cbr/\u003eresource-management/\u003cbr/\u003eprocessing-and-chaining/\"]\n    C2Config -.-\u003e C2Deployments[\"Different versions/\u003cbr/\u003econfigurations\"]\n    C3Config -.-\u003e C3Deployments[\"Minimal deployment\u003cbr/\u003efor testing\"]\n```\n\nSources: [system/clusters/README.md:45-77](), [README.md:100-111]()\n\n### Cluster Configuration Customization\n\nEach cluster directory contains environment-specific configuration:\n\n- **Public IP addresses** embedded in ingress hostnames (e.g., `185.52.193.87.nip.io` for creodias)\n- **DNS domains** for clusters with real DNS (e.g., `develop.eoepca.org`, `demo.eoepca.org`)\n- **Resource limits** tuned for cluster capacity\n- **Component versions** allowing different clusters to run different releases\n- **Feature flags** enabling/disabling specific building blocks\n\nTo add a new cluster, copy an existing cluster directory (e.g., `creodias` to `new-cluster`) and customize the configuration for the new environment. During bootstrap, specify the new path:\n\n```bash\nBRANCH=develop TARGET=new-cluster ./system/clusters/deployCluster.sh\n```\n\nThis creates an independent Flux installation that synchronizes only the `system/clusters/new-cluster` directory.\n\nSources: [system/clusters/README.md:45-50](), [README.md:100-107]()\n\n## Flux Management Operations\n\n### Checking Flux Status\n\nVerify Flux component health:\n\n```bash\nflux check\n```\n\nList all Flux resources:\n\n```bash\nflux get all -A\n```\n\nView GitRepository synchronization status:\n\n```bash\nflux get sources git -A\n```\n\nView Kustomization reconciliation status:\n\n```bash\nflux get kustomizations -A\n```\n\nSources: [system/clusters/README.md:24-29]()\n\n### Forcing Reconciliation\n\nManually trigger reconciliation without waiting for the interval:\n\n```bash\nflux reconcile source git flux-system\nflux reconcile kustomization flux-system\n```\n\nThis is useful after pushing changes to Git to see immediate effects.\n\nSources: [system/clusters/README.md:79-86]()\n\n### Suspending Reconciliation\n\nTemporarily suspend automatic reconciliation for maintenance:\n\n```bash\nflux suspend kustomization user-management\n```\n\nResume reconciliation:\n\n```bash\nflux resume kustomization user-management\n```\n\nSources: [system/clusters/README.md:79-95]()\n\n### Undeploying Flux\n\nThe `undeployCluster.sh` script removes Flux controllers from the cluster:\n\n```bash\n./system/clusters/undeployCluster.sh\n```\n\nNote that this only removes the Flux controllers themselves. Resources deployed by Flux (building blocks, infrastructure) remain in the cluster and must be cleaned up separately if desired.\n\nSources: [system/clusters/README.md:88-95]()\n\n## Integration with Other Systems\n\nFlux CD serves as the deployment foundation for all EOEPCA building blocks:\n\n- **Infrastructure Components**: cert-manager, ingress-nginx, sealed-secrets are deployed first via Flux\n- **User Management**: Keycloak identity-service, login-service, pdp-engine deployed via HelmReleases\n- **Resource Management**: workspace-api, resource-catalogue, data-access deployed via HelmReleases\n- **Processing**: ADES, application-hub, pde-hub deployed via HelmReleases\n\nThe GitOps model ensures that all components maintain consistent configuration across the cluster lifecycle. Changes to component versions, configuration values, or resource specifications are version-controlled in Git and automatically applied by Flux.\n\nFor detailed configuration of specific building blocks, see:\n- [Identity Service (Keycloak)](#4.1)\n- [Workspace API](#5.3)\n- [ADES](#6.1)\n- [Kubernetes Cluster Setup](#8.1)\n\nSources: [README.md:72-98](), [system/clusters/README.md:1-95]()"])</script><script>self.__next_f.push([1,"1f:T5bec,"])</script><script>self.__next_f.push([1,"# Network and Ingress\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n- [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml](system/clusters/creodias/system/demo/hr-eoepca-portal.yaml)\n- [system/clusters/creodias/system/demo/ss-django-secrets-create.sh](system/clusters/creodias/system/demo/ss-django-secrets-create.sh)\n- [system/clusters/creodias/system/demo/ss-django-secrets.yaml](system/clusters/creodias/system/demo/ss-django-secrets.yaml)\n- [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml](system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml)\n- [system/clusters/creodias/user-management/um-identity-service.yaml](system/clusters/creodias/user-management/um-identity-service.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the network ingress architecture of the EOEPCA platform, including external access patterns, TLS certificate management, domain naming conventions, and the integration of authentication at the ingress layer. It covers how services are exposed externally through the nginx ingress controller and how the multi-tenant workspace architecture impacts ingress configuration.\n\nFor information about identity and authentication flows beyond the ingress layer, see [User Management and Identity](#4). For details on the GitOps deployment of infrastructure components including ingress, see [GitOps and Flux CD](#3.2). For the broader system architecture context, see [Building Blocks Overview](#3.1).\n\n## Ingress Architecture Overview\n\nAll external traffic to the EOEPCA platform flows through a centralized nginx ingress controller deployed in the Kubernetes cluster. The ingress layer provides TLS termination, routing to backend services, and integration with authentication mechanisms.\n\n```mermaid\ngraph TB\n    subgraph \"External\"\n        User[\"External User/Client\"]\n        DNS[\"DNS\u003cbr/\u003e*.develop.eoepca.org\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Ingress Layer\"\n            IngressCtrl[\"nginx-ingress-controller\u003cbr/\u003eLoadBalancer Service\"]\n            CertMgr[\"cert-manager\u003cbr/\u003eTLS Certificate Management\"]\n            LetsEncrypt[\"Let's Encrypt\u003cbr/\u003eACME CA\"]\n        end\n        \n        subgraph \"Global Services (rm namespace)\"\n            DataAccess[\"data-access\u003cbr/\u003eService\"]\n            WorkspaceAPI[\"workspace-api\u003cbr/\u003eService\"]\n            ResourceCat[\"resource-catalogue\u003cbr/\u003eService\"]\n            RegistrationAPI[\"registration-api\u003cbr/\u003eService\"]\n        end\n        \n        subgraph \"User Management (um namespace)\"\n            IdentityKeycloak[\"identity-keycloak\u003cbr/\u003eService\"]\n            IdentityAPI[\"identity-api\u003cbr/\u003eService\"]\n            IdentityMgr[\"identity-manager\u003cbr/\u003eService\"]\n            IdentityGK[\"identity-gatekeeper\u003cbr/\u003eService\"]\n        end\n        \n        subgraph \"Workspace: user-ws\"\n            WSDataAccess[\"data-access\u003cbr/\u003eService\"]\n            WSResourceCat[\"resource-catalogue\u003cbr/\u003eService\"]\n            WSGuard[\"resource-guard\u003cbr/\u003ePEP Service\"]\n        end\n    end\n    \n    User --\u003e|HTTPS Request| DNS\n    DNS --\u003e|Route to| IngressCtrl\n    \n    IngressCtrl --\u003e|\"data-access.develop.eoepca.org\"| DataAccess\n    IngressCtrl --\u003e|\"workspace-api-open.develop.eoepca.org\"| WorkspaceAPI\n    IngressCtrl --\u003e|\"registration-api-open.develop.eoepca.org\"| RegistrationAPI\n    IngressCtrl --\u003e|\"identity.keycloak.develop.eoepca.org\"| IdentityKeycloak\n    IngressCtrl --\u003e|\"identity.api.develop.eoepca.org\"| IdentityAPI\n    IngressCtrl --\u003e|\"identity.manager.develop.eoepca.org\"| IdentityMgr\n    IngressCtrl --\u003e|\"identity.gatekeeper.develop.eoepca.org\"| IdentityGK\n    IngressCtrl --\u003e|\"data-access.user-ws.develop.eoepca.org\"| WSGuard\n    IngressCtrl --\u003e|\"resource-catalogue.user-ws.develop.eoepca.org\"| WSGuard\n    \n    WSGuard --\u003e|Forward Authorized| WSDataAccess\n    WSGuard --\u003e|Forward Authorized| WSResourceCat\n    \n    CertMgr -.-\u003e|Obtain Certificates| LetsEncrypt\n    CertMgr -.-\u003e|Provision TLS Secrets| IngressCtrl\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:35-47](), [system/clusters/creodias/user-management/um-identity-service.yaml:23-76](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:24-32]()\n\n## Ingress Controller Configuration\n\nThe platform uses the **nginx ingress controller** as the standard ingress implementation. All Ingress resources specify the nginx ingress class through annotations.\n\n### Ingress Class Annotation\n\nServices configure the ingress class using the `kubernetes.io/ingress.class` annotation:\n\n```yaml\nannotations:\n  kubernetes.io/ingress.class: nginx\n```\n\nSome newer configurations use the `ingressClassName` field instead:\n\n```yaml\nspec:\n  ingressClassName: nginx\n```\n\n### Key nginx Annotations\n\nThe platform uses several nginx-specific annotations for advanced routing and request handling:\n\n| Annotation | Purpose | Example Usage |\n|------------|---------|---------------|\n| `nginx.ingress.kubernetes.io/proxy-read-timeout` | Extended timeout for long-running requests | `\"600\"` for Data Access rendering |\n| `nginx.ingress.kubernetes.io/enable-cors` | Enable CORS headers | `\"true\"` for API services |\n| `nginx.ingress.kubernetes.io/configuration-snippet` | Custom nginx configuration for auth_request | Auth integration with gatekeeper |\n| `nginx.ingress.kubernetes.io/server-snippet` | Custom server block configuration | Internal auth location definitions |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:36-40](), [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31](), [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml:24-25]()\n\n## TLS Certificate Management\n\nAll services exposed through ingress use TLS encryption with certificates automatically provisioned and renewed by **cert-manager**.\n\n### Certificate Issuer\n\nThe platform uses a Let's Encrypt cluster issuer named `letsencrypt` configured for ACME certificate provisioning:\n\n```yaml\nannotations:\n  cert-manager.io/cluster-issuer: letsencrypt\n```\n\n### TLS Configuration Pattern\n\nEach ingress specifies TLS configuration with hostnames and secret names:\n\n```yaml\ntls:\n  - hosts:\n      - service-name.develop.eoepca.org\n    secretName: service-name-tls\n```\n\ncert-manager automatically creates and manages the referenced secrets, populating them with valid TLS certificates obtained from Let's Encrypt.\n\n```mermaid\nsequenceDiagram\n    participant Ingress as \"Ingress Resource\"\n    participant CertMgr as \"cert-manager\"\n    participant ACME as \"Let's Encrypt ACME\"\n    participant Secret as \"TLS Secret\"\n    participant nginx as \"nginx Ingress Controller\"\n    \n    Ingress-\u003e\u003eCertMgr: Annotation: cert-manager.io/cluster-issuer\n    CertMgr-\u003e\u003eCertMgr: Detect new certificate request\n    CertMgr-\u003e\u003eACME: Initiate ACME challenge\n    ACME-\u003e\u003eCertMgr: HTTP-01 challenge\n    CertMgr-\u003e\u003eACME: Complete challenge\n    ACME-\u003e\u003eCertMgr: Issue certificate\n    CertMgr-\u003e\u003eSecret: Create/Update TLS secret\n    nginx-\u003e\u003eSecret: Mount certificate\n    nginx-\u003e\u003enginx: Configure TLS for host\n    \n    Note over CertMgr,ACME: Automatic renewal before expiry\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:41,44-47](), [system/clusters/creodias/user-management/um-identity-service.yaml:25,31-34](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:29-32]()\n\n## Domain Naming Conventions\n\nThe EOEPCA platform follows consistent domain naming patterns to distinguish between global services, workspace-specific services, and different components within a service.\n\n### Global Service Pattern\n\nGlobal services accessible platform-wide use the pattern:\n\n```\n\u003cservice-name\u003e.develop.eoepca.org\n```\n\nExamples:\n- `data-access.develop.eoepca.org` - Global Data Access service\n- `workspace-api-open.develop.eoepca.org` - Workspace API\n- `registration-api-open.develop.eoepca.org` - Registration API\n- `resource-catalogue.develop.eoepca.org` - Global Resource Catalogue\n\n### Identity Service Component Pattern\n\nIdentity services use a three-part naming scheme to expose multiple components:\n\n```\nidentity.\u003ccomponent\u003e.develop.eoepca.org\n```\n\nExamples:\n- `identity.keycloak.develop.eoepca.org` - Keycloak authentication server\n- `identity.api.develop.eoepca.org` - Identity management API\n- `identity.manager.develop.eoepca.org` - Identity management UI\n- `identity.gatekeeper.develop.eoepca.org` - Gatekeeper proxy service\n\n### Workspace-Specific Service Pattern\n\nUser workspace services use the pattern:\n\n```\n\u003cservice-name\u003e.\u003cworkspace-name\u003e.develop.eoepca.org\n```\n\nExamples:\n- `data-access.eric-workspace.develop.eoepca.org` - Eric's Data Access instance\n- `resource-catalogue.alice-workspace.develop.eoepca.org` - Alice's Resource Catalogue\n\n**Domain Naming Convention Table**\n\n| Service Type | Pattern | Example |\n|--------------|---------|---------|\n| Global Service | `\u003cservice\u003e.develop.eoepca.org` | `data-access.develop.eoepca.org` |\n| Identity Component | `identity.\u003ccomponent\u003e.develop.eoepca.org` | `identity.keycloak.develop.eoepca.org` |\n| Workspace Service | `\u003cservice\u003e.\u003cworkspace\u003e.develop.eoepca.org` | `data-access.user-ws.develop.eoepca.org` |\n| Demo/Portal | `\u003capp\u003e.develop.eoepca.org` | `eoepca-portal.develop.eoepca.org` |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:43](), [system/clusters/creodias/user-management/um-identity-service.yaml:27,43,56,69](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:28]()\n\n## Global Service Ingress Configuration\n\nGlobal services are deployed in the `rm` (Resource Management) or `um` (User Management) namespaces with ingress resources that expose them at the top-level domain.\n\n### Data Access Service Ingress\n\nThe global Data Access service provides a comprehensive example of ingress configuration:\n\n```yaml\nglobal:\n  ingress:\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/tls-acme: \"true\"\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      cert-manager.io/cluster-issuer: letsencrypt\n    hosts:\n      - host: data-access.develop.eoepca.org\n    tls:\n      - hosts:\n          - data-access.develop.eoepca.org\n        secretName: data-access-tls\n```\n\nKey features:\n- **Extended timeout**: 600 seconds to support long-running rendering operations\n- **CORS enabled**: Allows cross-origin API requests from web clients\n- **Automatic TLS**: cert-manager provisions certificates\n- **Single host**: One hostname per service\n\n### Workspace API Ingress\n\nThe Workspace API orchestrates workspace provisioning and uses a simplified ingress pattern:\n\n```yaml\ningress:\n  enabled: true\n  hosts:\n    - host: workspace-api-open.develop.eoepca.org\n      paths: [\"/\"]\n  tls:\n    - hosts:\n        - workspace-api-open.develop.eoepca.org\n      secretName: workspace-api-tls\n```\n\nThe `-open` suffix indicates this endpoint does not require authentication at the ingress level, though the service itself may enforce authorization.\n\n### Identity Service Multi-Component Ingress\n\nThe Identity Service deploys multiple ingress resources for different components, each with dedicated hostnames:\n\n**Keycloak Component:**\n```yaml\nidentity-keycloak:\n  ingress:\n    annotations:\n      cert-manager.io/cluster-issuer: letsencrypt\n    hosts:\n      - host: identity.keycloak.develop.eoepca.org\n        paths:\n          - path: /\n            pathType: Prefix\n    tls:\n      - secretName: identity-keycloak-tls-certificate\n        hosts:\n          - identity.keycloak.develop.eoepca.org\n```\n\nThis pattern repeats for `identity-api`, `identity-manager`, and `identity-gatekeeper`, each with unique hostnames and TLS secrets.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:35-47](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:24-32](), [system/clusters/creodias/user-management/um-identity-service.yaml:23-76](), [system/clusters/creodias/resource-management/hr-registration-api.yaml:24-32]()\n\n## Workspace-Specific Ingress Architecture\n\nThe multi-tenant workspace architecture uses a template-based approach to generate unique ingress configurations for each user workspace. However, workspace services typically have ingress disabled at the service level because they are protected by a dedicated resource-guard (PEP) that handles ingress and authorization.\n\n### Template-Based Ingress Generation\n\nWorkspace ingress templates use placeholder substitution for dynamic values:\n\n```yaml\nglobal:\n  ingress:\n    enabled: false\n    tls:\n      - hosts:\n          - data-access.{{ workspace_name }}.develop.eoepca.org\n```\n\nThe `{{ workspace_name }}` placeholder is replaced with the actual workspace identifier (e.g., `eric-workspace`) during provisioning.\n\n### Workspace Ingress Disabled Pattern\n\nMost workspace service templates have `ingress.enabled: false` because they are accessed through the resource-guard:\n\n```yaml\nvs:\n  renderer:\n    ingress:\n      enabled: false\n  client:\n    ingress:\n      enabled: false\n```\n\nThe resource-guard service acts as a PEP (Policy Enforcement Point) and provides the actual ingress endpoint for the workspace.\n\n### Workspace Service Access Flow\n\n```mermaid\ngraph LR\n    User[\"User Request\u003cbr/\u003edata-access.user-ws\u003cbr/\u003e.develop.eoepca.org\"]\n    Ingress[\"nginx Ingress\u003cbr/\u003eController\"]\n    Guard[\"resource-guard\u003cbr/\u003ePEP Service\u003cbr/\u003e(user-ws namespace)\"]\n    DataAccess[\"data-access\u003cbr/\u003erenderer/client\u003cbr/\u003e(user-ws namespace)\"]\n    PDP[\"PDP Engine\u003cbr/\u003ePolicy Decision\"]\n    Identity[\"Identity Service\u003cbr/\u003eToken Validation\"]\n    \n    User --\u003e|HTTPS Request| Ingress\n    Ingress --\u003e|Route by hostname| Guard\n    Guard --\u003e|Check Policy| PDP\n    Guard --\u003e|Validate Token| Identity\n    Guard --\u003e|If Authorized| DataAccess\n    DataAccess --\u003e|Response| Guard\n    Guard --\u003e|Response| User\n```\n\n### Template Configuration Examples\n\n**Data Access Template:**\n- **File:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:24-31]()\n- **Pattern:** `data-access.{{ workspace_name }}.develop.eoepca.org`\n- **Ingress:** Disabled - protected by resource-guard\n\n**Resource Catalogue Template:**\n- **File:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:20-25]()\n- **Pattern:** `resource-catalogue.{{ workspace_name }}.develop.eoepca.org`\n- **Ingress:** Disabled - protected by resource-guard\n\n### Dynamic Values in Templates\n\nTemplates substitute several dynamic values during workspace provisioning:\n\n| Template Variable | Example Value | Usage |\n|-------------------|---------------|-------|\n| `{{ workspace_name }}` | `eric-workspace` | Hostname, namespace references |\n| `{{ bucket }}` | `eric-workspace-bucket` | S3 bucket references |\n| `{{ access_key_id }}` | `\u003cgenerated\u003e` | S3 credentials |\n| `{{ secret_access_key }}` | `\u003cgenerated\u003e` | S3 credentials |\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:24-31,40-42,111-112](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:20-33]()\n\n## Authentication Integration at Ingress Layer\n\nSome services integrate authentication directly at the nginx ingress layer using the `auth_request` mechanism, which delegates authentication decisions to a separate service before forwarding requests to the backend.\n\n### Auth Request Pattern\n\nThe auth_request pattern is implemented using nginx configuration snippets:\n\n```mermaid\nsequenceDiagram\n    participant Client as \"External Client\"\n    participant nginx as \"nginx Ingress\"\n    participant Gatekeeper as \"identity-gatekeeper\u003cbr/\u003eService\"\n    participant Backend as \"Protected Backend\u003cbr/\u003eService\"\n    \n    Client-\u003e\u003enginx: Request with credentials\n    nginx-\u003e\u003enginx: Check request method\n    alt OPTIONS (preflight)\n        nginx-\u003e\u003eClient: 200 OK (CORS)\n    else Other methods\n        nginx-\u003e\u003eGatekeeper: /auth subrequest\n        Gatekeeper-\u003e\u003eGatekeeper: Validate token\n        alt Valid token\n            Gatekeeper-\u003e\u003enginx: 200 OK\n            nginx-\u003e\u003eBackend: Forward request\n            Backend-\u003e\u003enginx: Response\n            nginx-\u003e\u003eClient: Response + CORS headers\n        else Invalid token\n            Gatekeeper-\u003e\u003enginx: 401/403\n            nginx-\u003e\u003eClient: 401/403\n        end\n    end\n```\n\n### Configuration Snippet Implementation\n\nThe identity dummy service demonstrates the auth_request integration pattern:\n\n```yaml\nannotations:\n  nginx.ingress.kubernetes.io/configuration-snippet: |\n    auth_request /auth;\n    # Preflighted requests\n    if ($request_method = OPTIONS ) {\n      return 200;\n    }\n    add_header Access-Control-Allow-Origin $http_origin always;\n    add_header Access-Control-Allow-Methods \"*\";\n    add_header Access-Control-Allow-Headers \"Authorization, Origin, Content-Type\";\n  nginx.ingress.kubernetes.io/server-snippet: |\n    location ^~ /auth {\n      internal;\n      proxy_pass http://identity-gatekeeper.um.svc.cluster.local:3000/$request_uri;\n      proxy_pass_request_body off;\n      proxy_set_header Content-Length \"\";\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-Host $host;\n      proxy_set_header X-Forwarded-Method $request_method;\n      proxy_set_header X-Forwarded-URI $request_uri;\n      proxy_busy_buffers_size 64k;\n      proxy_buffers 8 32k;\n      proxy_buffer_size 32k;\n    }\n```\n\n### Auth Request Components\n\n**Configuration Snippet:**\n- Invokes `auth_request /auth` for all requests\n- Handles CORS preflight OPTIONS requests\n- Adds CORS headers to responses\n\n**Server Snippet:**\n- Defines internal `/auth` location\n- Proxies to `identity-gatekeeper.um.svc.cluster.local:3000`\n- Forwards request metadata without body\n- Configures buffer sizes for large auth responses\n\n### Gatekeeper Service Reference\n\nThe auth_request delegates to the identity-gatekeeper service using cluster-internal DNS:\n\n```\nhttp://identity-gatekeeper.um.svc.cluster.local:3000/$request_uri\n```\n\nComponents of the service URL:\n- **Service name:** `identity-gatekeeper`\n- **Namespace:** `um` (User Management)\n- **Cluster domain:** `svc.cluster.local`\n- **Port:** `3000`\n- **Path:** Original request URI passed through\n\n**Sources:** [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31]()\n\n## CORS Configuration\n\nCross-Origin Resource Sharing (CORS) is configured at the ingress level to support web-based clients accessing EOEPCA APIs.\n\n### CORS Enablement\n\nServices that require CORS support use the nginx annotation:\n\n```yaml\nannotations:\n  nginx.ingress.kubernetes.io/enable-cors: \"true\"\n```\n\nThis enables basic CORS headers for cross-origin requests.\n\n### Advanced CORS Headers\n\nFor more granular control, services use configuration snippets to add specific CORS headers:\n\n```yaml\nadd_header Access-Control-Allow-Origin $http_origin always;\nadd_header Access-Control-Allow-Methods \"*\";\nadd_header Access-Control-Allow-Headers \"Authorization, Origin, Content-Type\";\n```\n\n### Preflight Request Handling\n\nOPTIONS preflight requests are handled explicitly to avoid unnecessary auth_request calls:\n\n```yaml\nif ($request_method = OPTIONS ) {\n  return 200;\n}\n```\n\nThis ensures preflight requests return immediately with appropriate CORS headers.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:40](), [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:12-17]()\n\n## Ingress Resource Mapping\n\nThe following table maps EOEPCA services to their ingress configurations:\n\n| Service | Hostname | Namespace | Protected | TLS Secret | File Reference |\n|---------|----------|-----------|-----------|------------|----------------|\n| Data Access (Global) | `data-access.develop.eoepca.org` | `rm` | No | `data-access-tls` | [hr-data-access.yaml:43-47]() |\n| Workspace API | `workspace-api-open.develop.eoepca.org` | `rm` | No | `workspace-api-tls` | [hr-workspace-api.yaml:27-32]() |\n| Registration API | `registration-api-open.develop.eoepca.org` | `rm` | No | `registration-api-tls` | [hr-registration-api.yaml:27-32]() |\n| Resource Catalogue | Disabled | `rm` | No | N/A | [hr-resource-catalogue.yaml:32-37]() |\n| Identity Keycloak | `identity.keycloak.develop.eoepca.org` | `um` | No | `identity-keycloak-tls-certificate` | [um-identity-service.yaml:27-34]() |\n| Identity API | `identity.api.develop.eoepca.org` | `um` | No | `identity-api-tls-certificate` | [um-identity-service.yaml:43-50]() |\n| Identity Manager | `identity.manager.develop.eoepca.org` | `um` | No | `identity-manager-tls-certificate` | [um-identity-service.yaml:56-63]() |\n| Identity Gatekeeper | `identity.gatekeeper.develop.eoepca.org` | `um` | No | `identity-gatekeeper-tls-certificate` | [um-identity-service.yaml:69-76]() |\n| EOEPCA Portal | `eoepca-portal.develop.eoepca.org` | `demo` | Yes | `eoepca-portal-tls-certificate` | [hr-eoepca-portal.yaml:28-35]() |\n| Workspace Data Access | `data-access.\u003cworkspace\u003e.develop.eoepca.org` | `\u003cworkspace\u003e` | Yes | Via resource-guard | [template-hr-data-access.yaml:28]() |\n| Workspace Catalogue | `resource-catalogue.\u003cworkspace\u003e.develop.eoepca.org` | `\u003cworkspace\u003e` | Yes | Via resource-guard | [template-hr-resource-catalogue.yaml:33]() |\n\n**Sources:** All files listed in table references\n\n## Configuration Patterns Summary\n\n### Standard Ingress Pattern\n\nAll EOEPCA services follow this general ingress structure:\n\n```yaml\ningress:\n  enabled: true\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    cert-manager.io/cluster-issuer: letsencrypt\n    # Additional service-specific annotations\n  hosts:\n    - host: service-name.develop.eoepca.org\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n    - hosts:\n        - service-name.develop.eoepca.org\n      secretName: service-name-tls\n```\n\n### Path-Based Routing\n\nServices use `Prefix` path type for flexible routing:\n- `/` - Routes all paths to the service\n- Allows services to handle internal routing\n\n### TLS Secret Naming Convention\n\nTLS secrets follow a consistent naming pattern:\n- Format: `\u003cservice-name\u003e-tls` or `\u003cservice-name\u003e-tls-certificate`\n- Automatically managed by cert-manager\n- Stored in the same namespace as the Ingress resource\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:35-47](), [system/clusters/creodias/user-management/um-identity-service.yaml:23-76](), [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml:22-35]()"])</script><script>self.__next_f.push([1,"20:T573b,"])</script><script>self.__next_f.push([1,"# User Management and Identity\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/system/demo/hr-django-portal.yaml](system/clusters/creodias/system/demo/hr-django-portal.yaml)\n- [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml](system/clusters/creodias/system/demo/hr-eoepca-portal.yaml)\n- [system/clusters/creodias/system/demo/ss-django-secrets-create.sh](system/clusters/creodias/system/demo/ss-django-secrets-create.sh)\n- [system/clusters/creodias/system/demo/ss-django-secrets.yaml](system/clusters/creodias/system/demo/ss-django-secrets.yaml)\n- [system/clusters/creodias/system/storage/hr-storage.yaml](system/clusters/creodias/system/storage/hr-storage.yaml)\n- [system/clusters/creodias/system/test/hr-cheese.yaml](system/clusters/creodias/system/test/hr-cheese.yaml)\n- [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml](system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml)\n- [system/clusters/creodias/user-management/um-identity-service.yaml](system/clusters/creodias/user-management/um-identity-service.yaml)\n- [system/clusters/creodias/user-management/um-login-service.yaml](system/clusters/creodias/user-management/um-login-service.yaml)\n- [system/clusters/creodias/user-management/um-pdp-engine.yaml](system/clusters/creodias/user-management/um-pdp-engine.yaml)\n- [system/clusters/creodias/user-management/um-user-profile.yaml](system/clusters/creodias/user-management/um-user-profile.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the User Management and Identity building block of the EOEPCA platform, which provides authentication and authorization capabilities for all platform services. The system implements User-Managed Access (UMA) 2.0 flows using OpenID Connect (OIDC) and enables fine-grained policy-based access control.\n\nFor detailed information on specific components, see:\n- Identity Service (Keycloak) implementation: [Identity Service (Keycloak)](#4.1)\n- Login Service (Gluu) implementation: [Login Service (Gluu)](#4.2)\n- Policy enforcement architecture: [Policy Enforcement (PEP/PDP)](#4.3)\n- Authentication flow details: [UMA Authentication Flow](#4.4)\n\n## Architecture Overview\n\nThe User Management building block consists of four primary subsystems deployed in the `um` namespace:\n\n| Component | Helm Chart | Primary Function |\n|-----------|------------|------------------|\n| Identity Service | `identity-service` | Keycloak-based identity provider, token management, UMA resource server |\n| Login Service | `login-service` | Gluu-based authentication frontend with LDAP user directory |\n| PDP Engine | `pdp-engine` | Policy Decision Point for authorization decisions |\n| User Profile | `user-profile` | User metadata and profile management |\n\n```mermaid\ngraph TB\n    subgraph \"um namespace\"\n        subgraph \"Identity Service\"\n            Keycloak[\"identity-keycloak\u003cbr/\u003e(Pod)\"]\n            IdentityAPI[\"identity-api\u003cbr/\u003e(Pod)\"]\n            IdentityMgr[\"identity-manager\u003cbr/\u003e(Pod)\"]\n            IdentityGK[\"identity-gatekeeper\u003cbr/\u003e(Pod)\"]\n            IdentityDB[(\"identity-postgres\u003cbr/\u003e(StatefulSet)\")]\n        end\n        \n        subgraph \"Login Service\"\n            OpenDJ[\"opendj\u003cbr/\u003e(LDAP Directory)\"]\n            OxAuth[\"oxauth\u003cbr/\u003e(Authentication)\"]\n            OxTrust[\"oxtrust\u003cbr/\u003e(Admin UI)\"]\n            OxPassport[\"oxpassport\u003cbr/\u003e(Federation)\"]\n        end\n        \n        PDP[\"pdp-engine\u003cbr/\u003e(Pod)\"]\n        UserProfile[\"user-profile\u003cbr/\u003e(Pod)\"]\n    end\n    \n    subgraph \"External Access\"\n        UserRequest[\"User Request\"]\n        ServiceRequest[\"Service Request\"]\n    end\n    \n    subgraph \"Protected Services\"\n        PEP1[\"PEP (Ingress)\u003cbr/\u003eADES\"]\n        PEP2[\"PEP (Ingress)\u003cbr/\u003eWorkspace API\"]\n        PEP3[\"PEP (Ingress)\u003cbr/\u003eResource Catalogue\"]\n    end\n    \n    UserRequest --\u003e|\"auth.develop.eoepca.org\"| OxAuth\n    OxAuth --\u003e|\"Validate Credentials\"| OpenDJ\n    OxAuth --\u003e|\"Issue ID Token\"| UserRequest\n    \n    ServiceRequest --\u003e|\"Protected Resource\"| PEP1\n    PEP1 --\u003e|\"Get UMA Ticket\"| Keycloak\n    PEP1 --\u003e|\"Check Policy\"| PDP\n    PDP --\u003e|\"Validate Token\"| Keycloak\n    \n    IdentityAPI --\u003e|\"Manage Resources\"| Keycloak\n    IdentityMgr --\u003e|\"Admin Interface\"| Keycloak\n    IdentityGK --\u003e|\"Auth Proxy\"| Keycloak\n    Keycloak --\u003e|\"Store Data\"| IdentityDB\n    \n    PDP --\u003e|\"Fetch User Data\"| UserProfile\n    UserProfile --\u003e|\"Sync with\"| OpenDJ\n```\n\n**Sources:** [system/clusters/creodias/user-management/um-identity-service.yaml:1-78](), [system/clusters/creodias/user-management/um-login-service.yaml:1-68](), [system/clusters/creodias/user-management/um-pdp-engine.yaml:1-28](), [system/clusters/creodias/user-management/um-user-profile.yaml:1-24]()\n\n## Identity Service Components\n\nThe Identity Service is deployed via the `um-identity-service` HelmRelease and consists of multiple sub-components:\n\n### Keycloak Identity Provider\n\nThe core identity provider is accessible at `identity.keycloak.develop.eoepca.org` and provides:\n- UMA 2.0 resource server capabilities\n- Token issuance and validation (ID tokens, access tokens, RPTs)\n- Client registration and management\n- Realm and user federation\n\n```mermaid\ngraph LR\n    subgraph \"identity-service Helm Chart\"\n        Keycloak[\"identity-keycloak\u003cbr/\u003eContainer: keycloak\"]\n        API[\"identity-api\u003cbr/\u003eContainer: identity-api\"]\n        Manager[\"identity-manager\u003cbr/\u003eContainer: identity-manager\"]\n        Gatekeeper[\"identity-gatekeeper\u003cbr/\u003eContainer: gatekeeper\"]\n        Postgres[\"identity-postgres\u003cbr/\u003eContainer: postgresql\"]\n    end\n    \n    subgraph \"Ingress Routes\"\n        IngressKC[\"identity.keycloak.develop.eoepca.org\u003cbr/\u003eTLS: identity-keycloak-tls-certificate\"]\n        IngressAPI[\"identity.api.develop.eoepca.org\u003cbr/\u003eTLS: identity-api-tls-certificate\"]\n        IngressMgr[\"identity.manager.develop.eoepca.org\u003cbr/\u003eTLS: identity-manager-tls-certificate\"]\n        IngressGK[\"identity.gatekeeper.develop.eoepca.org\u003cbr/\u003eTLS: identity-gatekeeper-tls-certificate\"]\n    end\n    \n    subgraph \"Storage\"\n        PVC[\"eoepca-userman-pvc\u003cbr/\u003e(NFS PersistentVolumeClaim)\"]\n    end\n    \n    IngressKC --\u003e|\"Route /auth/*\"| Keycloak\n    IngressAPI --\u003e|\"Route /\"| API\n    IngressMgr --\u003e|\"Route /\"| Manager\n    IngressGK --\u003e|\"Route /\"| Gatekeeper\n    \n    Keycloak --\u003e|\"Store realm config\"| Postgres\n    Keycloak --\u003e|\"Persistent data\"| PVC\n    API --\u003e|\"REST API to\"| Keycloak\n    Manager --\u003e|\"Web UI for\"| Keycloak\n    Gatekeeper --\u003e|\"Auth proxy to\"| Keycloak\n    Postgres --\u003e|\"Mount volume\"| PVC\n```\n\n**Configuration Details:**\n\n| Component | Hostname | Purpose |\n|-----------|----------|---------|\n| `identity-keycloak` | `identity.keycloak.develop.eoepca.org` | Main Keycloak server, UMA endpoints at `/auth/realms/{realm}/authz/*` |\n| `identity-api` | `identity.api.develop.eoepca.org` | REST API for resource registration and policy management |\n| `identity-manager` | `identity.manager.develop.eoepca.org` | Administrative web interface |\n| `identity-gatekeeper` | `identity.gatekeeper.develop.eoepca.org` | OAuth2 proxy for protected admin access |\n\n**Sources:** [system/clusters/creodias/user-management/um-identity-service.yaml:22-76]()\n\n### Persistent Storage\n\nAll Identity Service components share the `eoepca-userman-pvc` PersistentVolumeClaim, which is backed by NFS storage. This PVC is pre-created and referenced by the HelmRelease:\n\n```yaml\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\n```\n\n**Sources:** [system/clusters/creodias/user-management/um-identity-service.yaml:19-21](), [system/clusters/creodias/system/storage/hr-storage.yaml:27-28]()\n\n## Login Service Components\n\nThe Login Service is deployed via the `um-login-service` HelmRelease and provides the authentication frontend using Gluu Server 4.x components:\n\n```mermaid\ngraph TB\n    subgraph \"login-service Helm Chart\"\n        subgraph \"OpenDJ LDAP\"\n            OpenDJ[\"opendj\u003cbr/\u003eContainer: opendj\u003cbr/\u003ePort: 1636\"]\n            LDAPData[(\"LDAP Directory\u003cbr/\u003ecn=users,dc=example,dc=org\")]\n        end\n        \n        subgraph \"oxAuth (Authentication Server)\"\n            OxAuth[\"oxauth\u003cbr/\u003eContainer: oxauth\u003cbr/\u003eResources: 1000Mi RAM\"]\n            OxAuthCache[\"Embedded Cache\"]\n        end\n        \n        subgraph \"oxTrust (Admin Portal)\"\n            OxTrust[\"oxtrust\u003cbr/\u003eContainer: oxtrust\u003cbr/\u003eResources: 1500Mi RAM\"]\n        end\n        \n        subgraph \"oxPassport (Federation)\"\n            OxPassport[\"oxpassport\u003cbr/\u003eContainer: oxpassport\u003cbr/\u003eNode.js service\"]\n        end\n        \n        Nginx[\"nginx-ingress\u003cbr/\u003eHost: auth.develop.eoepca.org\u003cbr/\u003eTLS: gluu-tls-certificate\"]\n    end\n    \n    subgraph \"External\"\n        User[\"End User\"]\n        AdminUser[\"Administrator\"]\n    end\n    \n    User --\u003e|\"Login Request\"| Nginx\n    Nginx --\u003e|\"Route /oxauth/*\"| OxAuth\n    Nginx --\u003e|\"Route /identity/*\"| OxTrust\n    Nginx --\u003e|\"Route /passport/*\"| OxPassport\n    \n    AdminUser --\u003e|\"Admin Console\"| OxTrust\n    \n    OxAuth --\u003e|\"Query Users\"| OpenDJ\n    OxAuth --\u003e|\"Read LDAP\"| LDAPData\n    OxTrust --\u003e|\"Manage Users\"| OpenDJ\n    OxTrust --\u003e|\"Write LDAP\"| LDAPData\n    \n    OxPassport --\u003e|\"External IdP\"| OxAuth\n```\n\n**Component Resource Allocations:**\n\n| Component | CPU Request | Memory Request | Purpose |\n|-----------|-------------|----------------|---------|\n| `opendj` | 100m | 300Mi | LDAP directory for user credentials and attributes |\n| `oxauth` | 100m | 1000Mi | OAuth2/OIDC authentication server |\n| `oxtrust` | 100m | 1500Mi | Administrative web interface for user/client management |\n| `oxpassport` | 100m | 100Mi | External identity provider federation (optional) |\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:23-56]()\n\n### Domain Configuration\n\nThe Login Service is configured with the domain `auth.develop.eoepca.org` and requires the `nginxIp` parameter for proper routing:\n\n```yaml\nglobal:\n  domain: auth.develop.eoepca.org\n  nginxIp: 185.52.192.231\n  namespace: um\n```\n\nThis configuration ensures all Gluu components are accessible under a unified domain with proper ingress routing.\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:19-56]()\n\n## Policy Decision Point\n\nThe `pdp-engine` provides the Policy Decision Point (PDP) that makes authorization decisions based on XACML policies. It is deployed in the `um` namespace and integrates with both the Identity Service and Login Service:\n\n```mermaid\ngraph LR\n    subgraph \"PDP Engine\"\n        PDP[\"pdp-engine\u003cbr/\u003ePod\"]\n        PolicyStore[(\"Policy Storage\u003cbr/\u003eeoepca-userman-pvc\")]\n    end\n    \n    subgraph \"Policy Enforcement Points\"\n        PEP1[\"ADES PEP\u003cbr/\u003e(resource-guard)\"]\n        PEP2[\"Workspace PEP\u003cbr/\u003e(resource-guard)\"]\n        PEP3[\"Catalogue PEP\u003cbr/\u003e(resource-guard)\"]\n    end\n    \n    subgraph \"Identity Services\"\n        Keycloak[\"identity-keycloak\u003cbr/\u003eToken validation\"]\n        Gluu[\"oxauth\u003cbr/\u003eUser attributes\"]\n    end\n    \n    PEP1 --\u003e|\"authzRequest(user, resource, action)\"| PDP\n    PEP2 --\u003e|\"authzRequest(user, resource, action)\"| PDP\n    PEP3 --\u003e|\"authzRequest(user, resource, action)\"| PDP\n    \n    PDP --\u003e|\"Validate RPT\"| Keycloak\n    PDP --\u003e|\"Fetch user attributes\"| Gluu\n    PDP --\u003e|\"Load policies\"| PolicyStore\n    \n    PDP --\u003e|\"Decision: Permit/Deny\"| PEP1\n    PDP --\u003e|\"Decision: Permit/Deny\"| PEP2\n    PDP --\u003e|\"Decision: Permit/Deny\"| PEP3\n```\n\n**Configuration:**\n\nThe PDP Engine shares the same persistent volume and domain configuration as other user management components:\n\n```yaml\nglobal:\n  nginxIp: 185.52.192.231\n  domain: auth.develop.eoepca.org\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\n```\n\n**Sources:** [system/clusters/creodias/user-management/um-pdp-engine.yaml:1-28]()\n\n## User Profile Service\n\nThe `user-profile` service manages user metadata and profile information, acting as a bridge between the Identity Service and the Login Service:\n\n```yaml\nglobal:\n  domain: auth.develop.eoepca.org\n  nginxIp: 185.52.192.231\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\n```\n\nThis service synchronizes user profile data from the LDAP directory (OpenDJ) and makes it available to other EOEPCA components that require user context.\n\n**Sources:** [system/clusters/creodias/user-management/um-user-profile.yaml:1-24]()\n\n## Policy Enforcement Integration\n\nPolicy Enforcement Points (PEPs) are integrated into EOEPCA services via Nginx Ingress annotations. The following example shows how a test service is protected:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Ingress as \"Nginx Ingress\"\n    participant GK as \"identity-gatekeeper.um.svc\"\n    participant Service as \"Backend Service\"\n    \n    Client-\u003e\u003eIngress: \"GET /resource\"\n    Note over Ingress: \"nginx.ingress.kubernetes.io/configuration-snippet:\u003cbr/\u003eauth_request /auth\"\n    \n    Ingress-\u003e\u003eGK: \"Internal auth_request /auth\"\n    Note over GK: \"Proxy to identity-gatekeeper:3000\"\n    \n    alt Token Valid\n        GK--\u003e\u003eIngress: \"200 OK\"\n        Ingress-\u003e\u003eService: \"Forward request\"\n        Service--\u003e\u003eIngress: \"Response\"\n        Ingress--\u003e\u003eClient: \"200 + Data\"\n    else Token Invalid/Missing\n        GK--\u003e\u003eIngress: \"401/403\"\n        Ingress--\u003e\u003eClient: \"401/403 + WWW-Authenticate\"\n    end\n```\n\n**Nginx Configuration Snippet:**\n\nThe ingress configuration includes an internal authentication request to the `identity-gatekeeper` service:\n\n```yaml\nnginx.ingress.kubernetes.io/configuration-snippet: |\n  auth_request /auth;\n  # Preflighted requests\n  if ($request_method = OPTIONS ) {\n    return 200;\n  }\n  add_header Access-Control-Allow-Origin $http_origin always;\n  add_header Access-Control-Allow-Methods \"*\";\n  add_header Access-Control-Allow-Headers \"Authorization, Origin, Content-Type\";\n\nnginx.ingress.kubernetes.io/server-snippet: |\n  location ^~ /auth {\n    internal;\n    proxy_pass http://identity-gatekeeper.um.svc.cluster.local:3000/$request_uri;\n    proxy_pass_request_body off;\n    proxy_set_header Content-Length \"\";\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header X-Forwarded-Host $host;\n    proxy_set_header X-Forwarded-Method $request_method;\n    proxy_set_header X-Forwarded-URI $request_uri;\n    proxy_busy_buffers_size 64k;\n    proxy_buffers 8 32k;\n    proxy_buffer_size 32k;\n  }\n```\n\nThis pattern enables transparent authentication enforcement at the ingress layer without modifying backend services.\n\n**Sources:** [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31]()\n\n## OIDC Client Integration\n\nExternal services and portals integrate with the Identity Service using OIDC client credentials. The Django Portal example demonstrates this integration:\n\n```mermaid\ngraph LR\n    subgraph \"Django Portal (demo namespace)\"\n        Portal[\"django-portal\u003cbr/\u003ePod\"]\n        Secrets[\"django-secrets\u003cbr/\u003eSealedSecret\"]\n    end\n    \n    subgraph \"OIDC Configuration\"\n        ClientID[\"OIDC_RP_CLIENT_ID\u003cbr/\u003e(Gluu client)\"]\n        ClientSecret[\"OIDC_RP_CLIENT_SECRET\"]\n        KCClientID[\"KEYCLOAK_OIDC_RP_CLIENT_ID\u003cbr/\u003e(Keycloak client)\"]\n        KCClientSecret[\"KEYCLOAK_OIDC_RP_CLIENT_SECRET\"]\n    end\n    \n    subgraph \"Identity Providers\"\n        Gluu[\"oxauth\u003cbr/\u003eauth.develop.eoepca.org\"]\n        Keycloak[\"identity-keycloak\u003cbr/\u003eidentity.keycloak.develop.eoepca.org\"]\n    end\n    \n    Secrets --\u003e|\"Inject\"| ClientID\n    Secrets --\u003e|\"Inject\"| ClientSecret\n    Secrets --\u003e|\"Inject\"| KCClientID\n    Secrets --\u003e|\"Inject\"| KCClientSecret\n    \n    Portal --\u003e|\"Login via\"| ClientID\n    Portal --\u003e|\"Login via\"| KCClientID\n    \n    ClientID --\u003e|\"Authenticate\"| Gluu\n    KCClientID --\u003e|\"Authenticate\"| Keycloak\n```\n\n**Sealed Secret Structure:**\n\nOIDC client credentials are stored as Kubernetes SealedSecrets for secure GitOps deployment. The secret includes:\n\n| Key | Purpose |\n|-----|---------|\n| `OIDC_RP_CLIENT_ID` | Gluu/oxAuth client identifier |\n| `OIDC_RP_CLIENT_SECRET` | Gluu/oxAuth client secret |\n| `KEYCLOAK_OIDC_RP_CLIENT_ID` | Keycloak client identifier (alternative IdP) |\n| `KEYCLOAK_OIDC_RP_CLIENT_SECRET` | Keycloak client secret |\n| `DJANGO_SECRET` | Django application secret key |\n\n**Secret Creation Script:**\n\nThe `ss-django-secrets-create.sh` script automates SealedSecret generation:\n\n```bash\nkubectl -n \"${NAMESPACE}\" create secret generic \"${SECRET_NAME}\" \\\n  --from-literal=OIDC_RP_CLIENT_ID=\"${client_id}\" \\\n  --from-literal=OIDC_RP_CLIENT_SECRET=\"${client_secret}\" \\\n  --from-literal=DJANGO_SECRET=\"${django_secret}\" \\\n  --from-literal=KEYCLOAK_OIDC_RP_CLIENT_ID=\"${keycloak_client_id}\" \\\n  --from-literal=KEYCLOAK_OIDC_RP_CLIENT_SECRET=\"${keycloak_client_secret}\" \\\n  --dry-run=client -o yaml | \\\nkubeseal -o yaml --controller-name eoepca-sealed-secrets \\\n  --controller-namespace infra \u003e ss-${SECRET_NAME}.yaml\n```\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17](), [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:22-33]()\n\n## Portal Integration\n\nTwo demonstration portals are deployed to showcase authentication integration:\n\n### EOEPCA Portal\n\nA static portal providing an overview and access to EOEPCA services:\n\n```yaml\nconfigmap:\n  configuration: develop\ningress:\n  hosts:\n    - host: eoepca-portal.develop.eoepca.org\n      paths:\n        - path: /\n          pathType: Prefix\n```\n\n**Sources:** [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml:20-31]()\n\n### Django Portal\n\nA dynamic Django-based portal with full OIDC authentication:\n\n```yaml\ndomain: develop.eoepca.org\nauthHost: auth\nconfigmap:\n  user_prefix: develop-user\n```\n\nThe Django Portal uses the `authHost: auth` configuration to connect to `auth.develop.eoepca.org` for authentication and retrieves OIDC client credentials from the `django-secrets` SealedSecret.\n\n**Sources:** [system/clusters/creodias/system/demo/hr-django-portal.yaml:17-20]()\n\n## Deployment Dependencies\n\nThe User Management building block has the following deployment dependencies:\n\n```mermaid\ngraph TD\n    subgraph \"Prerequisites\"\n        NS[\"Namespace: um\"]\n        PVC[\"PersistentVolumeClaim:\u003cbr/\u003eeoepca-userman-pvc\"]\n        NFS[\"NFS Server\u003cbr/\u003e192.168.123.14\"]\n        StorageClass[\"StorageClass:\u003cbr/\u003eeoepca-nfs\"]\n        SealedSecretCtrl[\"sealed-secrets\u003cbr/\u003e(infra namespace)\"]\n        IngressCtrl[\"ingress-nginx\u003cbr/\u003e(ingress-nginx namespace)\"]\n        CertMgr[\"cert-manager\u003cbr/\u003e(cert-manager namespace)\"]\n    end\n    \n    subgraph \"User Management Services\"\n        Login[\"um-login-service\u003cbr/\u003eHelmRelease\"]\n        Identity[\"um-identity-service\u003cbr/\u003eHelmRelease\"]\n        PDP[\"um-pdp-engine\u003cbr/\u003eHelmRelease\"]\n        Profile[\"um-user-profile\u003cbr/\u003eHelmRelease\"]\n    end\n    \n    NFS --\u003e|\"Provision\"| StorageClass\n    StorageClass --\u003e|\"Create\"| PVC\n    PVC --\u003e|\"Mount\"| Login\n    PVC --\u003e|\"Mount\"| Identity\n    PVC --\u003e|\"Mount\"| PDP\n    PVC --\u003e|\"Mount\"| Profile\n    \n    IngressCtrl --\u003e|\"Route traffic to\"| Login\n    IngressCtrl --\u003e|\"Route traffic to\"| Identity\n    \n    CertMgr --\u003e|\"Issue TLS certs for\"| Login\n    CertMgr --\u003e|\"Issue TLS certs for\"| Identity\n    \n    Login --\u003e|\"Provides user store for\"| Identity\n    Identity --\u003e|\"Token validation for\"| PDP\n    Profile --\u003e|\"User metadata for\"| PDP\n```\n\n**Storage Configuration:**\n\nThe NFS-backed storage is configured via the `storage` HelmRelease:\n\n```yaml\nnfs:\n  server:\n    address: \"192.168.123.14\"\ndomain:\n  userman:\n    storageClass: eoepca-nfs\n```\n\nThis creates the `eoepca-nfs` StorageClass, which provisions PersistentVolumes from the NFS server for the `eoepca-userman-pvc` PersistentVolumeClaim.\n\n**Sources:** [system/clusters/creodias/system/storage/hr-storage.yaml:18-28]()\n\n## Helm Chart Versions\n\nThe User Management subsystem uses the following Helm chart versions (as of the latest deployment):\n\n| HelmRelease | Chart Name | Version | Repository |\n|-------------|------------|---------|------------|\n| `um-identity-service` | `identity-service` | 1.0.75 | eoepca |\n| `um-login-service` | `login-service` | 1.2.8 | eoepca |\n| `um-pdp-engine` | `pdp-engine` | 1.1.12 | eoepca |\n| `um-user-profile` | `user-profile` | 1.1.12 | eoepca |\n\nAll charts reference the `eoepca` HelmRepository in the `common` namespace, which is managed by Flux CD.\n\n**Sources:** [system/clusters/creodias/user-management/um-identity-service.yaml:10-17](), [system/clusters/creodias/user-management/um-login-service.yaml:7-14](), [system/clusters/creodias/user-management/um-pdp-engine.yaml:7-14](), [system/clusters/creodias/user-management/um-user-profile.yaml:7-14]()\n\n## Timeout Configuration\n\nUser Management services have extended timeout configurations to accommodate initialization and startup:\n\n| Service | Timeout | Reason |\n|---------|---------|--------|\n| `um-identity-service` | 5m | Keycloak and PostgreSQL initialization |\n| `um-login-service` | 25m | Gluu components require extended startup (OpenDJ, oxAuth, oxTrust) |\n| `um-pdp-engine` | 25m | Policy loading and identity service synchronization |\n| `um-user-profile` | 25m | LDAP synchronization and profile initialization |\n\nThese timeouts are configured in the HelmRelease specifications:\n\n```yaml\ntimeout: 25m0s\ninterval: 1m0s\n```\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:67-68](), [system/clusters/creodias/user-management/um-pdp-engine.yaml:26-27](), [system/clusters/creodias/user-management/um-user-profile.yaml:22-23]()\n\n## Summary\n\nThe User Management and Identity building block provides a comprehensive authentication and authorization infrastructure for the EOEPCA platform. Key characteristics include:\n\n- **Dual Identity Providers**: Gluu (Login Service) for user authentication and Keycloak (Identity Service) for UMA resource management\n- **Centralized Policy Enforcement**: PDP Engine makes authorization decisions based on XACML policies\n- **Flexible Integration**: PEPs can be deployed via Ingress annotations or as dedicated resource-guard instances\n- **Shared Storage**: All components use the `eoepca-userman-pvc` for persistent state\n- **GitOps Deployment**: Flux CD manages continuous deployment with HelmRelease specifications\n- **Sealed Secrets**: OIDC client credentials are encrypted for secure storage in Git\n\nFor implementation details of specific subsystems, refer to the child pages: [Identity Service (Keycloak)](#4.1), [Login Service (Gluu)](#4.2), [Policy Enforcement (PEP/PDP)](#4.3), and [UMA Authentication Flow](#4.4)."])</script><script>self.__next_f.push([1,"21:T4a20,"])</script><script>self.__next_f.push([1,"# Identity Service (Keycloak)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml](system/clusters/creodias/system/demo/hr-eoepca-portal.yaml)\n- [system/clusters/creodias/system/demo/ss-django-secrets-create.sh](system/clusters/creodias/system/demo/ss-django-secrets-create.sh)\n- [system/clusters/creodias/system/demo/ss-django-secrets.yaml](system/clusters/creodias/system/demo/ss-django-secrets.yaml)\n- [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml](system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml)\n- [system/clusters/creodias/user-management/um-identity-service.yaml](system/clusters/creodias/user-management/um-identity-service.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the Identity Service deployment within the EOEPCA platform, which provides centralized identity and access management based on Keycloak. The Identity Service handles user authentication, token issuance, and User-Managed Access (UMA) protocol flows. \n\nFor information about the login flows and user interface components, see [Login Service (Gluu)](#4.2). For details on how the Identity Service integrates with policy enforcement, see [Policy Enforcement (PEP/PDP)](#4.3). For a detailed explanation of the UMA authentication flow, see [UMA Authentication Flow](#4.4).\n\n## Overview\n\nThe Identity Service (`um-identity-service`) is deployed in the `um` namespace and serves as the core identity provider for all EOEPCA services. It implements OpenID Connect (OIDC) and OAuth2 protocols, with special support for UMA 2.0 ticket-based authorization flows. The service issues ID tokens, access tokens, and Requesting Party Tokens (RPTs) that are validated by Policy Enforcement Points throughout the platform.\n\nThe Identity Service consists of five primary components deployed together as a single Helm chart:\n- **identity-keycloak**: The main Keycloak server providing OIDC/OAuth2/UMA endpoints\n- **identity-postgres**: PostgreSQL database storing user accounts, clients, and configuration\n- **identity-api**: REST API for programmatic management of users and clients\n- **identity-manager**: Administrative web interface for configuration\n- **identity-gatekeeper**: OAuth2 proxy for protecting administrative endpoints\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:1-78]()\n\n## Component Architecture\n\n```mermaid\ngraph TB\n    subgraph \"External Access\"\n        Users[\"Users\"]\n        Services[\"EOEPCA Services\u003cbr/\u003e(ADES, Workspace API, etc)\"]\n    end\n    \n    subgraph \"um Namespace\"\n        subgraph \"um-identity-service HelmRelease\"\n            Keycloak[\"identity-keycloak\u003cbr/\u003eKeycloak Server\u003cbr/\u003ePort 8080\"]\n            API[\"identity-api\u003cbr/\u003eManagement API\u003cbr/\u003ePort 5000\"]\n            Manager[\"identity-manager\u003cbr/\u003eAdmin UI\u003cbr/\u003ePort 80\"]\n            Gatekeeper[\"identity-gatekeeper\u003cbr/\u003eOAuth2 Proxy\u003cbr/\u003ePort 3000\"]\n            Postgres[\"identity-postgres\u003cbr/\u003ePostgreSQL 13\u003cbr/\u003ePort 5432\"]\n        end\n    end\n    \n    subgraph \"Ingress Routes\"\n        IngressKC[\"identity.keycloak.develop.eoepca.org\u003cbr/\u003eTLS: identity-keycloak-tls-certificate\"]\n        IngressAPI[\"identity.api.develop.eoepca.org\u003cbr/\u003eTLS: identity-api-tls-certificate\"]\n        IngressMgr[\"identity.manager.develop.eoepca.org\u003cbr/\u003eTLS: identity-manager-tls-certificate\"]\n        IngressGK[\"identity.gatekeeper.develop.eoepca.org\u003cbr/\u003eTLS: identity-gatekeeper-tls-certificate\"]\n    end\n    \n    subgraph \"Storage\"\n        PVC[\"eoepca-userman-pvc\u003cbr/\u003eNFS-backed PersistentVolumeClaim\"]\n    end\n    \n    Users --\u003e|\"/.well-known/openid-configuration\u003cbr/\u003e/auth/realms/*/protocol/openid-connect\"| IngressKC\n    Users --\u003e|\"Admin Console Access\"| IngressMgr\n    Services --\u003e|\"Token Validation\u003cbr/\u003eUMA Ticket Exchange\"| IngressKC\n    Services --\u003e|\"Client Registration\u003cbr/\u003eUser Management\"| IngressAPI\n    \n    IngressKC --\u003e Keycloak\n    IngressAPI --\u003e API\n    IngressMgr --\u003e Gatekeeper\n    IngressGK --\u003e Gatekeeper\n    \n    Gatekeeper --\u003e|\"Protect\"| Manager\n    API --\u003e|\"Configure\"| Keycloak\n    Manager --\u003e|\"Configure\"| Keycloak\n    Keycloak --\u003e|\"Store Data\"| Postgres\n    \n    Postgres --\u003e|\"Persist\"| PVC\n```\n\n**Component Deployment Diagram**\n\nThis diagram shows the five services deployed by the `um-identity-service` HelmRelease and their ingress routing. Each component has a dedicated hostname with TLS certificate provisioned by cert-manager using the `letsencrypt` ClusterIssuer.\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:4-78]()\n\n## Keycloak Server (identity-keycloak)\n\nThe `identity-keycloak` component is the core Keycloak server that provides identity and access management functionality. It exposes standard OIDC endpoints for authentication and token issuance, as well as UMA 2.0 endpoints for ticket-based authorization.\n\n### Key Endpoints\n\nThe Keycloak server exposes endpoints under the base URL `https://identity.keycloak.develop.eoepca.org`:\n\n| Endpoint Pattern | Purpose |\n|-----------------|---------|\n| `/.well-known/openid-configuration` | OIDC discovery document |\n| `/auth/realms/{realm}/protocol/openid-connect/auth` | Authorization endpoint |\n| `/auth/realms/{realm}/protocol/openid-connect/token` | Token endpoint |\n| `/auth/realms/{realm}/protocol/openid-connect/userinfo` | User info endpoint |\n| `/auth/realms/{realm}/authz/uma-ticket` | UMA ticket issuance |\n| `/auth/realms/{realm}/authz/uma-rpt` | UMA RPT token exchange |\n\n### Ingress Configuration\n\nThe Keycloak ingress is configured at [system/clusters/creodias/user-management/um-identity-service.yaml:23-34]() with:\n- Hostname: `identity.keycloak.develop.eoepca.org`\n- TLS certificate: `identity-keycloak-tls-certificate` (managed by cert-manager)\n- ClusterIssuer: `letsencrypt`\n- Path routing: All requests (`/`) to the Keycloak service\n\n### Integration with Authentication Flow\n\nWhen a Policy Enforcement Point (PEP) needs to validate user access:\n1. PEP requests a UMA ticket from `/auth/realms/{realm}/authz/uma-ticket`\n2. User authenticates via `/auth/realms/{realm}/protocol/openid-connect/auth`\n3. User exchanges ticket + ID token for RPT at `/auth/realms/{realm}/authz/uma-rpt`\n4. PEP validates RPT with Keycloak before granting access\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:22-34]()\n\n## PostgreSQL Database (identity-postgres)\n\nThe `identity-postgres` component provides persistent storage for Keycloak configuration and runtime data. It stores user accounts, client registrations, sessions, realms, and authorization policies.\n\n### Storage Configuration\n\nThe PostgreSQL instance uses the shared NFS-backed PersistentVolumeClaim `eoepca-userman-pvc` for data persistence. This is configured at [system/clusters/creodias/user-management/um-identity-service.yaml:35-37]():\n\n```yaml\nidentity-postgres:\n  volumeClaim:\n    name: eoepca-userman-pvc\n```\n\nThe `eoepca-userman-pvc` claim is pre-provisioned (note `create: false` at [system/clusters/creodias/user-management/um-identity-service.yaml:19-21]()) and shared across user management components to enable data persistence across pod restarts and upgrades.\n\n### Database Schema\n\nPostgreSQL stores Keycloak tables including:\n- `user_entity`: User accounts and credentials\n- `client`: OAuth2/OIDC client registrations\n- `realm`: Realm configurations\n- `resource_server`: UMA resource server definitions\n- `resource_server_resource`: Protected resources for UMA\n- `resource_server_policy`: Authorization policies\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:35-37](), [system/clusters/creodias/user-management/um-identity-service.yaml:19-21]()\n\n## Management API (identity-api)\n\nThe `identity-api` component exposes a REST API for programmatic management of users, clients, and Keycloak configuration. This API is used by automation scripts and other EOEPCA services that need to register new clients or manage users dynamically.\n\n### API Endpoints\n\nThe API is accessible at `https://identity.api.develop.eoepca.org` and provides endpoints for:\n- User creation and management\n- OIDC client registration\n- Realm configuration\n- Role and group assignment\n\n### Ingress Configuration\n\nThe API ingress is defined at [system/clusters/creodias/user-management/um-identity-service.yaml:38-50]():\n\n| Configuration | Value |\n|--------------|-------|\n| Hostname | `identity.api.develop.eoepca.org` |\n| TLS Secret | `identity-api-tls-certificate` |\n| Path | `/` (all requests) |\n| Path Type | `Prefix` |\n| ClusterIssuer | `letsencrypt` |\n\n### Usage Example\n\nOther EOEPCA components use this API to dynamically register OIDC clients. For example, when a new workspace is created, the Workspace API can register a new client for that workspace via the identity-api.\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:38-50]()\n\n## Management Interface (identity-manager)\n\nThe `identity-manager` component provides a web-based administrative interface for Keycloak configuration. This interface is protected by the `identity-gatekeeper` OAuth2 proxy to ensure only authorized administrators can access it.\n\n### Access Control\n\nThe manager UI is accessible at `https://identity.manager.develop.eoepca.org` but requires authentication through the gatekeeper proxy. This two-tier architecture ensures:\n1. All requests pass through the gatekeeper for OAuth2 token validation\n2. Only users with valid tokens from Keycloak can access the manager\n3. Administrative operations are logged and auditable\n\n### Ingress Configuration\n\nThe manager ingress is configured at [system/clusters/creodias/user-management/um-identity-service.yaml:51-63]():\n\n```yaml\nidentity-manager:\n  ingress:\n    annotations:\n      cert-manager.io/cluster-issuer: letsencrypt\n    hosts:\n      - host: identity.manager.develop.eoepca.org\n        paths:\n          - path: /\n            pathType: Prefix\n    tls:\n      - secretName: identity-manager-tls-certificate\n        hosts:\n          - identity.manager.develop.eoepca.org\n```\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:51-63]()\n\n## OAuth2 Proxy (identity-gatekeeper)\n\nThe `identity-gatekeeper` component is an OAuth2 proxy that sits in front of the management interface to enforce authentication. It validates OAuth2 access tokens before forwarding requests to the manager.\n\n### Gatekeeper Flow\n\n```mermaid\nsequenceDiagram\n    participant Admin as \"Administrator\"\n    participant Ingress as \"identity.manager.develop.eoepca.org\"\n    participant GK as \"identity-gatekeeper\u003cbr/\u003eService (Port 3000)\"\n    participant Mgr as \"identity-manager\u003cbr/\u003eService\"\n    participant KC as \"identity-keycloak\"\n    \n    Admin-\u003e\u003eIngress: \"GET /admin\"\n    Ingress-\u003e\u003eGK: \"Forward Request\"\n    \n    alt No Valid Token\n        GK-\u003e\u003eAdmin: \"302 Redirect to Keycloak Login\"\n        Admin-\u003e\u003eKC: \"Authenticate (username/password)\"\n        KC-\u003e\u003eAdmin: \"Return Access Token\"\n        Admin-\u003e\u003eGK: \"Retry with Access Token\"\n    end\n    \n    GK-\u003e\u003eKC: \"Validate Access Token\"\n    KC--\u003e\u003eGK: \"Token Valid + User Claims\"\n    GK-\u003e\u003eMgr: \"Forward Request with User Context\"\n    Mgr--\u003e\u003eGK: \"Admin Interface HTML\"\n    GK--\u003e\u003eAdmin: \"Render Admin UI\"\n```\n\n**Gatekeeper Authentication Flow**\n\nThis diagram illustrates how the gatekeeper proxy enforces authentication before granting access to the management interface. The gatekeeper is deployed as a separate service to provide a reusable authentication layer.\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:64-76]()\n\n### Ingress Configuration\n\nThe gatekeeper has its own ingress at [system/clusters/creodias/user-management/um-identity-service.yaml:64-76]():\n\n| Configuration | Value |\n|--------------|-------|\n| Hostname | `identity.gatekeeper.develop.eoepca.org` |\n| TLS Secret | `identity-gatekeeper-tls-certificate` |\n| Port | 3000 |\n\nThe gatekeeper can also be referenced by other services for authentication. For example, the test dummy service uses it at [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:19-31]() with an nginx auth_request configuration.\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:64-76](), [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:19-31]()\n\n## Deployment Specifications\n\n### HelmRelease Configuration\n\nThe Identity Service is deployed via a Flux CD HelmRelease resource named `um-identity-service` in the `um` namespace. The HelmRelease specification at [system/clusters/creodias/user-management/um-identity-service.yaml:1-78]() defines:\n\n| Parameter | Value |\n|-----------|-------|\n| Chart Name | `identity-service` |\n| Chart Version | `1.0.75` |\n| Source Repository | `eoepca` HelmRepository in `common` namespace |\n| Reconciliation Interval | `1m0s` |\n| Timeout | `5m0s` |\n\n### Shared Storage\n\nThe Identity Service uses a pre-existing PersistentVolumeClaim for data persistence. The claim configuration at [system/clusters/creodias/user-management/um-identity-service.yaml:19-21]():\n\n```yaml\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\n```\n\nThe `create: false` setting indicates the PVC is provisioned separately and shared across user management components. This enables:\n- Persistent storage of PostgreSQL data across pod restarts\n- Shared configuration between Keycloak and other user management services\n- NFS-backed storage for multi-pod access\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:1-78]()\n\n## Integration with EOEPCA Services\n\n### OIDC Client Registration\n\nEOEPCA services register as OIDC clients in Keycloak to participate in authentication flows. For example, the EOEPCA portal registers a client with credentials stored in a SealedSecret. The secret creation script at [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:23-29]() shows how services store client credentials:\n\n```bash\nkubectl -n \"${NAMESPACE}\" create secret generic \"${SECRET_NAME}\" \\\n  --from-literal=OIDC_RP_CLIENT_ID=\"${client_id}\" \\\n  --from-literal=OIDC_RP_CLIENT_SECRET=\"${client_secret}\" \\\n  --from-literal=DJANGO_SECRET=\"${django_secret}\" \\\n  --from-literal=KEYCLOAK_OIDC_RP_CLIENT_ID=\"${keycloak_client_id}\" \\\n  --from-literal=KEYCLOAK_OIDC_RP_CLIENT_SECRET=\"${keycloak_client_secret}\"\n```\n\nThe resulting SealedSecret at [system/clusters/creodias/system/demo/ss-django-secrets.yaml:8-11]() contains encrypted client credentials that reference the Identity Service.\n\nSources: [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:23-29](), [system/clusters/creodias/system/demo/ss-django-secrets.yaml:8-11]()\n\n### Policy Enforcement Integration\n\nPolicy Enforcement Points throughout the platform reference the gatekeeper service for authentication. The test dummy service demonstrates this integration at [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31]() using nginx auth_request:\n\n```yaml\nnginx.ingress.kubernetes.io/configuration-snippet: |\n  auth_request /auth;\nnginx.ingress.kubernetes.io/server-snippet: |\n  location ^~ /auth {\n    internal;\n    proxy_pass http://identity-gatekeeper.um.svc.cluster.local:3000/$request_uri;\n    proxy_pass_request_body off;\n    proxy_set_header Content-Length \"\";\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header X-Forwarded-Host $host;\n    proxy_set_header X-Forwarded-Method $request_method;\n    proxy_set_header X-Forwarded-URI $request_uri;\n  }\n```\n\nThis pattern enables any service to leverage the Identity Service for authentication by:\n1. Configuring nginx to send auth_request to the gatekeeper\n2. Using the cluster-internal service name `identity-gatekeeper.um.svc.cluster.local:3000`\n3. Forwarding original request metadata via proxy headers\n\nSources: [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31]()\n\n## Service Endpoint Summary\n\n```mermaid\ngraph LR\n    subgraph \"External DNS\"\n        DNSKC[\"identity.keycloak.develop.eoepca.org\"]\n        DNSAPI[\"identity.api.develop.eoepca.org\"]\n        DNSMgr[\"identity.manager.develop.eoepca.org\"]\n        DNSGK[\"identity.gatekeeper.develop.eoepca.org\"]\n    end\n    \n    subgraph \"TLS Certificates (cert-manager)\"\n        TLSKc[\"identity-keycloak-tls-certificate\"]\n        TLSApi[\"identity-api-tls-certificate\"]\n        TLSMgr[\"identity-manager-tls-certificate\"]\n        TLSGK[\"identity-gatekeeper-tls-certificate\"]\n    end\n    \n    subgraph \"Kubernetes Services (um namespace)\"\n        SvcKC[\"identity-keycloak\u003cbr/\u003eService\"]\n        SvcAPI[\"identity-api\u003cbr/\u003eService\"]\n        SvcMgr[\"identity-manager\u003cbr/\u003eService\"]\n        SvcGK[\"identity-gatekeeper\u003cbr/\u003eService:3000\"]\n    end\n    \n    subgraph \"Cluster Internal Access\"\n        Internal[\"identity-gatekeeper.um.svc.cluster.local:3000\"]\n    end\n    \n    DNSKC --\u003e|\"cert-manager.io/cluster-issuer: letsencrypt\"| TLSKc\n    DNSAPI --\u003e|\"cert-manager.io/cluster-issuer: letsencrypt\"| TLSApi\n    DNSMgr --\u003e|\"cert-manager.io/cluster-issuer: letsencrypt\"| TLSMgr\n    DNSGK --\u003e|\"cert-manager.io/cluster-issuer: letsencrypt\"| TLSGK\n    \n    TLSKc --\u003e SvcKC\n    TLSApi --\u003e SvcAPI\n    TLSMgr --\u003e SvcMgr\n    TLSGK --\u003e SvcGK\n    \n    SvcGK -.-\u003e|\"Also accessible as\"| Internal\n```\n\n**Service Endpoint and TLS Configuration**\n\nThis diagram shows the complete endpoint configuration for all Identity Service components, including DNS names, TLS certificates, and internal cluster service names. All certificates are automatically provisioned by cert-manager using the `letsencrypt` ClusterIssuer.\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:23-76](), [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:21]()\n\n## Certificate Management\n\nAll Identity Service endpoints use TLS certificates managed by cert-manager. The `cert-manager.io/cluster-issuer: letsencrypt` annotation at multiple locations ([system/clusters/creodias/user-management/um-identity-service.yaml:25](), [system/clusters/creodias/user-management/um-identity-service.yaml:41](), [system/clusters/creodias/user-management/um-identity-service.yaml:53](), [system/clusters/creodias/user-management/um-identity-service.yaml:67]()) triggers automatic certificate provisioning:\n\n1. cert-manager detects the ingress with the annotation\n2. Creates a Certificate resource\n3. Initiates ACME challenge with Let's Encrypt\n4. Stores resulting certificate in the specified secret\n5. Automatically renews certificates before expiration\n\nThis ensures all Identity Service endpoints are accessible over HTTPS with valid, automatically-renewed certificates.\n\nSources: [system/clusters/creodias/user-management/um-identity-service.yaml:25](), [system/clusters/creodias/user-management/um-identity-service.yaml:41](), [system/clusters/creodias/user-management/um-identity-service.yaml:53](), [system/clusters/creodias/user-management/um-identity-service.yaml:67]()"])</script><script>self.__next_f.push([1,"22:T42e7,"])</script><script>self.__next_f.push([1,"# Login Service (Gluu)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/system/demo/hr-django-portal.yaml](system/clusters/creodias/system/demo/hr-django-portal.yaml)\n- [system/clusters/creodias/system/storage/hr-storage.yaml](system/clusters/creodias/system/storage/hr-storage.yaml)\n- [system/clusters/creodias/system/test/hr-cheese.yaml](system/clusters/creodias/system/test/hr-cheese.yaml)\n- [system/clusters/creodias/user-management/um-login-service.yaml](system/clusters/creodias/user-management/um-login-service.yaml)\n- [system/clusters/creodias/user-management/um-pdp-engine.yaml](system/clusters/creodias/user-management/um-pdp-engine.yaml)\n- [system/clusters/creodias/user-management/um-user-profile.yaml](system/clusters/creodias/user-management/um-user-profile.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Login Service provides OpenID Connect (OIDC) and OAuth2 authentication capabilities for the EOEPCA platform using the Gluu Server. It handles user authentication, identity federation, and token issuance. The service is deployed as a collection of interdependent components including an LDAP directory (OpenDJ), authorization server (oxAuth), administration interface (oxTrust), and identity provider proxy (oxPassport).\n\nFor identity management and user attribute storage, see [Identity Service (Keycloak)](#4.1). For authorization policy enforcement, see [Policy Enforcement (PEP/PDP)](#4.3). For details on the token exchange and UMA flow, see [UMA Authentication Flow](#4.4).\n\n## Component Architecture\n\nThe Login Service consists of four primary components that work together to provide authentication services:\n\n```mermaid\ngraph TB\n    subgraph \"Login Service Components\"\n        OpenDJ[\"OpenDJ\u003cbr/\u003e(LDAP Directory)\"]\n        oxAuth[\"oxAuth\u003cbr/\u003e(Authorization Server)\"]\n        oxTrust[\"oxTrust\u003cbr/\u003e(Admin Interface)\"]\n        oxPassport[\"oxPassport\u003cbr/\u003e(IdP Proxy)\"]\n    end\n    \n    subgraph \"External Access\"\n        Nginx[\"nginx-ingress\u003cbr/\u003eauth.develop.eoepca.org\"]\n        TLS[\"TLS Certificate\u003cbr/\u003egluu-tls-certificate\"]\n    end\n    \n    subgraph \"Storage\"\n        PVC[\"eoepca-userman-pvc\u003cbr/\u003e(Shared Volume)\"]\n    end\n    \n    subgraph \"External Systems\"\n        Users[\"End Users\"]\n        Identity[\"Identity Service\u003cbr/\u003e(Keycloak)\"]\n        ExternalIdP[\"External Identity Providers\"]\n    end\n    \n    Users --\u003e|\"HTTPS\"| Nginx\n    Nginx --\u003e|\"cert-manager.io/cluster-issuer: letsencrypt\"| TLS\n    Nginx --\u003e oxAuth\n    Nginx --\u003e oxTrust\n    \n    oxAuth --\u003e|\"Read/Write User Data\"| OpenDJ\n    oxTrust --\u003e|\"Manage Configuration\"| OpenDJ\n    oxTrust --\u003e|\"Configure\"| oxAuth\n    oxPassport --\u003e|\"Federate Identity\"| oxAuth\n    oxPassport --\u003e|\"Connect to\"| ExternalIdP\n    \n    OpenDJ --\u003e|\"Persist Data\"| PVC\n    oxAuth --\u003e|\"Persist Tokens\"| PVC\n    oxTrust --\u003e|\"Persist Config\"| PVC\n    \n    oxAuth --\u003e|\"Issue OIDC Tokens\"| Identity\n```\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:1-68]()\n\n### OpenDJ (LDAP Directory)\n\nOpenDJ serves as the backend LDAP directory for storing user credentials, configuration data, and identity attributes. It provides the persistent data layer for the Gluu authentication system.\n\n**Configuration:**\n- Volume claim: `eoepca-userman-pvc`\n- CPU request: 100m\n- Memory request: 300Mi\n- Persistence can be disabled to workaround immutable field errors in the `um-login-service-persistence-init-ss` job\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:23-33]()\n\n### oxAuth (Authorization Server)\n\noxAuth is the core authorization server implementing OpenID Connect and OAuth2 protocols. It handles authentication requests, issues tokens (ID tokens, access tokens, refresh tokens), and manages client registrations.\n\n**Configuration:**\n- Volume claim: `eoepca-userman-pvc`\n- CPU request: 100m\n- Memory request: 1000Mi\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:34-40]()\n\n### oxTrust (Administration Interface)\n\noxTrust provides a web-based administrative interface for managing the Gluu Server configuration, users, groups, and OIDC clients. Administrators use this interface to configure authentication policies and integration parameters.\n\n**Configuration:**\n- Volume claim: `eoepca-userman-pvc`\n- CPU request: 100m\n- Memory request: 1500Mi\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:41-47]()\n\n### oxPassport (Identity Provider Proxy)\n\noxPassport enables identity federation by proxying authentication requests to external identity providers. It supports integration with social login providers and enterprise identity systems.\n\n**Configuration:**\n- CPU request: 100m\n- Memory request: 100Mi\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:48-52]()\n\n## Deployment Configuration\n\nThe Login Service is deployed via a Flux CD HelmRelease in the `um` namespace:\n\n```mermaid\ngraph TB\n    subgraph \"Flux CD\"\n        FluxController[\"flux-controller\"]\n        HelmRelease[\"HelmRelease\u003cbr/\u003eum-login-service\"]\n    end\n    \n    subgraph \"Helm Repository\"\n        EoepcaRepo[\"HelmRepository: eoepca\u003cbr/\u003enamespace: common\"]\n        Chart[\"Chart: login-service\u003cbr/\u003eversion: 1.2.8\"]\n    end\n    \n    subgraph \"Kubernetes um Namespace\"\n        OpenDJPod[\"Pod: opendj\u003cbr/\u003erequests: 100m CPU, 300Mi RAM\"]\n        oxAuthPod[\"Pod: oxauth\u003cbr/\u003erequests: 100m CPU, 1000Mi RAM\"]\n        oxTrustPod[\"Pod: oxtrust\u003cbr/\u003erequests: 100m CPU, 1500Mi RAM\"]\n        oxPassportPod[\"Pod: oxpassport\u003cbr/\u003erequests: 100m CPU, 100Mi RAM\"]\n        NginxPod[\"Pod: nginx\"]\n    end\n    \n    subgraph \"Storage\"\n        SharedPVC[\"PersistentVolumeClaim\u003cbr/\u003eeoepca-userman-pvc\u003cbr/\u003estorageClass: eoepca-nfs\"]\n    end\n    \n    FluxController --\u003e|\"Reconciles every 1m\"| HelmRelease\n    HelmRelease --\u003e|\"References\"| EoepcaRepo\n    EoepcaRepo --\u003e|\"Provides\"| Chart\n    HelmRelease --\u003e|\"Deploys\"| OpenDJPod\n    HelmRelease --\u003e|\"Deploys\"| oxAuthPod\n    HelmRelease --\u003e|\"Deploys\"| oxTrustPod\n    HelmRelease --\u003e|\"Deploys\"| oxPassportPod\n    HelmRelease --\u003e|\"Deploys\"| NginxPod\n    \n    OpenDJPod --\u003e|\"Mounts\"| SharedPVC\n    oxAuthPod --\u003e|\"Mounts\"| SharedPVC\n    oxTrustPod --\u003e|\"Mounts\"| SharedPVC\n```\n\n**Deployment Parameters:**\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `metadata.name` | `um-login-service` | HelmRelease identifier |\n| `metadata.namespace` | `um` | Deployment namespace |\n| `spec.chart.spec.chart` | `login-service` | Helm chart name |\n| `spec.chart.spec.version` | `1.2.8` | Chart version |\n| `spec.timeout` | `25m0s` | Maximum deployment time |\n| `spec.interval` | `1m0s` | Reconciliation frequency |\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:1-15](), [system/clusters/creodias/user-management/um-login-service.yaml:67-68]()\n\n## Domain and Network Configuration\n\nThe Login Service is accessible via a dedicated authentication domain with TLS encryption:\n\n```mermaid\ngraph LR\n    subgraph \"External Network\"\n        Internet[\"Internet\"]\n        DNSRecord[\"DNS: auth.develop.eoepca.org\u003cbr/\u003e 185.52.192.231\"]\n    end\n    \n    subgraph \"Kubernetes Ingress\"\n        IngressNginx[\"ingress-nginx\u003cbr/\u003eIP: 185.52.192.231\"]\n        IngressRule[\"Ingress\u003cbr/\u003ehost: auth.develop.eoepca.org\"]\n    end\n    \n    subgraph \"Certificate Management\"\n        CertManager[\"cert-manager\"]\n        LetsEncrypt[\"ClusterIssuer: letsencrypt\"]\n        TLSSecret[\"Secret: gluu-tls-certificate\"]\n    end\n    \n    subgraph \"Login Service\"\n        NginxService[\"nginx Service\u003cbr/\u003e(within um namespace)\"]\n        GluuPods[\"Gluu Components\"]\n    end\n    \n    Internet --\u003e|\"HTTPS\"| DNSRecord\n    DNSRecord --\u003e IngressNginx\n    IngressNginx --\u003e IngressRule\n    IngressRule --\u003e|\"TLS Termination\"| TLSSecret\n    IngressRule --\u003e NginxService\n    NginxService --\u003e GluuPods\n    \n    CertManager --\u003e|\"Issues Certificate\"| TLSSecret\n    LetsEncrypt --\u003e|\"CA Authority\"| CertManager\n    IngressRule -.-\u003e|\"annotation:\u003cbr/\u003ecert-manager.io/cluster-issuer\"| LetsEncrypt\n```\n\n**Global Configuration:**\n\n| Parameter | Value |\n|-----------|-------|\n| `global.domain` | `auth.develop.eoepca.org` |\n| `global.nginxIp` | `185.52.192.231` |\n| `global.namespace` | `um` |\n\n**Ingress Configuration:**\n\n| Parameter | Value |\n|-----------|-------|\n| `nginx.ingress.hosts[0]` | `auth.develop.eoepca.org` |\n| `nginx.ingress.tls[0].hosts[0]` | `auth.develop.eoepca.org` |\n| `nginx.ingress.tls[0].secretName` | `gluu-tls-certificate` |\n| `nginx.ingress.annotations` | `cert-manager.io/cluster-issuer: letsencrypt` |\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:19-20](), [system/clusters/creodias/user-management/um-login-service.yaml:53-66]()\n\n## Storage Architecture\n\nAll Login Service components share a common NFS-backed persistent volume for data persistence:\n\n```mermaid\ngraph TB\n    subgraph \"NFS Server\"\n        NFSExport[\"NFS Export\u003cbr/\u003e192.168.123.14\"]\n    end\n    \n    subgraph \"Storage Class\"\n        StorageClass[\"StorageClass\u003cbr/\u003eeoepca-nfs\"]\n    end\n    \n    subgraph \"Persistent Volume\"\n        PVC[\"PersistentVolumeClaim\u003cbr/\u003eeoepca-userman-pvc\u003cbr/\u003enamespace: um\"]\n    end\n    \n    subgraph \"Login Service Mounts\"\n        ConfigMount[\"config.volumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n        OpenDJMount[\"opendj.volumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n        oxAuthMount[\"oxauth.volumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n        oxTrustMount[\"oxtrust.volumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n    end\n    \n    subgraph \"Component Data Directories\"\n        OpenDJData[\"OpenDJ Data\u003cbr/\u003e(LDAP entries)\"]\n        oxAuthData[\"oxAuth Data\u003cbr/\u003e(tokens, sessions)\"]\n        oxTrustData[\"oxTrust Data\u003cbr/\u003e(configuration)\"]\n        CommonConfig[\"Shared Configuration\u003cbr/\u003e(certificates, keys)\"]\n    end\n    \n    NFSExport --\u003e|\"Provides Storage\"| StorageClass\n    StorageClass --\u003e|\"Provisions\"| PVC\n    PVC --\u003e ConfigMount\n    PVC --\u003e OpenDJMount\n    PVC --\u003e oxAuthMount\n    PVC --\u003e oxTrustMount\n    \n    ConfigMount --\u003e CommonConfig\n    OpenDJMount --\u003e OpenDJData\n    oxAuthMount --\u003e oxAuthData\n    oxTrustMount --\u003e oxTrustData\n```\n\n**Volume Claim Configuration:**\n\nThe HelmRelease specifies `volumeClaim.create: false`, indicating that the PVC `eoepca-userman-pvc` must be created by the storage subsystem before deploying the Login Service. This shared volume approach allows all user management components (Login Service, PDP Engine, User Profile) to access common data.\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:16-22](), [system/clusters/creodias/user-management/um-login-service.yaml:28-29](), [system/clusters/creodias/user-management/um-login-service.yaml:35-36](), [system/clusters/creodias/user-management/um-login-service.yaml:42-43](), [system/clusters/creodias/system/storage/hr-storage.yaml:27-28]()\n\n## Resource Requirements\n\nThe following table summarizes the resource requests for each component:\n\n| Component | CPU Request | Memory Request | Purpose |\n|-----------|-------------|----------------|---------|\n| `opendj` | 100m | 300Mi | LDAP directory operations |\n| `oxauth` | 100m | 1000Mi | Token issuance and validation |\n| `oxtrust` | 100m | 1500Mi | Admin UI and configuration management |\n| `oxpassport` | 100m | 100Mi | Identity provider proxying |\n\n**Total Requests:** 400m CPU, 2900Mi (approximately 2.9 GB) memory\n\nThese are minimum resource requests that Kubernetes uses for scheduling. Actual resource usage may be higher depending on authentication load and concurrent user sessions.\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:30-52]()\n\n## Integration with User Management Ecosystem\n\nThe Login Service integrates with other user management components deployed in the `um` namespace:\n\n```mermaid\ngraph TB\n    subgraph \"um Namespace Components\"\n        LoginService[\"um-login-service\u003cbr/\u003e(HelmRelease)\"]\n        PDPEngine[\"um-pdp-engine\u003cbr/\u003e(HelmRelease)\"]\n        UserProfile[\"um-user-profile\u003cbr/\u003e(HelmRelease)\"]\n    end\n    \n    subgraph \"Shared Infrastructure\"\n        SharedPVC[\"eoepca-userman-pvc\u003cbr/\u003e(PersistentVolumeClaim)\"]\n        SharedDomain[\"auth.develop.eoepca.org\u003cbr/\u003e(Domain)\"]\n        SharedNginxIP[\"185.52.192.231\u003cbr/\u003e(Nginx IP)\"]\n    end\n    \n    subgraph \"External Dependencies\"\n        Keycloak[\"Identity Service\u003cbr/\u003e(Keycloak)\u003cbr/\u003enamespace: um\"]\n        HelmRepo[\"HelmRepository: eoepca\u003cbr/\u003enamespace: common\"]\n    end\n    \n    LoginService --\u003e|\"Mounts\"| SharedPVC\n    PDPEngine --\u003e|\"Mounts\"| SharedPVC\n    UserProfile --\u003e|\"Mounts\"| SharedPVC\n    \n    LoginService --\u003e|\"Uses Domain\"| SharedDomain\n    PDPEngine --\u003e|\"Uses Domain\"| SharedDomain\n    UserProfile --\u003e|\"Uses Domain\"| SharedDomain\n    \n    LoginService --\u003e|\"Ingress IP\"| SharedNginxIP\n    PDPEngine --\u003e|\"Ingress IP\"| SharedNginxIP\n    UserProfile --\u003e|\"Ingress IP\"| SharedNginxIP\n    \n    LoginService --\u003e|\"Fetches Chart\"| HelmRepo\n    PDPEngine --\u003e|\"Fetches Chart\"| HelmRepo\n    UserProfile --\u003e|\"Fetches Chart\"| HelmRepo\n    \n    LoginService -.-\u003e|\"Issues OIDC Tokens for\"| Keycloak\n```\n\n**Shared Configuration Parameters:**\n\nAll user management HelmReleases share common configuration:\n- `global.domain`: `auth.develop.eoepca.org`\n- `global.nginxIp`: `185.52.192.231`\n- `volumeClaim.name`: `eoepca-userman-pvc`\n- `volumeClaim.create`: `false`\n\nThis standardization ensures consistent deployment and simplifies maintenance.\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:53-56](), [system/clusters/creodias/user-management/um-pdp-engine.yaml:19-24](), [system/clusters/creodias/user-management/um-user-profile.yaml:16-21]()\n\n## Operational Considerations\n\n### Deployment Timeout\n\nThe HelmRelease specifies a timeout of `25m0s` to accommodate the initialization time required for Gluu components, particularly the LDAP schema import and certificate generation performed by OpenDJ.\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:67]()\n\n### Persistence Initialization\n\nThe deployment includes a StatefulSet job named `um-login-service-persistence-init-ss` that initializes the OpenDJ LDAP directory. If encountering \"failed to upgrade\" errors due to immutable field constraints, OpenDJ persistence can be temporarily disabled by setting `opendj.persistence.enabled: false`.\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:23-27]()\n\n### Flux CD Reconciliation\n\nThe Login Service is continuously monitored by Flux CD with a reconciliation interval of `1m0s`. Any drift between the desired state (defined in Git) and the actual cluster state triggers automatic remediation.\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:68]()\n\n## Authentication Flow Overview\n\nThe following diagram illustrates how the Login Service integrates into the EOEPCA authentication flow:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Service[\"Protected Service\u003cbr/\u003e(ADES/Workspace/etc)\"]\n    participant PEP[\"Resource Guard\u003cbr/\u003e(PEP)\"]\n    participant Keycloak[\"Identity Service\u003cbr/\u003e(Keycloak)\"]\n    participant Gluu[\"Login Service\u003cbr/\u003e(Gluu oxAuth)\"]\n    \n    User-\u003e\u003eService: \"Request Resource\"\n    Service-\u003e\u003ePEP: \"Forward Request\"\n    PEP-\u003e\u003eKeycloak: \"Get UMA Ticket\"\n    Keycloak--\u003e\u003ePEP: \"Return Ticket\"\n    PEP--\u003e\u003eUser: \"401 + WWW-Authenticate\"\n    \n    User-\u003e\u003eGluu: \"Authenticate\u003cbr/\u003e(username/password)\"\n    Gluu-\u003e\u003eGluu: \"Validate credentials\u003cbr/\u003eagainst OpenDJ\"\n    Gluu--\u003e\u003eUser: \"Return ID Token\"\n    \n    User-\u003e\u003eKeycloak: \"Exchange Ticket + ID Token\"\n    Keycloak-\u003e\u003eGluu: \"Validate ID Token\u003cbr/\u003e(OIDC UserInfo endpoint)\"\n    Gluu--\u003e\u003eKeycloak: \"Token Valid + User Claims\"\n    Keycloak--\u003e\u003eUser: \"Return RPT\"\n    \n    User-\u003e\u003ePEP: \"Retry with RPT\"\n    PEP-\u003e\u003eService: \"Forward Request\"\n    Service--\u003e\u003eUser: \"200 OK\"\n```\n\n**Key Points:**\n- Gluu oxAuth issues OIDC-compliant ID tokens\n- Keycloak validates these tokens via OIDC federation\n- User credentials are stored in OpenDJ LDAP directory\n- oxPassport enables login via external identity providers\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:34-40]()\n\n## Maintenance and Troubleshooting\n\n### Checking Component Status\n\n```bash\n# Check HelmRelease status\nkubectl get helmrelease um-login-service -n um\n\n# View all Login Service pods\nkubectl get pods -n um -l app.kubernetes.io/instance=um-login-service\n\n# Check individual component logs\nkubectl logs -n um -l app.kubernetes.io/component=opendj\nkubectl logs -n um -l app.kubernetes.io/component=oxauth\nkubectl logs -n um -l app.kubernetes.io/component=oxtrust\n```\n\n### Volume Mount Verification\n\n```bash\n# Verify PVC exists and is bound\nkubectl get pvc eoepca-userman-pvc -n um\n\n# Check volume mounts in pods\nkubectl describe pod \u003cpod-name\u003e -n um | grep -A5 \"Mounts:\"\n```\n\n### TLS Certificate Status\n\n```bash\n# Check certificate status\nkubectl get certificate gluu-tls-certificate -n um\n\n# View certificate details\nkubectl describe certificate gluu-tls-certificate -n um\n```\n\n**Sources:** [system/clusters/creodias/user-management/um-login-service.yaml:58-66]()"])</script><script>self.__next_f.push([1,"23:T6663,"])</script><script>self.__next_f.push([1,"# Policy Enforcement (PEP/PDP)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml](system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-ades.yaml](system/clusters/creodias/processing-and-chaining/proc-ades.yaml)\n- [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml](system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml](system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml)\n- [system/clusters/creodias/resource-management/ss-harbor.yaml](system/clusters/creodias/resource-management/ss-harbor.yaml)\n- [system/clusters/creodias/system/test/hr-dummy-service-guard.yaml](system/clusters/creodias/system/test/hr-dummy-service-guard.yaml)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the Policy Enforcement Point (PEP) and Policy Decision Point (PDP) components that provide authorization and access control for EOEPCA services. The PEP intercepts requests to protected services, while the PDP evaluates policies to determine whether access should be granted. Together, they implement User-Managed Access (UMA) 2.0 based authorization.\n\nFor authentication mechanisms and identity management, see [Identity Service (Keycloak)](#4.1). For the complete UMA authentication flow including ticket-based authorization, see [UMA Authentication Flow](#4.4).\n\nThis document covers:\n- PEP and PDP component architecture\n- Resource registration and policy configuration\n- The resource-guard deployment pattern\n- Policy evaluation mechanisms\n- Management tools for policy administration\n\n---\n\n## System Architecture\n\nThe EOEPCA platform deploys multiple PEP instances to protect different services (ADES, Workspace API, Resource Catalogue, Data Access), with a centralized PDP engine for policy evaluation. Each protected service is fronted by a `resource-guard` Helm chart deployment that includes both a `pep-engine` and a `uma-user-agent` component.\n\n```mermaid\ngraph TB\n    subgraph \"Protected Services\"\n        ADES[\"proc-ades\u003cbr/\u003e(ADES Service)\"]\n        WorkspaceAPI[\"workspace-api\u003cbr/\u003e(Workspace API)\"]\n        DataAccess[\"data-access-renderer\u003cbr/\u003e(Data Access)\"]\n        ResourceCat[\"resource-catalogue-service\u003cbr/\u003e(Resource Catalogue)\"]\n    end\n    \n    subgraph \"PEP Deployments (resource-guard chart)\"\n        AdesPEP[\"ades-pep\u003cbr/\u003e(pep-engine)\"]\n        AdesUMA[\"ades-uma-user-agent\"]\n        \n        WorkspacePEP[\"workspace-api-pep\u003cbr/\u003e(pep-engine)\"]\n        WorkspaceUMA[\"workspace-api-uma-user-agent\"]\n        \n        CombinedPEP[\"combined-rm-pep\u003cbr/\u003e(pep-engine)\"]\n        CombinedUMA[\"combined-rm-uma-user-agent\"]\n    end\n    \n    subgraph \"Policy Decision (um namespace)\"\n        PDP[\"pdp-engine\u003cbr/\u003e(Policy Decision Point)\"]\n        IdentitySvc[\"identity-service\u003cbr/\u003e(Keycloak)\"]\n    end\n    \n    subgraph \"External Access\"\n        User[\"User Request\"]\n        Ingress[\"nginx-ingress\"]\n    end\n    \n    User --\u003e|\"1. HTTP Request\"| Ingress\n    Ingress --\u003e|\"2. Route\"| AdesUMA\n    Ingress --\u003e|\"2. Route\"| WorkspaceUMA\n    Ingress --\u003e|\"2. Route\"| CombinedUMA\n    \n    AdesUMA --\u003e|\"3. Intercept\"| AdesPEP\n    WorkspaceUMA --\u003e|\"3. Intercept\"| WorkspacePEP\n    CombinedUMA --\u003e|\"3. Intercept\"| CombinedPEP\n    \n    AdesPEP --\u003e|\"4. Policy Check\"| PDP\n    WorkspacePEP --\u003e|\"4. Policy Check\"| PDP\n    CombinedPEP --\u003e|\"4. Policy Check\"| PDP\n    \n    PDP --\u003e|\"5. Validate Token\"| IdentitySvc\n    \n    AdesPEP -.-\u003e|\"6. Forward (if permitted)\"| ADES\n    WorkspacePEP -.-\u003e|\"6. Forward (if permitted)\"| WorkspaceAPI\n    CombinedPEP -.-\u003e|\"6. Forward (if permitted)\"| DataAccess\n    CombinedPEP -.-\u003e|\"6. Forward (if permitted)\"| ResourceCat\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:1-92](), [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:1-101](), [system/clusters/creodias/user-management/kustomization.yaml:12-12]()\n\n---\n\n## Resource Guard Deployment Pattern\n\nThe `resource-guard` Helm chart is the standard deployment pattern for protecting services. It packages two main components: `pep-engine` for policy enforcement and `uma-user-agent` for UMA authentication integration.\n\n### Chart Structure\n\n```mermaid\ngraph LR\n    subgraph \"resource-guard Helm Chart\"\n        Chart[\"HelmRelease\u003cbr/\u003eresource-guard v1.3.2\"]\n        \n        subgraph \"Values Configuration\"\n            GlobalVals[\"global:\u003cbr/\u003e- context\u003cbr/\u003e- domain\u003cbr/\u003e- nginxIp\"]\n            PEPVals[\"pep-engine:\u003cbr/\u003e- configMap\u003cbr/\u003e- defaultResources\u003cbr/\u003e- volumeClaim\"]\n            UMAVals[\"uma-user-agent:\u003cbr/\u003e- nginxIntegration\u003cbr/\u003e- client credentials\u003cbr/\u003e- unauthorizedResponse\"]\n        end\n        \n        subgraph \"Deployed Components\"\n            PEPDeploy[\"Deployment:\u003cbr/\u003epep-engine container\"]\n            UMADeploy[\"Deployment:\u003cbr/\u003euma-user-agent container\"]\n            IngressRes[\"Ingress:\u003cbr/\u003enginx integration\"]\n            PVC[\"PersistentVolumeClaim:\u003cbr/\u003epolicy storage\"]\n        end\n    end\n    \n    Chart --\u003e GlobalVals\n    Chart --\u003e PEPVals\n    Chart --\u003e UMAVals\n    \n    PEPVals --\u003e PEPDeploy\n    PEPVals --\u003e PVC\n    UMAVals --\u003e UMADeploy\n    UMAVals --\u003e IngressRes\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-14](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:7-14]()\n\n### Global Configuration\n\nEach resource-guard deployment requires global configuration to identify the service being protected:\n\n| Parameter | Description | Example |\n|-----------|-------------|---------|\n| `global.context` | Service identifier used in hostname | `ades`, `workspace-api`, `combined-rm` |\n| `global.domain` | Base domain for the platform | `develop.eoepca.org` |\n| `global.nginxIp` | Ingress controller IP address | `185.52.192.231` |\n| `global.certManager.clusterIssuer` | TLS certificate issuer | `letsencrypt` |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:19-24](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:19-24]()\n\n---\n\n## Policy Enforcement Point (pep-engine)\n\nThe `pep-engine` component is responsible for intercepting requests, managing resource registrations, and consulting the PDP for authorization decisions.\n\n### Configuration Structure\n\n```mermaid\ngraph TB\n    subgraph \"pep-engine ConfigMap\"\n        ASHost[\"asHostname: auth\u003cbr/\u003e(Authorization Server)\"]\n        PDPHost[\"pdpHostname: auth\u003cbr/\u003e(Policy Decision Point)\"]\n        LimitUses[\"limitUses: 100\u003cbr/\u003e(optional rate limiting)\"]\n    end\n    \n    subgraph \"Resource Definitions\"\n        DefRes[\"defaultResources:\u003cbr/\u003eAuto-registered on startup\"]\n        CustomRes[\"customDefaultResources:\u003cbr/\u003eUser-specific resources\"]\n    end\n    \n    subgraph \"Resource Properties\"\n        ResName[\"name: Human-readable name\"]\n        ResDesc[\"description: Purpose\"]\n        ResURI[\"resource_uri: URL path pattern\"]\n        ResScopes[\"scopes: Access scopes\"]\n        ResOwner[\"default_owner: User ID (UUID)\"]\n    end\n    \n    DefRes --\u003e ResName\n    DefRes --\u003e ResDesc\n    DefRes --\u003e ResURI\n    DefRes --\u003e ResScopes\n    DefRes --\u003e ResOwner\n    \n    CustomRes --\u003e ResName\n    CustomRes --\u003e ResDesc\n    CustomRes --\u003e ResURI\n    CustomRes --\u003e ResScopes\n    CustomRes --\u003e ResOwner\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:28-53](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:28-49]()\n\n### Resource Registration Examples\n\n#### ADES Service Protection\n\nThe ADES service uses per-user resource paths to isolate user workspaces:\n\n[system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:34-49]()\n\n```yaml\ncustomDefaultResources:\n- name: \"ADES Service for user 'eric'\"\n  description: \"Protected Access for eric to his space in the ADES\"\n  resource_uri: \"/eric\"\n  scopes: []\n  default_owner: \"fad43ef3-23ef-48b0-86f0-1cf29d97908e\"\n- name: \"ADES Service for user 'bob'\"\n  description: \"Protected Access for bob to his space in the ADES\"\n  resource_uri: \"/bob\"\n  scopes: []\n  default_owner: \"f0a19e32-5651-404e-8acd-128c2c284300\"\n```\n\nEach user gets a dedicated resource path (`/eric`, `/bob`) with their UUID as the `default_owner`, ensuring they can only access their own processing space.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:34-49]()\n\n#### Workspace API Protection\n\nThe Workspace API uses a combination of operator-only and public resources:\n\n[system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:35-53]()\n\n```yaml\ndefaultResources:\n  - name: \"Workspace API Base Path\"\n    description: \"Protected root path for operators only\"\n    resource_uri: \"/\"\n    scopes: []\n    default_owner: \"0000000000000\"\ncustomDefaultResources:\n  - name: \"Workspace API Swagger Docs\"\n    description: \"Public access to workspace API swagger docs\"\n    resource_uri: \"/docs\"\n    scopes:\n      - \"public_access\"\n    default_owner: \"0000000000000\"\n```\n\nThe root path is restricted to operators (owner ID `0000000000000`), while documentation endpoints have a `public_access` scope allowing unauthenticated access.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:35-53]()\n\n#### Template-Based Workspace Resources\n\nFor dynamically created workspaces, templates with placeholders are used:\n\n[system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:40-45]()\n\n```yaml\ndefaultResources:\n  - name: \"Workspace {{ workspace_name }} Root\"\n    description: \"Root URL of a users workspace\"\n    resource_uri: \"/\"\n    scopes: []\n    default_owner: {{ default_owner }}\n```\n\nThe Workspace API replaces `{{ workspace_name }}` and `{{ default_owner }}` when provisioning user-specific workspaces, creating isolated protected resources for each user.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:1-99]()\n\n### Volume Persistence\n\nThe `pep-engine` uses persistent storage to maintain resource registrations across restarts:\n\n| Deployment | Volume Claim | Creation |\n|------------|--------------|----------|\n| `ades-pep` | `eoepca-proc-pvc` | Pre-existing (shared) |\n| `workspace-api-pep` | `eoepca-resman-pvc` | Pre-existing (shared) |\n| `combined-rm-pep` | `eoepca-resman-pvc` | Pre-existing (shared) |\n| `dummy-service-pep` | `dummy-service-pep-pvc` | Not created (ephemeral) |\n| Workspace guards | `{{ workspace_name }}-pep-pvc` | Created per workspace |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:55-57](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:57-59](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:37-39]()\n\n---\n\n## UMA User Agent (uma-user-agent)\n\nThe `uma-user-agent` component handles the UMA authentication flow and integrates with nginx ingress to intercept requests before they reach the backend service.\n\n### Nginx Integration\n\n```mermaid\ngraph LR\n    subgraph \"Ingress Configuration\"\n        Ingress[\"nginx Ingress\u003cbr/\u003eResource\"]\n        Host[\"Host:\u003cbr/\u003eades.develop.eoepca.org\"]\n        Annot[\"Annotations:\u003cbr/\u003eproxy-read-timeout: 600\u003cbr/\u003eenable-cors: true\u003cbr/\u003erewrite-target\"]\n    end\n    \n    subgraph \"Path Routing\"\n        Path1[\"Path: /(.*)\"]\n        Svc1[\"Service: proc-ades\u003cbr/\u003ePort: 80\"]\n    end\n    \n    subgraph \"uma-user-agent Service\"\n        UMAAgent[\"uma-user-agent\u003cbr/\u003eintercepts request\"]\n        ClientCreds[\"OIDC Client Credentials\u003cbr/\u003e(Secret)\"]\n        UnAuthResp[\"unauthorizedResponse:\u003cbr/\u003eBearer realm=...\"]\n    end\n    \n    Ingress --\u003e Host\n    Ingress --\u003e Annot\n    Host --\u003e Path1\n    Path1 --\u003e UMAAgent\n    UMAAgent --\u003e ClientCreds\n    UMAAgent --\u003e UnAuthResp\n    UMAAgent -.-\u003e|\"After authorization\"| Svc1\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:65-77](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:67-80]()\n\n### Configuration Parameters\n\n| Parameter | Purpose | Example |\n|-----------|---------|---------|\n| `nginxIntegration.enabled` | Enable nginx auth proxy | `true` |\n| `nginxIntegration.hosts[].host` | Service hostname | `ades`, `workspace-api` |\n| `nginxIntegration.hosts[].paths[].path` | URL pattern to protect | `/(.*)`  |\n| `nginxIntegration.hosts[].paths[].service.name` | Backend service name | `proc-ades` |\n| `client.credentialsSecretName` | OIDC client secret name | `proc-uma-user-agent` |\n| `logging.level` | Log verbosity | `info`, `debug` |\n| `unauthorizedResponse` | 401 response header | `Bearer realm=\"https://...\"` |\n| `openAccess` | Disable authentication | `false` |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:61-83](), [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:46-94]()\n\n### Open Access Mode\n\nFor services that should be publicly accessible without authentication, `openAccess: true` can be set:\n\n[system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:94-94]()\n\nThis is used for the `combined-rm-guard` which protects public data access and catalogue services.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:94-94]()\n\n---\n\n## Policy Decision Point (pdp-engine)\n\nThe `pdp-engine` is a centralized component deployed in the `um` namespace that evaluates authorization policies for all PEP instances across the platform.\n\n### Deployment Structure\n\n```mermaid\ngraph TB\n    subgraph \"um namespace\"\n        PDP[\"Deployment: pdp-engine\"]\n        PDPSvc[\"Service: pdp-engine\"]\n        PDPConfig[\"ConfigMap:\u003cbr/\u003ePolicy evaluation logic\"]\n    end\n    \n    subgraph \"Policy Evaluation\"\n        LoadPolicies[\"Load policies\u003cbr/\u003efrom storage\"]\n        CheckUser[\"Verify user identity\u003cbr/\u003ewith identity-service\"]\n        EvalPolicy[\"Evaluate resource\u003cbr/\u003eownership \u0026 scopes\"]\n        Decision[\"Return: Permit/Deny\"]\n    end\n    \n    subgraph \"External Dependencies\"\n        IdentitySvc[\"identity-service\u003cbr/\u003e(Keycloak)\"]\n        PEPs[\"Multiple PEP instances\"]\n    end\n    \n    PDP --\u003e LoadPolicies\n    LoadPolicies --\u003e CheckUser\n    CheckUser --\u003e IdentitySvc\n    CheckUser --\u003e EvalPolicy\n    EvalPolicy --\u003e Decision\n    Decision --\u003e PEPs\n```\n\n**Sources:** [system/clusters/creodias/user-management/kustomization.yaml:12-12](), [bin/dump-policy.sh:27-27]()\n\n### Policy Evaluation Logic\n\nWhen a PEP sends a policy check request, the PDP performs these evaluations:\n\n1. **Token Validation**: Verifies the RPT (Requesting Party Token) with the Identity Service\n2. **Resource Lookup**: Finds the registered resource matching the requested URI\n3. **Ownership Check**: Compares the user's UUID from the token with the resource's `default_owner`\n4. **Scope Evaluation**: Checks if the request matches any allowed scopes (e.g., `public_access`)\n5. **Decision**: Returns `Permit` if ownership or scopes match, otherwise `Deny`\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:28-34](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:28-34]()\n\n---\n\n## Policy Management Tools\n\nThe EOEPCA platform provides command-line tools for managing policies across all PEP and PDP deployments.\n\n### Management Tools Interface\n\nEach `pep-engine` and `pdp-engine` container includes a `management_tools` CLI for policy administration:\n\n```mermaid\ngraph LR\n    subgraph \"management_tools CLI\"\n        List[\"list --all\u003cbr/\u003e(dump all policies)\"]\n        Remove[\"remove -r \u003cresource-id\u003e\u003cbr/\u003e(delete resource)\"]\n        Register[\"register\u003cbr/\u003e(add resource)\"]\n    end\n    \n    subgraph \"Storage\"\n        PVC[\"PersistentVolume\u003cbr/\u003e(policy database)\"]\n    end\n    \n    List --\u003e PVC\n    Remove --\u003e PVC\n    Register --\u003e PVC\n```\n\n**Sources:** [bin/dump-policy.sh:37-37](), [bin/unregister-resource.sh:23-23]()\n\n### Dumping Policies (dump-policy.sh)\n\nThe `dump-policy.sh` script extracts policy data from all PEP and PDP deployments:\n\n[bin/dump-policy.sh:20-28]()\n\n```bash\ndumpAll() {\n  dumpDeployment proc ades-pep\n  dumpDeployment rm combined-rm-pep\n  dumpDeployment rm workspace-api-pep\n  dumpDeployment test dummy-service-pep\n  dumpDeployment um pdp-engine nojson\n}\n```\n\nThe script executes `management_tools list --all` inside each container and saves the output as JSON:\n\n[bin/dump-policy.sh:30-40]()\n\n```bash\ndumpDeployment() {\n  namespace=\"${1}\"\n  deployment=\"${2}\"\n  print_json=\"$( [ \"${3}\" = \"nojson\" ] \u0026\u0026 echo \"\" || echo \"| jq\" )\"\n  dumpfile=\"${ORIG_DIR}/${deployment}.json\"\n\n  echo -n \"Dumping policy for ${namespace}/${deployment} to ${dumpfile}\"\n  cmd=\"kubectl -n \"${namespace}\" exec -it deploy/\"${deployment}\" -c \"${deployment}\" -- management_tools list --all ${print_json}\"\n  eval \"${cmd}\" \u003e \"${dumpfile}\"\n  echo \" done\"\n}\n```\n\n**Usage:**\n```bash\n# Dump all policies\n./bin/dump-policy.sh\n\n# Dump specific deployment\n./bin/dump-policy.sh rm workspace-api-pep\n```\n\n**Output:** Creates JSON files like `ades-pep.json`, `workspace-api-pep.json`, `pdp-engine.json` in the current directory.\n\n**Sources:** [bin/dump-policy.sh:1-43]()\n\n### Unregistering Resources (unregister-resource.sh)\n\nThe `unregister-resource.sh` script removes a resource from all PEP and PDP instances:\n\n[bin/unregister-resource.sh:19-24]()\n\n```bash\nresourceId=\"$1\"\n\n# combined-rm-pep\necho -n \"Delete resource ${resourceId} from combined-rm-pep...\"\nkubectl -n rm exec -it svc/combined-rm-pep -c combined-rm-pep -- management_tools remove -r ${resourceId}\necho \" done\"\n```\n\nThe script iterates through all known PEP deployments and the PDP:\n\n[bin/unregister-resource.sh:36-54]()\n\n```bash\n# ades-pep\nkubectl -n proc exec -it svc/ades-pep -c ades-pep -- management_tools remove -r ${resourceId}\n\n# workspace-api-pep\nkubectl -n rm exec -it svc/workspace-api-pep -c workspace-api-pep -- management_tools remove -r ${resourceId}\n\n# dummy-service-pep\nkubectl -n test exec -it svc/dummy-service-pep -c dummy-service-pep -- management_tools remove -r ${resourceId}\n\n# pdp\nkubectl -n um exec -it svc/pdp-engine -c pdp-engine -- management_tools remove -r ${resourceId}\n```\n\n**Usage:**\n```bash\n./bin/unregister-resource.sh \u003cresource-id\u003e\n```\n\nThis is useful for cleaning up stale resource registrations or removing test resources.\n\n**Sources:** [bin/unregister-resource.sh:1-55]()\n\n---\n\n## Policy Evaluation Flow\n\nThis sequence diagram shows the complete flow when a user requests a protected resource:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant NginxIngress as \"nginx-ingress\"\n    participant UMA as \"uma-user-agent\"\n    participant PEP as \"pep-engine\"\n    participant PDP as \"pdp-engine\"\n    participant Identity as \"identity-service\"\n    participant Backend as \"Backend Service\u003cbr/\u003e(ADES/Workspace API)\"\n    \n    User-\u003e\u003eNginxIngress: GET /eric/processes\n    NginxIngress-\u003e\u003eUMA: Forward request\n    \n    alt No Authorization Header\n        UMA-\u003e\u003eIdentity: Get UMA Ticket\n        Identity--\u003e\u003eUMA: ticket=abc123\n        UMA--\u003e\u003eUser: 401 Unauthorized\u003cbr/\u003eWWW-Authenticate: UMA ticket=abc123\n        User-\u003e\u003eUser: Authenticate \u0026 get RPT\n        User-\u003e\u003eNginxIngress: Retry with Authorization: Bearer RPT\n        NginxIngress-\u003e\u003eUMA: Forward with RPT\n    end\n    \n    UMA-\u003e\u003ePEP: Check authorization\u003cbr/\u003e(resource_uri=/eric, user_token=RPT)\n    \n    PEP-\u003e\u003ePEP: Load resource definition\u003cbr/\u003efrom PersistentVolume\n    \n    PEP-\u003e\u003ePDP: Evaluate policy\u003cbr/\u003e(resource_id, user_id, action)\n    \n    PDP-\u003e\u003eIdentity: Validate RPT token\n    Identity--\u003e\u003ePDP: Token valid, user_id=fad43ef3...\n    \n    PDP-\u003e\u003ePDP: Check resource.default_owner\u003cbr/\u003e== user_id\n    \n    alt Policy Permits\n        PDP--\u003e\u003ePEP: Decision: Permit\n        PEP--\u003e\u003eUMA: Allow\n        UMA-\u003e\u003eBackend: Forward request\n        Backend--\u003e\u003eUMA: Response\n        UMA--\u003e\u003eUser: 200 OK + Response\n    else Policy Denies\n        PDP--\u003e\u003ePEP: Decision: Deny\n        PEP--\u003e\u003eUMA: Deny\n        UMA--\u003e\u003eUser: 403 Forbidden\n    end\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:1-92]()\n\n---\n\n## Multi-Service Protection Patterns\n\n### Protecting Multiple Endpoints (Combined Guard)\n\nThe `combined-rm-guard` demonstrates protecting multiple backend services through a single PEP instance:\n\n[system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:52-84]()\n\n```yaml\nnginxIntegration:\n  enabled: true\n  hosts:\n    - host: resource-catalogue\n      paths:\n        - path: /(.*)\n          service:\n            name: resource-catalogue-service\n            port: 80\n    - host: data-access\n      paths:\n        - path: /(ows.*)\n          service:\n            name: data-access-renderer\n            port: 80\n        - path: /(opensearch.*)\n          service:\n            name: data-access-renderer\n            port: 80\n        - path: /cache/(.*)\n          service:\n            name: data-access-cache\n            port: 80\n```\n\nThis configuration protects both Resource Catalogue and Data Access services through regex path matching, routing different URL patterns to different backend services.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:50-88]()\n\n### Per-Workspace Protection\n\nDynamically provisioned workspaces use the template pattern to create isolated resource guards:\n\n[system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:50-84]()\n\n```yaml\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: \"resource-catalogue.{{ workspace_name }}\"\n        paths:\n          - path: \"/(.*)\"\n            service:\n              name: \"resource-catalogue-service\"\n              port: 80\n      - host: \"data-access.{{ workspace_name }}\"\n        paths:\n          - path: \"/(ows.*)\"\n            service:\n              name: \"vs-renderer\"\n              port: 80\n```\n\nEach workspace gets its own subdomain (`resource-catalogue.eric-workspace.develop.eoepca.org`) with a dedicated PEP that enforces the workspace owner's access rights.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:49-98]()\n\n---\n\n## Testing PEP/PDP\n\n### Test Service Deployment\n\nThe platform includes a test service guard for validation:\n\n[system/clusters/creodias/system/test/hr-dummy-service-guard.yaml:1-81]()\n\nThis deployment protects a dummy service with two user-specific resources for testing authorization logic:\n\n```yaml\ncustomDefaultResources:\n- name: \"Eric's space\"\n  description: \"Protected Access for eric to his space in the dummy service\"\n  resource_uri: \"/ericspace\"\n  scopes: []\n  default_owner: \"fad43ef3-23ef-48b0-86f0-1cf29d97908e\"\n- name: \"Bob's space\"\n  description: \"Protected Access for bob to his space in the dummy service\"\n  resource_uri: \"/bobspace\"\n  scopes: []\n  default_owner: \"f0a19e32-5651-404e-8acd-128c2c284300\"\n```\n\n**Sources:** [system/clusters/creodias/system/test/hr-dummy-service-guard.yaml:34-44]()\n\n### Acceptance Tests\n\nThe Robot Framework tests validate PEP/PDP behavior through the Resource Catalogue:\n\n[test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:1-96]()\n\nTests include:\n- CSW operations with authenticated access\n- OpenSearch queries through protected endpoints\n- Workspace-specific catalogue access validation\n\n**Sources:** [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:1-96](), [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:1-156]()\n\n---\n\n## Summary Table: PEP Deployments\n\n| Deployment | Namespace | Context | Protected Service(s) | Resource Pattern |\n|------------|-----------|---------|---------------------|------------------|\n| `ades-pep` | `proc` | `ades` | ADES processing service | Per-user paths: `/eric`, `/bob`, `/alice` |\n| `workspace-api-pep` | `rm` | `workspace-api` | Workspace provisioning API | Root path + public docs |\n| `combined-rm-pep` | `rm` | `combined-rm` | Resource Catalogue + Data Access | Multiple hosts and paths |\n| `dummy-service-pep` | `test` | `dummy-service` | Test service | User-specific paths: `/ericspace`, `/bobspace` |\n| `resource-guard` (template) | `{workspace}-namespace` | `{workspace_name}` | Workspace Resource Catalogue + Data Access | Workspace root path |\n\n**Sources:** [bin/dump-policy.sh:20-27](), [bin/unregister-resource.sh:21-48]()"])</script><script>self.__next_f.push([1,"24:T5b6d,"])</script><script>self.__next_f.push([1,"# UMA Authentication Flow\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml](system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-ades.yaml](system/clusters/creodias/processing-and-chaining/proc-ades.yaml)\n- [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml](system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml](system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml)\n- [system/clusters/creodias/resource-management/ss-harbor.yaml](system/clusters/creodias/resource-management/ss-harbor.yaml)\n- [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml](system/clusters/creodias/system/demo/hr-eoepca-portal.yaml)\n- [system/clusters/creodias/system/demo/ss-django-secrets-create.sh](system/clusters/creodias/system/demo/ss-django-secrets-create.sh)\n- [system/clusters/creodias/system/demo/ss-django-secrets.yaml](system/clusters/creodias/system/demo/ss-django-secrets.yaml)\n- [system/clusters/creodias/system/test/hr-dummy-service-guard.yaml](system/clusters/creodias/system/test/hr-dummy-service-guard.yaml)\n- [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml](system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml)\n- [system/clusters/creodias/user-management/um-identity-service.yaml](system/clusters/creodias/user-management/um-identity-service.yaml)\n- [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot](test/acceptance/02__Processing/01__ADES/01__API_PROC.robot)\n- [test/acceptance/02__Processing/01__ADES/02__WPS.robot](test/acceptance/02__Processing/01__ADES/02__WPS.robot)\n- [test/acceptance/__init__.robot](test/acceptance/__init__.robot)\n- [test/client/.gitignore](test/client/.gitignore)\n- [test/client/DemoClient.py](test/client/DemoClient.py)\n- [test/client/debug/jwt-output-by-pep.json](test/client/debug/jwt-output-by-pep.json)\n- [test/client/main.py](test/client/main.py)\n- [test/client/requirements.txt](test/client/requirements.txt)\n- [test/client/setup.sh](test/client/setup.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a detailed technical explanation of the User-Managed Access (UMA) 2.0 authentication and authorization flow implemented throughout the EOEPCA platform. UMA is the protocol used to secure access to protected resources across all EOEPCA services, including ADES, Workspace API, Data Access, and Resource Catalogue services.\n\nThe document focuses specifically on the ticket-based authorization mechanism, token exchange processes, and the interaction between clients, Policy Enforcement Points (PEPs), and the Identity Service. For information about the PEP/PDP architecture and resource registration, see [Policy Enforcement (PEP/PDP)](#4.3). For details about the Identity Service deployment, see [Identity Service (Keycloak)](#4.1).\n\n**Sources:** [test/client/DemoClient.py:239-291](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:61-82]()\n\n## UMA Overview\n\nUser-Managed Access (UMA) is an OAuth2-based protocol that enables resource owners to control access to their protected resources. In EOEPCA, UMA provides fine-grained, policy-based access control where users can own and control access to their workspaces, processing jobs, and data resources.\n\n### Key Components\n\n| Component | Role | Implementation |\n|-----------|------|----------------|\n| **Authorization Server (AS)** | Issues tokens and tickets | Identity Service (Keycloak) at `auth.develop.eoepca.org` |\n| **Resource Server (RS)** | Hosts protected resources | ADES, Workspace API, Data Access, etc. |\n| **Policy Enforcement Point (PEP)** | Intercepts requests and enforces policies | `pep-engine` deployed with each protected service |\n| **UMA User Agent** | Handles UMA flow and token exchange | `uma-user-agent` component |\n| **Policy Decision Point (PDP)** | Evaluates access policies | PDP Engine at `auth.develop.eoepca.org` |\n| **Client** | Requests access to resources | User applications, DemoClient, Robot tests |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:28-82](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:28-85]()\n\n## Token Types\n\nThe UMA flow involves several token types, each serving a specific purpose:\n\n### ID Token\n\nA JWT issued by the Identity Service after user authentication via username/password. The ID Token contains user identity claims and is used to prove the user's identity during the UMA flow.\n\n**Obtained via:** Password grant to token endpoint with client credentials\n\n```\nPOST /oxauth/restv1/token\ngrant_type=password\nusername={username}\npassword={password}\nclient_id={client_id}\nclient_secret={client_secret}\nscope=openid user_name profile is_operator\n```\n\n**Sources:** [test/client/DemoClient.py:124-139]()\n\n### UMA Ticket\n\nA short-lived token issued by the PEP when an unauthorized request is made. The ticket represents a pending authorization request and must be exchanged for an RPT.\n\n**Received via:** `WWW-Authenticate` header in 401 response from PEP\n\n```\nWWW-Authenticate: Bearer realm=\"...\", ticket={uma_ticket}\n```\n\n**Sources:** [test/client/DemoClient.py:274-279]()\n\n### RPT (Requesting Party Token)\n\nThe Requesting Party Token is the access token used to actually access protected resources. It is obtained by exchanging a UMA ticket + ID Token at the Authorization Server's token endpoint.\n\n**Obtained via:** UMA ticket grant\n\n```\nPOST /oxauth/restv1/token\ngrant_type=urn:ietf:params:oauth:grant-type:uma-ticket\nticket={uma_ticket}\nclaim_token={id_token}\nclaim_token_format=http://openid.net/specs/openid-connect-core-1_0.html#IDToken\nclient_id={client_id}\nclient_secret={client_secret}\n```\n\n**Sources:** [test/client/DemoClient.py:185-212]()\n\n### Access Token (Cached RPT)\n\nOnce obtained, the RPT is reused for subsequent requests to the same resource until it expires. The client caches this token to avoid repeating the UMA flow unnecessarily.\n\n**Sources:** [test/client/DemoClient.py:259-261]()\n\n## Complete UMA Authentication Flow\n\nThe following diagram shows the complete UMA authentication flow with actual code references from the DemoClient implementation:\n\n```mermaid\nsequenceDiagram\n    participant Client as Client\u003cbr/\u003e(DemoClient)\n    participant UMA as uma-user-agent\u003cbr/\u003e(PEP Proxy)\n    participant PEP as pep-engine\u003cbr/\u003e(Resource Server)\n    participant Backend as Backend Service\u003cbr/\u003e(ADES/Workspace/etc)\n    participant AS as Authorization Server\u003cbr/\u003e(Keycloak)\n\n    Note over Client: uma_http_request()\u003cbr/\u003eline 239\n    \n    rect rgba(200, 200, 200, 0.1)\n        Note over Client,AS: Initial Request (No Token)\n        Client-\u003e\u003eUMA: GET /resource\u003cbr/\u003eX-User-Id: {id_token}\n        Note over Client: line 257-265\n        UMA-\u003e\u003ePEP: Forward request\n        PEP-\u003e\u003eAS: Check resource registration\n        AS--\u003e\u003ePEP: Resource not authorized\n        PEP--\u003e\u003eUMA: 401 Unauthorized\n        UMA--\u003e\u003eClient: 401 + WWW-Authenticate\u003cbr/\u003eticket={uma_ticket}\n        Note over Client: line 270-279\n    end\n\n    rect rgba(200, 200, 200, 0.1)\n        Note over Client,AS: Token Exchange\n        Client-\u003e\u003eAS: POST /token\u003cbr/\u003egrant_type=uma-ticket\u003cbr/\u003eticket={ticket}\u003cbr/\u003eclaim_token={id_token}\n        Note over Client: get_access_token_from_ticket()\u003cbr/\u003eline 185-212\n        AS-\u003e\u003eAS: Validate ID Token\n        AS-\u003e\u003eAS: Check policies (PDP)\n        AS--\u003e\u003eClient: 200 OK\u003cbr/\u003e{access_token: RPT}\n        Note over Client: line 207-208\n    end\n\n    rect rgba(200, 200, 200, 0.1)\n        Note over Client,AS: Retry with RPT\n        Client-\u003e\u003eUMA: GET /resource\u003cbr/\u003eAuthorization: Bearer {RPT}\u003cbr/\u003eX-User-Id: {id_token}\n        Note over Client: line 260-265\n        UMA-\u003e\u003ePEP: Forward request with RPT\n        PEP-\u003e\u003eAS: Validate RPT\n        AS--\u003e\u003ePEP: Token valid + permissions\n        PEP-\u003e\u003eBackend: Forward request\n        Backend--\u003e\u003ePEP: 200 OK + response\n        PEP--\u003e\u003eUMA: 200 OK + response\n        UMA--\u003e\u003eClient: 200 OK + response\n        Note over Client: line 267-268\n    end\n```\n\n**Sources:** [test/client/DemoClient.py:239-291](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:61-82]()\n\n## Detailed Flow Steps\n\n### Step 1: Initial Request Without Token\n\nWhen a client first attempts to access a protected resource, it may not have an access token. The request is sent with only the ID Token in the `X-User-Id` header:\n\n```\nGET https://ades.develop.eoepca.org/eric/wps3/processes\nX-User-Id: {id_token}\n```\n\nThe `uma-user-agent` component intercepts this request and forwards it to the `pep-engine`. Since no valid RPT is present, the PEP queries the Authorization Server to check if the resource is registered and if the user has access.\n\n**Sources:** [test/client/DemoClient.py:256-264]()\n\n### Step 2: 401 Response with UMA Ticket\n\nThe PEP responds with a `401 Unauthorized` status and includes a `WWW-Authenticate` header containing a UMA ticket:\n\n```\nHTTP/1.1 401 Unauthorized\nWWW-Authenticate: Bearer realm=\"https://portal.develop.eoepca.org/oidc/authenticate/\",ticket={uma_ticket}\n```\n\nThe client parses this response to extract the ticket from the header.\n\n**Implementation in DemoClient:**\n\n```python\n# line 270-279\nelif r.status_code == 401:\n    self.trace(log_prefix, \"Received a 401 (Unauthorized) response to access attempt\")\n    if id_token is not None:\n        location_header = r.headers[\"WWW-Authenticate\"]\n        for item in location_header.split(\",\"):\n            if item.split(\"=\")[0] == \"ticket\":\n                ticket = item.split(\"=\")[1]\n                break\n```\n\n**Sources:** [test/client/DemoClient.py:270-279](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:84]()\n\n### Step 3: Token Exchange (Ticket + ID Token  RPT)\n\nThe client exchanges the UMA ticket and ID Token for an RPT by making a request to the Authorization Server's token endpoint:\n\n**Token Endpoint Discovery:**\n\nThe token endpoint URL is obtained from the UMA2 configuration at `/.well-known/uma2-configuration`:\n\n```python\n# line 74-85\ndef get_token_endpoint(self):\n    if self.token_endpoint == None:\n        headers = { 'content-type': \"application/json\" }\n        r = self.http_request(\"GET\", self.base_url + \"/.well-known/uma2-configuration\", headers=headers)\n        self.token_endpoint = r.json()[\"token_endpoint\"]\n    return self.token_endpoint\n```\n\n**Token Exchange Request:**\n\n```python\n# line 185-212\ndef get_access_token_from_ticket(self, ticket, id_token):\n    client_id, client_secret = self.get_client_credentials()\n    headers = { 'content-type': \"application/x-www-form-urlencoded\", \"cache-control\": \"no-cache\" }\n    data = {\n        \"claim_token_format\": \"http://openid.net/specs/openid-connect-core-1_0.html#IDToken\",\n        \"claim_token\": id_token,\n        \"ticket\": ticket,\n        \"grant_type\": \"urn:ietf:params:oauth:grant-type:uma-ticket\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n        \"scope\": \"openid\"\n    }\n    token_endpoint = self.get_token_endpoint()\n    r = self.http_request(\"POST\", token_endpoint, headers=headers, data=data)\n    access_token = r.json()[\"access_token\"]\n    return access_token\n```\n\nThe Authorization Server validates the ID Token, evaluates policies through the PDP, and issues an RPT if the user is authorized.\n\n**Sources:** [test/client/DemoClient.py:185-212](), [test/client/DemoClient.py:74-85]()\n\n### Step 4: Retry with RPT\n\nThe client retries the original request, this time including the RPT in the `Authorization` header:\n\n```\nGET https://ades.develop.eoepca.org/eric/wps3/processes\nAuthorization: Bearer {RPT}\nX-User-Id: {id_token}\n```\n\nThe `uma-user-agent` forwards the request to `pep-engine`, which validates the RPT with the Authorization Server. If valid, the PEP forwards the request to the backend service.\n\n**Loop Control in Implementation:**\n\n```python\n# line 246-291\nwhile repeat and count \u003c 2:\n    count += 1\n    repeat = False\n    # Set ID Token in header\n    headers[\"X-User-Id\"] = id_token\n    # use access token if we have one\n    if access_token is not None:\n        headers[\"Authorization\"] = f\"Bearer {access_token}\"\n    # attempt access\n    r = self.http_request(method, url, headers=headers, json=json, data=data)\n    # if response is OK then nothing else to do\n    if r.ok:\n        self.trace(log_prefix, \"Successfully accessed resource\")\n    # if we got a 401 then initiate the UMA flow\n    elif r.status_code == 401:\n        # ... ticket exchange ...\n        access_token = self.get_access_token_from_ticket(ticket, id_token)\n        repeat = True  # Retry with new RPT\n```\n\n**Sources:** [test/client/DemoClient.py:246-291]()\n\n## RPT Token Structure\n\nThe RPT returned by the Authorization Server is a JWT containing permissions and claims. Example structure:\n\n```json\n{\n  \"header\": {\n    \"alg\": \"RS256\",\n    \"kid\": \"RSA1\"\n  },\n  \"payload\": {\n    \"active\": true,\n    \"exp\": 1604406738,\n    \"iat\": 1604403138,\n    \"permissions\": [\n      {\n        \"resource_id\": \"c90141d2-223a-4e51-ab00-99e31898e77b\",\n        \"resource_scopes\": [\"Authenticated\", \"user_name\", \"openid\"],\n        \"exp\": 1604406738\n      }\n    ],\n    \"client_id\": \"cbc4f5c1-f444-4b0c-8ed2-6949f0f88476\",\n    \"pct_claims\": {\n      \"sub\": [\"24243808-b7a1-4c28-9b47-c0e5c84e7882\"],\n      \"user_name\": [\"demo\"],\n      \"iss\": [\"https://test.192.168.49.2.nip.io\"]\n    }\n  }\n}\n```\n\nThe `permissions` array contains the resources the token grants access to, including the `resource_id` and allowed scopes. The `pct_claims` section contains user identity information from the ID Token.\n\n**Sources:** [test/client/debug/jwt-output-by-pep.json:1-52]()\n\n## Client Registration\n\nBefore performing the UMA flow, clients must register with the Authorization Server to obtain client credentials (`client_id` and `client_secret`):\n\n```python\n# line 87-111\ndef register_client(self, redirect_uris = [\"\"]):\n    if not \"client_id\" in self.state:\n        if self.scim_client == None:\n            self.scim_client = EOEPCA_Scim(self.base_url + \"/\")\n        self.client = self.scim_client.registerClient(\n            \"Demo Client\",\n            grantTypes = [\"client_credentials\", \"password\", \"urn:ietf:params:oauth:grant-type:uma-ticket\"],\n            redirectURIs = redirect_uris,\n            logoutURI = \"\",\n            responseTypes = [\"code\",\"token\",\"id_token\"],\n            scopes = ['openid',  'email', 'user_name ','uma_protection', 'permission', 'is_operator', 'profile'],\n            subject_type = \"public\",\n            token_endpoint_auth_method = ENDPOINT_AUTH_CLIENT_POST)\n        self.state[\"client_id\"] = self.client[\"client_id\"]\n        self.state[\"client_secret\"] = self.client[\"client_secret\"]\n```\n\nThe client must be registered with the `urn:ietf:params:oauth:grant-type:uma-ticket` grant type to support UMA flows.\n\n**Sources:** [test/client/DemoClient.py:87-111]()\n\n## Resource Registration\n\nBefore a resource can be protected, it must be registered with the PEP. This associates a resource URI with metadata and ownership:\n\n```python\n# line 141-183\ndef register_protected_resource(self, resource_api_url, uri, id_token, name, scopes, ownershipId=None):\n    headers = { 'content-type': \"application/json\", \"Authorization\": f\"Bearer {id_token}\" }\n    data = { \"resource_scopes\":scopes, \"icon_uri\":uri, \"name\":name}\n    if ownershipId != None:\n        data[\"uuid\"] = ownershipId\n    r = self.http_request(\"POST\", f\"{resource_api_url}/resources\", headers=headers, json=data)\n    resource_id= response_json['id']\n```\n\n**Example: Registering User Workspace**\n\n```python\n# From main.py lines 64-65\ndemo.register_protected_resource(wsapi_resource_api_url, wsapi_user_prefix, user_id_token, \n    f\"Workspace for user {USER_NAME}\", [])\n```\n\nThis registers the workspace path `/workspaces/develop-user-eric` with the user as the default owner.\n\n**Sources:** [test/client/DemoClient.py:141-183](), [test/client/main.py:54-75]()\n\n## UMA Flow Configuration in Deployments\n\n### UMA User Agent Configuration\n\nThe `uma-user-agent` component is configured in each resource-guard deployment:\n\n```yaml\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: ades\n        paths:\n          - path: /(.*)\n            service:\n              name: proc-ades\n              port: 80\n  client:\n    credentialsSecretName: \"proc-uma-user-agent\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://auth.develop.eoepca.org/oxauth/auth/passport/passportlogin.htm\"'\n  openAccess: false\n```\n\nKey configuration parameters:\n\n| Parameter | Purpose |\n|-----------|---------|\n| `nginxIntegration.enabled` | Enables nginx ingress integration for request interception |\n| `client.credentialsSecretName` | Secret containing OAuth2 client credentials for the UMA agent |\n| `unauthorizedResponse` | The `WWW-Authenticate` header value returned on 401 responses |\n| `openAccess` | If `true`, allows unauthenticated access; if `false`, enforces UMA |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:61-83]()\n\n### PEP Engine Configuration\n\nThe `pep-engine` component registers default resources on deployment:\n\n```yaml\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  customDefaultResources:\n  - name: \"ADES Service for user 'eric'\"\n    description: \"Protected Access for eric to his space in the ADES\"\n    resource_uri: \"/eric\"\n    scopes: []\n    default_owner: \"fad43ef3-23ef-48b0-86f0-1cf29d97908e\"\n```\n\nThe `default_owner` field contains the user's UUID (the `sub` claim from their ID Token), establishing initial ownership of the resource.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:28-49]()\n\n## Practical Example: ADES Access\n\nThe following diagram shows a complete practical example of accessing the ADES service:\n\n```mermaid\nflowchart TD\n    Start[\"Client Application\"]\n    \n    Register[\"register_client()\u003cbr/\u003eGet client_id/secret\"]\n    GetIDToken[\"get_id_token()\u003cbr/\u003eusername/password\u003cbr/\u003e ID Token\"]\n    RegResource[\"register_protected_resource()\u003cbr/\u003e/eric  resource_id\"]\n    \n    ListProc[\"proc_list_processes()\u003cbr/\u003eGET /eric/wps3/processes\"]\n    UMAFlow[\"uma_http_request()\"]\n    \n    Check401{\"/401\u003cbr/\u003eresponse?\"}\n    ExtractTicket[\"Extract ticket from\u003cbr/\u003eWWW-Authenticate header\"]\n    ExchangeToken[\"get_access_token_from_ticket()\u003cbr/\u003eticket + ID Token  RPT\"]\n    \n    Retry[\"Retry request with\u003cbr/\u003eAuthorization: Bearer RPT\"]\n    Success[\"200 OK\u003cbr/\u003eProcess list returned\"]\n    \n    Start --\u003e Register\n    Register --\u003e GetIDToken\n    GetIDToken --\u003e RegResource\n    RegResource --\u003e ListProc\n    \n    ListProc --\u003e UMAFlow\n    UMAFlow --\u003e Check401\n    \n    Check401 --\u003e|\"Yes\"| ExtractTicket\n    ExtractTicket --\u003e ExchangeToken\n    ExchangeToken --\u003e Retry\n    Retry --\u003e Success\n    \n    Check401 --\u003e|\"No (cached RPT)\"| Success\n    \n    style Register fill:#f9f9f9\n    style GetIDToken fill:#f9f9f9\n    style RegResource fill:#f9f9f9\n    style UMAFlow fill:#e8e8e8\n    style ExchangeToken fill:#e8e8e8\n```\n\n**Implementation Flow:**\n\n1. **Client Registration:** [test/client/main.py:46-47]()\n2. **Get ID Token:** [test/client/main.py:50-52]()\n3. **Register Resource:** [test/client/main.py:54-56]()\n4. **Call Protected Endpoint:** [test/client/main.py:158-160]()\n5. **UMA Flow Handling:** [test/client/DemoClient.py:239-291]()\n\n**Sources:** [test/client/main.py:38-160](), [test/client/DemoClient.py:383-395]()\n\n## Token Caching and Reuse\n\nThe DemoClient implementation caches the RPT to avoid repeating the UMA flow on every request:\n\n```python\n# line 239-291\ndef uma_http_request(self, method, url, headers=None, id_token=None, access_token=None, json=None, data=None):\n    # ... loop control ...\n    while repeat and count \u003c 2:\n        # use access token if we have one\n        if access_token is not None:\n            self.trace(log_prefix, \"Attempting to use existing access token\")\n            headers[\"Authorization\"] = f\"Bearer {access_token}\"\n        else:\n            self.trace(log_prefix, \"No existing access token - making a naive attempt\"\n        # ... perform request ...\n        if r.ok:\n            # Success - return with potentially reusable access_token\n            return r, access_token\n        elif r.status_code == 401:\n            # Get new token and retry\n            access_token = self.get_access_token_from_ticket(ticket, id_token)\n            repeat = True\n    return r, access_token\n```\n\nCallers should preserve the returned `access_token` for subsequent requests:\n\n```python\n# From Robot test line 63-66\n${resp}  ${access_token}  @{processes} =  Proc List Processes  ${API_PROC_SERVICE_URL}  ${ID_TOKEN}  ${ACCESS_TOKEN}\nShould Be True  $access_token is not None\nSet Suite Variable  ${ACCESS_TOKEN}  ${access_token}\n```\n\n**Sources:** [test/client/DemoClient.py:239-291](), [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot:63-66]()\n\n## Non-UMA Alternative\n\nFor testing or specific use cases, the DemoClient also provides a non-UMA flow that uses ID Token directly:\n\n```python\n# line 231-237\ndef notuma_http_request(self, method, url, headers=None, id_token=None, access_token=None, json=None, data=None):\n    if headers is None:\n        headers = {}\n    if id_token is not None:\n        headers[\"Authorization\"] = f\"Bearer {id_token}\"\n    r = self.http_request(method, url, headers=headers, json=json, data=data)\n    return r, None\n```\n\nThis is used for endpoints like the Workspace API creation endpoint that don't go through the UMA flow:\n\n```python\n# line 310-327\ndef wsapi_create(self, service_base_url, name, owner=None, id_token=None, access_token=None):\n    # ...\n    r, access_token = self.notuma_http_request(\"POST\", service_base_url + \"/workspaces\", \n        headers=headers, id_token=id_token, access_token=access_token, json=body_data)\n```\n\n**Sources:** [test/client/DemoClient.py:231-237](), [test/client/DemoClient.py:310-327]()\n\n## Summary\n\nThe UMA authentication flow in EOEPCA provides fine-grained, policy-based access control through a ticket-based authorization mechanism:\n\n1. **Client Registration:** Obtain OAuth2 client credentials\n2. **User Authentication:** Get ID Token via password grant\n3. **Resource Access Attempt:** Request protected resource with ID Token\n4. **Ticket Issuance:** Receive 401 with UMA ticket\n5. **Token Exchange:** Exchange ticket + ID Token for RPT\n6. **Authorized Access:** Retry with RPT to access resource\n7. **Token Reuse:** Cache RPT for subsequent requests\n\nThis flow is transparently handled by the `uma_http_request()` helper function, which manages the ticket exchange and retry logic automatically. All protected services in EOEPCA (ADES, Workspace API, Data Access, etc.) use this same UMA pattern through their deployed `resource-guard` instances.\n\n**Sources:** [test/client/DemoClient.py:239-291](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-89](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:1-91]()"])</script><script>self.__next_f.push([1,"25:T6e39,"])</script><script>self.__next_f.push([1,"# Resource Management\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Resource Management building block provides data cataloging, discovery, access, and workspace management capabilities within the EOEPCA platform. This system enables users to discover Earth Observation data through OGC-compliant catalog services, visualize and access data via standardized web services, and manage isolated workspaces with dedicated storage and catalog instances.\n\nThis page covers the overall architecture and integration of Resource Management services. For detailed information on specific components, see:\n- [Data Access Services](#5.1) - OGC WMS/WCS/WMTS rendering and visualization\n- [Resource Catalogue](#5.2) - pycsw-based metadata catalog\n- [Workspace API](#5.3) - Workspace provisioning and orchestration\n- [Data Registration and Harvesting](#5.4) - External data ingestion pipeline\n- [Multi-Tenant Workspaces](#5.5) - Per-user workspace isolation architecture\n\n## Architecture Overview\n\nThe Resource Management building block consists of both global services deployed in the `rm` namespace and per-user workspace services deployed in dedicated namespaces. The architecture supports multi-tenancy through a template-based provisioning system.\n\n**Diagram: Resource Management Component Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"rm namespace - Global Services\"\n        WorkspaceAPI[\"workspace-api\u003cbr/\u003eHelmRelease\"]\n        GlobalRC[\"resource-catalogue\u003cbr/\u003eHelmRelease\"]\n        GlobalDA[\"data-access\u003cbr/\u003eHelmRelease\"]\n        RegAPI[\"registration-api\u003cbr/\u003eHelmRelease\"]\n        BucketAPI[\"minio-bucket-api\"]\n        \n        RCDB[(\"resource-catalogue-db\u003cbr/\u003ePostgreSQL\")]\n        RedisGlobal[(\"data-access-redis-master\u003cbr/\u003eRedis\")]\n    end\n    \n    subgraph \"External Data Sources\"\n        CREODIAS[\"CREODIAS OpenSearch\u003cbr/\u003edatahub.creodias.eu\"]\n    end\n    \n    subgraph \"Storage Layer\"\n        CloudFerro[\"CloudFerro S3\u003cbr/\u003edata.cloudferro.com\"]\n        MinIO[\"MinIO\u003cbr/\u003eminio.develop.eoepca.org\"]\n    end\n    \n    subgraph \"workspace_name namespace\"\n        WSDA[\"vs (data-access)\u003cbr/\u003eHelmRelease\"]\n        WSRC[\"rm-resource-catalogue\u003cbr/\u003eHelmRelease\"]\n        WSGuard[\"resource-guard\u003cbr/\u003ePEP\"]\n        \n        WSDB[(\"vs-database\u003cbr/\u003ePostgreSQL\")]\n        WSRedis[(\"vs-redis-master\u003cbr/\u003eRedis\")]\n        WSNFS[\"PVC managed-nfs-storage\"]\n        WSBucket[\"S3 Bucket\u003cbr/\u003eworkspace_name\"]\n    end\n    \n    CREODIAS --\u003e|\"harvester polls\"| GlobalDA\n    GlobalDA --\u003e|\"enqueue items\"| RedisGlobal\n    RedisGlobal --\u003e|\"registrar consumes\"| GlobalDA\n    GlobalDA --\u003e|\"write metadata\"| RCDB\n    GlobalRC --\u003e|\"query\"| RCDB\n    \n    WorkspaceAPI --\u003e|\"create namespace\"| WSDA\n    WorkspaceAPI --\u003e|\"provision bucket\"| BucketAPI\n    BucketAPI --\u003e|\"create\"| WSBucket\n    WorkspaceAPI --\u003e|\"instantiate templates\"| WSRC\n    \n    WSDA --\u003e|\"read/write\"| WSBucket\n    WSDA --\u003e|\"register items\"| WSRedis\n    WSDA --\u003e|\"write metadata\"| WSDB\n    WSRC --\u003e|\"query\"| WSDB\n    WSDA --\u003e|\"persistence\"| WSNFS\n    WSRC --\u003e|\"persistence\"| WSNFS\n    \n    GlobalDA --\u003e|\"read data\"| CloudFerro\n    GlobalDA --\u003e|\"cache\"| MinIO\n    WSDA --\u003e|\"read data\"| WSBucket\n    \n    WSGuard --\u003e|\"enforce ownership\"| WSDA\n    WSGuard --\u003e|\"enforce ownership\"| WSRC\n    \n    RegAPI --\u003e|\"enqueue to\"| RedisGlobal\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:1-50]()\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:1-82]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:1-50]()\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml:1-37]()\n\n## Global Services Deployment\n\nThe global Resource Management services are deployed in the `rm` namespace and provide platform-wide data access and catalog capabilities.\n\n### Global Services Table\n\n| Service | HelmRelease Name | Chart | Version | Purpose |\n|---------|-----------------|-------|---------|---------|\n| Workspace API | `workspace-api` | `rm-workspace-api` | 1.4.0 | Orchestrates workspace provisioning |\n| Resource Catalogue | `resource-catalogue` | `rm-resource-catalogue` | 1.4.0 | Global metadata catalog (pycsw) |\n| Data Access | `data-access` | `data-access` | 1.4.0 | Global OGC services for public data |\n| Registration API | `registration-api` | `rm-registration-api` | 1.4.0 | Data registration endpoint |\n\n### Global Data Access Configuration\n\nThe global `data-access` service provides read-only access to platform-wide data collections stored in CloudFerro S3 and cached in MinIO.\n\n**Key Configuration:**\n\n```yaml\nstorage:\n  data:\n    data:\n      type: S3\n      endpoint_url: http://data.cloudferro.com\n      region_name: RegionOne\n  cache:\n    type: S3\n    endpoint_url: \"https://cf2.cloudferro.com:8080/cache-bucket\"\n    bucket: cache-bucket\n```\n\n**Ingress Configuration:**\n\n```yaml\ningress:\n  hosts:\n    - host: data-access.develop.eoepca.org\n  tls:\n    - hosts:\n        - data-access.develop.eoepca.org\n      secretName: data-access-tls\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:42-47]()\n\n### Global Resource Catalogue Configuration\n\nThe global `resource-catalogue` is based on pycsw and stores metadata in a PostgreSQL database with a 5Gi volume.\n\n**Database Configuration:**\n\n```yaml\ndb:\n  volume_size: 5Gi\n  config:\n    shared_buffers: 2GB\n    effective_cache_size: 6GB\n    maintenance_work_mem: 512MB\n    max_connections: 300\n```\n\n**Catalog Metadata:**\n\n```yaml\npycsw:\n  config:\n    server:\n      url: https://resource-catalogue.develop.eoepca.org/\n    manager:\n      transactions: \"true\"\n      allowed_ips: \"*\"\n    metadata:\n      identification_title: EOEPCA Resource Catalogue\n      provider_name: EOEPCA\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:19-31]()\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:44-57]()\n\n## Data Harvesting and Registration Pipeline\n\nThe Resource Management system implements a queue-based pipeline for harvesting external data sources and registering metadata into the catalog.\n\n**Diagram: Harvesting and Registration Flow**\n\n```mermaid\nsequenceDiagram\n    participant Harvester as \"harvester\u003cbr/\u003e(data-access)\"\n    participant OpenSearch as \"CREODIAS OpenSearch\u003cbr/\u003edatahub.creodias.eu\"\n    participant Redis as \"data-access-redis-master\u003cbr/\u003eregister_queue\"\n    participant Registrar as \"registrar\u003cbr/\u003e(data-access)\"\n    participant Backend as \"registrar_pycsw.backend\"\n    participant DB as \"resource-catalogue-db\u003cbr/\u003ePostgreSQL\"\n    participant PyCSW as \"pycsw\u003cbr/\u003e(resource-catalogue)\"\n    \n    Harvester-\u003e\u003eOpenSearch: \"Poll OpenSearch API\u003cbr/\u003eSentinel2/Landsat8/Sentinel1\"\n    OpenSearch--\u003e\u003eHarvester: \"Return STAC items\u003cbr/\u003ewith metadata\"\n    Harvester-\u003e\u003eHarvester: \"postprocess_sentinel2()\u003cbr/\u003etransform to STAC\"\n    Harvester-\u003e\u003eRedis: \"Enqueue to register_queue\"\n    \n    Registrar-\u003e\u003eRedis: \"Consume from register_queue\"\n    Redis--\u003e\u003eRegistrar: \"STAC item\"\n    \n    Registrar-\u003e\u003eRegistrar: \"Route to ItemBackend\u003cbr/\u003ebased on queue\"\n    Registrar-\u003e\u003eBackend: \"ItemBackend.register()\"\n    Backend-\u003e\u003eDB: \"INSERT metadata\u003cbr/\u003e(ISO 19115 format)\"\n    \n    Registrar-\u003e\u003eRedis: \"Enqueue to seed_queue\u003cbr/\u003e(success callback)\"\n    \n    PyCSW-\u003e\u003eDB: \"Query metadata\u003cbr/\u003evia CSW/OpenSearch\"\n    DB--\u003e\u003ePyCSW: \"Return records\"\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:949-1087]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:878-947]()\n\n### Harvester Configuration\n\nThe `harvester` component polls external OpenSearch endpoints to discover and ingest data. It is configured with multiple harvesters for different satellite missions.\n\n**Harvester Definitions:**\n\n| Harvester | Data Source | Queue | Postprocessor |\n|-----------|-------------|-------|---------------|\n| `Sentinel2` | `datahub.creodias.eu/resto/api/collections/Sentinel2` | `register` | `harvester_eoepca.postprocess.postprocess_sentinel2` |\n| `Landsat8` | `datahub.creodias.eu/resto/api/collections/Landsat8` | `register` | `harvester_eoepca.postprocess.postprocess_landsat8` |\n| `Sentinel1-GRD` | `datahub.creodias.eu/resto/api/collections/Sentinel1` | `register` | `harvester_eoepca.postprocess.postprocess_sentinel1` |\n| `Sentinel3` | `datahub.creodias.eu/resto/api/collections/Sentinel3` | `register` | `harvester_eoepca.postprocess.postprocess_sentinel3` |\n\n**Example Harvester Configuration:**\n\n```yaml\nharvesters:\n  Sentinel2:\n    resource:\n      type: OpenSearch\n      opensearch:\n        url: https://datahub.creodias.eu/resto/api/collections/Sentinel2/describe.xml\n        query:\n          time:\n            begin: 2019-09-10T00:00:00Z\n            end: 2019-09-11T00:00:00Z\n          bbox: 14.9,47.7,16.4,48.7\n    postprocessors:\n      - type: external\n        process: harvester_eoepca.postprocess.postprocess_sentinel2\n    queue: register\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:962-985]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:986-1009]()\n\n### Registrar Queue Routes\n\nThe `registrar` component consumes items from Redis queues and routes them to backend handlers based on the queue name. Each route is configured with specific backends that write metadata to the catalog database.\n\n**Registrar Routes:**\n\n| Route | Queue | Backend | Purpose |\n|-------|-------|---------|---------|\n| `collections` | `register_collection_queue` | `registrar_pycsw.backend.CollectionBackend` | Register STAC collections |\n| `items` | `register_queue` | `registrar_pycsw.backend.ItemBackend` | Register STAC items |\n| `ades` | `register_ades_queue` | `registrar_pycsw.backend.ADESBackend` | Register ADES services |\n| `application` | `register_application_queue` | `registrar_pycsw.backend.CWLBackend` | Register CWL applications |\n| `catalogue` | `register_catalogue_queue` | `registrar_pycsw.backend.CatalogueBackend` | Register catalog endpoints |\n\n**Example Route Configuration:**\n\n```yaml\nroutes:\n  collections:\n    path: registrar.route.stac.Collection\n    queue: register_collection_queue\n    replace: true\n    backends:\n      - path: registrar_pycsw.backend.CollectionBackend\n        kwargs:\n          repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:894-947]()\n\n## Product Types and Collections\n\nThe Data Access service defines product types that map satellite data assets to coverage types and browsing configurations. These definitions enable OGC WMS/WCS rendering of specific bands and derived products.\n\n**Diagram: Product Type to Coverage Mapping**\n\n```mermaid\ngraph LR\n    subgraph \"Collections\"\n        S2L2A[\"S2L2A Collection\"]\n        S2L1C[\"S2L1C Collection\"]\n        L8L1TP[\"L8L1TP Collection\"]\n        S1IWGRD1C[\"S1IWGRD1C Collection\"]\n    end\n    \n    subgraph \"Product Types\"\n        S2MSI2A[\"S2MSI2A\u003cbr/\u003efilter: s2:product_type=S2MSI2A\"]\n        S2MSI1C[\"S2MSI1C\u003cbr/\u003efilter: s2:product_type=S2MSI1C\"]\n        L8MSI1TP[\"L8MSI1TP\u003cbr/\u003efilter: platform=landsat-8\u003cbr/\u003elandsat:correction=L1TP\"]\n        S1GRD[\"S1IWGRD1C_VVVH\u003cbr/\u003efilter: sar:product_type=GRD\"]\n    end\n    \n    subgraph \"Coverages\"\n        S2L2A_B04[\"S2L2A_B04\u003cbr/\u003easset: B04\"]\n        S2L2A_B03[\"S2L2A_B03\u003cbr/\u003easset: B03\"]\n        S2L2A_B02[\"S2L2A_B02\u003cbr/\u003easset: B02\"]\n        L8_B4[\"L8L1TP_B04\u003cbr/\u003easset: SR_B4\"]\n        S1_VV[\"S1IWGRD1C_VV\u003cbr/\u003easset: vv\"]\n    end\n    \n    subgraph \"Browses\"\n        TrueColor[\"TRUE_COLOR\u003cbr/\u003eRGB: B04/B03/B02\u003cbr/\u003erange: [0,4000]\"]\n        FalseColor[\"FALSE_COLOR\u003cbr/\u003eRGB: B08/B04/B03\"]\n        NDVI[\"NDVI\u003cbr/\u003eexpression: (B08-B04)/(B08+B04)\"]\n    end\n    \n    S2L2A --\u003e S2MSI2A\n    S2L1C --\u003e S2MSI1C\n    L8L1TP --\u003e L8MSI1TP\n    S1IWGRD1C --\u003e S1GRD\n    \n    S2MSI2A --\u003e S2L2A_B04\n    S2MSI2A --\u003e S2L2A_B03\n    S2MSI2A --\u003e S2L2A_B02\n    L8MSI1TP --\u003e L8_B4\n    S1GRD --\u003e S1_VV\n    \n    S2MSI2A --\u003e TrueColor\n    S2MSI2A --\u003e FalseColor\n    S2MSI2A --\u003e NDVI\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:511-671]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:217-290]()\n\n### Example Product Type Definition\n\nThe `S2MSI2A` product type for Sentinel-2 Level 2A data demonstrates the mapping structure:\n\n```yaml\nproductTypes:\n  - name: S2MSI2A\n    filter:\n      s2:product_type: S2MSI2A\n    collections:\n      - S2L2A\n    coverages:\n      S2L2A_B04:\n        assets:\n          - B04\n      S2L2A_B03:\n        assets:\n          - B03\n    defaultBrowse: TRUE_COLOR\n    browses:\n      TRUE_COLOR:\n        asset: visual-10m\n        red:\n          expression: B04\n          range: [0, 4000]\n          nodata: 0\n        green:\n          expression: B03\n          range: [0, 4000]\n          nodata: 0\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:593-651]()\n\n## Workspace Provisioning Architecture\n\nThe Workspace API orchestrates the creation of isolated, per-user environments by instantiating HelmRelease templates with user-specific values. This enables multi-tenant resource isolation.\n\n**Diagram: Workspace Provisioning Flow**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WorkspaceAPI as \"workspace-api\u003cbr/\u003e(rm namespace)\"\n    participant K8s as \"Kubernetes API\"\n    participant BucketAPI as \"minio-bucket-api\u003cbr/\u003ebucket endpoint\"\n    participant MinIO as \"MinIO S3\"\n    participant Flux as \"Flux HelmController\"\n    participant Templates as \"workspace-charts\u003cbr/\u003eConfigMap\"\n    \n    User-\u003e\u003eWorkspaceAPI: \"POST /workspaces\u003cbr/\u003e{name: 'eric-workspace'}\"\n    WorkspaceAPI-\u003e\u003eK8s: \"Create namespace\u003cbr/\u003e'eric-workspace'\"\n    K8s--\u003e\u003eWorkspaceAPI: \"Namespace created\"\n    \n    WorkspaceAPI-\u003e\u003eBucketAPI: \"POST /bucket\u003cbr/\u003e{bucket: 'eric-workspace'}\"\n    BucketAPI-\u003e\u003eMinIO: \"Create bucket\u003cbr/\u003ewith access keys\"\n    MinIO--\u003e\u003eBucketAPI: \"Bucket created\u003cbr/\u003ecredentials returned\"\n    BucketAPI--\u003e\u003eWorkspaceAPI: \"access_key_id, secret_access_key\"\n    \n    WorkspaceAPI-\u003e\u003eK8s: \"Create Secret 'bucket'\u003cbr/\u003ewith S3 credentials\"\n    \n    WorkspaceAPI-\u003e\u003eTemplates: \"Read template-hr-data-access.yaml\"\n    Templates--\u003e\u003eWorkspaceAPI: \"Template with placeholders\"\n    \n    WorkspaceAPI-\u003e\u003eWorkspaceAPI: \"Substitute:\u003cbr/\u003e{{ workspace_name }} -\u003e 'eric-workspace'\u003cbr/\u003e{{ bucket }} -\u003e 'eric-workspace'\u003cbr/\u003e{{ access_key_id }} -\u003e 'AKIAIOSFODNN7EXAMPLE'\u003cbr/\u003e{{ secret_access_key }} -\u003e 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\"\n    \n    WorkspaceAPI-\u003e\u003eK8s: \"Apply HelmRelease\u003cbr/\u003e'vs' in eric-workspace\"\n    WorkspaceAPI-\u003e\u003eK8s: \"Apply HelmRelease\u003cbr/\u003e'rm-resource-catalogue' in eric-workspace\"\n    WorkspaceAPI-\u003e\u003eK8s: \"Apply HelmRelease\u003cbr/\u003e'resource-guard' in eric-workspace\"\n    \n    Flux-\u003e\u003eK8s: \"Detect new HelmReleases\"\n    Flux-\u003e\u003eFlux: \"Deploy data-access chart\"\n    Flux-\u003e\u003eFlux: \"Deploy resource-catalogue chart\"\n    \n    K8s--\u003e\u003eWorkspaceAPI: \"Workspace provisioned\"\n    WorkspaceAPI--\u003e\u003eUser: \"200 OK\u003cbr/\u003eworkspace details\"\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:1-50]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:1-50]()\n\n### Workspace API Configuration\n\nThe Workspace API is configured with template locations, bucket provisioning endpoints, and protection settings.\n\n**Key Configuration Values:**\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `prefixForName` | `develop-user` | Prefix for workspace names |\n| `workspaceSecretName` | `bucket` | Name of S3 credential secret |\n| `namespaceForBucketResource` | `rm` | Namespace for bucket operator |\n| `s3Endpoint` | `https://minio.develop.eoepca.org` | MinIO endpoint |\n| `bucketEndpointUrl` | `http://minio-bucket-api:8080/bucket` | Bucket provisioning API |\n| `workspaceChartsConfigMap` | `workspace-charts` | ConfigMap with HelmRelease templates |\n| `pepBaseUrl` | `http://workspace-api-pep:5576/resources` | PEP for resource protection |\n| `autoProtectionEnabled` | `True` | Automatically register resources with PEP |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:35-49]()\n\n### Template Structure\n\nThe workspace templates use placeholder substitution to generate user-specific HelmReleases. The template system replaces variables like `{{ workspace_name }}`, `{{ bucket }}`, `{{ access_key_id }}`, and `{{ secret_access_key }}`.\n\n**Template Placeholders:**\n\n```yaml\nmetadata:\n  name: vs  # fixed name per workspace\nspec:\n  values:\n    global:\n      storage:\n        data:\n          data:\n            endpoint_url: https://minio.develop.eoepca.org\n            access_key_id: {{ access_key_id }}\n            secret_access_key: {{ secret_access_key }}\n            bucket: {{ bucket }}\n      metadata:\n        title: Data Access Service - {{ workspace_name }}\n    vs:\n      registrar:\n        config:\n          routes:\n            items:\n              backends:\n                - path: \"registrar_pycsw.backend.ItemBackend\"\n                  kwargs:\n                    ows_url: \"https://data-access.{{ workspace_name }}.develop.eoepca.org/ows\"\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:56-58]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:108-112]()\n\n## Workspace-Specific Services\n\nEach workspace receives its own instances of data-access and resource-catalogue services, isolated within a dedicated namespace.\n\n### Workspace Data Access\n\nThe workspace-specific `vs` (view server) HelmRelease provides OGC services over the user's S3 bucket.\n\n**Key Differences from Global Service:**\n\n| Aspect | Global | Workspace |\n|--------|--------|-----------|\n| Namespace | `rm` | `{{ workspace_name }}` |\n| S3 Endpoint | `http://data.cloudferro.com` (read-only) | `https://minio.develop.eoepca.org` (read-write) |\n| Bucket | CloudFerro public buckets | User-specific bucket |\n| Redis | `data-access-redis-master` | `vs-redis-master` |\n| Database | Shared PostgreSQL | Dedicated `vs-database` PostgreSQL |\n| Ingress | `data-access.develop.eoepca.org` | `data-access.{{ workspace_name }}.develop.eoepca.org` |\n| Replicas | `renderer: 4` | `renderer: 1` |\n\n**Workspace Storage Configuration:**\n\n```yaml\nstorage:\n  data:\n    data:\n      type: \"S3\"\n      endpoint_url: https://minio.develop.eoepca.org\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      bucket: {{ bucket }}\n      region_name: RegionOne\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:70-82]()\n\n### Workspace Resource Catalogue\n\nEach workspace has a dedicated pycsw instance with its own PostgreSQL database for storing user-specific metadata.\n\n**Workspace Catalogue Configuration:**\n\n```yaml\nglobal:\n  namespace: \"{{ workspace_name }}\"\ndb:\n  volume_storage_type: managed-nfs-storage-retain\npycsw:\n  config:\n    server:\n      url: \"https://resource-catalogue.{{ workspace_name }}.develop.eoepca.org\"\n      federatedcatalogues: https://resource-catalogue.develop.eoepca.org/collections/S2MSI2A\n    metadata:\n      identification_title: Resource Catalogue - {{ workspace_name }}\n```\n\n**Federated Search:**\n\nThe workspace catalogue is configured with `federatedcatalogues` pointing to the global catalogue, enabling federated search across the user's private data and platform-wide public data.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:16-36]()\n\n### Workspace Harvester\n\nThe workspace data-access includes a harvester configured to monitor a static STAC catalog in the user's S3 bucket.\n\n**Workspace Harvester Configuration:**\n\n```yaml\nharvester:\n  config:\n    redis:\n      host: vs-redis-master\n      port: 6379\n    harvesters:\n      harvest-bucket-catalog:\n        queue: \"register_queue\"\n        resource:\n          type: \"STACCatalog\"\n          staccatalog:\n            filesystem: s3bucket\n            root_path: \"/home/catalog.json\"\n    filesystems:\n      s3bucket:\n        type: s3\n        s3:\n          access_key_id: {{ access_key_id }}\n          secret_access_key: {{ secret_access_key }}\n          endpoint_url: https://minio.develop.eoepca.org\n```\n\nThis allows users to maintain a `catalog.json` file in their bucket that the harvester will periodically process and register into their workspace catalogue.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:166-195]()\n\n## Storage Architecture\n\nResource Management uses a multi-tier storage architecture combining S3 object storage, PostgreSQL databases, Redis queues, and NFS persistent volumes.\n\n**Diagram: Storage Tiers and Access Patterns**\n\n```mermaid\ngraph TB\n    subgraph \"S3 Object Storage\"\n        CloudFerroS3[\"CloudFerro S3\u003cbr/\u003edata.cloudferro.com\u003cbr/\u003e(Public EO Data)\"]\n        MinIOCache[\"MinIO Cache\u003cbr/\u003ecf2.cloudferro.com:8080/cache-bucket\u003cbr/\u003e(Rendered tiles)\"]\n        MinIOWorkspace[\"MinIO User Buckets\u003cbr/\u003eminio.develop.eoepca.org\u003cbr/\u003e(User data)\"]\n    end\n    \n    subgraph \"Database Layer\"\n        GlobalRCDB[\"resource-catalogue-db\u003cbr/\u003ePostgreSQL\u003cbr/\u003e(Global metadata)\"]\n        WorkspaceDB[\"vs-database\u003cbr/\u003ePostgreSQL per workspace\u003cbr/\u003e(User metadata)\"]\n    end\n    \n    subgraph \"Queue Layer\"\n        GlobalRedis[\"data-access-redis-master\u003cbr/\u003e(Global queues:\u003cbr/\u003eregister, register_collection,\u003cbr/\u003eregister_ades, seed)\"]\n        WorkspaceRedis[\"vs-redis-master\u003cbr/\u003e(Workspace queues:\u003cbr/\u003eregister_queue,\u003cbr/\u003eregister_collection_queue)\"]\n    end\n    \n    subgraph \"Persistent Volumes\"\n        WSNFS[\"managed-nfs-storage\u003cbr/\u003ePVC per workspace\u003cbr/\u003e(Database + Redis persistence)\"]\n    end\n    \n    subgraph \"Global Services\"\n        GlobalRenderer[\"renderer\u003cbr/\u003e(4 replicas)\"]\n        GlobalRegistrar[\"registrar\"]\n        GlobalHarvester[\"harvester\"]\n    end\n    \n    subgraph \"Workspace Services\"\n        WSRenderer[\"renderer\u003cbr/\u003e(1 replica)\"]\n        WSRegistrar[\"registrar\"]\n        WSHarvester[\"harvester\"]\n    end\n    \n    GlobalHarvester --\u003e|\"Harvest\"| CloudFerroS3\n    GlobalRenderer --\u003e|\"Read STAC items\"| CloudFerroS3\n    GlobalRenderer --\u003e|\"Write tiles\"| MinIOCache\n    GlobalHarvester --\u003e|\"Enqueue\"| GlobalRedis\n    GlobalRegistrar --\u003e|\"Consume\"| GlobalRedis\n    GlobalRegistrar --\u003e|\"Write metadata\"| GlobalRCDB\n    \n    WSHarvester --\u003e|\"Read catalog.json\"| MinIOWorkspace\n    WSRenderer --\u003e|\"Read STAC items\"| MinIOWorkspace\n    WSHarvester --\u003e|\"Enqueue\"| WorkspaceRedis\n    WSRegistrar --\u003e|\"Consume\"| WorkspaceRedis\n    WSRegistrar --\u003e|\"Write metadata\"| WorkspaceDB\n    \n    WorkspaceDB --\u003e|\"Persist\"| WSNFS\n    WorkspaceRedis --\u003e|\"Persist\"| WSNFS\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-53]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-240]()\n\n## Registration API\n\nThe Registration API provides an HTTP endpoint for external clients to submit data for registration into the catalog system.\n\n**Registration API Configuration:**\n\n```yaml\nfullnameOverride: registration-api\ningress:\n  enabled: true\n  hosts:\n    - host: registration-api-open.develop.eoepca.org\n      paths: [\"/\"]\nworkspaceK8sNamespace: \"rm\"\nredisServiceName: \"data-access-redis-master\"\n```\n\nThe Registration API enqueues items to the same Redis instance used by the harvester, allowing both automated harvesting and manual registration to flow through the same registrar pipeline.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml:17-36]()\n\n## Resource Scaling Configuration\n\nThe global and workspace services are configured with different resource allocations based on expected load.\n\n### Global Service Resources\n\n| Component | Replicas | CPU Request | Memory Request | Memory Limit |\n|-----------|----------|-------------|----------------|--------------|\n| `renderer` | 4 | 0.5 | 1Gi | 3Gi |\n| `registrar` | 1 | 100m | 100Mi | - |\n| `harvester` | 1 | 100m | 100Mi | - |\n| `scheduler` | 1 | 100m | 100Mi | - |\n\n### Workspace Service Resources\n\n| Component | Replicas | CPU Request | Memory Request | Memory Limit |\n|-----------|----------|-------------|----------------|--------------|\n| `renderer` | 1 | 100m | 300Mi | 3Gi |\n| `registrar` | 1 | 100m | 100Mi | - |\n| `harvester` | 1 | 100m | 100Mi | - |\n| `scheduler` | 1 | 100m | 100Mi | - |\n| `ingestor` | 0 | - | - | - |\n| `preprocessor` | 0 | - | - | - |\n| `cache` | 0 | - | - | - |\n| `seeder` | 0 | - | - | - |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:865-876]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:878-886]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:70-82]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:251-268]()\n\n## Integration with Other Building Blocks\n\nResource Management integrates with User Management for authentication/authorization and with Processing \u0026 Chaining for data discovery and access.\n\n### Integration Points\n\n| Integration | Component | Purpose |\n|-------------|-----------|---------|\n| **User Management** | `workspace-api-pep` | Protects workspace API endpoints with UMA authentication |\n| **User Management** | `resource-guard` (per workspace) | Protects workspace-specific services |\n| **User Management** | `umaClientSecretName: rm-uma-user-agent` | OAuth2 client credentials for workspace API |\n| **Processing \u0026 Chaining** | Resource Catalogue CSW | ADES queries catalog to discover input data |\n| **Processing \u0026 Chaining** | Data Access OGC services | ADES stage-in from WCS endpoints |\n| **Processing \u0026 Chaining** | S3 Buckets | ADES reads inputs from and writes outputs to workspace buckets |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:44-49]()\n\n## Summary\n\nThe Resource Management building block provides:\n\n1. **Global data access** via OGC WMS/WCS/WMTS services over CloudFerro S3 storage\n2. **Platform-wide catalog** using pycsw for metadata discovery via CSW and OpenSearch\n3. **Automated harvesting** from external OpenSearch endpoints (CREODIAS)\n4. **Queue-based registration** pipeline with Redis and configurable backends\n5. **Multi-tenant workspaces** with isolated namespaces, S3 buckets, databases, and catalogs\n6. **Template-based provisioning** for per-user service instantiation\n7. **Federated search** enabling users to query both private and public data\n8. **Integration with identity services** for policy-based access control\n\nFor implementation details of individual components, refer to the subsystem pages listed at the beginning of this document."])</script><script>self.__next_f.push([1,"26:T768a,"])</script><script>self.__next_f.push([1,"# Data Access Services\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Data Access Services component provides OGC-compliant visualization and access capabilities for Earth Observation data within the EOEPCA platform. This service enables users to view, query, and retrieve satellite imagery through standardized OGC WMS (Web Map Service), WCS (Web Coverage Service), and WMTS (Web Map Tile Service) interfaces. The service is built on the EOX ViewServer framework and integrates with S3 storage backends, the Resource Catalogue, and supports both global and workspace-specific deployments.\n\nFor information about data cataloging and metadata management, see [Resource Catalogue](#5.2). For workspace provisioning and multi-tenant isolation, see [Multi-Tenant Workspaces](#5.5). For data registration workflows, see [Data Registration and Harvesting](#5.4).\n\n## Architecture Overview\n\nThe Data Access service consists of multiple cooperating components that handle data rendering, registration, harvesting, caching, and client visualization.\n\n**Architecture: Data Access Service Components**\n\n```mermaid\ngraph TB\n    subgraph \"External Clients\"\n        OGC[\"OGC Clients\u003cbr/\u003e(WMS/WCS/WMTS)\"]\n        WebUI[\"Web UI Client\"]\n    end\n    \n    subgraph \"Data Access Service (vs)\"\n        Renderer[\"vs.renderer\u003cbr/\u003e4 replicas\u003cbr/\u003eOGC service endpoint\"]\n        Registrar[\"vs.registrar\u003cbr/\u003eRegistration worker\"]\n        Harvester[\"vs.harvester\u003cbr/\u003eOpenSearch harvester\"]\n        Client[\"vs.client\u003cbr/\u003eWeb visualization UI\"]\n        Scheduler[\"vs.scheduler\u003cbr/\u003eBackground task scheduler\"]\n        Seeder[\"vs.seeder\u003cbr/\u003eTile pre-generation\"]\n        Cache[\"vs.cache\u003cbr/\u003eTile cache manager\"]\n        Redis[(\"Redis\u003cbr/\u003eregister_queue\u003cbr/\u003eseed_queue\")]\n    end\n    \n    subgraph \"Storage Backends\"\n        S3Data[\"S3 Data Storage\u003cbr/\u003eCloudFerro eodata\u003cbr/\u003eendpoint: data.cloudferro.com\"]\n        S3Cache[\"S3 Cache Storage\u003cbr/\u003ecf2.cloudferro.com:8080\u003cbr/\u003ebucket: cache-bucket\"]\n        S3Workspace[\"S3 Workspace Storage\u003cbr/\u003eMinIO\u003cbr/\u003eminio.develop.eoepca.org\"]\n    end\n    \n    subgraph \"Resource Catalogue\"\n        PyCSW[\"pycsw Database\u003cbr/\u003epostgresql://...@resource-catalogue-db\"]\n    end\n    \n    subgraph \"External Data Sources\"\n        CreoDIAS[\"CreoDIAS OpenSearch\u003cbr/\u003edatahub.creodias.eu\"]\n    end\n    \n    OGC --\u003e|GetCapabilities\u003cbr/\u003eGetMap\u003cbr/\u003eGetCoverage| Renderer\n    WebUI --\u003e Client\n    Client --\u003e|HTTP| Renderer\n    \n    Harvester --\u003e|Poll| CreoDIAS\n    Harvester --\u003e|Enqueue items| Redis\n    Redis --\u003e|Consume| Registrar\n    Registrar --\u003e|Write metadata| PyCSW\n    Registrar --\u003e|Trigger seeding| Redis\n    Redis --\u003e|Consume seed_queue| Seeder\n    \n    Renderer --\u003e|Read data| S3Data\n    Renderer --\u003e|Read data| S3Workspace\n    Renderer --\u003e|Query metadata| PyCSW\n    Renderer --\u003e|Store tiles| S3Cache\n    \n    Scheduler --\u003e|Schedule tasks| Harvester\n    Scheduler --\u003e|Schedule tasks| Seeder\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:1-1144]()\n\n## Component Breakdown\n\n### Renderer\n\nThe renderer component is the primary OGC service endpoint, handling WMS, WCS, and WMTS requests. It runs with 4 replicas for high availability.\n\n| Property | Value |\n|----------|-------|\n| Component | `vs.renderer` |\n| Replica Count | 4 |\n| CPU Requests | 0.5 cores |\n| Memory Requests | 1 GiB |\n| CPU Limits | 1.5 cores |\n| Memory Limits | 3 GiB |\n| Ingress Host | `data-access.develop.eoepca.org` |\n\n**Configuration:**\n- Reads data from S3 storage backends using GDAL virtual file system (`CPL_VSIL_CURL_ALLOWED_EXTENSIONS`)\n- Queries metadata from Resource Catalogue PostgreSQL database\n- Generates dynamic visualizations based on product type browse configurations\n- Supports multiple grids and zoom levels\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:864-876]()\n\n### Registrar\n\nThe registrar is a worker process that consumes items from Redis queues and registers them in both the EOxServer instance and the Resource Catalogue.\n\n**Queue Routes:**\n\n```mermaid\ngraph LR\n    subgraph \"Redis Queues\"\n        RegQueue[\"register_queue\u003cbr/\u003e(items)\"]\n        CollQueue[\"register_collection_queue\u003cbr/\u003e(collections)\"]\n        ADESQueue[\"register_ades_queue\u003cbr/\u003e(ADES instances)\"]\n        AppQueue[\"register_application_queue\u003cbr/\u003e(CWL apps)\"]\n        CatQueue[\"register_catalogue_queue\u003cbr/\u003e(catalogues)\"]\n        JSONQueue[\"register_json_queue\u003cbr/\u003e(JSON metadata)\"]\n        XMLQueue[\"register_xml_queue\u003cbr/\u003e(XML metadata)\"]\n    end\n    \n    subgraph \"Registrar Routes\"\n        ItemRoute[\"registrar.route.stac.ItemRoute\"]\n        CollRoute[\"registrar.route.stac.CollectionRoute\"]\n        ADESRoute[\"registrar.route.json.JSONRoute\"]\n        AppRoute[\"registrar.route.json.JSONRoute\"]\n        CatRoute[\"registrar.route.json.JSONRoute\"]\n        JSONRoute[\"registrar.route.json.JSONRoute\"]\n        XMLRoute[\"registrar.route.json.JSONRoute\"]\n    end\n    \n    subgraph \"Backends\"\n        ItemBackend[\"registrar_pycsw.backend.ItemBackend\"]\n        CollBackend[\"registrar_pycsw.backend.CollectionBackend\"]\n        ADESBackend[\"registrar_pycsw.backend.ADESBackend\"]\n        CWLBackend[\"registrar_pycsw.backend.CWLBackend\"]\n        CatBackend[\"registrar_pycsw.backend.CatalogueBackend\"]\n        JSONBackend[\"registrar_pycsw.backend.JSONBackend\"]\n        XMLBackend[\"registrar_pycsw.backend.XMLBackend\"]\n    end\n    \n    RegQueue --\u003e ItemRoute\n    CollQueue --\u003e CollRoute\n    ADESQueue --\u003e ADESRoute\n    AppQueue --\u003e AppRoute\n    CatQueue --\u003e CatRoute\n    JSONQueue --\u003e JSONRoute\n    XMLQueue --\u003e XMLRoute\n    \n    ItemRoute --\u003e ItemBackend\n    CollRoute --\u003e CollBackend\n    ADESRoute --\u003e ADESBackend\n    AppRoute --\u003e CWLBackend\n    CatRoute --\u003e CatBackend\n    JSONRoute --\u003e JSONBackend\n    XMLRoute --\u003e XMLBackend\n```\n\n**Backend Configuration:**\n\n| Route | Queue | Backend Class | Purpose |\n|-------|-------|---------------|---------|\n| items | `register` | `registrar_pycsw.backend.ItemBackend` | Register STAC items |\n| collections | `register_collection_queue` | `registrar_pycsw.backend.CollectionBackend` | Register STAC collections |\n| ades | `register_ades_queue` | `registrar_pycsw.backend.ADESBackend` | Register ADES instances |\n| application | `register_application_queue` | `registrar_pycsw.backend.CWLBackend` | Register CWL applications |\n| catalogue | `register_catalogue_queue` | `registrar_pycsw.backend.CatalogueBackend` | Register catalogue metadata |\n| json | `register_json_queue` | `registrar_pycsw.backend.JSONBackend` | Register generic JSON |\n| xml | `register_xml_queue` | `registrar_pycsw.backend.XMLBackend` | Register generic XML |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:878-948]()\n\n### Harvester\n\nThe harvester polls external OpenSearch endpoints to discover new data products and enqueues them for registration.\n\n**Configured Harvesters:**\n\n| Harvester Name | Collection | Product Type | OpenSearch URL |\n|----------------|------------|--------------|----------------|\n| Sentinel2 | Sentinel2 | S2MSI1C, S2MSI2A | `datahub.creodias.eu/resto/api/collections/Sentinel2` |\n| Landsat8 | Landsat8 | L8MSI1TP, L8MSI1GT | `datahub.creodias.eu/resto/api/collections/Landsat8` |\n| Sentinel1-GRD | Sentinel1 | S1IWGRD1C_VVVH | `datahub.creodias.eu/resto/api/collections/Sentinel1` (GRD-COG) |\n| Sentinel1-SLC | Sentinel1 | S1SLC | `datahub.creodias.eu/resto/api/collections/Sentinel1` (SLC) |\n| Sentinel3 | Sentinel3 | S3A_OL_2_LFR | `datahub.creodias.eu/resto/api/collections/Sentinel3` (OL_2_LFR___) |\n\n**Harvester Configuration Example:**\n\n```yaml\nSentinel2:\n  resource:\n    type: OpenSearch\n    opensearch:\n      url: https://datahub.creodias.eu/resto/api/collections/Sentinel2/describe.xml\n      format:\n        type: 'application/json'\n        json:\n          property_mapping:\n            start_datetime: 'startDate'\n            end_datetime: 'completionDate'\n            productIdentifier: 'productIdentifier'\n      query:\n        time:\n          begin: 2019-09-10T00:00:00Z\n          end: 2019-09-11T00:00:00Z\n        collection: null\n        bbox: 14.9,47.7,16.4,48.7\n  postprocessors:\n    - type: external\n      process: harvester_eoepca.postprocess.postprocess_sentinel2\n      kwargs: {}\n  queue: register\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:949-1087]()\n\n### Client\n\nThe client is a web-based visualization interface for browsing and viewing data layers.\n\n**Configuration:**\n- Service URL: `https://data-access.develop.eoepca.org/`\n- EOxServer download enabled\n- Time domain configuration for temporal filtering\n- Custom date ranges for display and selection\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:1089-1107]()\n\n### Scheduler\n\nThe scheduler manages periodic background tasks such as harvesting cycles and cache seeding operations.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:1136-1144]()\n\n### Seeder\n\nThe seeder pre-generates map tiles for faster access. It is triggered by items in the `seed_queue` after successful registration.\n\n**Sources:** Configuration references in registrar `defaultSuccessQueue: seed_queue` [system/clusters/creodias/resource-management/hr-data-access.yaml:893]()\n\n### Cache\n\nThe cache component stores pre-rendered tiles in S3 storage to reduce rendering load and improve response times.\n\n**Cache Storage Configuration:**\n\n| Property | Value |\n|----------|-------|\n| Type | S3 |\n| Endpoint | `https://cf2.cloudferro.com:8080/cache-bucket` |\n| Host | `cf2.cloudferro.com:8080` |\n| Region | RegionOne |\n| Bucket | `cache-bucket` |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:58-64]()\n\n## OGC Service Interfaces\n\nThe Data Access service exposes OGC-compliant REST endpoints through the renderer component.\n\n**Service Metadata Configuration**\n\n```mermaid\ngraph TB\n    subgraph \"OGC Service Metadata\"\n        Title[\"title: EOEPCA Data Access Service\u003cbr/\u003edeveloped by EOX\"]\n        Abstract[\"abstract: EOEPCA Data Access Service\u003cbr/\u003edeveloped by EOX\"]\n        URL[\"url: https://ecma/cache/ows\"]\n        Contact[\"contactOrganization: EOX IT Services GmbH\u003cbr/\u003econtactEmail: office@eox.at\u003cbr/\u003econtactPosition: CTO\"]\n        Provider[\"providerName: EOX\u003cbr/\u003eproviderUrl: https://eox.at\"]\n        INSPIRE[\"inspireProfile: true\u003cbr/\u003edefaultLanguage: eng\"]\n    end\n    \n    subgraph \"Supported Operations\"\n        WMS[\"WMS Operations\u003cbr/\u003eGetCapabilities\u003cbr/\u003eGetMap\u003cbr/\u003eGetFeatureInfo\"]\n        WCS[\"WCS Operations\u003cbr/\u003eGetCapabilities\u003cbr/\u003eDescribeCoverage\u003cbr/\u003eGetCoverage\"]\n        WMTS[\"WMTS Operations\u003cbr/\u003eGetCapabilities\u003cbr/\u003eGetTile\"]\n    end\n    \n    Title --\u003e WMS\n    Abstract --\u003e WCS\n    Contact --\u003e WMTS\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:66-89]()\n\n## Data Model\n\nThe Data Access service uses a hierarchical model: **Product Types**  **Collections**  **Layers**  **Coverage Types**.\n\n### Product Types\n\nProduct types define how STAC items are mapped to coverages and browses (visualizations).\n\n**Product Type Structure:**\n\n```mermaid\ngraph TB\n    subgraph \"Product Type: S2MSI2A\"\n        Filter[\"filter:\u003cbr/\u003es2:product_type: S2MSI2A\"]\n        Collections[\"collections:\u003cbr/\u003e- S2L2A\"]\n        Coverages[\"coverages:\u003cbr/\u003eS2L2A_B01, S2L2A_B02, ...\u003cbr/\u003eS2L2A_B12\"]\n        Browses[\"browses:\u003cbr/\u003eTRUE_COLOR\u003cbr/\u003eFALSE_COLOR\u003cbr/\u003eNDVI\"]\n        Masks[\"masks:\u003cbr/\u003eclouds (validity: false)\"]\n    end\n    \n    Filter --\u003e Collections\n    Collections --\u003e Coverages\n    Collections --\u003e Browses\n    Collections --\u003e Masks\n    \n    subgraph \"Browse: TRUE_COLOR\"\n        Red[\"red: B04\u003cbr/\u003erange: [0, 4000]\u003cbr/\u003enodata: 0\"]\n        Green[\"green: B03\u003cbr/\u003erange: [0, 4000]\u003cbr/\u003enodata: 0\"]\n        Blue[\"blue: B02\u003cbr/\u003erange: [0, 4000]\u003cbr/\u003enodata: 0\"]\n        Asset[\"asset: visual-10m\"]\n    end\n    \n    Browses --\u003e Red\n    Browses --\u003e Green\n    Browses --\u003e Blue\n    Browses --\u003e Asset\n```\n\n**Example Product Types:**\n\n| Product Type | Filter | Collections | Coverages | Browses |\n|--------------|--------|-------------|-----------|---------|\n| S2MSI1C | `s2:product_type: S2MSI1C` | S2L1C | 13 bands (B01-B12) | TRUE_COLOR, FALSE_COLOR, NDVI |\n| S2MSI2A | `s2:product_type: S2MSI2A` | S2L2A | 12 bands (B01-B12, no B10) | TRUE_COLOR, FALSE_COLOR, NDVI |\n| L8MSI1TP | `platform: landsat-8`\u003cbr/\u003e`landsat:correction: L1TP` | L8L1TP | 7 bands (B01-B07) | TRUE_COLOR |\n| S1IWGRD1C_VVVH | `constellation: sentinel-1`\u003cbr/\u003e`sar:instrument_mode: IW`\u003cbr/\u003e`sar:product_type: GRD`\u003cbr/\u003e`sar:polarizations: [\"VV\", \"VH\"]` | S1IWGRD1C | VV, VH | COMPOSITE |\n| S3A_OL_2_LFR | `constellation: Sentinel-3`\u003cbr/\u003e`s3:productType: OL_2_LFR___` | S3A_OL_2_LFR | (empty) | THUMBNAIL |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:510-862]()\n\n### Collections\n\nCollections group product types for organization and discovery.\n\n**Collection Configuration:**\n\n```yaml\ncollections:\n  S2L1C:\n    product_types:\n      - S2MSI1C\n    coverage_types:\n      - S2L1C_B01\n      - S2L1C_B02\n      # ... all bands\n      \n  S2L2A:\n    product_types:\n      - S2MSI2A\n    product_levels:\n      - Level-2A\n    coverage_types:\n      - S2L2A_B01\n      - S2L2A_B02\n      # ... all bands\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:217-290]()\n\n### Layers\n\nLayers define how collections are presented in WMS/WMTS services.\n\n**Layer Properties:**\n\n| Property | Description | Example |\n|----------|-------------|---------|\n| `id` | Layer identifier | `S2L2A__TRUE_COLOR` |\n| `title` | Human-readable title | \"Sentinel-2 Level 2A True Color\" |\n| `abstract` | Description | \"Sentinel-2 Level 2A True Color\" |\n| `displayColor` | Color for UI display | `#eb3700` |\n| `grids` | Supported grids and zoom levels | `WGS84`, zoom: 13 |\n| `parentLayer` | Parent collection | `S2L2A` |\n| `search` | Search configuration | histogram bin count, threshold |\n\n**Example Layer Configuration:**\n\n```yaml\nlayers:\n  - id: S2L2A\n    title: Sentinel-2 Level 2A True Color\n    abstract: Sentinel-2 Level 2A True Color\n    displayColor: '#eb3700'\n    grids:\n      - name: WGS84\n        zoom: 13\n    parentLayer: S2L2A\n    search:\n      histogramBinCount: 15\n      histogramThreshold: 80\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:91-216]()\n\n### Coverage Types\n\nCoverage types define the band metadata for individual coverages.\n\n**Coverage Type Structure:**\n\n| Field | Description |\n|-------|-------------|\n| `name` | Coverage type identifier |\n| `data_type` | Data type (e.g., Uint16) |\n| `bands` | Array of band definitions |\n| `bands[].identifier` | Band identifier |\n| `bands[].name` | Band common name |\n| `bands[].definition` | OGC definition URI |\n| `bands[].description` | Band description |\n| `bands[].nil_values` | No-data values |\n| `bands[].uom` | Unit of measure |\n| `bands[].wavelength` | Central wavelength |\n\n**Example: Landsat-8 Red Band:**\n\n```yaml\n- name: \"L8L1TP_B04\"\n  data_type: \"Uint16\"\n  bands:\n    - identifier: \"SR_B4\"\n      name: \"red\"\n      definition: \"http://www.opengis.net/def/property/OGC/0/Radiance\"\n      description: \"SR_B4\"\n      nil_values:\n        - reason: \"http://www.opengis.net/def/nil/OGC/0/unknown\"\n          value: 0\n      uom: \"W/m2/um\"\n      wavelength: 0.65\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:291-509]()\n\n## Storage Configuration\n\nThe Data Access service integrates with multiple S3 storage backends for data and cache.\n\n**Storage Backends Configuration**\n\n```mermaid\ngraph LR\n    subgraph \"Storage Backends\"\n        DataS3[\"Data Storage\u003cbr/\u003etype: S3\u003cbr/\u003eendpoint: data.cloudferro.com\u003cbr/\u003eregion: RegionOne\u003cbr/\u003evalidate_bucket_name: false\"]\n        CacheS3[\"Cache Storage\u003cbr/\u003etype: S3\u003cbr/\u003eendpoint: cf2.cloudferro.com:8080\u003cbr/\u003ebucket: cache-bucket\u003cbr/\u003eregion: RegionOne\"]\n        WorkspaceS3[\"Workspace Storage\u003cbr/\u003etype: S3\u003cbr/\u003eendpoint: minio.develop.eoepca.org\u003cbr/\u003ebucket: (user-specific)\u003cbr/\u003eregion: RegionOne\"]\n    end\n    \n    subgraph \"Environment Variables\"\n        AWSURL[\"AWS_ENDPOINT_URL_S3:\u003cbr/\u003eminio.develop.eoepca.org\"]\n        AWSHTTPS[\"AWS_HTTPS: FALSE\"]\n        VSILEXT[\"CPL_VSIL_CURL_ALLOWED_EXTENSIONS:\u003cbr/\u003e.TIF,.tif,.xml,.jp2,.jpg,.png,.nc\"]\n    end\n    \n    DataS3 -.-\u003e VSILEXT\n    WorkspaceS3 -.-\u003e AWSURL\n    WorkspaceS3 -.-\u003e AWSHTTPS\n```\n\n**Global Storage Configuration:**\n\n| Storage Type | Endpoint | Bucket | Region | Purpose |\n|--------------|----------|--------|--------|---------|\n| data.data | `http://data.cloudferro.com` | (auto-detected) | RegionOne | Source EO data |\n| cache | `https://cf2.cloudferro.com:8080` | cache-bucket | RegionOne | Rendered tile cache |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64]()\n\n## Data Registration Pipeline\n\nThe registration pipeline processes incoming data items and makes them available through the OGC services.\n\n**Registration Flow**\n\n```mermaid\nsequenceDiagram\n    participant H as Harvester\n    participant R as Redis\n    participant Reg as Registrar\n    participant EOX as EOxServer Backend\n    participant PY as pycsw Backend\n    participant SQ as seed_queue\n    participant Seed as Seeder\n    \n    H-\u003e\u003eR: Enqueue STAC Item\u003cbr/\u003e(register_queue)\n    R-\u003e\u003eReg: Consume from register_queue\n    Reg-\u003e\u003eReg: Route via registrar.route.stac.ItemRoute\n    Reg-\u003e\u003eEOX: registrar.backend.eoxserver.ItemBackend\u003cbr/\u003eCreate coverage in EOxServer\n    Reg-\u003e\u003ePY: registrar_pycsw.backend.ItemBackend\u003cbr/\u003eInsert ISO 19115 metadata\n    PY--\u003e\u003ePY: Write to PostgreSQL:\u003cbr/\u003epostgresql://...@resource-catalogue-db/pycsw\n    Reg-\u003e\u003eSQ: Enqueue to seed_queue\u003cbr/\u003e(defaultSuccessQueue)\n    SQ-\u003e\u003eSeed: Consume seed task\n    Seed-\u003e\u003eSeed: Pre-generate tiles\u003cbr/\u003e(minzoom to maxzoom)\n```\n\n**Workspace Registration Configuration:**\n\nIn workspace-specific instances, the registrar uses a simplified configuration without the default route, focusing on item registration:\n\n```yaml\nconfig:\n  disableDefaultRoute: true\n  routes:\n    items:\n      path: registrar.route.stac.ItemRoute\n      queue: register_queue\n      replace: true\n      backends:\n        - path: \"registrar.backend.eoxserver.ItemBackend\"\n          kwargs:\n            instance_base_path: \"/var/www/pvs/dev\"\n            instance_name: \"pvs_instance\"\n            product_types: []\n            auto_create_product_types: True\n            automatic_visibilities: [\"wms\", \"wcs\"]\n        - path: \"registrar_pycsw.backend.ItemBackend\"\n          kwargs:\n            repository_database_uri: \"postgresql://postgres:mypass@resource-catalogue-db/pycsw\"\n            ows_url: \"https://data-access.{{ workspace_name }}.develop.eoepca.org/ows\"\n            public_s3_url: \"https://minio.develop.eoepca.org/{{ bucket }}\"\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:84-164]()\n\n## Harvesting Pipeline\n\nThe harvesting pipeline continuously polls external OpenSearch endpoints to discover and ingest new data products.\n\n**Harvesting Process**\n\n```mermaid\ngraph TB\n    subgraph \"External Sources\"\n        CrS2[\"CreoDIAS Sentinel-2\u003cbr/\u003edatahub.creodias.eu/resto/.../Sentinel2\"]\n        CrL8[\"CreoDIAS Landsat-8\u003cbr/\u003edatahub.creodias.eu/resto/.../Landsat8\"]\n        CrS1[\"CreoDIAS Sentinel-1\u003cbr/\u003edatahub.creodias.eu/resto/.../Sentinel1\"]\n        CrS3[\"CreoDIAS Sentinel-3\u003cbr/\u003edatahub.creodias.eu/resto/.../Sentinel3\"]\n    end\n    \n    subgraph \"Harvester Component\"\n        HS2[\"Harvester: Sentinel2\u003cbr/\u003etime: 2019-09-10 to 2019-09-11\u003cbr/\u003ebbox: 14.9,47.7,16.4,48.7\"]\n        HL8[\"Harvester: Landsat8\u003cbr/\u003etime: 2020-09-01 to 2020-09-05\u003cbr/\u003ebbox: 19.7,34.7,28.5,42.0\"]\n        HS1GRD[\"Harvester: Sentinel1-GRD\u003cbr/\u003eproductType: GRD-COG\"]\n        HS1SLC[\"Harvester: Sentinel1-SLC\u003cbr/\u003eproductType: SLC\"]\n        HS3[\"Harvester: Sentinel3\u003cbr/\u003eproductType: OL_2_LFR___\"]\n    end\n    \n    subgraph \"Postprocessors\"\n        PP2[\"harvester_eoepca.postprocess\u003cbr/\u003e.postprocess_sentinel2\"]\n        PP8[\"harvester_eoepca.postprocess\u003cbr/\u003e.postprocess_landsat8\"]\n        PP1[\"harvester_eoepca.postprocess\u003cbr/\u003e.postprocess_sentinel1\"]\n        PP3[\"harvester_eoepca.postprocess\u003cbr/\u003e.postprocess_sentinel3\"]\n    end\n    \n    subgraph \"Queue\"\n        RegQ[\"register queue\u003cbr/\u003e(Redis)\"]\n    end\n    \n    CrS2 --\u003e|OpenSearch query\u003cbr/\u003eJSON format| HS2\n    CrL8 --\u003e|OpenSearch query\u003cbr/\u003eJSON format| HL8\n    CrS1 --\u003e|OpenSearch query\u003cbr/\u003eJSON format| HS1GRD\n    CrS1 --\u003e|OpenSearch query\u003cbr/\u003eJSON format| HS1SLC\n    CrS3 --\u003e|OpenSearch query\u003cbr/\u003eJSON format| HS3\n    \n    HS2 --\u003e PP2\n    HL8 --\u003e PP8\n    HS1GRD --\u003e PP1\n    HS1SLC --\u003e PP1\n    HS3 --\u003e PP3\n    \n    PP2 --\u003e RegQ\n    PP8 --\u003e RegQ\n    PP1 --\u003e RegQ\n    PP3 --\u003e RegQ\n```\n\n**Harvester Properties:**\n\nEach harvester is configured with:\n- `resource.type`: Always \"OpenSearch\"\n- `resource.opensearch.url`: OpenSearch description document URL\n- `resource.opensearch.format`: Response format (application/json)\n- `resource.opensearch.format.json.property_mapping`: Maps OpenSearch properties to STAC fields\n- `resource.opensearch.query`: Query parameters (time, bbox, collection, extra_params)\n- `filter`: Additional filtering rules\n- `postprocessors`: External processing functions to transform results\n- `queue`: Target queue for enqueuing results\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:957-1087]()\n\n## Workspace Integration\n\nThe Data Access service supports per-workspace deployments for multi-tenant isolation. Each workspace gets its own instance with isolated storage and catalogue.\n\n**Workspace Data Access Template**\n\n```mermaid\ngraph TB\n    subgraph \"Workspace API\"\n        WS[\"Workspace API\u003cbr/\u003eworkspace-api.rm\"]\n    end\n    \n    subgraph \"Template\"\n        Tmpl[\"template-hr-data-access.yaml\u003cbr/\u003eHelmRelease template\"]\n    end\n    \n    subgraph \"User Workspace: eric-workspace\"\n        VSEric[\"Data Access: vs\u003cbr/\u003enamespace: eric-workspace\"]\n        S3Eric[\"S3 Bucket\u003cbr/\u003ebucket: eric-workspace\u003cbr/\u003eaccess_key_id: {{ access_key_id }}\u003cbr/\u003esecret_access_key: {{ secret_access_key }}\"]\n        CatEric[\"Resource Catalogue DB\u003cbr/\u003epostgresql://...@resource-catalogue-db/pycsw\"]\n        HarvEric[\"Harvester\u003cbr/\u003eharvest-bucket-catalog\u003cbr/\u003eroot_path: /home/catalog.json\"]\n    end\n    \n    WS --\u003e|Instantiate with\u003cbr/\u003eworkspace_name=eric\u003cbr/\u003ebucket=eric-workspace\u003cbr/\u003ecredentials| Tmpl\n    Tmpl --\u003e|Deploy| VSEric\n    VSEric --\u003e|Read/Write| S3Eric\n    VSEric --\u003e|Register metadata| CatEric\n    HarvEric --\u003e|Harvest STAC| S3Eric\n```\n\n**Template Variables:**\n\nThe workspace template uses the following placeholders that are replaced during instantiation:\n\n| Variable | Purpose | Example Value |\n|----------|---------|---------------|\n| `{{ workspace_name }}` | Namespace and hostname segment | `eric-workspace` |\n| `{{ bucket }}` | S3 bucket name | `eric-workspace` |\n| `{{ access_key_id }}` | S3 access key | (from workspace secret) |\n| `{{ secret_access_key }}` | S3 secret key | (from workspace secret) |\n\n**Workspace-Specific Harvester:**\n\nEach workspace includes a harvester that monitors a STAC catalog in the user's S3 bucket:\n\n```yaml\nharvesters:\n  harvest-bucket-catalog:\n    queue: \"register_queue\"\n    resource:\n      type: \"STACCatalog\"\n      staccatalog:\n        filesystem: s3bucket\n        root_path: \"/home/catalog.json\"\nfilesystems:\n  s3bucket:\n    type: s3\n    s3:\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      endpoint_url: https://minio.develop.eoepca.org\n      region: RegionOne\n      public: False\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:1-269]()\n\n## Deployment Architecture\n\n**Global vs Workspace Instances**\n\n```mermaid\ngraph TB\n    subgraph \"Global Instance (rm namespace)\"\n        GlobalDA[\"data-access\u003cbr/\u003eHelmRelease\u003cbr/\u003ehost: data-access.develop.eoepca.org\"]\n        GlobalRenderer[\"renderer: 4 replicas\u003cbr/\u003eCPU: 1.5 limit / 0.5 request\u003cbr/\u003eMemory: 3Gi limit / 1Gi request\"]\n        GlobalHarvester[\"harvester\u003cbr/\u003eSentinel-2, Landsat-8\u003cbr/\u003eSentinel-1, Sentinel-3\"]\n        GlobalCache[\"cache storage\u003cbr/\u003eS3: cf2.cloudferro.com:8080\u003cbr/\u003ebucket: cache-bucket\"]\n        GlobalData[\"data storage\u003cbr/\u003eS3: data.cloudferro.com\"]\n    end\n    \n    subgraph \"Workspace Instance (eric-workspace)\"\n        WSData[\"vs\u003cbr/\u003eHelmRelease\u003cbr/\u003ehost: data-access.eric-workspace.develop.eoepca.org\"]\n        WSRenderer[\"renderer: 1 replica\u003cbr/\u003eCPU: 100m request\u003cbr/\u003eMemory: 300Mi request / 3Gi limit\"]\n        WSHarvester[\"harvester\u003cbr/\u003eharvest-bucket-catalog\u003cbr/\u003e(user's STAC catalog)\"]\n        WSStorage[\"data storage\u003cbr/\u003eS3: minio.develop.eoepca.org\u003cbr/\u003ebucket: eric-workspace\"]\n    end\n    \n    GlobalDA --\u003e GlobalRenderer\n    GlobalDA --\u003e GlobalHarvester\n    GlobalRenderer --\u003e GlobalData\n    GlobalRenderer --\u003e GlobalCache\n    \n    WSData --\u003e WSRenderer\n    WSData --\u003e WSHarvester\n    WSRenderer --\u003e WSStorage\n```\n\n**Key Differences:**\n\n| Aspect | Global Instance | Workspace Instance |\n|--------|-----------------|-------------------|\n| Namespace | `rm` | `{{ workspace_name }}` |\n| Renderer Replicas | 4 | 1 |\n| Renderer Resources | Higher (1.5 CPU / 3Gi RAM) | Lower (0.1 CPU / 300Mi RAM) |\n| Data Source | CloudFerro eodata | User S3 bucket |\n| Cache | S3 cache bucket | Disabled |\n| Harvesting | External OpenSearch | User STAC catalog |\n| Ingress | Enabled (public) | Disabled (via Resource Guard) |\n| Seeder | Configured | Disabled (`replicaCount: 0`) |\n| Ingestor | Disabled | Disabled |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:864-876](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:69-269]()\n\n## Configuration Reference\n\n### Environment Variables\n\n| Variable | Value | Purpose |\n|----------|-------|---------|\n| `REGISTRAR_REPLACE` | `\"true\"` | Enable replacement of existing registrations |\n| `CPL_VSIL_CURL_ALLOWED_EXTENSIONS` | `.TIF,.TIFF,.tif,.tiff,.xml,.jp2,.jpg,.jpeg,.png,.nc` | GDAL allowed file extensions for remote access |\n| `AWS_ENDPOINT_URL_S3` | `https://minio.develop.eoepca.org` | S3 endpoint for AWS SDK |\n| `AWS_HTTPS` | `\"FALSE\"` | Disable HTTPS for internal MinIO |\n| `startup_scripts` | `/registrar_pycsw/registrar_pycsw/initialize-collections.sh` | Initialization scripts |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:26-33]()\n\n### Ingress Configuration\n\n```yaml\ningress:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    kubernetes.io/tls-acme: \"true\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n    nginx.ingress.kubernetes.io/enable-cors: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt\n  hosts:\n    - host: data-access.develop.eoepca.org\n  tls:\n    - hosts:\n        - data-access.develop.eoepca.org\n      secretName: data-access-tls\n```\n\n**Annotations:**\n- Extended read timeout (600s) for long-running rendering operations\n- CORS enabled for cross-origin client access\n- Automatic TLS certificate provisioning via cert-manager\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:35-47]()\n\n### Redis Configuration\n\nRedis serves as the message queue for the registration and seeding pipelines.\n\n**Connection Details:**\n\n| Property | Value |\n|----------|-------|\n| Global Instance Host | `data-access-redis-master` |\n| Workspace Instance Host | `vs-redis-master` |\n| Port | 6379 |\n| Password | None (usePassword: false) |\n\n**Persistence:**\n- Storage Class: `managed-nfs-storage`\n- Size: 1Gi\n- Cluster Mode: Disabled\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:958-960](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:230-240]()\n\n### Database Configuration\n\nThe Data Access service uses PostgreSQL for EOxServer instance data and connects to the Resource Catalogue PostgreSQL for metadata.\n\n**Workspace Database Configuration:**\n\n```yaml\ndatabase:\n  persistence:\n    enabled: true\n    existingClaim: \"\"  # claim auto-generated\n    storageClass: managed-nfs-storage\n    size: \"100Gi\"\n```\n\n**Resource Catalogue Connection:**\n\nAll backends connect to the shared pycsw database:\n\n```\npostgresql://postgres:mypass@resource-catalogue-db/pycsw\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-228](), [system/clusters/creodias/resource-management/hr-data-access.yaml:891-902]()\n\n### Client Configuration\n\nThe web client provides time-based filtering and download capabilities.\n\n**Time Domain Configuration:**\n\n```yaml\nclient:\n  config:\n    eoxserverDownloadEnabled: true\n    timeDomain:\n      - \"2002-01-01T00:00:00Z\"\n      - \"customClientDateFuture1\"\n    displayTimeDomain:\n      - \"customClientDatePast1\"\n      - \"customClientDateFuture1\"\n    selectedTimeDomain:\n      - \"customClientDatePast2\"\n      - \"today\"\n    customClientDaysPast1: 90\n    customClientDaysPast2: 1\n    customClientDaysFuture1: 7\n```\n\n**Time Domain Explanation:**\n- `timeDomain`: Overall available time range\n- `displayTimeDomain`: Time range shown in UI by default (past 90 days to future 7 days)\n- `selectedTimeDomain`: Initial selection (past 1 day to today)\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:1093-1106]()"])</script><script>self.__next_f.push([1,"27:T588f,"])</script><script>self.__next_f.push([1,"# Resource Catalogue\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\nThe Resource Catalogue provides standards-based metadata discovery and query capabilities for Earth Observation data, applications, and services in the EOEPCA platform. It implements OGC Catalogue Service for the Web (CSW) 3.0 and OpenSearch interfaces using pycsw, storing ISO 19115 metadata in PostgreSQL. Both a global catalogue and per-workspace catalogues are deployed to support multi-tenant data discovery.\n\nFor information about data visualization and access services, see [Data Access Services](#5.1). For details on how metadata enters the catalogue, see [Data Registration and Harvesting](#5.4). For workspace provisioning including catalogue instantiation, see [Multi-Tenant Workspaces](#5.5).\n\n---\n\n## Architecture Overview\n\nThe Resource Catalogue is built on pycsw, a Python implementation of OGC CSW. It serves as the central metadata repository for discovering EO products, collections, applications, ADES services, and other catalogued resources. The architecture supports both a global platform-wide catalogue and isolated per-workspace catalogues.\n\n```mermaid\ngraph TB\n    subgraph \"External Clients\"\n        CSWClient[\"CSW Client\u003cbr/\u003e(owslib.csw)\"]\n        OSClient[\"OpenSearch Client\u003cbr/\u003e(pyops)\"]\n        Browser[\"Web Browser\"]\n    end\n    \n    subgraph \"Resource Catalogue Deployment\"\n        Ingress[\"Nginx Ingress\u003cbr/\u003eresource-catalogue.develop.eoepca.org\"]\n        PyCSW[\"pycsw Server\u003cbr/\u003ePort 8000\"]\n        PYCSWDB[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\u003cbr/\u003edatabase: pycsw\")]\n    end\n    \n    subgraph \"Data Sources\"\n        Registrar[\"Registrar Service\u003cbr/\u003eMultiple Backend Routes\"]\n        DataAccess[\"Data Access Renderer\u003cbr/\u003eMetadata Generation\"]\n        Harvester[\"Harvester\u003cbr/\u003eOpenSearch Polling\"]\n    end\n    \n    subgraph \"Query Endpoints\"\n        CSWEndpoint[\"/csw\u003cbr/\u003eOGC CSW 3.0\"]\n        OSEndpoint[\"/opensearch\u003cbr/\u003eOpenSearch 1.1\"]\n        Collections[\"/collections\u003cbr/\u003eSTAC Collections\"]\n    end\n    \n    CSWClient --\u003e|GetCapabilities\u003cbr/\u003eGetRecords\u003cbr/\u003eGetRecordById| Ingress\n    OSClient --\u003e|OpenSearch Query| Ingress\n    Browser --\u003e|HTTP GET| Ingress\n    \n    Ingress --\u003e PyCSW\n    PyCSW --\u003e CSWEndpoint\n    PyCSW --\u003e OSEndpoint\n    PyCSW --\u003e Collections\n    \n    PyCSW --\u003e|Query/Insert| PYCSWDB\n    \n    Registrar --\u003e|ItemBackend\u003cbr/\u003eCollectionBackend\u003cbr/\u003eADESBackend\u003cbr/\u003eCWLBackend| PYCSWDB\n    DataAccess --\u003e|Product Metadata| Registrar\n    Harvester --\u003e|Harvested Records| Registrar\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:1-82]()\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:1-156]()\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:1-96]()\n\n---\n\n## Technology Stack\n\n### pycsw Core\n\nThe Resource Catalogue uses pycsw, an OGC-compliant CSW server implementation written in Python. pycsw provides:\n\n- **OGC CSW 3.0** interface for structured metadata queries\n- **OpenSearch 1.1** interface for REST-based discovery\n- **INSPIRE** profile support for European spatial data infrastructure compliance\n- **Transactional CSW** for programmatic metadata insertion\n\nThe pycsw deployment is configured through the HelmRelease values structure.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:38-82]()\n\n### PostgreSQL Backend\n\nMetadata records are stored in PostgreSQL using the ISO 19115 schema. The database deployment includes performance tuning parameters:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `shared_buffers` | 2GB | Memory for caching data |\n| `effective_cache_size` | 6GB | Query planner estimate |\n| `maintenance_work_mem` | 512MB | Index maintenance |\n| `work_mem` | 4MB | Per-operation memory |\n| `checkpoint_completion_target` | 0.9 | Checkpoint spreading |\n\nThe database is deployed with persistent storage via a 5Gi volume.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:19-31]()\n\n---\n\n## Deployment Patterns\n\n### Global Resource Catalogue\n\nThe platform-wide catalogue is deployed in the `rm` namespace and is accessible at `https://resource-catalogue.develop.eoepca.org`. It contains metadata for:\n\n- **EO Products**: Sentinel-2, Landsat-8, Sentinel-1, Sentinel-3 scenes\n- **Collections**: Product type groupings (S2MSI2A, L8MSI1TP, etc.)\n- **Applications**: CWL workflow packages\n- **ADES Services**: Processing service endpoints\n- **Other Catalogues**: Federated catalogue references\n\nConfiguration for the global catalogue:\n\n```yaml\npycsw:\n  config:\n    server:\n      url: https://resource-catalogue.develop.eoepca.org/\n    manager:\n      transactions: \"true\"\n      allowed_ips: \"*\"\n    metadata:\n      identification_title: EOEPCA Resource Catalogue\n      identification_abstract: Based on pycsw, a Python OGC CSW server implementation\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:38-82]()\n\n### Per-Workspace Catalogues\n\nEach user workspace receives its own isolated Resource Catalogue instance deployed to the user's namespace. These workspace catalogues:\n\n- Store user-specific STAC items generated by ADES processing jobs\n- Reference the global catalogue via federated search\n- Use the same pycsw image but with workspace-specific configuration\n- Persist data on managed NFS storage\n\nThe workspace catalogue template uses placeholders for dynamic values:\n\n```yaml\nglobal:\n  namespace: \"{{ workspace_name }}\"\npycsw:\n  config:\n    server:\n      url: \"https://resource-catalogue.{{ workspace_name }}.develop.eoepca.org\"\n      federatedcatalogues: https://resource-catalogue.develop.eoepca.org/collections/S2MSI2A\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:1-68]()\n\n```mermaid\ngraph TB\n    subgraph \"Global Namespace: rm\"\n        GlobalCat[\"resource-catalogue\u003cbr/\u003eHelmRelease\"]\n        GlobalDB[(\"resource-catalogue-db\u003cbr/\u003ePostgreSQL\u003cbr/\u003ePVC: 5Gi\")]\n        GlobalCat --\u003e GlobalDB\n    end\n    \n    subgraph \"Workspace: eric-workspace\"\n        WSCat1[\"rm-resource-catalogue\u003cbr/\u003eHelmRelease\u003cbr/\u003e(from template)\"]\n        WSDB1[(\"resource-catalogue-db\u003cbr/\u003ePostgreSQL\u003cbr/\u003ePVC: managed-nfs-storage-retain\")]\n        WSCat1 --\u003e WSDB1\n        WSCat1 -.-\u003e|federatedcatalogues| GlobalCat\n    end\n    \n    subgraph \"Workspace: alice-workspace\"\n        WSCat2[\"rm-resource-catalogue\u003cbr/\u003eHelmRelease\u003cbr/\u003e(from template)\"]\n        WSDB2[(\"resource-catalogue-db\u003cbr/\u003ePostgreSQL\u003cbr/\u003ePVC: managed-nfs-storage-retain\")]\n        WSCat2 --\u003e WSDB2\n        WSCat2 -.-\u003e|federatedcatalogues| GlobalCat\n    end\n    \n    WorkspaceAPI[\"Workspace API\u003cbr/\u003eHelmRelease Orchestrator\"]\n    Template[\"template-hr-resource-catalogue.yaml\u003cbr/\u003eConfigMap\"]\n    \n    WorkspaceAPI --\u003e|Instantiate with\u003cbr/\u003eworkspace_name| Template\n    Template --\u003e|Generate| WSCat1\n    Template --\u003e|Generate| WSCat2\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:1-68]()\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:46]()\n\n---\n\n## Query Interfaces\n\n### OGC CSW Interface\n\nThe CSW endpoint supports standard OGC operations accessed at `/csw`:\n\n| Operation | Purpose | Example Usage |\n|-----------|---------|---------------|\n| `GetCapabilities` | Retrieve service metadata | Discover supported operations and constraints |\n| `GetRecords` | Query metadata records | Search with filters, sorting, paging |\n| `GetRecordById` | Fetch specific record | Retrieve by identifier |\n| `DescribeRecord` | Get record schema | Understand metadata structure |\n| `GetDomain` | Query property values | Discover valid property ranges |\n| `Transaction` | Insert/update/delete | Programmatic metadata management |\n\nThe CSW interface uses ISO 19115 output schema by default and supports filtering with OGC Filter Encoding:\n\n```python\n# Example CSW query using owslib\nfrom owslib.csw import CatalogueServiceWeb\nfrom owslib.fes import PropertyIsEqualTo, BBox, And\n\ncsw = CatalogueServiceWeb('https://resource-catalogue.develop.eoepca.org/csw')\n\n# Spatial and temporal filters\nbbox_filter = BBox([37, 13.9, 37.9, 15.1])\ncollection_filter = PropertyIsEqualTo('apiso:ParentIdentifier', 'S2MSI2A')\n\ncsw.getrecords2(\n    constraints=[And([bbox_filter, collection_filter])],\n    outputschema='http://www.isotc211.org/2005/gmd',\n    maxrecords=10\n)\n```\n\n**Sources:**\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:74-98]()\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:51-73]()\n\n### OpenSearch Interface\n\nThe OpenSearch endpoint at `/opensearch` provides a REST-based alternative for metadata discovery. It supports:\n\n- **Description document**: XML describing search parameters at `/opensearch`\n- **Query interface**: URL-based search with query parameters\n- **STAC extensions**: EO-specific search parameters\n\nOpenSearch parameters include:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `eo:parentIdentifier` | string | Collection/product type |\n| `eo:cloudCover` | integer | Maximum cloud coverage percentage |\n| `bbox` | bbox | Geographic bounding box |\n| `start` | datetime | Temporal range start |\n| `end` | datetime | Temporal range end |\n\nExample OpenSearch queries:\n\n```\n# All records\nGET /opensearch/?mode=opensearch\u0026service=CSW\u0026version=3.0.0\u0026request=GetRecords\u0026elementsetname=full\u0026resulttype=results\u0026typenames=csw:Record\n\n# Filter by collection\nGET /opensearch/?mode=opensearch\u0026service=CSW\u0026version=3.0.0\u0026request=GetRecords\u0026elementsetname=full\u0026resulttype=results\u0026typenames=csw:Record\u0026eo:parentIdentifier=S2MSI2A\n\n# Filter by cloud cover\nGET /opensearch/?eo:cloudCover=]20\n```\n\n**Sources:**\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:85-96]()\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:139-156]()\n\n---\n\n## Integration with Registrar Service\n\nMetadata enters the Resource Catalogue through the Registrar Service, which implements multiple backend routes for different record types. The registrar consumes messages from Redis queues and writes to the PostgreSQL database using specialized backend classes.\n\n```mermaid\ngraph LR\n    subgraph \"Redis Queues\"\n        ItemQ[\"register_queue\u003cbr/\u003eSTAC Items\"]\n        CollQ[\"register_collection_queue\u003cbr/\u003eSTAC Collections\"]\n        ADESQ[\"register_ades_queue\u003cbr/\u003eADES Services\"]\n        AppQ[\"register_application_queue\u003cbr/\u003eCWL Applications\"]\n        CatQ[\"register_catalogue_queue\u003cbr/\u003eCatalogue Services\"]\n        JSONQ[\"register_json_queue\u003cbr/\u003eGeneric JSON\"]\n        XMLQ[\"register_xml_queue\u003cbr/\u003eGeneric XML\"]\n    end\n    \n    subgraph \"Registrar Backends\"\n        ItemBE[\"registrar_pycsw.backend.ItemBackend\"]\n        CollBE[\"registrar_pycsw.backend.CollectionBackend\"]\n        ADESBE[\"registrar_pycsw.backend.ADESBackend\"]\n        CWLBE[\"registrar_pycsw.backend.CWLBackend\"]\n        CatBE[\"registrar_pycsw.backend.CatalogueBackend\"]\n        JSONBE[\"registrar_pycsw.backend.JSONBackend\"]\n        XMLBE[\"registrar_pycsw.backend.XMLBackend\"]\n    end\n    \n    subgraph \"Database\"\n        PYCSWDB[(\"PostgreSQL\u003cbr/\u003epycsw database\u003cbr/\u003eISO 19115 schema\")]\n    end\n    \n    ItemQ --\u003e ItemBE\n    CollQ --\u003e CollBE\n    ADESQ --\u003e ADESBE\n    AppQ --\u003e CWLBE\n    CatQ --\u003e CatBE\n    JSONQ --\u003e JSONBE\n    XMLQ --\u003e XMLBE\n    \n    ItemBE --\u003e PYCSWDB\n    CollBE --\u003e PYCSWDB\n    ADESBE --\u003e PYCSWDB\n    CWLBE --\u003e PYCSWDB\n    CatBE --\u003e PYCSWDB\n    JSONBE --\u003e PYCSWDB\n    XMLBE --\u003e PYCSWDB\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:878-948]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:84-164]()\n\n### Backend Route Configuration\n\nEach backend route is configured in the Data Access registrar component with specific parameters:\n\n#### ItemBackend\n\nHandles STAC Item registration from EO products:\n\n```yaml\nroutes:\n  items:\n    path: registrar.route.stac.ItemRoute\n    queue: register_queue\n    replace: true\n    backends:\n      - path: registrar_pycsw.backend.ItemBackend\n        kwargs:\n          repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n          ows_url: https://data-access.develop.eoepca.org/ows\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:888-892]()\n\n#### CollectionBackend\n\nManages STAC Collection metadata:\n\n```yaml\nroutes:\n  collections:\n    path: registrar.route.stac.Collection\n    queue: register_collection_queue\n    replace: true\n    backends:\n      - path: registrar_pycsw.backend.CollectionBackend\n        kwargs:\n          repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:895-903]()\n\n#### ADESBackend\n\nRegisters ADES service endpoints:\n\n```yaml\nroutes:\n  ades:\n    path: registrar.route.json.JSONRoute\n    queue: register_ades_queue\n    replace: true\n    backends:\n      - path: registrar_pycsw.backend.ADESBackend\n        kwargs:\n          repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:904-912]()\n\n#### CWLBackend\n\nCatalogs CWL application packages:\n\n```yaml\nroutes:\n  application:\n    path: registrar.route.json.JSONRoute\n    queue: register_application_queue\n    replace: true\n    backends:\n      - path: registrar_pycsw.backend.CWLBackend\n        kwargs:\n          repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:913-921]()\n\n---\n\n## Server Configuration\n\n### Service Metadata\n\nThe pycsw server configuration defines service-level metadata:\n\n| Configuration Key | Purpose | Example Value |\n|-------------------|---------|---------------|\n| `server.url` | Service endpoint URL | `https://resource-catalogue.develop.eoepca.org/` |\n| `manager.transactions` | Enable CSW-T operations | `\"true\"` |\n| `manager.allowed_ips` | Transaction access control | `\"*\"` |\n| `metadata.identification_title` | Service title | `EOEPCA Resource Catalogue` |\n| `metadata.provider_name` | Organization name | `EOEPCA` |\n| `metadata.provider_url` | Organization website | `https://eoepca.org/` |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:44-82]()\n\n### INSPIRE Profile\n\nThe catalogue supports the INSPIRE metadata profile for European spatial data:\n\n```yaml\ninspire:\n  enabled: \"true\"\n  languages_supported: eng,gre\n  default_language: eng\n  date: YYYY-MM-DD\n  gemet_keywords: Utility and governmental services\n  conformity_service: notEvaluated\n  contact_name: Organization Name\n  contact_email: Email Address\n  temp_extent: YYYY-MM-DD/YYYY-MM-DD\n```\n\nWhen enabled, the catalogue validates and serves metadata according to INSPIRE technical guidelines.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:72-82]()\n\n---\n\n## Federated Search\n\nWorkspace catalogues implement federated search to query both local user data and global platform resources. The `federatedcatalogues` configuration parameter specifies remote catalogue endpoints:\n\n```yaml\npycsw:\n  config:\n    server:\n      federatedcatalogues: https://resource-catalogue.develop.eoepca.org/collections/S2MSI2A\n```\n\nThis enables a workspace catalogue to:\n\n1. Query local user-generated STAC items\n2. Automatically search the global catalogue for platform datasets\n3. Merge results from multiple catalogue sources\n4. Provide unified discovery across personal and shared resources\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WSCat as \"Workspace Catalogue\u003cbr/\u003e(eric-workspace)\"\n    participant GlobalCat as \"Global Catalogue\u003cbr/\u003e(rm namespace)\"\n    \n    User-\u003e\u003eWSCat: CSW GetRecords\u003cbr/\u003e(eo:parentIdentifier=S2MSI2A)\n    \n    WSCat-\u003e\u003eWSCat: Query local PostgreSQL\u003cbr/\u003e(user processing outputs)\n    \n    WSCat-\u003e\u003eGlobalCat: Federated query\u003cbr/\u003efederatedcatalogues endpoint\n    GlobalCat--\u003e\u003eWSCat: Platform S2MSI2A records\n    \n    WSCat-\u003e\u003eWSCat: Merge local + federated results\n    WSCat--\u003e\u003eUser: Combined result set\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:33-34]()\n\n---\n\n## Operational Management\n\n### Resource Registration\n\nData products are registered into the catalogue by pushing their identifiers to Redis queues. The `register-S2-L2A-data.sh` script demonstrates bulk registration:\n\n```bash\nkubectl -n rm exec --stdin --tty data-access-redis-master-0 -- \\\n  redis-cli lpush register_queue \\\n  EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_20200902T090559_N0214_R050_T34SFH_20200902T113910.SAFE/ \\\n  EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_20200902T090559_N0214_R050_T35SLB_20200902T113910.SAFE/\n```\n\nThe registrar service consumes from `register_queue` and invokes `ItemBackend` to create ISO 19115 records in PostgreSQL.\n\n**Sources:**\n- [system/clusters/data/register-S2-L2A-data.sh:1-10]()\n- [system/clusters/data/register-S2-L1C-data.sh:1-10]()\n\n### Policy Management\n\nAlthough the global Resource Catalogue in the reference deployment does not have ingress protection enabled, workspace catalogues can be protected by Resource Guards (PEPs). The `unregister-resource.sh` script includes catalogue-specific resource cleanup, indicating support for protected deployments:\n\n```bash\n# resource-catalogue-pep\necho -n \"Delete resource ${resourceId} from resource-catalogue-pep...\"\nkubectl -n rm exec -it svc/resource-catalogue-pep -c resource-catalogue-pep -- \\\n  management_tools remove -r ${resourceId}\n```\n\n**Sources:**\n- [bin/unregister-resource.sh:26-29]()\n\n### Testing and Validation\n\nThe acceptance test suite validates catalogue functionality using `owslib.csw` for CSW queries and `pyops` for OpenSearch:\n\n```python\n# CSW operations validation\ncsw = CatalogueServiceWeb('https://resource-catalogue.develop.eoepca.org/csw')\noperations = [op.name for op in csw.operations]\n# Expects: ['GetCapabilities', 'GetRecords', 'GetRecordById', ...]\n\n# Record queries with filters\nbbox_query = BBox([37, 13.9, 37.9, 15.1])\nbegin = PropertyIsGreaterThanOrEqualTo(propertyname='apiso:TempExtent_begin', \n                                        literal='2021-04-02 00:00')\ncsw.getrecords2(constraints=[And([bbox_query, begin])])\n\n# OpenSearch queries\nclient = pyops.Client(description_xml_url='https://resource-catalogue.develop.eoepca.org/opensearch')\nresults = client.search(params={\"{eo:cloudCover?}\": {\"value\": \"]20\"}})\n```\n\n**Sources:**\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:17-98]()\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:20-96]()\n\n---\n\n## Data Model\n\n### ISO 19115 Metadata Records\n\nThe catalogue stores metadata following ISO 19115 geographic information standards. Key record properties include:\n\n| Property | ISO Path | Purpose |\n|----------|----------|---------|\n| Identifier | `apiso:Identifier` | Unique record ID |\n| Title | `apiso:Title` | Human-readable name |\n| Abstract | `apiso:Abstract` | Description |\n| Parent Identifier | `apiso:ParentIdentifier` | Collection reference |\n| Temporal Extent | `apiso:TempExtent_begin/end` | Time range |\n| Bounding Box | `apiso:BoundingBox` | Geographic extent |\n| Type | `apiso:Type` | Resource type (dataset, service, etc.) |\n| References | Links | Related resources (WMS, data download, etc.) |\n\n**Sources:**\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:79-98]()\n\n### STAC Integration\n\nWhile pycsw natively supports ISO 19115, the registrar backends translate STAC metadata into ISO records. The backends extract:\n\n- **STAC Item**  ISO 19115 Dataset record\n- **STAC Collection**  ISO 19115 Series/Collection record  \n- **STAC assets**  ISO 19115 Distribution information (links to data files, WMS services)\n\nThis allows the catalogue to serve both ISO-compliant CSW clients and STAC-aware applications through appropriate output schemas.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:888-903]()\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json:9]()"])</script><script>self.__next_f.push([1,"28:T48a1,"])</script><script>self.__next_f.push([1,"# Workspace API\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n\n\u003c/details\u003e\n\n\n\nThe Workspace API is an orchestrator service that provisions and manages isolated, user-specific workspaces within the EOEPCA platform. Each workspace provides a user with dedicated instances of Data Access services, Resource Catalogue, and S3 storage, all protected by workspace-specific Policy Enforcement Points (PEPs). This enables multi-tenant data isolation while maintaining a consistent service architecture across all users.\n\nFor information about the global Data Access and Resource Catalogue services, see [Data Access Services](#5.1) and [Resource Catalogue](#5.2). For details on the multi-tenant workspace architecture and template patterns, see [Multi-Tenant Workspaces](#5.5).\n\n## Architecture Overview\n\nThe Workspace API operates as a control plane service deployed in the global `rm` namespace. It orchestrates the creation of user-specific Kubernetes resources by instantiating HelmRelease templates, provisioning S3 buckets, and configuring Harbor registry access.\n\n**Workspace API Architecture**\n\n```mermaid\ngraph TB\n    User[\"User Request\u003cbr/\u003e(authenticated)\"]\n    \n    subgraph \"rm Namespace (Global)\"\n        WorkspaceAPI[\"workspace-api\u003cbr/\u003eOrchestrator Service\"]\n        Templates[\"ConfigMap:\u003cbr/\u003eworkspace-charts\"]\n        MinioAPI[\"minio-bucket-api\u003cbr/\u003eS3 Provisioning\"]\n        PEP[\"workspace-api-pep\u003cbr/\u003eResource Guard\"]\n    end\n    \n    subgraph \"workspace-name Namespace\"\n        NS[\"Namespace:\u003cbr/\u003eworkspace-name\"]\n        \n        subgraph \"Workspace Services\"\n            WSDA[\"HelmRelease: vs\u003cbr/\u003e(Data Access)\"]\n            WSRC[\"HelmRelease:\u003cbr/\u003erm-resource-catalogue\"]\n            WSGuard[\"HelmRelease:\u003cbr/\u003eresource-guard\"]\n        end\n        \n        subgraph \"Storage Resources\"\n            PVC[\"PVC:\u003cbr/\u003emanaged-nfs-storage\"]\n            Bucket[\"S3 Bucket:\u003cbr/\u003eworkspace-name\"]\n        end\n        \n        WSDA --\u003e PVC\n        WSDA --\u003e Bucket\n        WSRC --\u003e PVC\n        WSGuard --\u003e WSDA\n        WSGuard --\u003e WSRC\n    end\n    \n    subgraph \"External Services\"\n        Harbor[\"Harbor Registry\"]\n        Minio[\"MinIO S3\"]\n        Identity[\"Identity Service\"]\n    end\n    \n    User --\u003e PEP\n    PEP --\u003e WorkspaceAPI\n    WorkspaceAPI --\u003e Templates\n    WorkspaceAPI --\u003e MinioAPI\n    WorkspaceAPI --\u003e Harbor\n    WorkspaceAPI --\u003e NS\n    \n    Templates --\u003e WSDA\n    Templates --\u003e WSRC\n    Templates --\u003e WSGuard\n    \n    MinioAPI --\u003e Minio\n    Minio --\u003e Bucket\n    \n    WSGuard --\u003e Identity\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:1-50]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:1-269]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:1-68]()\n\n## Deployment Configuration\n\nThe Workspace API is deployed via a HelmRelease in the `rm` namespace using the `rm-workspace-api` Helm chart.\n\n| Configuration Parameter | Value | Purpose |\n|------------------------|-------|---------|\n| `fullnameOverride` | `workspace-api` | Service name in cluster |\n| `prefixForName` | `develop-user` | Prefix for generated workspace names |\n| `workspaceSecretName` | `bucket` | Name for S3 credentials secret |\n| `namespaceForBucketResource` | `rm` | Namespace where bucket resources are created |\n| `s3Endpoint` | `https://minio.develop.eoepca.org` | MinIO S3 endpoint URL |\n| `s3Region` | `RegionOne` | S3 region identifier |\n| `harborUrl` | `https://harbor.develop.eoepca.org` | Harbor registry URL |\n| `harborUsername` | `admin` | Harbor admin username |\n| `harborPasswordSecretName` | `harbor` | Secret containing Harbor password |\n| `umaClientSecretName` | `rm-uma-user-agent` | UMA client credentials for PEP integration |\n| `workspaceChartsConfigMap` | `workspace-charts` | ConfigMap containing HelmRelease templates |\n| `bucketEndpointUrl` | `http://minio-bucket-api:8080/bucket` | Internal bucket provisioning API |\n| `pepBaseUrl` | `http://workspace-api-pep:5576/resources` | PEP API for auto-protection |\n| `autoProtectionEnabled` | `True` | Automatically register resources with PEP |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:16-50]()\n\n## Workspace Provisioning Flow\n\nThe Workspace API follows a multi-step provisioning sequence when creating a new workspace for a user.\n\n**Workspace Creation Sequence**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant PEP as workspace-api-pep\n    participant API as workspace-api\n    participant K8s as Kubernetes API\n    participant BucketAPI as minio-bucket-api\n    participant Harbor as Harbor Registry\n    participant Flux as Flux Controllers\n    \n    User-\u003e\u003ePEP: POST /workspace\u003cbr/\u003e(with ID Token)\n    PEP-\u003e\u003ePEP: Validate UMA Token\n    PEP-\u003e\u003eAPI: Forward Request\n    \n    API-\u003e\u003eAPI: Generate Workspace Name\u003cbr/\u003e(prefixForName + username)\n    \n    API-\u003e\u003eK8s: Create Namespace\n    Note over K8s: namespace: develop-user-eric\n    K8s--\u003e\u003eAPI: Namespace Created\n    \n    API-\u003e\u003eBucketAPI: POST /bucket\u003cbr/\u003e{name: workspace-name}\n    BucketAPI-\u003e\u003eBucketAPI: Create S3 Bucket\n    BucketAPI-\u003e\u003eBucketAPI: Generate Access Keys\n    BucketAPI--\u003e\u003eAPI: {bucket, access_key, secret_key}\n    \n    API-\u003e\u003eK8s: Create Secret\u003cbr/\u003e(bucket credentials)\n    Note over K8s: secret: bucket\u003cbr/\u003ein workspace namespace\n    \n    API-\u003e\u003eHarbor: Create Robot Account\u003cbr/\u003e(workspace-specific)\n    Harbor--\u003e\u003eAPI: Robot Credentials\n    \n    API-\u003e\u003eK8s: Create Secret\u003cbr/\u003e(harbor credentials)\n    \n    API-\u003e\u003eAPI: Render Templates\u003cbr/\u003e(substitute placeholders)\n    Note over API: {{ workspace_name }}\u003cbr/\u003e{{ bucket }}\u003cbr/\u003e{{ access_key_id }}\u003cbr/\u003e{{ secret_access_key }}\n    \n    API-\u003e\u003eK8s: Create HelmRelease\u003cbr/\u003e(vs - Data Access)\n    API-\u003e\u003eK8s: Create HelmRelease\u003cbr/\u003e(rm-resource-catalogue)\n    API-\u003e\u003eK8s: Create HelmRelease\u003cbr/\u003e(resource-guard)\n    \n    Flux-\u003e\u003eK8s: Reconcile HelmReleases\n    Note over Flux: Deploy actual services\n    \n    K8s--\u003e\u003eAPI: All Resources Created\n    API--\u003e\u003ePEP: Workspace Created\n    PEP--\u003e\u003eUser: 200 OK\u003cbr/\u003e{workspace_details}\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:34-50]()\n\n## Template System\n\nThe Workspace API uses a template-based approach to generate workspace-specific HelmRelease manifests. Templates are stored in a ConfigMap (`workspace-charts`) and contain placeholders that are substituted during provisioning.\n\n### Data Access Template\n\nThe Data Access template (`template-hr-data-access.yaml`) defines a complete Data Access service instance with workspace-specific configuration.\n\n**Key Template Placeholders:**\n\n| Placeholder | Example Value | Usage |\n|------------|---------------|-------|\n| `{{ workspace_name }}` | `develop-user-eric` | Namespace, DNS hostnames |\n| `{{ bucket }}` | `develop-user-eric` | S3 bucket name |\n| `{{ access_key_id }}` | `AKIAIOSFODNN7EXAMPLE` | S3 credentials |\n| `{{ secret_access_key }}` | `wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY` | S3 credentials |\n\n**Template Structure:**\n\n```yaml\n# Data Access Service Configuration\nglobal:\n  ingress:\n    tls:\n      - hosts:\n          - data-access.{{ workspace_name }}.develop.eoepca.org\n  \n  storage:\n    data:\n      data:\n        type: \"S3\"\n        endpoint_url: https://minio.develop.eoepca.org\n        access_key_id: {{ access_key_id }}\n        secret_access_key: {{ secret_access_key }}\n        bucket: {{ bucket }}\n  \n  metadata:\n    title: Data Access Service - {{ workspace_name }}\n```\n\nThe template configures a reduced-capacity Data Access instance with:\n- Single renderer replica (vs. 4 in global service)\n- Workspace-specific S3 storage backend\n- PostgreSQL persistence on NFS (`managed-nfs-storage`)\n- Redis queue for registration events\n- Harvester configured to read from user's bucket catalog\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:1-269]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:27-44]()\n\n### Resource Catalogue Template\n\nThe Resource Catalogue template provisions a pycsw instance for workspace metadata management.\n\n**Key Configuration:**\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `namespace` | `{{ workspace_name }}` | Deployment namespace |\n| `url` | `https://resource-catalogue.{{ workspace_name }}.develop.eoepca.org` | CSW endpoint |\n| `federatedcatalogues` | `https://resource-catalogue.develop.eoepca.org/collections/S2MSI2A` | Federated search to global catalogue |\n| `identification_title` | `Resource Catalogue - {{ workspace_name }}` | Service title |\n| `volume_storage_type` | `managed-nfs-storage-retain` | Persistent storage with retention policy |\n\nThe template enables federated catalogue search, allowing workspace catalogues to query both local and global collections while maintaining metadata isolation.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:1-68]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:17-34]()\n\n## Registrar Configuration\n\nEach workspace Data Access service includes a registrar component that processes metadata registration events from a Redis queue.\n\n**Registrar Routes:**\n\nThe workspace registrar defines multiple routes for different data types:\n\n| Route | Queue | Backend | Purpose |\n|-------|-------|---------|---------|\n| `items` | `register_queue` | `ItemBackend` | STAC Item registration to pycsw |\n| `collections` | `register_collection_queue` | `CollectionBackend` | STAC Collection registration |\n| `ades` | `register_ades_queue` | `ADESBackend` | ADES service registration |\n| `application` | `register_application_queue` | `CWLBackend` | CWL application registration |\n| `catalogue` | `register_catalogue_queue` | `CatalogueBackend` | Catalogue service registration |\n\nAll backends connect to the workspace-specific PostgreSQL database at `postgresql://postgres:mypass@resource-catalogue-db/pycsw`.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:84-164]()\n\n## Harvester Configuration\n\nThe workspace harvester is configured to process STAC catalogs stored in the user's S3 bucket.\n\n**S3 Bucket Harvester:**\n\n```yaml\nharvesters:\n  harvest-bucket-catalog:\n    queue: \"register_queue\"\n    resource:\n      type: \"STACCatalog\"\n      staccatalog:\n        filesystem: s3bucket\n        root_path: \"/home/catalog.json\"\n\nfilesystems:\n  s3bucket:\n    type: s3\n    s3:\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      endpoint_url: https://minio.develop.eoepca.org\n      region: RegionOne\n```\n\nThis configuration enables the harvester to automatically discover and register STAC items from a static catalog (`/home/catalog.json`) in the user's bucket, integrating processing outputs into the workspace catalogue.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:166-195]()\n\n## Storage Integration\n\nWorkspaces integrate with two storage backends for different persistence requirements.\n\n### S3 Bucket Provisioning\n\nThe Workspace API interacts with the `minio-bucket-api` service to provision dedicated S3 buckets for each workspace.\n\n**Bucket API Endpoint:**\n\n- **URL:** `http://minio-bucket-api:8080/bucket`\n- **Namespace:** `rm`\n- **Function:** Creates buckets on MinIO and generates IAM access credentials\n\n**Bucket Naming:**\n\nBuckets follow the pattern: `{prefixForName}-{username}`, resulting in names like `develop-user-eric`.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:38-47]()\n\n### NFS Persistent Volumes\n\nWorkspace services use NFS-backed PersistentVolumeClaims for database and cache persistence.\n\n**Storage Configuration:**\n\n| Service | PVC Usage | Storage Class | Typical Size |\n|---------|-----------|---------------|--------------|\n| PostgreSQL Database | Metadata storage | `managed-nfs-storage` | 100Gi |\n| Redis | Queue persistence | `managed-nfs-storage` | 1Gi |\n\nThe `managed-nfs-storage` storage class is defined globally and provides NFS-backed volumes suitable for multi-pod access patterns.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-240]()\n\n## Resource Protection\n\nThe Workspace API integrates with the Policy Enforcement infrastructure to automatically protect workspace resources.\n\n### Auto-Protection Flow\n\n**Protection Registration**\n\n```mermaid\ngraph LR\n    API[\"workspace-api\"]\n    PEP[\"workspace-api-pep\"]\n    Resource[\"Workspace Service\u003cbr/\u003e(Data Access / Catalogue)\"]\n    PDP[\"pdp-engine\"]\n    Identity[\"identity-service\"]\n    \n    API --\u003e|\"1. Register Resource\"| PEP\n    PEP --\u003e|\"2. Create Protection Policy\"| PDP\n    PDP --\u003e|\"3. Store Policy\"| Identity\n    Resource --\u003e|\"4. Enforce on Access\"| PEP\n    \n    Note1[\"Resource URI Pattern:\u003cbr/\u003e/data-access/{workspace_name}\u003cbr/\u003e/catalogue/{workspace_name}\"]\n    Note2[\"Default Owner:\u003cbr/\u003eUser who created workspace\"]\n    Note3[\"Policy Type:\u003cbr/\u003eOwnership-based access\"]\n```\n\nWhen `autoProtectionEnabled` is `True`, the Workspace API automatically registers each workspace service with the PEP at `http://workspace-api-pep:5576/resources`, creating ownership policies that restrict access to the workspace owner.\n\n**UMA Client Configuration:**\n\n- **Secret:** `rm-uma-user-agent` in `rm` namespace\n- **Purpose:** Provides UMA credentials for PEP registration\n- **Scope:** Allows Workspace API to register resources on behalf of users\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:44-49]()\n\n## Harbor Registry Integration\n\nEach workspace receives dedicated registry access through Harbor robot accounts.\n\n**Harbor Configuration:**\n\n| Parameter | Value |\n|-----------|-------|\n| URL | `https://harbor.develop.eoepca.org` |\n| Admin Username | `admin` |\n| Password Secret | `harbor` (in `rm` namespace) |\n\nThe Workspace API creates a robot account per workspace with permissions to:\n- Pull public images\n- Push/pull workspace-specific repositories\n\nThis enables users to:\n1. Store application container images in private repositories\n2. Deploy applications from private registries to ADES\n3. Maintain separation between user workspaces\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:41-43]()\n\n## Service Endpoints\n\nOnce provisioned, workspace services are accessible via dedicated subdomains following consistent naming patterns.\n\n**Workspace Endpoint Patterns:**\n\n| Service | Endpoint Pattern | Example |\n|---------|------------------|---------|\n| Data Access - OGC Services | `https://data-access.{workspace_name}.develop.eoepca.org/ows` | `https://data-access.develop-user-eric.develop.eoepca.org/ows` |\n| Data Access - Client UI | `https://data-access.{workspace_name}.develop.eoepca.org/` | `https://data-access.develop-user-eric.develop.eoepca.org/` |\n| Resource Catalogue - CSW | `https://resource-catalogue.{workspace_name}.develop.eoepca.org` | `https://resource-catalogue.develop-user-eric.develop.eoepca.org` |\n| MinIO S3 - Bucket Access | `https://minio.develop.eoepca.org/{workspace_name}` | `https://minio.develop.eoepca.org/develop-user-eric` |\n\nAll endpoints are protected by workspace-specific Resource Guards that enforce ownership policies via UMA authentication.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:28-28]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:33-33]()\n\n## Resource Scaling\n\nWorkspace services are deployed with reduced resource allocations compared to global services to support multi-tenancy at scale.\n\n**Resource Allocation Comparison:**\n\n| Component | Global Service | Workspace Service |\n|-----------|----------------|-------------------|\n| Renderer Replicas | 4 | 1 |\n| Renderer CPU Request | 500m | 100m |\n| Renderer Memory Request | 1Gi | 300Mi |\n| Renderer Memory Limit | 3Gi | 3Gi |\n| Registrar CPU Request | 100m | 100m |\n| Registrar Memory Request | 100Mi | 100Mi |\n| Harvester CPU Request | 100m | 100m |\n| Scheduler CPU Request | 100m | 100m |\n| Ingestor Replicas | 1 | 0 (disabled) |\n| Preprocessor Replicas | 1 | 0 (disabled) |\n| Cache Replicas | 1 | 0 (disabled) |\n| Seeder Replicas | 1 | 0 (disabled) |\n\nWorkspace instances disable resource-intensive components (ingestor, preprocessor, cache, seeder) as users typically access data through the global services or direct S3 access.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:865-876]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:70-82]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:251-268]()\n\n## Related Services\n\nThe Workspace API coordinates with several other Resource Management components:\n\n- **[Data Access Services](#5.1)** - Global data visualization and OGC service endpoints\n- **[Resource Catalogue](#5.2)** - Global metadata catalogue with federated search capabilities  \n- **[Registration API](#5.4)** - Registration service for manual data registration\n- **[Multi-Tenant Workspaces](#5.5)** - Detailed workspace architecture and isolation patterns\n\nThe Workspace API serves as the provisioning control plane for the multi-tenant workspace architecture described in section 5.5."])</script><script>self.__next_f.push([1,"29:T5901,"])</script><script>self.__next_f.push([1,"# Data Registration and Harvesting\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the data registration and harvesting subsystem within EOEPCA's Resource Management building block. This subsystem is responsible for:\n\n1. **Harvesting** metadata from external Earth Observation data sources via OpenSearch APIs\n2. **Processing** harvested metadata through configurable postprocessors\n3. **Registering** processed metadata to the Resource Catalogue (pycsw database)\n4. Enabling both automated continuous harvesting and manual data registration workflows\n\nFor information about querying the registered data, see [Resource Catalogue](#5.2). For information about visualizing and accessing registered data, see [Data Access Services](#5.1).\n\n---\n\n## System Architecture\n\nThe data registration and harvesting pipeline consists of three primary components that work together to ingest external EO data metadata into the EOEPCA platform.\n\n```mermaid\ngraph TB\n    subgraph \"External Data Sources\"\n        CreoDIAS[\"CreoDIAS OpenSearch\u003cbr/\u003edatahub.creodias.eu/resto\"]\n        OtherOS[\"Other OpenSearch\u003cbr/\u003eEndpoints\"]\n        STACCat[\"STAC Catalogs\u003cbr/\u003e(User Workspace)\"]\n    end\n    \n    subgraph \"Data Access Service (rm namespace)\"\n        Harvester[\"vs.harvester\u003cbr/\u003eComponent\"]\n        Redis[\"data-access-redis-master\u003cbr/\u003eRedis Queues\"]\n        Registrar[\"vs.registrar\u003cbr/\u003eComponent\"]\n    end\n    \n    subgraph \"Redis Queues\"\n        RegQueue[\"register_queue\"]\n        RegCollQueue[\"register_collection_queue\"]\n        RegADESQueue[\"register_ades_queue\"]\n        RegAppQueue[\"register_application_queue\"]\n        RegCatQueue[\"register_catalogue_queue\"]\n        RegJSONQueue[\"register_json_queue\"]\n        RegXMLQueue[\"register_xml_queue\"]\n        SeedQueue[\"seed_queue\"]\n    end\n    \n    subgraph \"Registrar Backends\"\n        ItemBackend[\"registrar_pycsw.backend.ItemBackend\"]\n        CollBackend[\"registrar_pycsw.backend.CollectionBackend\"]\n        ADESBackend[\"registrar_pycsw.backend.ADESBackend\"]\n        CWLBackend[\"registrar_pycsw.backend.CWLBackend\"]\n        CatalogueBackend[\"registrar_pycsw.backend.CatalogueBackend\"]\n        JSONBackend[\"registrar_pycsw.backend.JSONBackend\"]\n        XMLBackend[\"registrar_pycsw.backend.XMLBackend\"]\n    end\n    \n    subgraph \"Storage\"\n        PyCSWDB[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db/pycsw\")]\n    end\n    \n    CreoDIAS --\u003e|Poll OpenSearch| Harvester\n    OtherOS --\u003e|Poll OpenSearch| Harvester\n    STACCat --\u003e|Read STAC| Harvester\n    \n    Harvester --\u003e|Enqueue Items| RegQueue\n    RegQueue --\u003e Redis\n    RegCollQueue --\u003e Redis\n    RegADESQueue --\u003e Redis\n    RegAppQueue --\u003e Redis\n    RegCatQueue --\u003e Redis\n    RegJSONQueue --\u003e Redis\n    RegXMLQueue --\u003e Redis\n    \n    Redis --\u003e|Consume| Registrar\n    \n    Registrar --\u003e|Route: items| ItemBackend\n    Registrar --\u003e|Route: collections| CollBackend\n    Registrar --\u003e|Route: ades| ADESBackend\n    Registrar --\u003e|Route: application| CWLBackend\n    Registrar --\u003e|Route: catalogue| CatalogueBackend\n    Registrar --\u003e|Route: json| JSONBackend\n    Registrar --\u003e|Route: xml| XMLBackend\n    \n    ItemBackend --\u003e|Write Metadata| PyCSWDB\n    CollBackend --\u003e|Write Metadata| PyCSWDB\n    ADESBackend --\u003e|Write Metadata| PyCSWDB\n    CWLBackend --\u003e|Write Metadata| PyCSWDB\n    CatalogueBackend --\u003e|Write Metadata| PyCSWDB\n    JSONBackend --\u003e|Write Metadata| PyCSWDB\n    XMLBackend --\u003e|Write Metadata| PyCSWDB\n    \n    Registrar -.-\u003e|Success| SeedQueue\n```\n\n**Diagram: Data Registration and Harvesting Architecture**\n\nThe harvester continuously polls external OpenSearch endpoints, processes results through postprocessors, and enqueues metadata to Redis. The registrar consumes from Redis queues and routes items to appropriate backends that write to the pycsw PostgreSQL database.\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:949-1087](), [system/clusters/creodias/resource-management/hr-data-access.yaml:878-947]()\n\n---\n\n## Harvester Service Configuration\n\nThe `vs.harvester` component is responsible for polling external data sources and enqueueing metadata for registration. It is configured via the `harvester` section in the Data Access HelmRelease.\n\n### Harvester Component Structure\n\n```mermaid\ngraph LR\n    subgraph \"Harvester Pod\"\n        HarvesterProc[\"harvester process\"]\n        Config[\"config.harvesters\u003cbr/\u003eConfiguration\"]\n    end\n    \n    subgraph \"Resource Types\"\n        OpenSearch[\"OpenSearch\u003cbr/\u003eType: OpenSearch\"]\n        STACType[\"STAC Catalog\u003cbr/\u003eType: STACCatalog\"]\n    end\n    \n    subgraph \"Postprocessors\"\n        S2Post[\"harvester_eoepca.postprocess.\u003cbr/\u003epostprocess_sentinel2\"]\n        L8Post[\"harvester_eoepca.postprocess.\u003cbr/\u003epostprocess_landsat8\"]\n        S1Post[\"harvester_eoepca.postprocess.\u003cbr/\u003epostprocess_sentinel1\"]\n        S3Post[\"harvester_eoepca.postprocess.\u003cbr/\u003epostprocess_sentinel3\"]\n    end\n    \n    Config --\u003e HarvesterProc\n    HarvesterProc --\u003e OpenSearch\n    HarvesterProc --\u003e STACType\n    \n    OpenSearch --\u003e S2Post\n    OpenSearch --\u003e L8Post\n    OpenSearch --\u003e S1Post\n    OpenSearch --\u003e S3Post\n    \n    S2Post --\u003e RedisQueue[\"Redis Queue\"]\n    L8Post --\u003e RedisQueue\n    S1Post --\u003e RedisQueue\n    S3Post --\u003e RedisQueue\n    STACType --\u003e RedisQueue\n```\n\n**Diagram: Harvester Component and Processing Pipeline**\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:949-1087]()\n\n### Configured Harvesters\n\nThe global Data Access service is configured with multiple harvesters for different satellite missions. Each harvester definition specifies:\n\n| Harvester Name | Resource Type | OpenSearch URL | Query Parameters | Postprocessor | Target Queue |\n|---------------|---------------|----------------|------------------|---------------|--------------|\n| `Sentinel2` | OpenSearch | `datahub.creodias.eu/resto/.../Sentinel2/describe.xml` | bbox, time range | `postprocess_sentinel2` | `register` |\n| `Landsat8` | OpenSearch | `datahub.creodias.eu/resto/.../Landsat8/describe.xml` | bbox, time range | `postprocess_landsat8` | `register` |\n| `Sentinel1-GRD` | OpenSearch | `datahub.creodias.eu/resto/.../Sentinel1/describe.xml` | bbox, time, productType=GRD-COG | `postprocess_sentinel1` | `register` |\n| `Sentinel1-SLC` | OpenSearch | `datahub.creodias.eu/resto/.../Sentinel1/describe.xml` | bbox, time, productType=SLC | `postprocess_sentinel1` | `register` |\n| `Sentinel3` | OpenSearch | `datahub.creodias.eu/resto/.../Sentinel3/describe.xml` | bbox, time, productType=OL_2_LFR___ | `postprocess_sentinel3` | `register` |\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:962-1087]()\n\n### Harvester Configuration Example\n\nThe Sentinel-2 harvester configuration demonstrates the key parameters:\n\n```yaml\nSentinel2:\n  resource:\n    type: OpenSearch\n    opensearch:\n      url: https://datahub.creodias.eu/resto/api/collections/Sentinel2/describe.xml\n      format:\n        type: 'application/json'\n        json:\n          property_mapping:\n            start_datetime: 'startDate'\n            end_datetime: 'completionDate'\n            productIdentifier: 'productIdentifier'\n      query:\n        time:\n          begin: 2019-09-10T00:00:00Z\n          end: 2019-09-11T00:00:00Z\n        collection: null\n        bbox: 14.9,47.7,16.4,48.7\n  filter: {}\n  postprocessors:\n    - type: external\n      process: harvester_eoepca.postprocess.postprocess_sentinel2\n      kwargs: {}\n  queue: register\n```\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:962-985]()\n\n### Redis Connection Configuration\n\nThe harvester connects to the Redis queue system via configuration:\n\n```yaml\nconfig:\n  redis:\n    host: data-access-redis-master\n    port: 6379\n```\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:958-960]()\n\n### Workspace-Specific Harvester Configuration\n\nUser workspace Data Access instances include a different harvester configuration that reads STAC catalogs from the user's S3 bucket:\n\n```yaml\nharvesters:\n  harvest-bucket-catalog:\n    queue: \"register_queue\"\n    resource:\n      type: \"STACCatalog\"\n      staccatalog:\n        filesystem: s3bucket\n        root_path: \"/home/catalog.json\"\nfilesystems:\n  s3bucket:\n    type: s3\n    s3:\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      endpoint_url: https://minio.develop.eoepca.org\n      region: RegionOne\n```\n\nThis allows users to register data within their workspace by maintaining a STAC catalog at `/home/catalog.json` in their bucket.\n\nSources: [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:174-195]()\n\n---\n\n## Registrar Service Configuration\n\nThe `vs.registrar` component consumes metadata from Redis queues and registers it to the Resource Catalogue database through specialized backends.\n\n### Registrar Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Registrar Service\"\n        RegistrarProc[\"registrar process\"]\n        RouteEngine[\"Route Engine\"]\n    end\n    \n    subgraph \"Input Queues\"\n        Q1[\"register_queue\u003cbr/\u003e(default items)\"]\n        Q2[\"register_collection_queue\"]\n        Q3[\"register_ades_queue\"]\n        Q4[\"register_application_queue\"]\n        Q5[\"register_catalogue_queue\"]\n        Q6[\"register_json_queue\"]\n        Q7[\"register_xml_queue\"]\n    end\n    \n    subgraph \"Route Handlers\"\n        R1[\"registrar.route.stac.ItemRoute\"]\n        R2[\"registrar.route.stac.CollectionRoute\"]\n        R3[\"registrar.route.json.JSONRoute\"]\n        R4[\"registrar.route.json.JSONRoute\"]\n        R5[\"registrar.route.json.JSONRoute\"]\n        R6[\"registrar.route.json.JSONRoute\"]\n        R7[\"registrar.route.json.JSONRoute\"]\n    end\n    \n    subgraph \"Backend Writers\"\n        B1[\"registrar_pycsw.backend.ItemBackend\"]\n        B2[\"registrar_pycsw.backend.CollectionBackend\"]\n        B3[\"registrar_pycsw.backend.ADESBackend\"]\n        B4[\"registrar_pycsw.backend.CWLBackend\"]\n        B5[\"registrar_pycsw.backend.CatalogueBackend\"]\n        B6[\"registrar_pycsw.backend.JSONBackend\"]\n        B7[\"registrar_pycsw.backend.XMLBackend\"]\n    end\n    \n    Q1 --\u003e RegistrarProc\n    Q2 --\u003e RegistrarProc\n    Q3 --\u003e RegistrarProc\n    Q4 --\u003e RegistrarProc\n    Q5 --\u003e RegistrarProc\n    Q6 --\u003e RegistrarProc\n    Q7 --\u003e RegistrarProc\n    \n    RegistrarProc --\u003e RouteEngine\n    \n    RouteEngine --\u003e|items route| R1\n    RouteEngine --\u003e|collections route| R2\n    RouteEngine --\u003e|ades route| R3\n    RouteEngine --\u003e|application route| R4\n    RouteEngine --\u003e|catalogue route| R5\n    RouteEngine --\u003e|json route| R6\n    RouteEngine --\u003e|xml route| R7\n    \n    R1 --\u003e B1\n    R2 --\u003e B2\n    R3 --\u003e B3\n    R4 --\u003e B4\n    R5 --\u003e B5\n    R6 --\u003e B6\n    R7 --\u003e B7\n    \n    B1 --\u003e DB[(\"PostgreSQL\u003cbr/\u003epycsw database\")]\n    B2 --\u003e DB\n    B3 --\u003e DB\n    B4 --\u003e DB\n    B5 --\u003e DB\n    B6 --\u003e DB\n    B7 --\u003e DB\n```\n\n**Diagram: Registrar Route and Backend Architecture**\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:878-947]()\n\n### Default Configuration\n\nThe registrar is configured with default behaviors and multiple specialized routes:\n\n```yaml\nregistrar:\n  config:\n    defaultBackends:\n      - path: registrar_pycsw.backend.ItemBackend\n        kwargs:\n          repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n          ows_url: https://data-access.develop.eoepca.org/ows\n    defaultSuccessQueue: seed_queue\n```\n\nThe `defaultSuccessQueue` setting causes successfully registered items to be enqueued to `seed_queue` for cache seeding operations.\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:888-893]()\n\n### Registrar Routes and Backends\n\nEach route configuration defines how a specific type of metadata is processed:\n\n| Route Name | Queue | Route Handler | Backend | Purpose |\n|------------|-------|---------------|---------|---------|\n| `collections` | `register_collection_queue` | `registrar.route.stac.Collection` | `CollectionBackend` | STAC collection registration |\n| `ades` | `register_ades_queue` | `registrar.route.json.JSONRoute` | `ADESBackend` | ADES service registration |\n| `application` | `register_application_queue` | `registrar.route.json.JSONRoute` | `CWLBackend` | CWL application package registration |\n| `catalogue` | `register_catalogue_queue` | `registrar.route.json.JSONRoute` | `CatalogueBackend` | Catalogue endpoint registration |\n| `json` | `register_json_queue` | `registrar.route.json.JSONRoute` | `JSONBackend` | Generic JSON metadata |\n| `xml` | `register_xml_queue` | `registrar.route.json.JSONRoute` | `XMLBackend` | Generic XML metadata |\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:894-947]()\n\n### Backend Database Connection\n\nAll backends connect to the same pycsw PostgreSQL database but use different metadata schemas and indexing strategies:\n\n```yaml\nbackends:\n  - path: registrar_pycsw.backend.ItemBackend\n    kwargs:\n      repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n      ows_url: https://data-access.develop.eoepca.org/ows\n```\n\nThe `ows_url` parameter provides the OGC Web Services endpoint URL that will be associated with registered items.\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:889-892]()\n\n### Replace Behavior\n\nRoutes can be configured with `replace: true` to update existing entries rather than creating duplicates:\n\n```yaml\nroutes:\n  collections:\n    path: registrar.route.stac.Collection\n    queue: register_collection_queue\n    replace: true\n```\n\nThis is particularly important for collections and applications that may be re-registered with updated metadata.\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:895-898]()\n\n---\n\n## Registration Pipeline Flow\n\nThe complete registration pipeline involves multiple stages from external data source to queryable catalogue entry.\n\n```mermaid\nsequenceDiagram\n    participant ExtOS as External OpenSearch\u003cbr/\u003e(CreoDIAS)\n    participant Harvester as vs.harvester\n    participant Post as Postprocessor\u003cbr/\u003e(harvester_eoepca)\n    participant Redis as Redis Queue\u003cbr/\u003e(register_queue)\n    participant Registrar as vs.registrar\n    participant Backend as ItemBackend\n    participant PyCSW as PostgreSQL\u003cbr/\u003e(pycsw DB)\n    participant RC as Resource Catalogue\u003cbr/\u003e(CSW/OpenSearch)\n    \n    Harvester-\u003e\u003eExtOS: Poll OpenSearch\u003cbr/\u003e(time window, bbox)\n    ExtOS--\u003e\u003eHarvester: Return results\u003cbr/\u003e(JSON format)\n    \n    Harvester-\u003e\u003ePost: Process metadata\n    Post-\u003e\u003ePost: Transform to STAC\n    Post-\u003e\u003ePost: Add CloudFerro paths\n    Post--\u003e\u003eHarvester: STAC Item JSON\n    \n    Harvester-\u003e\u003eRedis: lpush register_queue\u003cbr/\u003e(STAC Item)\n    \n    Redis-\u003e\u003eRegistrar: Consume queue\n    Registrar-\u003e\u003eRegistrar: Route to items handler\n    Registrar-\u003e\u003eBackend: Process STAC Item\n    \n    Backend-\u003e\u003eBackend: Transform to ISO 19115\n    Backend-\u003e\u003ePyCSW: INSERT metadata\n    PyCSW--\u003e\u003eBackend: Success\n    \n    Backend--\u003e\u003eRegistrar: Registration complete\n    Registrar-\u003e\u003eRedis: lpush seed_queue\u003cbr/\u003e(if configured)\n    \n    Note over RC: Metadata now queryable\n    \n    RC-\u003e\u003ePyCSW: CSW GetRecords query\n    PyCSW--\u003e\u003eRC: Return records\n```\n\n**Diagram: Complete Registration Pipeline Sequence**\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:949-1087](), [system/clusters/creodias/resource-management/hr-data-access.yaml:878-947]()\n\n### Registration Pipeline Steps\n\n1. **Polling**: Harvester polls external OpenSearch endpoint with spatial/temporal constraints\n2. **Postprocessing**: Results are transformed to STAC format with platform-specific metadata\n3. **Enqueueing**: STAC items are pushed to Redis queue (`register_queue`)\n4. **Consumption**: Registrar consumes from queue and routes to appropriate handler\n5. **Backend Processing**: Backend transforms STAC to ISO 19115 metadata\n6. **Database Write**: Metadata is inserted into pycsw PostgreSQL database\n7. **Cache Seeding**: (Optional) Item is enqueued to `seed_queue` for cache generation\n8. **Catalogue Availability**: Metadata becomes queryable via CSW/OpenSearch interfaces\n\n---\n\n## Manual Data Registration\n\nFor testing or manual data ingestion, items can be directly pushed to Redis queues using `kubectl` and `redis-cli`.\n\n### Manual Registration Script\n\nThe repository includes convenience scripts for manual registration:\n\n```bash\n#!/usr/bin/env bash\n\nkubectl -n rm exec --stdin --tty data-access-redis-master-0 -- \\\n  redis-cli lpush register_queue \\\n    EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_20200902T090559_N0214_R050_T34SFH_20200902T113910.SAFE/ \\\n    EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_20200902T090559_N0214_R050_T35SLB_20200902T113910.SAFE/ \\\n    EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_20200902T090559_N0214_R050_T34SGJ_20200902T113910.SAFE/\n```\n\nThis executes `redis-cli lpush` inside the Redis pod to add CloudFerro EODATA paths directly to the `register_queue`.\n\nSources: [system/clusters/data/register-S2-L2A-data.sh:9](), [system/clusters/data/register-S2-L1C-data.sh:9]()\n\n### Manual Registration Process\n\nTo manually register data:\n\n1. **Identify Redis pod**: `data-access-redis-master-0` in namespace `rm`\n2. **Execute redis-cli**: Use `kubectl exec` to run commands inside the pod\n3. **Push to queue**: Use `lpush register_queue \u003cpath\u003e` with CloudFerro EODATA paths\n4. **Monitor processing**: Watch registrar logs for processing confirmation\n5. **Verify registration**: Query Resource Catalogue to confirm metadata is available\n\nThe registrar will automatically consume these queue entries and process them through the standard pipeline.\n\n---\n\n## Environment Variables and Startup\n\nThe Data Access service uses several environment variables to control registration behavior:\n\n| Variable | Value | Purpose |\n|----------|-------|---------|\n| `REGISTRAR_REPLACE` | `\"true\"` | Enable replacement of existing entries |\n| `CPL_VSIL_CURL_ALLOWED_EXTENSIONS` | `.TIF,.TIFF,.tif,.tiff,.xml,.jp2,.jpg,.jpeg,.png,.nc` | GDAL virtual file system extensions |\n| `AWS_ENDPOINT_URL_S3` | `https://minio.develop.eoepca.org` | S3 endpoint for data access |\n| `AWS_HTTPS` | `\"FALSE\"` | Disable HTTPS for MinIO connections |\n\nAdditionally, startup scripts can be configured:\n\n```yaml\nenv:\n  startup_scripts:\n    - /registrar_pycsw/registrar_pycsw/initialize-collections.sh\n```\n\nThis initialization script registers default collections at service startup.\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:27-33]()\n\n---\n\n## Resource Requirements\n\nThe harvester and registrar components are configured with resource limits to ensure stable operation:\n\n```yaml\nharvester:\n  resources:\n    requests:\n      cpu: 100m\n      memory: 100Mi\n\nregistrar:\n  replicaCount: 1\n  resources:\n    requests:\n      cpu: 100m\n      memory: 100Mi\n```\n\nSources: [system/clusters/creodias/resource-management/hr-data-access.yaml:953-956](), [system/clusters/creodias/resource-management/hr-data-access.yaml:879-886]()\n\n---\n\n## Integration with Resource Catalogue\n\nThe registered metadata becomes queryable through the Resource Catalogue service's CSW and OpenSearch interfaces. The catalogue uses the pycsw database populated by the registrar backends.\n\nExample test query demonstrating registered data retrieval:\n\n```python\n# Query for Sentinel-2 data\nfilter_list = [PropertyIsEqualTo('apiso:ParentIdentifier', 'S2MSI1C')]\ncsw.getrecords2(constraints=filter_list, outputschema='http://www.isotc211.org/2005/gmd')\n```\n\nThe Resource Catalogue federates queries across both global and workspace-specific catalogues, enabling users to discover platform data and their own registered results.\n\nSources: [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:74-98]()\n\n---\n\n## Summary\n\nThe Data Registration and Harvesting subsystem provides automated and manual mechanisms for ingesting EO data metadata into the EOEPCA platform. Key components include:\n\n- **Harvester**: Polls external OpenSearch endpoints and STAC catalogs\n- **Redis Queues**: Decouple harvesting from registration with message queues\n- **Registrar**: Routes and processes different metadata types via specialized backends\n- **pycsw Database**: Stores ISO 19115 metadata for CSW/OpenSearch queries\n\nThis architecture enables continuous ingestion of satellite data from CloudFerro/CreoDIAS while also supporting user workspace data registration through STAC catalogs."])</script><script>self.__next_f.push([1,"2a:T4e66,"])</script><script>self.__next_f.push([1,"# Multi-Tenant Workspaces\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the multi-tenant workspace architecture in EOEPCA, which enables isolated, per-user environments for data storage, processing, and cataloging. Each workspace is provisioned on-demand with dedicated S3 storage, a private resource catalogue, data access services, and policy-based access control.\n\nFor information about the Workspace API that orchestrates this provisioning, see [Workspace API](#5.3). For details on policy enforcement and access control mechanisms, see [Policy Enforcement (PEP/PDP)](#4.3).\n\n## Architecture Overview\n\nThe workspace system provides isolated environments through a template-based provisioning mechanism. When a user requests a workspace, the Workspace API instantiates HelmRelease templates with user-specific values to create dedicated Kubernetes resources.\n\n```mermaid\ngraph TB\n    subgraph \"Global Layer (rm namespace)\"\n        WS_API[\"Workspace API\u003cbr/\u003eworkspace-api\"]\n        GlobalRC[\"Global Resource Catalogue\u003cbr/\u003eresource-catalogue\"]\n        GlobalDA[\"Global Data Access\u003cbr/\u003edata-access\"]\n        BucketAPI[\"Bucket Provisioning\u003cbr/\u003eminio-bucket-api\"]\n        Harbor[\"Container Registry\u003cbr/\u003eHarbor\"]\n        Templates[\"Template ConfigMap\u003cbr/\u003eworkspace-charts\"]\n    end\n    \n    subgraph \"User Workspace: eric-workspace\"\n        NS1[\"Kubernetes Namespace\u003cbr/\u003eeric-workspace\"]\n        RG1[\"Resource Guard\u003cbr/\u003eeric-specific PEP\"]\n        RC1[\"Resource Catalogue\u003cbr/\u003erm-resource-catalogue\"]\n        DA1[\"Data Access Service\u003cbr/\u003evs\"]\n        Bucket1[\"S3 Bucket\u003cbr/\u003eeric-workspace\"]\n        PVC1[\"Persistent Volume\u003cbr/\u003eNFS storage\"]\n    end\n    \n    subgraph \"User Workspace: alice-workspace\"\n        NS2[\"Kubernetes Namespace\u003cbr/\u003ealice-workspace\"]\n        RG2[\"Resource Guard\u003cbr/\u003ealice-specific PEP\"]\n        RC2[\"Resource Catalogue\u003cbr/\u003erm-resource-catalogue\"]\n        DA2[\"Data Access Service\u003cbr/\u003evs\"]\n        Bucket2[\"S3 Bucket\u003cbr/\u003ealice-workspace\"]\n        PVC2[\"Persistent Volume\u003cbr/\u003eNFS storage\"]\n    end\n    \n    WS_API --\u003e|\"1. Read templates\"| Templates\n    WS_API --\u003e|\"2. Create namespace\"| NS1\n    WS_API --\u003e|\"3. Provision bucket\"| BucketAPI\n    BucketAPI --\u003e Bucket1\n    WS_API --\u003e|\"4. Configure registry\"| Harbor\n    WS_API --\u003e|\"5. Instantiate services\"| RG1\n    WS_API --\u003e|\"5. Instantiate services\"| RC1\n    WS_API --\u003e|\"5. Instantiate services\"| DA1\n    \n    DA1 --\u003e Bucket1\n    DA1 --\u003e PVC1\n    RC1 --\u003e PVC1\n    RG1 -.-\u003e|\"Protects\"| DA1\n    RG1 -.-\u003e|\"Protects\"| RC1\n    \n    WS_API -.-\u003e|\"Same process\"| NS2\n    NS2 -.-\u003e RG2\n    NS2 -.-\u003e RC2\n    NS2 -.-\u003e DA2\n    DA2 -.-\u003e Bucket2\n    \n    GlobalRC --\u003e|\"Federated search\"| RC1\n    GlobalRC --\u003e|\"Federated search\"| RC2\n    GlobalDA -.-\u003e|\"Read-only access\"| Bucket1\n    GlobalDA -.-\u003e|\"Read-only access\"| Bucket2\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:1-50](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:1-269](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:1-68]()\n\n## Workspace Provisioning Flow\n\nThe following sequence illustrates the steps when a user requests a workspace through the Workspace API:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WS_API as Workspace API\u003cbr/\u003eworkspace-api\n    participant K8s as Kubernetes API\n    participant BucketAPI as Bucket API\u003cbr/\u003eminio-bucket-api:8080\n    participant Harbor as Harbor Registry\n    participant PEP as Resource Guard PEP\u003cbr/\u003eworkspace-api-pep:5576\n    participant Templates as ConfigMap\u003cbr/\u003eworkspace-charts\n    \n    User-\u003e\u003eWS_API: Request workspace creation\n    WS_API-\u003e\u003eK8s: Create namespace\u003cbr/\u003e(e.g., eric-workspace)\n    K8s--\u003e\u003eWS_API: Namespace created\n    \n    WS_API-\u003e\u003eBucketAPI: POST /bucket\u003cbr/\u003e{name: \"eric-workspace\"}\n    BucketAPI--\u003e\u003eWS_API: Bucket credentials\u003cbr/\u003e(access_key_id, secret_access_key)\n    \n    WS_API-\u003e\u003eHarbor: Create project\u003cbr/\u003eConfigure access\n    Harbor--\u003e\u003eWS_API: Registry configured\n    \n    WS_API-\u003e\u003eTemplates: Read HelmRelease templates\u003cbr/\u003e(template-hr-data-access.yaml\u003cbr/\u003etemplate-hr-resource-catalogue.yaml\u003cbr/\u003etemplate-hr-resource-guard.yaml)\n    Templates--\u003e\u003eWS_API: Template definitions\n    \n    WS_API-\u003e\u003eWS_API: Substitute placeholders:\u003cbr/\u003e{{ workspace_name }}\u003cbr/\u003e{{ access_key_id }}\u003cbr/\u003e{{ secret_access_key }}\u003cbr/\u003e{{ bucket }}\u003cbr/\u003e{{ default_owner }}\n    \n    WS_API-\u003e\u003eK8s: Apply HelmRelease\u003cbr/\u003erm-resource-catalogue\n    WS_API-\u003e\u003eK8s: Apply HelmRelease\u003cbr/\u003evs (data-access)\n    WS_API-\u003e\u003eK8s: Apply HelmRelease\u003cbr/\u003eresource-guard\n    \n    K8s--\u003e\u003eWS_API: Services deployed\n    \n    alt autoProtectionEnabled=True\n        WS_API-\u003e\u003ePEP: Register workspace resources\u003cbr/\u003ePOST /resources\n        PEP--\u003e\u003eWS_API: Resources protected\n    end\n    \n    WS_API--\u003e\u003eUser: Workspace ready\u003cbr/\u003e(endpoints, credentials)\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:35-49]()\n\n## Template System\n\nWorkspace components are provisioned using HelmRelease templates stored in a ConfigMap named `workspace-charts`. These templates contain placeholder variables that are substituted at runtime with user-specific values.\n\n### Template Placeholders\n\n| Placeholder | Description | Example Value |\n|-------------|-------------|---------------|\n| `{{ workspace_name }}` | User's workspace namespace | `eric-workspace` |\n| `{{ bucket }}` | S3 bucket name | `eric-workspace` |\n| `{{ access_key_id }}` | S3 access key | Generated by bucket API |\n| `{{ secret_access_key }}` | S3 secret key | Generated by bucket API |\n| `{{ default_owner }}` | Workspace owner identity | User's ID token subject |\n\n### Data Access Template Structure\n\nThe data access template configures a complete EOX View Server instance with workspace-specific storage and catalogue integration:\n\n```mermaid\ngraph TB\n    Template[\"template-hr-data-access.yaml\"]\n    \n    subgraph \"Key Template Sections\"\n        Ingress[\"Ingress Configuration\u003cbr/\u003eLine 24-31\u003cbr/\u003eHost: data-access.{{ workspace_name }}.develop.eoepca.org\"]\n        Storage[\"Storage Configuration\u003cbr/\u003eLines 33-44\u003cbr/\u003eS3 bucket: {{ bucket }}\u003cbr/\u003eCredentials: {{ access_key_id }}, {{ secret_access_key }}\"]\n        Registrar[\"Registrar Routes\u003cbr/\u003eLines 93-164\u003cbr/\u003ePostgreSQL: resource-catalogue-db\u003cbr/\u003eOWS URL: workspace-specific\"]\n        Harvester[\"Harvester Configuration\u003cbr/\u003eLines 174-195\u003cbr/\u003eHarvest from /home/catalog.json\u003cbr/\u003ein user's S3 bucket\"]\n        Database[\"PostgreSQL Persistence\u003cbr/\u003eLines 219-228\u003cbr/\u003eStorageClass: managed-nfs-storage\u003cbr/\u003eSize: 100Gi\"]\n        Redis[\"Redis Cache\u003cbr/\u003eLines 230-240\u003cbr/\u003eStorageClass: managed-nfs-storage\u003cbr/\u003eSize: 1Gi\"]\n    end\n    \n    Template --\u003e Ingress\n    Template --\u003e Storage\n    Template --\u003e Registrar\n    Template --\u003e Harvester\n    Template --\u003e Database\n    Template --\u003e Redis\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:24-31](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:93-164](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:174-195]()\n\n### Resource Catalogue Template Structure\n\nThe resource catalogue template provisions a pycsw-based catalogue with federated search capabilities:\n\n```mermaid\ngraph TB\n    Template[\"template-hr-resource-catalogue.yaml\"]\n    \n    subgraph \"Key Template Sections\"\n        Namespace[\"Namespace\u003cbr/\u003eLine 17\u003cbr/\u003enamespace: {{ workspace_name }}\"]\n        URL[\"Server URL\u003cbr/\u003eLine 33\u003cbr/\u003eurl: https://resource-catalogue.{{ workspace_name }}.develop.eoepca.org\"]\n        Federation[\"Federated Catalogues\u003cbr/\u003eLine 34\u003cbr/\u003efederatedcatalogues:\u003cbr/\u003ehttps://resource-catalogue.develop.eoepca.org/collections/S2MSI2A\"]\n        Title[\"Identification\u003cbr/\u003eLine 36\u003cbr/\u003eidentification_title: Resource Catalogue - {{ workspace_name }}\"]\n        Storage[\"Database Storage\u003cbr/\u003eLine 19\u003cbr/\u003evolume_storage_type: managed-nfs-storage-retain\"]\n    end\n    \n    Template --\u003e Namespace\n    Template --\u003e URL\n    Template --\u003e Federation\n    Template --\u003e Title\n    Template --\u003e Storage\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:17-67]()\n\n## Workspace Components\n\nEach provisioned workspace contains the following Kubernetes resources:\n\n### Service Deployments\n\n| Component | HelmRelease Name | Purpose |\n|-----------|-----------------|---------|\n| Resource Catalogue | `rm-resource-catalogue` | pycsw-based CSW/OpenSearch catalogue for workspace metadata |\n| Data Access | `vs` | EOX View Server for WMS/WCS/WMTS visualization |\n| Resource Guard | `resource-guard` | PEP enforcing ownership policies |\n\n### Storage Resources\n\n| Resource Type | Configuration | Purpose |\n|---------------|---------------|---------|\n| S3 Bucket | Provisioned via `minio-bucket-api:8080` | Object storage for user data and results |\n| PVC (Database) | `managed-nfs-storage`, 100Gi | PostgreSQL data for resource catalogue |\n| PVC (Redis) | `managed-nfs-storage`, 1Gi | Redis persistence for harvester queues |\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-240]()\n\n## Storage Isolation\n\n### S3 Bucket Provisioning\n\nS3 buckets are provisioned through the bucket API endpoint, which generates unique credentials per workspace:\n\n```mermaid\ngraph LR\n    WS_API[\"Workspace API\"]\n    BucketAPI[\"minio-bucket-api\u003cbr/\u003e:8080/bucket\"]\n    MinIO[\"MinIO Server\u003cbr/\u003eminio.develop.eoepca.org\"]\n    \n    WS_API --\u003e|\"POST /bucket\u003cbr/\u003e{name: 'eric-workspace'}\"| BucketAPI\n    BucketAPI --\u003e|\"Create bucket\"| MinIO\n    BucketAPI --\u003e|\"Generate IAM credentials\"| MinIO\n    BucketAPI -.-\u003e|\"Return credentials\"| WS_API\n    \n    MinIO --\u003e|\"Bucket: eric-workspace\"| Bucket1[\"S3 Bucket\u003cbr/\u003eeric-workspace\"]\n    MinIO --\u003e|\"Bucket: alice-workspace\"| Bucket2[\"S3 Bucket\u003cbr/\u003ealice-workspace\"]\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:38-47]()\n\n### Bucket Configuration in Data Access\n\nThe data access service is configured with workspace-specific bucket credentials:\n\n| Configuration Key | Template Value |\n|-------------------|----------------|\n| `global.storage.data.data.type` | `S3` |\n| `global.storage.data.data.endpoint_url` | `https://minio.develop.eoepca.org` |\n| `global.storage.data.data.access_key_id` | `{{ access_key_id }}` |\n| `global.storage.data.data.secret_access_key` | `{{ secret_access_key }}` |\n| `global.storage.data.data.bucket` | `{{ bucket }}` |\n| `global.storage.data.data.region_name` | `RegionOne` |\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()\n\n### Static Catalogue Harvesting\n\nEach workspace data access service includes a harvester configured to continuously harvest a static STAC catalogue from the user's bucket:\n\n```yaml\n# Lines 180-195 in template-hr-data-access.yaml\nharvesters:\n  harvest-bucket-catalog:\n    queue: \"register_queue\"\n    resource:\n      type: \"STACCatalog\"\n      staccatalog:\n        filesystem: s3bucket\n        root_path: \"/home/catalog.json\"\nfilesystems:\n  s3bucket:\n    type: s3\n    s3:\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      endpoint_url: https://minio.develop.eoepca.org\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:180-195]()\n\n## Access Control and Ownership\n\n### Resource Guard PEP\n\nEach workspace is protected by a dedicated Resource Guard (Policy Enforcement Point) that enforces ownership policies. The Workspace API configures automatic resource protection:\n\n| Configuration Key | Value |\n|-------------------|-------|\n| `pepBaseUrl` | `http://workspace-api-pep:5576/resources` |\n| `autoProtectionEnabled` | `True` |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:48-49]()\n\n### Workspace Service Isolation\n\nServices within a workspace are isolated through:\n\n1. **Namespace Isolation**: Each workspace runs in a dedicated Kubernetes namespace\n2. **Ingress Routing**: Subdomain-based routing (e.g., `data-access.eric-workspace.develop.eoepca.org`)\n3. **PEP Enforcement**: Resource Guard validates user identity against `default_owner` policy\n4. **Credential Isolation**: Unique S3 credentials per workspace\n\n```mermaid\ngraph TB\n    User[\"User: eric\"]\n    \n    subgraph \"eric-workspace Namespace\"\n        RG[\"Resource Guard PEP\u003cbr/\u003eValidates: owner == eric\"]\n        DA[\"Data Access\u003cbr/\u003edata-access.eric-workspace.develop.eoepca.org\"]\n        RC[\"Resource Catalogue\u003cbr/\u003eresource-catalogue.eric-workspace.develop.eoepca.org\"]\n        Bucket[\"S3 Bucket: eric-workspace\u003cbr/\u003eCredentials: eric-specific\"]\n    end\n    \n    User --\u003e|\"Request with ID token\"| RG\n    RG --\u003e|\"Policy check: PERMIT\"| DA\n    RG --\u003e|\"Policy check: PERMIT\"| RC\n    DA --\u003e Bucket\n    RC -.-\u003e|\"Metadata about\"| Bucket\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:17](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:28]()\n\n## Federated Catalogue Architecture\n\nThe workspace resource catalogues participate in a federated search hierarchy:\n\n```mermaid\ngraph TB\n    GlobalRC[\"Global Resource Catalogue\u003cbr/\u003eresource-catalogue.develop.eoepca.org\u003cbr/\u003e(rm namespace)\"]\n    \n    subgraph \"Workspace Catalogues\"\n        RC1[\"eric-workspace Catalogue\u003cbr/\u003eresource-catalogue.eric-workspace.develop.eoepca.org\"]\n        RC2[\"alice-workspace Catalogue\u003cbr/\u003eresource-catalogue.alice-workspace.develop.eoepca.org\"]\n        RC3[\"bob-workspace Catalogue\u003cbr/\u003eresource-catalogue.bob-workspace.develop.eoepca.org\"]\n    end\n    \n    GlobalRC --\u003e|\"Federated search\"| RC1\n    GlobalRC --\u003e|\"Federated search\"| RC2\n    GlobalRC --\u003e|\"Federated search\"| RC3\n    \n    RC1 -.-\u003e|\"federatedcatalogues config\u003cbr/\u003eLine 34\"| GlobalRC\n    RC2 -.-\u003e|\"federatedcatalogues config\"| GlobalRC\n    RC3 -.-\u003e|\"federatedcatalogues config\"| GlobalRC\n```\n\nEach workspace catalogue is configured with the global catalogue as a federated endpoint, enabling cross-workspace discovery while maintaining isolation.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:34]()\n\n## Workspace API Configuration\n\nThe Workspace API is deployed in the `rm` namespace with the following key configuration:\n\n| Configuration Key | Value | Purpose |\n|-------------------|-------|---------|\n| `prefixForName` | `develop-user` | Prefix for generated workspace names |\n| `workspaceSecretName` | `bucket` | Name of secret storing bucket credentials |\n| `namespaceForBucketResource` | `rm` | Namespace where bucket resources are created |\n| `s3Endpoint` | `https://minio.develop.eoepca.org` | MinIO S3 endpoint |\n| `s3Region` | `RegionOne` | S3 region identifier |\n| `harborUrl` | `https://harbor.develop.eoepca.org` | Container registry URL |\n| `harborUsername` | `admin` | Harbor admin user |\n| `harborPasswordSecretName` | `harbor` | Secret containing Harbor password |\n| `umaClientSecretName` | `rm-uma-user-agent` | UMA client for authentication |\n| `workspaceChartsConfigMap` | `workspace-charts` | ConfigMap containing templates |\n| `bucketEndpointUrl` | `http://minio-bucket-api:8080/bucket` | Bucket provisioning API |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:35-49]()\n\n## Registrar Backend Configuration\n\nThe workspace data access service includes a registrar component with multiple backend routes for different metadata types:\n\n### Registrar Routes\n\n| Route Name | Queue Name | Backend Class | Purpose |\n|------------|-----------|---------------|---------|\n| `items` | `register_queue` | `registrar_pycsw.backend.ItemBackend` | Register STAC items |\n| `collections` | `register_collection_queue` | `registrar_pycsw.backend.CollectionBackend` | Register STAC collections |\n| `ades` | `register_ades_queue` | `registrar_pycsw.backend.ADESBackend` | Register ADES services |\n| `application` | `register_application_queue` | `registrar_pycsw.backend.CWLBackend` | Register CWL applications |\n| `catalogue` | `register_catalogue_queue` | `registrar_pycsw.backend.CatalogueBackend` | Register catalogue metadata |\n| `json` | `register_json_queue` | `registrar_pycsw.backend.JSONBackend` | Register JSON metadata |\n| `xml` | `register_xml_queue` | `registrar_pycsw.backend.XMLBackend` | Register XML metadata |\n\nAll backends connect to the workspace-specific PostgreSQL database at `postgresql://postgres:mypass@resource-catalogue-db/pycsw`.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:93-164]()\n\n## Resource Scaling\n\nWorkspace services are configured with minimal resource requests suitable for single-user environments:\n\n### Data Access Service Resources\n\n| Component | Replicas | CPU Request | Memory Request | Memory Limit |\n|-----------|----------|-------------|----------------|--------------|\n| Renderer | 1 | 100m | 300Mi | 3Gi |\n| Registrar | 1 | 100m | 100Mi | - |\n| Harvester | 1 | 100m | 100Mi | - |\n| Scheduler | 1 | 100m | 100Mi | - |\n\n### Disabled Components\n\nThe following data access components are disabled (0 replicas) in workspace deployments to reduce resource consumption:\n- Ingestor\n- Preprocessor\n- Cache\n- Seeder (limited to zoom levels 0-6 when enabled)\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:70-268]()\n\n## Global vs Workspace Services\n\nThe EOEPCA deployment maintains both global services (in the `rm` namespace) and per-workspace services:\n\n### Global Services\n\n| Service | Purpose | Access |\n|---------|---------|--------|\n| Global Data Access | Platform-wide data visualization | Read-only access to all workspace buckets |\n| Global Resource Catalogue | Federated catalogue search | Aggregates metadata from all workspaces |\n| Workspace API | Workspace provisioning orchestrator | Creates and manages workspaces |\n| Registration API | Data registration endpoint | Enqueues metadata for processing |\n\n### Workspace Services\n\n| Service | Purpose | Access |\n|---------|---------|--------|\n| Workspace Data Access | User-specific data visualization | Full read/write access to workspace bucket |\n| Workspace Resource Catalogue | Private metadata catalogue | Federated with global catalogue |\n| Resource Guard | Policy enforcement | Validates ownership for all workspace requests |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:1-50](), [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:1-82](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:1-50](), [system/clusters/creodias/resource-management/hr-registration-api.yaml:1-37]()"])</script><script>self.__next_f.push([1,"2b:T4a83,"])</script><script>self.__next_f.push([1,"# Processing and Chaining\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [release-notes/release-0.3.md](release-notes/release-0.3.md)\n- [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml](system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-ades.yaml](system/clusters/creodias/processing-and-chaining/proc-ades.yaml)\n- [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml](system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml](system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml)\n- [system/clusters/creodias/resource-management/ss-harbor.yaml](system/clusters/creodias/resource-management/ss-harbor.yaml)\n- [system/clusters/creodias/system/test/hr-dummy-service-guard.yaml](system/clusters/creodias/system/test/hr-dummy-service-guard.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides an overview of the Processing and Chaining subsystem within EOEPCA, which enables users to deploy, execute, and develop Earth Observation processing applications. The subsystem encompasses three primary components: the Application Deployment and Execution Service (ADES), Application Hub, and Processor Development Environment (PDE).\n\nThis page covers the high-level architecture and integration patterns. For detailed information on specific components, see:\n- ADES implementation and OGC API Processes interface: [ADES](#6.1)\n- Application Hub and JupyterHub deployment: [Application Hub](#6.2)\n- PDE setup and development workflows: [Processor Development Environment](#6.3)\n- PEP/PDP integration for processing services: [Resource Guards and Access Control](#6.4)\n- CWL application package development: [CWL Application Packages](#6.5)\n\nFor data access and cataloging capabilities that support processing, see [Resource Management](#5).\n\n## Overview\n\nThe Processing and Chaining subsystem provides the execution infrastructure for running user-defined processing workflows on Earth Observation data. It implements OGC standards (WPS 2.0, OGC API - Processes) and uses the Common Workflow Language (CWL) as the application description format. Processing jobs execute as Kubernetes pods orchestrated by the Calrissian workflow engine, enabling scalable, cloud-native execution patterns.\n\nThe subsystem supports the complete application lifecycle:\n1. **Development** - Users develop and test applications in the PDE or Application Hub\n2. **Deployment** - Applications packaged as CWL + Docker are deployed to ADES\n3. **Execution** - ADES orchestrates data stage-in, processing, and stage-out\n4. **Results** - Outputs are written to user workspaces with STAC metadata\n\n## Component Architecture\n\nThe Processing and Chaining subsystem consists of three major components deployed in the `proc` namespace:\n\n```mermaid\ngraph TB\n    subgraph \"proc namespace\"\n        ADES[\"ADES Service\u003cbr/\u003e(proc-ades)\"]\n        ADESGuard[\"ADES Resource Guard\u003cbr/\u003e(proc-ades-guard)\"]\n        AppHub[\"Application Hub\u003cbr/\u003e(JupyterHub)\"]\n        PDE[\"Processor Development\u003cbr/\u003eEnvironment\"]\n    end\n    \n    subgraph \"User Interaction\"\n        User[\"User/Developer\"]\n    end\n    \n    subgraph \"Storage Layer\"\n        S3In[\"S3 Input Storage\u003cbr/\u003e(CloudFerro eodata)\"]\n        S3Out[\"S3 Output Storage\u003cbr/\u003e(MinIO/Workspace)\"]\n        NFS[\"NFS Persistent Volumes\u003cbr/\u003e(managed-nfs-storage)\"]\n    end\n    \n    subgraph \"Resource Management\"\n        RC[\"Resource Catalogue\"]\n        WS[\"Workspace API\"]\n    end\n    \n    subgraph \"Identity \u0026 Access\"\n        Identity[\"Identity Service\"]\n        PDP[\"PDP Engine\"]\n    end\n    \n    User --\u003e|Develop Applications| PDE\n    User --\u003e|Interactive Notebooks| AppHub\n    User --\u003e|Deploy CWL Apps| ADES\n    User --\u003e|Execute Jobs| ADES\n    \n    ADES --\u003e|Protected by| ADESGuard\n    ADESGuard --\u003e|UMA Flow| Identity\n    ADESGuard --\u003e|Policy Check| PDP\n    \n    ADES --\u003e|Query Metadata| RC\n    ADES --\u003e|Get Workspace Info| WS\n    ADES --\u003e|Stage-in Data| S3In\n    ADES --\u003e|Stage-out Results| S3Out\n    ADES --\u003e|Persistent Storage| NFS\n    \n    PDE --\u003e|Deploy to| ADES\n    AppHub --\u003e|Deploy to| ADES\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:1-143](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90]()\n\n### ADES (Application Deployment and Execution Service)\n\nThe ADES is the core processing engine, deployed via HelmRelease `proc-ades` in the `proc` namespace. It provides OGC-compliant interfaces for deploying and executing CWL-based application packages.\n\n| Configuration Aspect | Value | Description |\n|---------------------|-------|-------------|\n| Namespace | `proc` | Kubernetes namespace for processing services |\n| Chart | `eoepca/ades:2.0.24` | Helm chart and version |\n| Storage Class | `managed-nfs-storage` | NFS-based persistent storage |\n| Workflow Executor | Calrissian | CWL engine with native Kubernetes integration |\n| Max RAM per Job | `8Gi` | Resource limit for processing pods |\n| Max CPU Cores | `4` | CPU limit for concurrent processing |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:1-143]()\n\n### Application Hub (JupyterHub)\n\nThe Application Hub provides interactive notebook environments for data exploration and algorithm prototyping. Users access JupyterLab instances with pre-configured access to EOEPCA services.\n\n### Processor Development Environment (PDE)\n\nThe PDE is a complete development environment including JupyterLab, Theia IDE, local S3 storage (MinIO), and CI/CD tools (Jenkins) for building and testing application packages before deployment to ADES.\n\n**Sources:** [release-notes/release-0.3.md:251-255]()\n\n## ADES Configuration and Integration\n\nThe ADES integrates with multiple EOEPCA subsystems through its `workflowExecutor` configuration:\n\n```mermaid\ngraph LR\n    subgraph \"ADES Configuration\"\n        WE[\"workflowExecutor\"]\n        Inputs[\"inputs:\u003cbr/\u003eADES_STAGEIN_AWS_*\u003cbr/\u003eADES_STAGEOUT_AWS_*\"]\n        RM[\"useResourceManager: true\u003cbr/\u003eresourceManagerEndpoint\"]\n        Platform[\"platformDomain\"]\n        Storage[\"processingStorageClass\u003cbr/\u003eprocessingVolumeTmpSize\u003cbr/\u003eprocessingVolumeOutputSize\"]\n    end\n    \n    subgraph \"External Services\"\n        CloudFerro[\"CloudFerro eodata\u003cbr/\u003ehttp://data.cloudferro.com\"]\n        MinIO[\"MinIO\u003cbr/\u003ehttps://minio.develop.eoepca.org\"]\n        WorkspaceAPI[\"Workspace API\u003cbr/\u003ehttps://workspace-api.develop.eoepca.org\"]\n        Auth[\"Auth Service\u003cbr/\u003ehttps://auth.develop.eoepca.org\"]\n    end\n    \n    subgraph \"Kubernetes Resources\"\n        StorageClass[\"managed-nfs-storage\"]\n        PVC[\"Persistent Volume Claims\"]\n    end\n    \n    Inputs --\u003e|Stage-in from| CloudFerro\n    Inputs --\u003e|Stage-out to| MinIO\n    RM --\u003e|Get Workspace| WorkspaceAPI\n    Platform --\u003e|Authenticate| Auth\n    Storage --\u003e|Provision| StorageClass\n    StorageClass --\u003e|Create| PVC\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:82-122]()\n\n### Key Configuration Parameters\n\nThe ADES `workflowExecutor` section defines environment variables injected into all workflow stages:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `APP` | `ades` | Application identifier prefix `ADES_APP` |\n| `STAGEIN_AWS_SERVICEURL` | `http://data.cloudferro.com` | Source S3 endpoint for input data |\n| `STAGEIN_AWS_ACCESS_KEY_ID` | `test` | Credentials for stage-in |\n| `STAGEIN_AWS_REGION` | `RegionOne` | AWS region for CloudFerro |\n| `STAGEOUT_AWS_SERVICEURL` | `https://minio.develop.eoepca.org` | Destination S3 endpoint for outputs |\n| `useResourceManager` | `\"true\"` | Enable Workspace API integration |\n| `resourceManagerWorkspacePrefix` | `develop-user` | Workspace naming convention |\n| `resourceManagerEndpoint` | `https://workspace-api.develop.eoepca.org` | Workspace API URL |\n| `platformDomain` | `https://auth.develop.eoepca.org` | Identity service URL |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:88-101]()\n\n### Processing Resource Limits\n\nThe ADES enforces resource limits for each job execution:\n\n| Resource | Configuration Parameter | Value |\n|----------|------------------------|-------|\n| Temporary Volume | `processingVolumeTmpSize` | `6Gi` |\n| Output Volume | `processingVolumeOutputSize` | `6Gi` |\n| Maximum RAM | `processingMaxRam` | `8Gi` |\n| Maximum CPU Cores | `processingMaxCores` | `4` |\n| Workspace Retention | `processingKeepWorkspace` | `false` (cleaned after success) |\n| Failed Workspace Retention | `processingKeepWorkspaceIfFailed` | `True` |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:109-120]()\n\n## Workflow Execution Model\n\nADES executes CWL workflows using Calrissian, which translates CWL steps into Kubernetes pods. The execution follows a stage-in, process, stage-out pattern:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant ADES as \"ADES Service\u003cbr/\u003e(proc-ades)\"\n    participant PEP as \"Resource Guard\u003cbr/\u003e(proc-ades-guard)\"\n    participant Calrissian\n    participant K8sPods as \"K8s Processing Pods\"\n    participant RC as \"Resource Catalogue\"\n    participant S3In as \"S3 Input Storage\u003cbr/\u003e(CloudFerro)\"\n    participant S3Out as \"S3 Output Storage\u003cbr/\u003e(MinIO)\"\n    \n    User-\u003e\u003ePEP: POST /eric/processes/{processId}/execution\n    PEP-\u003e\u003ePEP: UMA Authentication\n    PEP-\u003e\u003eADES: Forward Authenticated Request\n    \n    ADES-\u003e\u003eRC: Query Input Data Locations\n    RC--\u003e\u003eADES: Return Metadata\n    \n    ADES-\u003e\u003eCalrissian: Create CWL Workflow Job\n    \n    Calrissian-\u003e\u003eK8sPods: Create Stage-in Pod\n    K8sPods-\u003e\u003eS3In: Download Input Data\n    K8sPods--\u003e\u003eCalrissian: Stage-in Complete\n    \n    Calrissian-\u003e\u003eK8sPods: Create Processing Pod(s)\n    K8sPods-\u003e\u003eK8sPods: Execute Application Logic\n    K8sPods--\u003e\u003eCalrissian: Processing Complete\n    \n    Calrissian-\u003e\u003eK8sPods: Create Stage-out Pod\n    K8sPods-\u003e\u003eS3Out: Upload Results + STAC Manifest\n    K8sPods--\u003e\u003eCalrissian: Stage-out Complete\n    \n    Calrissian--\u003e\u003eADES: Job Complete\n    ADES--\u003e\u003eUser: Return Job Status + Results Location\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:82-122](), [release-notes/release-0.3.md:30-40]()\n\n### Workflow Stages\n\nEach CWL workflow execution involves three stages:\n\n1. **Stage-in**: Downloads input data from S3-compatible storage using parameters from `STAGEIN_AWS_*` environment variables. Input specifications use STAC manifests to describe data locations.\n\n2. **Processing**: Executes user-defined Docker containers with mounted volumes (`processingVolumeTmpSize` for temporary data, `processingVolumeOutputSize` for results). Pods are subject to resource limits (`processingMaxRam`, `processingMaxCores`).\n\n3. **Stage-out**: Uploads results to user workspace S3 bucket (`STAGEOUT_AWS_SERVICEURL`) and generates STAC manifest describing outputs.\n\n**Sources:** [release-notes/release-0.3.md:66-76](), [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:108-120]()\n\n## Multi-Tenancy and Access Control\n\nThe Processing subsystem implements multi-tenancy through per-user resource protection. The `proc-ades-guard` HelmRelease deploys a resource-guard chart that creates PEP and UMA User Agent components.\n\n```mermaid\ngraph TB\n    subgraph \"User Spaces in ADES\"\n        EricSpace[\"/eric\u003cbr/\u003eowner: fad43ef3-...-1cf29d97908e\"]\n        BobSpace[\"/bob\u003cbr/\u003eowner: f0a19e32-...-128c2c284300\"]\n        AliceSpace[\"/alice\u003cbr/\u003eowner: 5fa1b608-...-46c79ec75b78\"]\n    end\n    \n    subgraph \"Resource Guard Components\"\n        PEP[\"pep-engine\u003cbr/\u003e(customDefaultResources)\"]\n        UMA[\"uma-user-agent\u003cbr/\u003e(nginxIntegration)\"]\n    end\n    \n    subgraph \"Backend Service\"\n        ADESService[\"proc-ades service\u003cbr/\u003e(port 80)\"]\n    end\n    \n    subgraph \"External Traffic\"\n        Ingress[\"Ingress\u003cbr/\u003eades.develop.eoepca.org\"]\n    end\n    \n    Ingress --\u003e|\"/(.*)| UMA\n    UMA --\u003e|Policy Enforcement| PEP\n    PEP --\u003e|Owns| EricSpace\n    PEP --\u003e|Owns| BobSpace\n    PEP --\u003e|Owns| AliceSpace\n    UMA --\u003e|\"Proxy to /$1\"| ADESService\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90]()\n\n### Per-User Resource Registration\n\nThe ADES guard pre-registers protected resources for specific users:\n\n| User | Resource URI | Owner ID | Description |\n|------|-------------|----------|-------------|\n| eric | `/eric` | `fad43ef3-23ef-48b0-86f0-1cf29d97908e` | Eric's ADES workspace |\n| bob | `/bob` | `f0a19e32-5651-404e-8acd-128c2c284300` | Bob's ADES workspace |\n| alice | `/alice` | `5fa1b608-2b28-4686-b571-46c79ec75b78` | Alice's ADES workspace |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:35-49]()\n\n### Resource Guard Configuration\n\nThe `proc-ades-guard` HelmRelease configures two sub-charts:\n\n1. **pep-engine**: Policy Enforcement Point that registers resources with the PDP\n   - `configMap.asHostname`: `auth` - Authorization Server hostname\n   - `configMap.pdpHostname`: `auth` - Policy Decision Point hostname\n   - `customDefaultResources`: Array of user-specific protected resources\n   - `volumeClaim.name`: `eoepca-proc-pvc` - Shared persistent storage\n\n2. **uma-user-agent**: Nginx-integrated UMA proxy\n   - `nginxIntegration.enabled`: `true` - Deploy as Ingress controller\n   - `nginxIntegration.hosts[0].host`: `ades` - Hostname for routing\n   - `nginxIntegration.hosts[0].paths[0].path`: `/(.*)` - Capture all paths\n   - `nginxIntegration.hosts[0].paths[0].service.name`: `proc-ades` - Backend service\n   - `client.credentialsSecretName`: `proc-uma-user-agent` - OIDC credentials\n   - `unauthorizedResponse`: Bearer realm with authentication URL\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:28-83]()\n\n## Persistence and Storage\n\nThe Processing subsystem uses multiple storage mechanisms for different purposes:\n\n```mermaid\ngraph TB\n    subgraph \"ADES Persistence\"\n        UserData[\"User Data Volume\u003cbr/\u003e(userDataSize: 10Gi)\"]\n        ProcServices[\"Process Services Volume\u003cbr/\u003e(procServicesSize: 5Gi)\"]\n    end\n    \n    subgraph \"Job Execution Volumes\"\n        TmpVol[\"Temporary Volume\u003cbr/\u003e(processingVolumeTmpSize: 6Gi)\"]\n        OutVol[\"Output Volume\u003cbr/\u003e(processingVolumeOutputSize: 6Gi)\"]\n    end\n    \n    subgraph \"Storage Class\"\n        NFS[\"managed-nfs-storage\u003cbr/\u003e(storageClass)\"]\n    end\n    \n    subgraph \"PEP Persistence\"\n        PEPPVC[\"eoepca-proc-pvc\u003cbr/\u003e(shared across proc namespace)\"]\n    end\n    \n    NFS --\u003e|Provision| UserData\n    NFS --\u003e|Provision| ProcServices\n    NFS --\u003e|Provision| TmpVol\n    NFS --\u003e|Provision| OutVol\n    NFS --\u003e|Provision| PEPPVC\n    \n    UserData -.-\u003e|AccessMode| RWO1[\"ReadWriteOnce\"]\n    ProcServices -.-\u003e|AccessMode| RWO2[\"ReadWriteOnce\"]\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:131-139](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:56-57]()\n\n### Storage Configuration Details\n\nThe ADES HelmRelease defines persistent storage through the `persistence` section:\n\n| Volume Purpose | Configuration Key | Size | Access Mode | Storage Class |\n|----------------|------------------|------|-------------|---------------|\n| User Data | `userDataSize` | `10Gi` | `ReadWriteOnce` | `managed-nfs-storage` |\n| Process Services | `procServicesSize` | `5Gi` | `ReadWriteOnce` | `managed-nfs-storage` |\n| Job Temporary | `processingVolumeTmpSize` | `6Gi` | Dynamic (per job) | `managed-nfs-storage` |\n| Job Output | `processingVolumeOutputSize` | `6Gi` | Dynamic (per job) | `managed-nfs-storage` |\n\nThe `clusterAdminRoleName: cluster-admin` configuration grants ADES the necessary permissions to create and manage pods dynamically through the Kubernetes API.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:76-139]()\n\n## Integration with Resource Management\n\nThe ADES integrates with the Resource Management subsystem to discover input data and provision user workspaces:\n\n```mermaid\ngraph LR\n    subgraph \"ADES Integration Points\"\n        ADES[\"ADES Service\"]\n        Config[\"workflowExecutor config\"]\n    end\n    \n    subgraph \"Resource Management\"\n        WS[\"Workspace API\u003cbr/\u003e(workspace-api.develop.eoepca.org)\"]\n        RC[\"Resource Catalogue\u003cbr/\u003e(OGC CSW/OpenSearch)\"]\n    end\n    \n    subgraph \"Workspace Resolution\"\n        WSPrefix[\"resourceManagerWorkspacePrefix\u003cbr/\u003edevelop-user\"]\n        WSEndpoint[\"resourceManagerEndpoint\"]\n        UseRM[\"useResourceManager: true\"]\n    end\n    \n    ADES --\u003e|Query Metadata| RC\n    ADES --\u003e|Resolve User Workspace| WS\n    Config --\u003e|Configure| UseRM\n    UseRM --\u003e|Construct URL| WSPrefix\n    UseRM --\u003e|Connect to| WSEndpoint\n    WSEndpoint --\u003e|Points to| WS\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:98-101]()\n\n### Workspace API Integration\n\nWhen `useResourceManager` is enabled, the ADES:\n1. Extracts the username from the JWT token using `usernameJwtJsonPath: \"user_name\"`\n2. Constructs the workspace name as `{resourceManagerWorkspacePrefix}-{username}`\n3. Queries the Workspace API at `resourceManagerEndpoint` for workspace details\n4. Uses the workspace-specific S3 bucket for stage-out operations\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:98-106]()\n\n## Deployment Structure\n\nThe Processing and Chaining subsystem is deployed through Flux CD HelmRelease resources:\n\n| Resource Name | Type | Chart | Version | Purpose |\n|---------------|------|-------|---------|---------|\n| `proc-ades` | HelmRelease | `eoepca/ades` | `2.0.24` | Core ADES service |\n| `proc-ades-guard` | HelmRelease | `eoepca/resource-guard` | `1.3.2` | PEP/UMA protection for ADES |\n\nBoth resources are located in [system/clusters/creodias/processing-and-chaining/]() and reconciled by Flux CD with `interval: 1m0s`.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:1-143](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90]()\n\n## Release History\n\nProcessing and Chaining capabilities were first introduced in Release 0.3, which included:\n\n- OGC WPS 2.0 and OGC API Processes interfaces\n- CWL application package deployment with Docker containers\n- Calrissian workflow engine integration for native Kubernetes execution\n- Data stage-in via OpenSearch catalogue references\n- Data stage-out to S3 buckets with STAC manifests\n- Dedicated user contexts within ADES\n- Rule-based policies for resource protection\n- Dynamic registration of per-user ADES resources\n- Integration with Resource Catalogue for input data discovery\n- Processor Development Environment with JupyterLab, Theia IDE, MinIO, and Jenkins\n\n**Sources:** [release-notes/release-0.3.md:1-97](), [release-notes/release-0.3.md:220-249]()"])</script><script>self.__next_f.push([1,"2c:T635e,"])</script><script>self.__next_f.push([1,"# ADES (Application Deployment and Execution Service)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml](system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-ades.yaml](system/clusters/creodias/processing-and-chaining/proc-ades.yaml)\n- [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml](system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml](system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml)\n- [system/clusters/creodias/resource-management/ss-harbor.yaml](system/clusters/creodias/resource-management/ss-harbor.yaml)\n- [system/clusters/creodias/system/test/hr-dummy-service-guard.yaml](system/clusters/creodias/system/test/hr-dummy-service-guard.yaml)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe ADES (Application Deployment and Execution Service) is the core processing engine of the EOEPCA platform, responsible for deploying and executing user-defined Earth Observation processing applications. It implements the OGC API Processes standard (formerly WPS - Web Processing Service) and orchestrates Common Workflow Language (CWL) workflows using Calrissian as the execution engine within Kubernetes.\n\nThis document covers the ADES service architecture, deployment configuration, workflow execution, data stage-in/stage-out mechanisms, and integration with platform services. For information about developing applications for ADES, see [CWL Application Packages](#6.5). For the development environment used to create processors, see [Processor Development Environment (PDE)](#6.3). For security and access control patterns, see [Resource Guards and Access Control](#6.4).\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:1-143]()\n\n## Architecture Overview\n\nThe ADES service acts as the orchestrator for processing workflows, bridging user requests with Kubernetes-based execution infrastructure. It manages the complete lifecycle of processing jobs including application deployment, input data stage-in, workflow execution, and output data stage-out.\n\n```mermaid\ngraph TB\n    subgraph \"User Layer\"\n        User[\"User Request\u003cbr/\u003e(OGC API Processes)\"]\n        AppHub[\"Application Hub\u003cbr/\u003e(JupyterHub)\"]\n    end\n    \n    subgraph \"ADES Service (proc namespace)\"\n        PEP[\"proc-ades-guard\u003cbr/\u003e(UMA User Agent + PEP)\"]\n        ADES[\"proc-ades\u003cbr/\u003e(ADES Service)\"]\n        PVC_User[\"eoepca-proc-pvc\u003cbr/\u003e(User Data - 10Gi)\"]\n        PVC_Proc[\"eoepca-proc-pvc\u003cbr/\u003e(Process Services - 5Gi)\"]\n    end\n    \n    subgraph \"Workflow Execution\"\n        Calrissian[\"Calrissian\u003cbr/\u003e(CWL Executor)\"]\n        WorkflowPods[\"Processing Pods\u003cbr/\u003e(K8s Jobs)\"]\n        TmpVol[\"Temp Volumes\u003cbr/\u003e(6Gi per job)\"]\n        OutVol[\"Output Volumes\u003cbr/\u003e(6Gi per job)\"]\n    end\n    \n    subgraph \"Data Sources\"\n        RC[\"Resource Catalogue\u003cbr/\u003e(Data Discovery)\"]\n        S3In[\"CloudFerro S3\u003cbr/\u003e(EODATA)\"]\n    end\n    \n    subgraph \"User Workspace\"\n        WSApi[\"Workspace API\u003cbr/\u003e(Resource Manager)\"]\n        S3Out[\"MinIO S3\u003cbr/\u003e(User Bucket)\"]\n        STAC[\"STAC Catalog\u003cbr/\u003e(Output Metadata)\"]\n    end\n    \n    subgraph \"Identity \u0026 Auth\"\n        Identity[\"Identity Service\u003cbr/\u003e(Keycloak)\"]\n        PDP[\"PDP Engine\u003cbr/\u003e(Policy Decision)\"]\n    end\n    \n    User --\u003e|\"HTTP Request\"| PEP\n    AppHub --\u003e|\"Deploy/Execute\"| PEP\n    PEP --\u003e|\"UMA Auth\"| Identity\n    PEP --\u003e|\"Policy Check\"| PDP\n    PEP --\u003e|\"Authorized\"| ADES\n    \n    ADES --\u003e|\"Query Inputs\"| RC\n    ADES --\u003e|\"Get User Workspace\"| WSApi\n    ADES --\u003e|\"Submit CWL\"| Calrissian\n    ADES --\u003e|\"Store State\"| PVC_User\n    ADES --\u003e|\"Store Process Defs\"| PVC_Proc\n    \n    Calrissian --\u003e|\"Create Jobs\"| WorkflowPods\n    WorkflowPods --\u003e|\"Use\"| TmpVol\n    WorkflowPods --\u003e|\"Use\"| OutVol\n    \n    WorkflowPods --\u003e|\"Stage-In\"| S3In\n    WorkflowPods --\u003e|\"Stage-Out\"| S3Out\n    WorkflowPods --\u003e|\"Register STAC\"| STAC\n    \n    ADES -.-\u003e|\"Query Metadata\"| RC\n```\n\n**Diagram: ADES Architecture and Integration Points**\n\nThe ADES deployment consists of:\n- **proc-ades**: Main service exposing OGC API Processes endpoints\n- **proc-ades-guard**: Resource Guard providing UMA-based authentication and authorization\n- **Calrissian**: CWL workflow executor that creates Kubernetes pods for processing\n- **Persistent Volumes**: Storage for user data (10Gi) and process service definitions (5Gi)\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:1-143](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90]()\n\n## Deployment Configuration\n\nThe ADES service is deployed as a HelmRelease in the `proc` namespace using the `eoepca/ades` Helm chart.\n\n### Service Configuration\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| Chart Version | 2.0.24 | ADES Helm chart version |\n| Replica Count | 1 | Single instance deployment |\n| Service Type | ClusterIP | Internal cluster access |\n| Service Port | 80 | HTTP service port |\n| Storage Class | managed-nfs-storage | NFS-backed persistent storage |\n| Ingress Host | ades-open.develop.eoepca.org | Public access endpoint |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:1-46]()\n\n### Resource Limits\n\nThe ADES service itself (not the processing pods) has the following resource configuration:\n\n```yaml\nresources:\n  limits:\n    cpu: 2\n    memory: 4Gi\n  requests:\n    cpu: 500m\n    memory: 2Gi\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:59-69]()\n\n### Persistent Storage\n\nTwo persistent volume claims are configured:\n\n1. **User Data Volume**: 10Gi storage for user-specific data and job state\n2. **Process Services Volume**: 5Gi storage for deployed application definitions\n\n```yaml\npersistence:\n  enabled: true\n  storageClass: \"managed-nfs-storage\"\n  userDataAccessMode: ReadWriteOnce\n  userDataSize: 10Gi\n  procServicesAccessMode: ReadWriteOnce\n  procServicesSize: 5Gi\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:131-139]()\n\n## Workflow Execution with Calrissian\n\nThe ADES uses Calrissian as its workflow executor, which translates CWL (Common Workflow Language) workflows into Kubernetes jobs and pods.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant ADES as \"proc-ades\u003cbr/\u003e(ADES Service)\"\n    participant Calrissian\n    participant K8s as \"Kubernetes API\"\n    participant Pod as \"Processing Pod\"\n    participant S3In as \"CloudFerro S3\"\n    participant S3Out as \"User Workspace S3\"\n    \n    User-\u003e\u003eADES: POST /processes/{processId}/execution\n    ADES-\u003e\u003eADES: Validate CWL workflow\n    ADES-\u003e\u003eCalrissian: Submit CWL workflow\n    \n    Calrissian-\u003e\u003eK8s: Create Stage-In Job\n    K8s-\u003e\u003ePod: Start Stage-In Pod\n    Pod-\u003e\u003eS3In: Download input data\n    Pod-\u003e\u003ePod: Validate inputs\n    \n    Calrissian-\u003e\u003eK8s: Create Processing Job(s)\n    K8s-\u003e\u003ePod: Start Processing Pod\n    Pod-\u003e\u003ePod: Execute application\n    Pod-\u003e\u003ePod: Generate outputs\n    \n    Calrissian-\u003e\u003eK8s: Create Stage-Out Job\n    K8s-\u003e\u003ePod: Start Stage-Out Pod\n    Pod-\u003e\u003eS3Out: Upload results\n    Pod-\u003e\u003eS3Out: Create STAC catalog\n    \n    Calrissian-\u003e\u003eADES: Workflow complete\n    ADES-\u003e\u003eUser: Return job status + outputs\n```\n\n**Diagram: ADES Workflow Execution Sequence**\n\n### Workflow Executor Configuration\n\nThe workflow executor is configured with environment variables that are passed to all workflow stages:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `useKubeProxy` | True | Use Kubernetes proxy for API access |\n| `clusterAdminRoleName` | cluster-admin | Role for Calrissian to create pods |\n| `processingStorageClass` | managed-nfs-storage | Storage class for job volumes |\n| `processingVolumeTmpSize` | 6Gi | Temporary storage per job |\n| `processingVolumeOutputSize` | 6Gi | Output storage per job |\n| `processingMaxRam` | 8Gi | Maximum RAM per job |\n| `processingMaxCores` | 4 | Maximum CPU cores per job |\n| `processingKeepWorkspace` | false | Clean workspace on success |\n| `processingKeepWorkspaceIfFailed` | True | Keep workspace on failure for debugging |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:76-122]()\n\n### Workflow Input Environment Variables\n\nAll workflows receive the following prefixed environment variables (prefixed with `ADES_`):\n\n```yaml\nworkflowExecutor:\n  inputs:\n    APP: ades\n    STAGEIN_AWS_SERVICEURL: http://data.cloudferro.com\n    STAGEIN_AWS_ACCESS_KEY_ID: test\n    STAGEIN_AWS_SECRET_ACCESS_KEY: test\n    STAGEIN_AWS_REGION: RegionOne\n    STAGEOUT_AWS_SERVICEURL: https://minio.develop.eoepca.org\n```\n\nThese become `ADES_APP`, `ADES_STAGEIN_AWS_SERVICEURL`, etc. in the workflow execution environment.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:88-97]()\n\n## Data Stage-In and Stage-Out\n\nThe ADES implements a three-stage workflow execution pattern: stage-in, processing, and stage-out.\n\n```mermaid\ngraph LR\n    subgraph \"Stage-In Phase\"\n        RC[\"Resource Catalogue\u003cbr/\u003e(Query Metadata)\"]\n        S3Src[\"CloudFerro S3\u003cbr/\u003eEODATA bucket\"]\n        StageIn[\"Stage-In Pod\"]\n        TmpStore[\"Tmp Volume\u003cbr/\u003e(6Gi)\"]\n    end\n    \n    subgraph \"Processing Phase\"\n        ProcPod[\"Processing Pod\u003cbr/\u003e(CWL CommandLineTool)\"]\n        Docker[\"Docker Image\u003cbr/\u003e(Application)\"]\n        OutStore[\"Output Volume\u003cbr/\u003e(6Gi)\"]\n    end\n    \n    subgraph \"Stage-Out Phase\"\n        StageOut[\"Stage-Out Pod\"]\n        UserS3[\"MinIO S3\u003cbr/\u003eUser Workspace\"]\n        STACGen[\"STAC Generator\"]\n        STACCat[\"STAC Catalog\u003cbr/\u003e(catalog.json)\"]\n    end\n    \n    RC --\u003e|\"OpenSearch Query\"| StageIn\n    S3Src --\u003e|\"Download\"| StageIn\n    StageIn --\u003e|\"Write\"| TmpStore\n    \n    TmpStore --\u003e|\"Read Inputs\"| ProcPod\n    Docker --\u003e|\"Execute\"| ProcPod\n    ProcPod --\u003e|\"Write Results\"| OutStore\n    \n    OutStore --\u003e|\"Read Results\"| StageOut\n    StageOut --\u003e|\"Upload Files\"| UserS3\n    StageOut --\u003e|\"Generate\"| STACGen\n    STACGen --\u003e|\"Create\"| STACCat\n    STACCat --\u003e|\"Upload\"| UserS3\n```\n\n**Diagram: ADES Data Stage-In/Stage-Out Pipeline**\n\n### Stage-In Configuration\n\nStage-in retrieves input data from CloudFerro's EODATA S3 bucket:\n\n- **Service URL**: `http://data.cloudferro.com`\n- **Authentication**: Uses static credentials (`test`/`test`)\n- **Region**: `RegionOne`\n- **Access Pattern**: Read-only access to public EODATA bucket\n\nInput references from the Resource Catalogue are resolved to S3 paths, and data is downloaded to temporary volumes attached to processing pods.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:91-94]()\n\n### Stage-Out Configuration\n\nStage-out uploads results to user-specific MinIO buckets managed by the Workspace API:\n\n- **Service URL**: `https://minio.develop.eoepca.org`\n- **Authentication**: Uses workspace-specific credentials from Resource Manager\n- **Output Format**: STAC catalog with asset references\n- **Destination**: User workspace bucket (e.g., `develop-user-eric-workspace`)\n\nThe stage-out process creates a STAC catalog describing all output products, including metadata, spatial/temporal extents, and asset links.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:96-96]()\n\n## Integration with Platform Services\n\nThe ADES integrates with multiple EOEPCA platform services to provide end-to-end processing capabilities.\n\n### Resource Manager Integration\n\nThe ADES uses the Workspace API (Resource Manager) to determine user workspace locations and credentials:\n\n```yaml\nworkflowExecutor:\n  useResourceManager: \"true\"\n  resourceManagerWorkspacePrefix: \"develop-user\"\n  resourceManagerEndpoint: \"https://workspace-api.develop.eoepca.org\"\n  platformDomain: \"https://auth.develop.eoepca.org\"\n  usernameJwtJsonPath: \"user_name\"\n```\n\nThe Resource Manager provides:\n- User workspace bucket names\n- S3 credentials for stage-out\n- Namespace information for isolated execution\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:98-106]()\n\n### Resource Catalogue Integration\n\nThe ADES queries the Resource Catalogue to:\n- Resolve input data references from OpenSearch queries\n- Discover available datasets by spatial/temporal constraints\n- Retrieve metadata for input products\n\nInput references in execution requests can be:\n- Direct URLs to data products\n- OpenSearch queries that return catalog entries\n- STAC catalog references\n\n**Example Input Reference from Test Data:**\n\n```json\n{\n  \"id\": \"input_reference\",\n  \"input\": {\n    \"format\": {\n      \"mimeType\": \"application/json\"\n    },\n    \"href\": \"https://resource-catalogue.develop.eoepca.org/?mode=opensearch\u0026service=CSW\u0026version=3.0.0\u0026request=GetRecords\u0026elementsetname=full\u0026resulttype=results\u0026typenames=csw:Record\u0026recordids=S2B_MSIL1C_20210402T095029_N0300_R079_T33SVB_20210402T121737.SAFE\"\n  }\n}\n```\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json:3-11]()\n\n### Identity Service Integration\n\nAll ADES requests are authenticated via the UMA (User-Managed Access) pattern:\n- User obtains ID token from Identity Service (Keycloak)\n- ID token is exchanged for RPT (Requesting Party Token) via UMA ticket\n- RPT contains user identity used to determine workspace location\n\nThe `usernameJwtJsonPath` configuration (`user_name`) specifies the JWT claim used to extract the username for workspace resolution.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:106-106]()\n\n## Access Control and Security\n\nThe ADES is protected by a Resource Guard deployment that enforces user-based access control.\n\n### PEP Configuration\n\nThe `proc-ades-guard` HelmRelease deploys a Policy Enforcement Point (PEP) with user-specific resource protection:\n\n```yaml\npep-engine:\n  customDefaultResources:\n  - name: \"ADES Service for user 'eric'\"\n    description: \"Protected Access for eric to his space in the ADES\"\n    resource_uri: \"/eric\"\n    scopes: []\n    default_owner: \"fad43ef3-23ef-48b0-86f0-1cf29d97908e\"\n  - name: \"ADES Service for user 'bob'\"\n    description: \"Protected Access for bob to his space in the ADES\"\n    resource_uri: \"/bob\"\n    scopes: []\n    default_owner: \"f0a19e32-5651-404e-8acd-128c2c284300\"\n  - name: \"ADES Service for user 'alice'\"\n    description: \"Protected Access for alice to her space in the ADES\"\n    resource_uri: \"/alice\"\n    scopes: []\n    default_owner: \"5fa1b608-2b28-4686-b571-46c79ec75b78\"\n```\n\nEach user has a protected path (e.g., `/eric`) that only they can access. The `default_owner` is the user's Keycloak UUID.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:34-49]()\n\n### UMA User Agent Configuration\n\nThe UMA User Agent acts as an authentication proxy:\n\n```yaml\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: ades\n        paths:\n          - path: /(.*)\n            service:\n              name: proc-ades\n              port: 80\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n  client:\n    credentialsSecretName: \"proc-uma-user-agent\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://auth.develop.eoepca.org/oxauth/auth/passport/passportlogin.htm\"'\n  openAccess: false\n```\n\nThe proxy:\n- Intercepts all requests to `https://ades.develop.eoepca.org`\n- Validates UMA tokens with the Identity Service\n- Enforces timeout of 600 seconds for long-running requests\n- Enables CORS for browser-based clients\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:61-83]()\n\n### Resource Management Operations\n\nAdministrators can manage ADES resources using the `management_tools` CLI available in the PEP container:\n\n**Dump all registered resources:**\n```bash\nkubectl -n proc exec -it deploy/ades-pep -c ades-pep -- management_tools list --all | jq\n```\n\n**Unregister a specific resource:**\n```bash\nkubectl -n proc exec -it svc/ades-pep -c ades-pep -- management_tools remove -r \u003cresource-id\u003e\n```\n\n**Sources:** [bin/dump-policy.sh:21-21](), [bin/unregister-resource.sh:36-39]()\n\n## Application Package Format\n\nThe ADES supports two application package formats: CWL (Common Workflow Language) and OGC Application Packages embedded in ATOM feeds.\n\n### CWL Application Package\n\nCWL packages define workflows with typed inputs, Docker container references, and output specifications.\n\n**Example CWL Workflow Structure:**\n\n```mermaid\ngraph TB\n    subgraph \"CWL Workflow Definition\"\n        Workflow[\"Workflow\u003cbr/\u003e(id: nhi)\"]\n        CLT[\"CommandLineTool\u003cbr/\u003e(id: clt)\"]\n        DockerReq[\"DockerRequirement\u003cbr/\u003e(dockerPull)\"]\n    end\n    \n    subgraph \"Workflow Inputs\"\n        InRef[\"input_reference\u003cbr/\u003e(Directory[])\"]\n        Threshold[\"threshold\u003cbr/\u003e(string)\"]\n        AOI[\"aoi\u003cbr/\u003e(string?, optional)\"]\n    end\n    \n    subgraph \"Processing Step\"\n        Step1[\"step_1\u003cbr/\u003e(scatter: input_reference)\"]\n        BaseCmd[\"baseCommand: nhi\"]\n        EnvVars[\"EnvVarRequirement\u003cbr/\u003e(PATH settings)\"]\n    end\n    \n    subgraph \"Workflow Outputs\"\n        WfOut[\"wf_outputs\u003cbr/\u003e(Directory array)\"]\n        Results[\"results\u003cbr/\u003e(glob: .)\"]\n    end\n    \n    Workflow --\u003e|\"defines\"| CLT\n    Workflow --\u003e|\"uses\"| DockerReq\n    InRef --\u003e Step1\n    Threshold --\u003e Step1\n    AOI --\u003e Step1\n    Step1 --\u003e|\"runs\"| CLT\n    CLT --\u003e|\"baseCommand\"| BaseCmd\n    CLT --\u003e|\"requires\"| EnvVars\n    Step1 --\u003e|\"produces\"| Results\n    Results --\u003e WfOut\n```\n\n**Diagram: CWL Workflow Structure**\n\n**Key CWL Elements:**\n\n| Element | Purpose | Example Value |\n|---------|---------|---------------|\n| `$graph` | Defines workflow and tool definitions | Array of Workflow and CommandLineTool |\n| `class: Workflow` | Top-level workflow definition | `id: nhi` |\n| `class: CommandLineTool` | Executable tool definition | `id: clt` |\n| `DockerRequirement` | Container image specification | `dockerPull: registry.hub.docker.com/eoepcaci/nhi:dev0.0.3` |\n| `EnvVarRequirement` | Environment variables | `PATH: /usr/local/sbin:...:/srv/conda/envs/env_app_snuggs/bin` |\n| `ResourceRequirement` | Resource constraints | Empty (uses ADES defaults) |\n| `ScatterFeatureRequirement` | Parallel processing support | Enables scatter over inputs |\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:1-79]()\n\n### Application Deployment Request\n\nApplications are deployed by submitting a CWL package to the ADES:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"applicationPackage\",\n      \"input\": {\n        \"format\": {\n          \"mimeType\": \"application/cwl\"\n        },\n        \"value\": {\n          \"href\": \"https://raw.githubusercontent.com/EOEPCA/app-snuggs/main/app-package.cwl\"\n        }\n      }\n    }\n  ],\n  \"outputs\": [\n    {\n      \"format\": {\n        \"mimeType\": \"string\",\n        \"schema\": \"string\",\n        \"encoding\": \"string\"\n      },\n      \"id\": \"deployResult\",\n      \"transmissionMode\": \"value\"\n    }\n  ],\n  \"mode\": \"auto\",\n  \"response\": \"raw\"\n}\n```\n\nThe ADES retrieves the CWL from the provided `href`, validates it, and registers it as a process available for execution.\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json:1-28]()\n\n### ATOM Feed Application Package\n\nATOM feeds embed CWL workflows within XML:\n\n```xml\n\u003centry\u003e\n  \u003cid\u003ehttps://catalog.terradue.com:443/eoepca-services/search?format=atom\u0026amp;uid=app-s-expression\u003c/id\u003e\n  \u003ctitle type=\"text\"\u003es expressions\u003c/title\u003e\n  \u003csummary type=\"html\"\u003eApplies s expressions to EO acquisitions\u003c/summary\u003e\n  \u003cowc:offering code=\"http://www.opengis.net/eoc/applicationContext/cwl\"\n    xmlns:owc=\"http://www.opengis.net/owc/1.0\"\u003e\n    \u003cowc:content type=\"application/cwl\"\u003e\n      $graph:\n      - baseCommand: s-expression\n        class: CommandLineTool\n        hints:\n          DockerRequirement:\n            dockerPull: eoepca/s-expression:dev0.0.2\n        \u003c!-- CWL content continues --\u003e\n    \u003c/owc:content\u003e\n  \u003c/owc:offering\u003e\n\u003c/entry\u003e\n```\n\nThis format allows application catalogs to be discovered via OpenSearch and deployed directly from catalog entries.\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml:8-92]()\n\n## Processing Resources and Constraints\n\nThe ADES enforces resource limits on processing jobs to ensure fair resource usage and prevent cluster exhaustion.\n\n### Per-Job Resource Limits\n\n| Resource | Limit | Configuration Parameter |\n|----------|-------|------------------------|\n| Temporary Storage | 6Gi | `processingVolumeTmpSize` |\n| Output Storage | 6Gi | `processingVolumeOutputSize` |\n| Maximum RAM | 8Gi | `processingMaxRam` |\n| Maximum CPU Cores | 4 | `processingMaxCores` |\n| Storage Class | managed-nfs-storage | `processingStorageClass` |\n\n### Workspace Cleanup Policy\n\nThe ADES implements a cleanup policy for processing workspaces:\n\n- **On Success** (`processingKeepWorkspace: false`): Workspace is deleted to free resources\n- **On Failure** (`processingKeepWorkspaceIfFailed: True`): Workspace is retained for debugging\n\nThis allows users to inspect failed job data while ensuring successful jobs don't accumulate storage.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:109-120]()\n\n### Image Pull Secrets\n\nThe ADES can be configured with image pull secrets to access private container registries:\n\n```yaml\nworkflowExecutor:\n  imagePullSecrets: []\n```\n\nIn production deployments, this would reference a Kubernetes secret containing credentials for Harbor or other private registries.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:122-122]()\n\n## Operational Procedures\n\n### Monitoring Workflow Execution\n\nView running processing pods:\n```bash\nkubectl -n proc get pods -l app=proc-ades\n```\n\nCheck Calrissian job status:\n```bash\nkubectl -n proc get jobs\n```\n\nView workflow logs:\n```bash\nkubectl -n proc logs -l job-name=\u003cjob-name\u003e --tail=100 -f\n```\n\n### Troubleshooting Failed Jobs\n\nWhen `processingKeepWorkspaceIfFailed` is enabled, failed job data is retained in persistent volumes. Access the data:\n\n```bash\n# Find the PVC for a failed job\nkubectl -n proc get pvc\n\n# Mount and inspect\nkubectl -n proc exec -it deploy/proc-ades -- ls -la /home/ades/workspaces\n```\n\n### Policy Management\n\nThe ADES PEP can be managed using the policy management scripts:\n\n**Dump current ADES policies:**\n```bash\n./bin/dump-policy.sh proc ades-pep\n```\n\n**Unregister a resource from ADES:**\n```bash\n./bin/unregister-resource.sh \u003cresource-id\u003e\n```\n\n**Sources:** [bin/dump-policy.sh:21-21](), [bin/unregister-resource.sh:36-39]()\n\n### Accessing ADES Endpoints\n\n**Protected endpoint (requires authentication):**\n- `https://ades.develop.eoepca.org/\u003cusername\u003e/` - User-specific ADES instance\n\n**Open endpoint (for testing):**\n- `https://ades-open.develop.eoepca.org/` - Unprotected ADES instance\n\n**API Documentation:**\n- `/processes` - List available processes\n- `/processes/{processId}` - Process description\n- `/processes/{processId}/execution` - Execute process\n- `/jobs/{jobId}` - Job status and results\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:51-54]()\n\n## Related Documentation\n\n- For developing CWL applications, see [CWL Application Packages](#6.5)\n- For interactive development environments, see [Processor Development Environment (PDE)](#6.3)\n- For deploying applications via JupyterHub, see [Application Hub (JupyterHub)](#6.2)\n- For understanding the PEP/UMA authentication flow, see [UMA Authentication Flow](#4.4)\n- For workspace provisioning and S3 bucket management, see [Workspace API](#5.3)\n- For querying available datasets, see [Resource Catalogue](#5.2)\n- For S3 storage architecture details, see [S3 Storage Architecture](#7.1)"])</script><script>self.__next_f.push([1,"2d:T44aa,"])</script><script>self.__next_f.push([1,"# Application Hub (JupyterHub)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh)\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml](system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Application Hub provides a multi-user JupyterHub environment for interactive development, testing, and execution of Earth Observation processing applications. It enables users to develop Common Workflow Language (CWL) application packages in Jupyter notebooks before deploying them to the ADES for production execution. The hub integrates with EOEPCA's identity management system via OAuth2, provisions isolated user environments with configurable resource profiles, and provides access to workspace storage for data persistence.\n\nFor information about deploying applications to production, see [ADES (Application Deployment and Execution Service)](#6.1). For details on the development environment setup, see [Processor Development Environment (PDE)](#6.3).\n\n---\n\n## System Overview\n\nThe Application Hub is deployed as a Kubernetes-native JupyterHub instance in the `proc` namespace. It serves as the primary development interface for users to interact with EOEPCA services, providing authenticated notebook environments with pre-configured access to the platform's resource management and processing capabilities.\n\n**Deployment Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"External Access\"\n        User[\"User Browser\"]\n    end\n    \n    subgraph \"Ingress Layer\"\n        Ingress[\"Nginx Ingress\u003cbr/\u003eapplicationhub.develop.eoepca.org\"]\n        TLS[\"TLS Certificate\u003cbr/\u003eapplicationhub-tls\"]\n    end\n    \n    subgraph \"proc Namespace\"\n        Hub[\"JupyterHub Hub\u003cbr/\u003eapplication-hub\"]\n        Proxy[\"JupyterHub Proxy\"]\n        HubDB[(\"SQLite DB\u003cbr/\u003esqlite-pvc\u003cbr/\u003emanaged-nfs-storage\")]\n        Secrets[\"SealedSecret\u003cbr/\u003eapplication-hub-secrets\"]\n    end\n    \n    subgraph \"User Pods\"\n        Notebook1[\"Single-user Notebook\u003cbr/\u003ejupyter/minimal-notebook\"]\n        Notebook2[\"Single-user Notebook\u003cbr/\u003eeoepca/pde-container\"]\n        UserPVC[\"User PVC\u003cbr/\u003emanaged-nfs-storage\"]\n    end\n    \n    subgraph \"um Namespace\"\n        Identity[\"Identity Service\u003cbr/\u003eKeycloak\"]\n        OAuth[\"OAuth2 Endpoints\u003cbr/\u003eauth.develop.eoepca.org\"]\n    end\n    \n    User --\u003e|HTTPS| Ingress\n    Ingress --\u003e TLS\n    Ingress --\u003e Proxy\n    Proxy --\u003e Hub\n    Hub --\u003e HubDB\n    Hub --\u003e Secrets\n    \n    Hub --\u003e|Spawns| Notebook1\n    Hub --\u003e|Spawns| Notebook2\n    Notebook1 --\u003e UserPVC\n    Notebook2 --\u003e UserPVC\n    \n    Hub --\u003e|OAuth2 Flow| OAuth\n    OAuth --\u003e Identity\n    User --\u003e|Authenticate| OAuth\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:1-99]()\n\n---\n\n## HelmRelease Configuration\n\nThe Application Hub is deployed via a Flux CD HelmRelease that references the `eoepca/application-hub` Helm chart. The deployment is configured through the `proc-application-hub.yaml` manifest.\n\n| Configuration Aspect | Value | Description |\n|---------------------|-------|-------------|\n| HelmRelease Name | `application-hub` | Flux resource identifier |\n| Namespace | `proc` | Kubernetes namespace for processing services |\n| Chart Version | `2.0.52` | EOEPCA application-hub chart version |\n| Helm Repository | `eoepca` (common namespace) | Chart source reference |\n| Reconciliation Interval | `1m0s` | Flux checks for updates every minute |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:1-15](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:99]()\n\n### Ingress Configuration\n\nThe hub is exposed via an ingress resource with TLS termination:\n\n```mermaid\ngraph LR\n    Internet[\"Internet Traffic\"] --\u003e|HTTPS| IngressHost[\"applicationhub.develop.eoepca.org\"]\n    IngressHost --\u003e TLSSecret[\"Secret: applicationhub-tls\"]\n    IngressHost --\u003e Path[\"Path: /\u003cbr/\u003eType: ImplementationSpecific\"]\n    Path --\u003e HubService[\"Service: application-hub\"]\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:16-27]()\n\n---\n\n## OAuth2 Authentication Integration\n\nThe Application Hub implements OAuth2 authentication against EOEPCA's Identity Service, enabling single sign-on and centralized user management. The authentication flow uses the authorization code grant with PKCE.\n\n**OAuth2 Configuration Flow**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant JupyterHub as \"JupyterHub Hub\u003cbr/\u003eapplication-hub\"\n    participant OAuth as \"OAuth2 Endpoints\u003cbr/\u003eauth.develop.eoepca.org\"\n    participant Identity as \"Identity Service\u003cbr/\u003eKeycloak\"\n    \n    User-\u003e\u003eJupyterHub: Access https://applicationhub.develop.eoepca.org\n    JupyterHub-\u003e\u003eJupyterHub: No valid session\n    JupyterHub-\u003e\u003eOAuth: Redirect to OAUTH2_AUTHORIZE_URL\n    OAuth-\u003e\u003eUser: Display login page\n    User-\u003e\u003eOAuth: Enter credentials\n    OAuth-\u003e\u003eIdentity: Validate credentials\n    Identity--\u003e\u003eOAuth: User authenticated\n    OAuth-\u003e\u003eJupyterHub: Redirect to OAUTH_CALLBACK_URL\u003cbr/\u003ewith authorization code\n    JupyterHub-\u003e\u003eOAuth: Exchange code at OAUTH2_TOKEN_URL\u003cbr/\u003eusing OAUTH_CLIENT_ID/SECRET\n    OAuth--\u003e\u003eJupyterHub: Return access token\n    JupyterHub-\u003e\u003eOAuth: GET OAUTH2_USERDATA_URL\u003cbr/\u003ewith access token\n    OAuth--\u003e\u003eJupyterHub: User info (user_name)\n    JupyterHub-\u003e\u003eJupyterHub: Extract username via OAUTH2_USERNAME_KEY\n    JupyterHub-\u003e\u003eUser: Spawn single-user notebook\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:38-43]()\n\n### OAuth2 Environment Variables\n\nThe hub's OAuth2 integration is configured via environment variables in the hub pod:\n\n| Environment Variable | Value | Purpose |\n|---------------------|-------|---------|\n| `OAUTH_CALLBACK_URL` | `https://applicationhub.develop.eoepca.org/hub/oauth_callback` | Redirect URI after authentication |\n| `OAUTH2_AUTHORIZE_URL` | `https://auth.develop.eoepca.org/oxauth/restv1/authorize` | OAuth2 authorization endpoint |\n| `OAUTH2_TOKEN_URL` | `https://auth.develop.eoepca.org/oxauth/restv1/token` | Token exchange endpoint |\n| `OAUTH2_USERDATA_URL` | `https://auth.develop.eoepca.org/oxauth/restv1/userinfo` | User profile endpoint |\n| `OAUTH2_USERNAME_KEY` | `user_name` | JSON key for extracting username from userinfo response |\n| `OAUTH_LOGOUT_REDIRECT_URL` | `https://applicationhub.develop.eoepca.org` | Post-logout redirect target |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:38-43]()\n\n### Secret Management\n\nOAuth2 credentials and cryptographic keys are stored in a SealedSecret:\n\n```mermaid\ngraph TB\n    SealedSecret[\"SealedSecret\u003cbr/\u003eapplication-hub-secrets\u003cbr/\u003enamespace: proc\"]\n    Secret[\"Secret\u003cbr/\u003eapplication-hub-secrets\"]\n    \n    SecretKeys[\"Encrypted Keys:\u003cbr/\u003e- JUPYTERHUB_CRYPT_KEY\u003cbr/\u003e- OAUTH_CLIENT_ID\u003cbr/\u003e- OAUTH_CLIENT_SECRET\"]\n    \n    HubPod[\"Hub Pod\"]\n    EnvVars[\"Environment Variables\"]\n    \n    SealedSecret --\u003e|\"Decrypted by\u003cbr/\u003esealed-secrets controller\"| Secret\n    Secret --\u003e SecretKeys\n    HubPod --\u003e|\"References via\u003cbr/\u003eexistingSecret\"| Secret\n    SecretKeys --\u003e|\"Mounted as\"| EnvVars\n```\n\nThe sealed secrets are created using the `application-hub-sealed-secrets-create.sh` script, which generates a Kubernetes Secret and pipes it through `kubeseal` for encryption. The sealed secret is then committed to Git and decrypted at runtime by the sealed-secrets controller.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:34](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:47-63](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-17](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36]()\n\n---\n\n## Single-User Notebook Environments\n\nWhen a user authenticates, JupyterHub spawns a dedicated single-user notebook server pod. The environment configuration is controlled by the `singleuser` and `profileList` sections of the HelmRelease.\n\n**User Environment Spawning**\n\n```mermaid\ngraph TB\n    Hub[\"JupyterHub Hub\"]\n    KubeSpawner[\"KubeSpawner\u003cbr/\u003eKubernetes Pod Spawner\"]\n    \n    subgraph \"User Pod Specification\"\n        Profile[\"Selected Profile\"]\n        Image[\"Container Image\"]\n        Resources[\"CPU/Memory Limits\"]\n        Storage[\"PVC Mount\"]\n    end\n    \n    subgraph \"Created Resources\"\n        Pod[\"Pod:\u003cbr/\u003e{username}-{hash}\"]\n        PVC[\"PVC:\u003cbr/\u003eclaim-{username}\"]\n        NFSStorage[\"NFS Storage\u003cbr/\u003emanaged-nfs-storage\"]\n    end\n    \n    Hub --\u003e|\"User login\"| KubeSpawner\n    KubeSpawner --\u003e Profile\n    Profile --\u003e Image\n    Profile --\u003e Resources\n    Profile --\u003e Storage\n    \n    KubeSpawner --\u003e|\"Create\"| Pod\n    KubeSpawner --\u003e|\"Provision\"| PVC\n    PVC --\u003e NFSStorage\n    Pod --\u003e|\"Mount\"| PVC\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:85-98]()\n\n### Default Container Image\n\nThe default single-user image is `jupyter/minimal-notebook:2343e33dec46`, providing a baseline Python environment. The image can be overridden via environment variable or profile selection:\n\n```\nJUPYTERHUB_SINGLE_USER_IMAGE: \"eoepca/pde-container:1.0.3\"\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:37](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:86-88]()\n\n### Profile List Configuration\n\nUsers can select from multiple environment profiles at spawn time. The profile list defines different resource allocations and container images:\n\n| Profile Display Name | Description | Default | Resource Overrides |\n|---------------------|-------------|---------|-------------------|\n| `Minimal environment` | Baseline Python environment | `True` | None (uses singleuser defaults) |\n| `EOEPCA profile` | Enhanced environment for EO processing | `False` | `cpu_limit: 4`, `mem_limit: 8G` |\n\nThe profile configuration uses `kubespawner_override` to customize pod specifications. Note that profile definitions support arbitrary KubeSpawner configuration keys.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:89-98]()\n\n---\n\n## Storage and Persistence\n\nThe Application Hub provisions persistent storage at multiple levels to ensure data durability across pod restarts.\n\n**Storage Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Hub Persistence\"\n        HubPod[\"Hub Pod\"]\n        HubDB[\"Hub Database\u003cbr/\u003esqlite-pvc\"]\n        HubPVC[\"PVC\u003cbr/\u003e1Gi\u003cbr/\u003emanaged-nfs-storage\"]\n    end\n    \n    subgraph \"User Persistence\"\n        UserPod1[\"User Notebook Pod\u003cbr/\u003ealice\"]\n        UserPod2[\"User Notebook Pod\u003cbr/\u003ebob\"]\n        UserPVC1[\"PVC\u003cbr/\u003eclaim-alice\"]\n        UserPVC2[\"PVC\u003cbr/\u003eclaim-bob\"]\n    end\n    \n    subgraph \"Storage Backend\"\n        StorageClass[\"StorageClass\u003cbr/\u003emanaged-nfs-storage\"]\n        NFSServer[\"NFS Server\"]\n        NFSExport[\"/data/eoepca\"]\n    end\n    \n    HubPod --\u003e HubDB\n    HubDB --\u003e HubPVC\n    HubPVC --\u003e StorageClass\n    \n    UserPod1 --\u003e UserPVC1\n    UserPod2 --\u003e UserPVC2\n    UserPVC1 --\u003e StorageClass\n    UserPVC2 --\u003e StorageClass\n    \n    StorageClass --\u003e NFSServer\n    NFSServer --\u003e NFSExport\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:44](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:73-83]()\n\n### Hub Database Configuration\n\nThe JupyterHub hub maintains its state in a SQLite database stored on a PersistentVolume:\n\n| Configuration Key | Value | Description |\n|------------------|-------|-------------|\n| `jupyterhub.hub.db.type` | `sqlite-pvc` | Use SQLite with PVC backing |\n| `jupyterhub.hub.db.pvc.storage` | `1Gi` | Database volume size |\n| `jupyterhub.hub.db.pvc.storageClassName` | `managed-nfs-storage` | NFS-backed storage class |\n| `jupyterhub.hub.db.pvc.accessModes` | `[ReadWriteOnce]` | Single-writer access mode |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:73-83]()\n\n### User Notebook Storage\n\nEach user's notebook environment receives a dedicated PersistentVolumeClaim provisioned by the `managed-nfs-storage` StorageClass. The storage class is configured via the `STORAGE_CLASS` environment variable. User home directories persist across pod restarts, maintaining notebooks, data, and configuration files.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:44]()\n\n---\n\n## Integration with EOEPCA Services\n\nThe Application Hub environment is pre-configured to interact with other EOEPCA building blocks, enabling users to develop applications that leverage platform capabilities.\n\n**Service Integration Points**\n\n```mermaid\ngraph TB\n    Notebook[\"User Notebook\u003cbr/\u003eeoepca/pde-container\"]\n    \n    subgraph \"Resource Management\"\n        WorkspaceAPI[\"Workspace API\u003cbr/\u003edevelop-user-{username}\"]\n        ResourceCat[\"Resource Catalogue\u003cbr/\u003eCSW/OpenSearch\"]\n        DataAccess[\"Data Access\u003cbr/\u003eOGC WMS/WCS\"]\n    end\n    \n    subgraph \"Processing Services\"\n        ADES[\"ADES\u003cbr/\u003eOGC Processes API\"]\n        CWLDeploy[\"CWL Deployment\"]\n    end\n    \n    subgraph \"Identity\"\n        OAuth[\"OAuth2 Tokens\"]\n    end\n    \n    Notebook --\u003e|\"Query metadata\"| ResourceCat\n    Notebook --\u003e|\"Preview data\"| DataAccess\n    Notebook --\u003e|\"Access workspace storage\"| WorkspaceAPI\n    Notebook --\u003e|\"Deploy application\"| ADES\n    Notebook --\u003e|\"Package workflow\"| CWLDeploy\n    \n    Notebook --\u003e|\"Authenticated via\"| OAuth\n    OAuth --\u003e|\"Secures\"| WorkspaceAPI\n    OAuth --\u003e|\"Secures\"| ADES\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:37](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:45]()\n\n### Workspace Integration\n\nThe `RESOURCE_MANAGER_WORKSPACE_PREFIX` environment variable configures the naming convention for user workspaces. With the value `develop-user`, user workspaces are accessed at `develop-user-{username}`. This allows notebooks to programmatically construct workspace URLs and access user-specific resource catalogues and data access services.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:45]()\n\n### PDE Container Image\n\nThe `JUPYTERHUB_SINGLE_USER_IMAGE` can be set to `eoepca/pde-container:1.0.3`, which provides a Processor Development Environment with pre-installed tools for CWL development, STAC manipulation, and EOEPCA service interaction. This image includes libraries for interacting with the ADES and workspace services.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:37]()\n\n---\n\n## Environment Configuration Summary\n\nThe following table summarizes all environment variables configured in the hub pod:\n\n| Environment Variable | Source | Value/Description |\n|---------------------|--------|-------------------|\n| `JUPYTERHUB_ENV` | Static | `dev` - Development environment identifier |\n| `JUPYTERHUB_SINGLE_USER_IMAGE` | Static | `eoepca/pde-container:1.0.3` - Default user container |\n| `OAUTH_CALLBACK_URL` | Static | `https://applicationhub.develop.eoepca.org/hub/oauth_callback` |\n| `OAUTH2_USERDATA_URL` | Static | `https://auth.develop.eoepca.org/oxauth/restv1/userinfo` |\n| `OAUTH2_TOKEN_URL` | Static | `https://auth.develop.eoepca.org/oxauth/restv1/token` |\n| `OAUTH2_AUTHORIZE_URL` | Static | `https://auth.develop.eoepca.org/oxauth/restv1/authorize` |\n| `OAUTH_LOGOUT_REDIRECT_URL` | Static | `https://applicationhub.develop.eoepca.org` |\n| `OAUTH2_USERNAME_KEY` | Static | `user_name` - Key in userinfo response |\n| `STORAGE_CLASS` | Static | `managed-nfs-storage` - PVC storage class |\n| `RESOURCE_MANAGER_WORKSPACE_PREFIX` | Static | `develop-user` - Workspace naming prefix |\n| `JUPYTERHUB_CRYPT_KEY` | Secret | Reference to `application-hub-secrets` |\n| `OAUTH_CLIENT_ID` | Secret | Reference to `application-hub-secrets` |\n| `OAUTH_CLIENT_SECRET` | Secret | Reference to `application-hub-secrets` |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:35-63]()\n\n---\n\n## Deployment and Lifecycle Management\n\nThe Application Hub follows EOEPCA's GitOps deployment model. Configuration changes are committed to the repository, and Flux CD automatically reconciles the cluster state.\n\n**Deployment Workflow**\n\n```mermaid\ngraph LR\n    GitRepo[\"Git Repository\u003cbr/\u003eproc-application-hub.yaml\"]\n    FluxSource[\"Flux SourceController\u003cbr/\u003eMonitors repository\"]\n    FluxHelm[\"Flux HelmController\u003cbr/\u003eManages HelmRelease\"]\n    HelmChart[\"HelmChart\u003cbr/\u003eeoepca/application-hub:2.0.52\"]\n    K8sResources[\"Kubernetes Resources\u003cbr/\u003eDeployment, Service, Ingress\"]\n    \n    GitRepo --\u003e|\"Poll every 1m\"| FluxSource\n    FluxSource --\u003e|\"Detects change\"| FluxHelm\n    FluxHelm --\u003e|\"Fetch\"| HelmChart\n    HelmChart --\u003e|\"Render\"| FluxHelm\n    FluxHelm --\u003e|\"Apply\"| K8sResources\n```\n\nThe reconciliation interval is set to `1m0s`, meaning Flux checks for configuration changes and chart updates every minute and automatically applies them to the cluster.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:99]()"])</script><script>self.__next_f.push([1,"2e:T304b,"])</script><script>self.__next_f.push([1,"# Processor Development Environment (PDE)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [release-notes/release-0.3.md](release-notes/release-0.3.md)\n- [system/clusters/creodias/processing-and-chaining/namespace.yaml](system/clusters/creodias/processing-and-chaining/namespace.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Processor Development Environment (PDE) provides an integrated development platform for creating, testing, and debugging CWL-based application packages that execute on the ADES. The PDE bundles multiple development tools into a cohesive environment, including interactive notebooks, IDEs, local object storage, continuous integration, and containerization capabilities.\n\nFor information about deploying applications to ADES, see [ADES](#6.1). For creating CWL application packages, see [CWL Application Packages](#6.5). For the end-user application development experience via JupyterHub, see [Application Hub](#6.2).\n\n**Sources:** [release-notes/release-0.3.md:1-13](), [release-notes/release-0.3.md:42-48]()\n\n---\n\n## Overview\n\nThe PDE is a comprehensive development environment that supports the full lifecycle of Earth Observation processor development, from initial experimentation to production-ready CWL workflows. Unlike the Application Hub which provides user-facing JupyterHub access, the PDE is specifically focused on processor development with additional tooling for CI/CD, containerization, and local testing.\n\nThe PDE version 0.3 is documented in the proc-pde repository and provides a self-contained environment for developers working on application packages.\n\n**Sources:** [release-notes/release-0.3.md:251-255]()\n\n---\n\n## PDE Architecture\n\nThe following diagram illustrates the components of the Processor Development Environment and their relationships:\n\n```mermaid\ngraph TB\n    subgraph \"Processor Development Environment\"\n        JupyterLab[\"JupyterLab\u003cbr/\u003eInteractive Development\"]\n        Theia[\"Theia IDE\u003cbr/\u003eIntegrated Development Environment\"]\n        MinIO[\"MinIO\u003cbr/\u003eLocal S3 Object Storage\"]\n        Jenkins[\"Jenkins\u003cbr/\u003eContinuous Integration\"]\n        DockerInDocker[\"Docker-in-Docker\u003cbr/\u003eContainer Build Environment\"]\n        TestingTools[\"Testing Tools\u003cbr/\u003eApplication Package Validation\"]\n    end\n    \n    subgraph \"Developer Workflow\"\n        Dev[Developer]\n        Code[CWL Workflows\u003cbr/\u003eDocker Images]\n        TestData[Test EO Data]\n    end\n    \n    subgraph \"Deployment Target\"\n        ADES[\"ADES\u003cbr/\u003eApplication Deployment \u0026\u003cbr/\u003eExecution Service\"]\n        K8s[Kubernetes Cluster]\n    end\n    \n    Dev --\u003e|Develop Code| JupyterLab\n    Dev --\u003e|Edit Code| Theia\n    \n    JupyterLab --\u003e|Store Data| MinIO\n    Theia --\u003e|Store Data| MinIO\n    \n    TestData --\u003e|Upload| MinIO\n    MinIO --\u003e|Provide Test Inputs| JupyterLab\n    MinIO --\u003e|Provide Test Inputs| TestingTools\n    \n    Theia --\u003e|Commit Changes| Jenkins\n    Jenkins --\u003e|Build Containers| DockerInDocker\n    \n    Code --\u003e|Validate| TestingTools\n    TestingTools --\u003e|Use| MinIO\n    TestingTools --\u003e|Execute Locally| DockerInDocker\n    \n    Code --\u003e|Deploy| ADES\n    ADES --\u003e|Execute on| K8s\n    \n    JupyterLab -.-\u003e|Prototype| Code\n    Theia -.-\u003e|Develop| Code\n```\n\n**Diagram: PDE Component Architecture and Development Workflow**\n\nThis architecture shows how the PDE components work together to support the development lifecycle. Developers interact with JupyterLab for experimentation and Theia IDE for structured development. Code changes trigger Jenkins builds that use Docker-in-Docker for containerization. Testing tools validate application packages using MinIO as a local data store before deployment to the production ADES environment.\n\n**Sources:** [release-notes/release-0.3.md:42-48]()\n\n---\n\n## Core Components\n\n### JupyterLab Interface\n\nJupyterLab provides an interactive notebook environment for rapid prototyping and experimentation with Earth Observation data and algorithms. Developers can:\n\n- Interact with code and data through notebook interfaces\n- Prototype processing algorithms using Python, R, or other kernels\n- Visualize intermediate results and test outputs\n- Access MinIO storage for reading test data and writing outputs\n\nThe JupyterLab component enables iterative development with immediate feedback, making it ideal for the initial stages of processor development.\n\n**Sources:** [release-notes/release-0.3.md:43]()\n\n### Theia IDE\n\nTheia provides a full-featured integrated development environment similar to Visual Studio Code. The IDE supports:\n\n- Syntax highlighting and code completion for CWL and other languages\n- Integrated file explorer and terminal access\n- Git integration for version control\n- Debugging capabilities for application code\n- Extension support for additional language and tool support\n\nDevelopers use Theia for structured development of CWL application packages, Dockerfiles, and supporting scripts after initial prototyping in JupyterLab.\n\n**Sources:** [release-notes/release-0.3.md:44]()\n\n### MinIO S3 Object Storage\n\nMinIO provides a local S3-compatible object storage system within the PDE, enabling:\n\n- Storage of test EO data products\n- Collection of processing results during development\n- Simulation of S3-based stage-in/stage-out patterns used by ADES\n- Testing of S3 API integration without external dependencies\n\nThe MinIO instance allows developers to test their application packages with realistic storage patterns before deployment to production ADES environments that use CloudFerro eodata or workspace-specific S3 buckets.\n\n**Sources:** [release-notes/release-0.3.md:45]()\n\n### Jenkins Continuous Integration\n\nJenkins provides automated build and test capabilities within the PDE:\n\n- Automated container image builds triggered by code commits\n- Continuous integration pipelines for application packages\n- Test execution and validation reporting\n- Integration with Docker-in-Docker for image building\n\nThe Jenkins instance enables developers to establish CI/CD practices early in the development process, ensuring code quality and facilitating collaboration among team members.\n\n**Sources:** [release-notes/release-0.3.md:46]()\n\n### Docker-in-Docker\n\nThe Docker-in-Docker component provides containerization capabilities with an Ubuntu host:\n\n- Building Docker images for application packages\n- Testing containerized workflows locally before ADES deployment\n- Simulating Kubernetes pod execution patterns\n- Debugging container runtime issues\n\nThis capability is essential for developing CWL application packages that rely on Docker containers for processor execution, as it allows developers to test container builds and runtime behavior in an environment similar to the production ADES/Calrissian execution environment.\n\n**Sources:** [release-notes/release-0.3.md:47]()\n\n### Application Package Testing Tools\n\nThe PDE includes specialized tools for validating CWL application packages:\n\n- CWL workflow validation and linting\n- Local execution of CWL workflows using cwltool or similar engines\n- STAC manifest generation and validation\n- Input/output parameter testing\n\nThese tools enable developers to verify their application packages meet ADES requirements before deployment, reducing deployment failures and iteration cycles.\n\n**Sources:** [release-notes/release-0.3.md:48]()\n\n---\n\n## Development Workflow\n\nThe following sequence diagram illustrates a typical development workflow using the PDE:\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant JL as JupyterLab\n    participant Theia as Theia IDE\n    participant MinIO as MinIO S3\n    participant Jenkins as Jenkins CI\n    participant DinD as Docker-in-Docker\n    participant Tools as Testing Tools\n    participant ADES as ADES Service\n    \n    Dev-\u003e\u003eMinIO: Upload test EO data\n    MinIO--\u003e\u003eDev: Data available\n    \n    Dev-\u003e\u003eJL: Prototype algorithm in notebook\n    JL-\u003e\u003eMinIO: Read test data\n    MinIO--\u003e\u003eJL: Return data\n    JL-\u003e\u003eMinIO: Write intermediate results\n    Dev-\u003e\u003eJL: Validate approach\n    \n    Dev-\u003e\u003eTheia: Create CWL workflow\n    Dev-\u003e\u003eTheia: Create Dockerfile\n    Dev-\u003e\u003eTheia: Commit to Git\n    \n    Theia-\u003e\u003eJenkins: Trigger CI build\n    Jenkins-\u003e\u003eDinD: Build Docker image\n    DinD--\u003e\u003eJenkins: Image built\n    \n    Jenkins-\u003e\u003eTools: Run CWL validation\n    Tools-\u003e\u003eTools: Validate workflow syntax\n    Tools-\u003e\u003eDinD: Execute workflow locally\n    DinD-\u003e\u003eMinIO: Read test inputs\n    MinIO--\u003e\u003eDinD: Return data\n    DinD-\u003e\u003eMinIO: Write outputs\n    Tools--\u003e\u003eJenkins: Validation results\n    Jenkins--\u003e\u003eDev: Build status\n    \n    alt Tests Pass\n        Dev-\u003e\u003eADES: Deploy application package\n        ADES--\u003e\u003eDev: Deployment successful\n        Dev-\u003e\u003eADES: Execute test job\n        ADES--\u003e\u003eDev: Job results\n    else Tests Fail\n        Dev-\u003e\u003eTheia: Fix issues\n        Note over Dev,Jenkins: Iterate until tests pass\n    end\n```\n\n**Diagram: PDE Development and Testing Workflow**\n\nThis workflow demonstrates the iterative development process:\n1. Upload test data to MinIO\n2. Prototype algorithms in JupyterLab\n3. Formalize as CWL workflows in Theia IDE\n4. Commit changes to trigger Jenkins CI\n5. Build containers with Docker-in-Docker\n6. Validate with testing tools\n7. Deploy to ADES when tests pass\n\n**Sources:** [release-notes/release-0.3.md:42-48]()\n\n---\n\n## Integration with ADES\n\nThe PDE is designed to produce application packages compatible with the ADES execution environment. Key integration points include:\n\n| PDE Component | ADES Integration |\n|--------------|------------------|\n| CWL Workflows | Must conform to ADES/Calrissian CWL execution patterns |\n| Docker Images | Tagged and pushed to registries accessible by ADES |\n| STAC Manifests | Generated during testing to match ADES stage-in/stage-out formats |\n| S3 Storage | MinIO simulates CloudFerro eodata and workspace bucket patterns |\n| Testing Tools | Validate compliance with OGC API Processes requirements |\n\nDevelopers use the PDE to ensure their application packages will execute correctly on ADES with Calrissian as the CWL engine. The local testing environment in the PDE mirrors the production ADES environment, including:\n\n- CWL execution patterns with container isolation\n- S3-based data access for inputs and outputs\n- STAC-based input/output catalogs\n- Kubernetes resource constraints\n\n**Sources:** [release-notes/release-0.3.md:30-41](), [release-notes/release-0.3.md:222-249]()\n\n---\n\n## Deployment Context\n\nThe PDE operates within the `proc` namespace of the Kubernetes cluster, alongside other processing and chaining components like ADES and the Application Hub. While the Application Hub provides end-user access to Jupyter notebooks for interactive analysis, the PDE is a more comprehensive development environment specifically designed for building production-ready application packages.\n\nThe PDE version 0.3 includes all necessary tools and dependencies to develop, build, test, and deploy CWL-based processors without requiring additional external tools or services.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/namespace.yaml:1-5](), [release-notes/release-0.3.md:251-255]()\n\n---\n\n## Component Versions (Release 0.3)\n\n| Component | Version | Notes |\n|-----------|---------|-------|\n| PDE | 0.3 | Complete development environment |\n| JupyterLab | Bundled | Interactive notebook interface |\n| Theia IDE | Bundled | Full IDE capabilities |\n| MinIO | Bundled | Local S3-compatible storage |\n| Jenkins | Bundled | CI/CD automation |\n| Docker-in-Docker | Ubuntu host | Container build environment |\n\nThe PDE is distributed as a cohesive package with all components pre-configured for processor development workflows.\n\n**Sources:** [release-notes/release-0.3.md:251-255]()\n\n---\n\n## Summary\n\nThe Processor Development Environment provides a comprehensive, self-contained platform for developing CWL-based Earth Observation processors. By bundling JupyterLab, Theia IDE, MinIO storage, Jenkins CI, Docker-in-Docker, and specialized testing tools, the PDE enables developers to prototype, build, test, and deploy application packages efficiently. The environment simulates production ADES execution patterns, reducing deployment failures and accelerating the development cycle for Earth Observation processing applications.\n\n**Sources:** [release-notes/release-0.3.md:42-48](), [release-notes/release-0.3.md:251-255]()"])</script><script>self.__next_f.push([1,"2f:T54c7,"])</script><script>self.__next_f.push([1,"# Resource Guards and Access Control\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml](system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-ades.yaml](system/clusters/creodias/processing-and-chaining/proc-ades.yaml)\n- [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml](system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml](system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml)\n- [system/clusters/creodias/resource-management/ss-harbor.yaml](system/clusters/creodias/resource-management/ss-harbor.yaml)\n- [system/clusters/creodias/system/test/hr-dummy-service-guard.yaml](system/clusters/creodias/system/test/hr-dummy-service-guard.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the resource guard deployment pattern used to protect services in the EOEPCA platform with policy-based access control. Resource guards act as Policy Enforcement Points (PEPs) that intercept requests to backend services, validate user authentication via UMA (User-Managed Access), and enforce authorization policies through the Policy Decision Point (PDP). This page focuses on how resource guards are configured, deployed, and integrated with protected services.\n\nFor information about the overall authentication and authorization flow, see [UMA Authentication Flow](#4.4). For details on policy management and the PDP engine, see [Policy Enforcement (PEP/PDP)](#4.3).\n\n---\n\n## Overview\n\nA **resource guard** is a composite Kubernetes deployment that sits in front of a protected backend service and enforces access control policies. Each resource guard consists of two main components deployed together via the `resource-guard` Helm chart:\n\n| Component | Function |\n|-----------|----------|\n| `pep-engine` | Registers protected resources with the Authorization Server (AS), maintains policy state, and communicates with the PDP for policy decisions |\n| `uma-user-agent` | Handles UMA authentication flow, validates tokens, and integrates with nginx ingress to route authenticated requests to backend services |\n\nResource guards are deployed throughout the platform to protect:\n- ADES service for processing workflows\n- Workspace API for workspace management\n- Resource Catalogue and Data Access services (both global and per-workspace)\n- Custom application services requiring access control\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:1-92]()\n\n---\n\n## Resource Guard Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Resource Guard Deployment\"\n        UMA[\"uma-user-agent\u003cbr/\u003ePod\"]\n        PEP[\"pep-engine\u003cbr/\u003ePod\"]\n        PVC[\"PVC\u003cbr/\u003e(policy state)\"]\n    end\n    \n    subgraph \"Nginx Ingress\"\n        Ingress[\"Ingress Resource\u003cbr/\u003ehost: ades.develop.eoepca.org\"]\n    end\n    \n    subgraph \"Backend Service\"\n        ADES[\"proc-ades\u003cbr/\u003eService:80\"]\n    end\n    \n    subgraph \"Identity Services\"\n        AS[\"Authorization Server\u003cbr/\u003eauth.develop.eoepca.org\"]\n        PDP[\"Policy Decision Point\u003cbr/\u003eauth.develop.eoepca.org/pdp\"]\n    end\n    \n    Client[\"External Client\"] --\u003e|\"1. HTTPS Request\"| Ingress\n    Ingress --\u003e|\"2. Forward\"| UMA\n    UMA --\u003e|\"3. Validate Token\"| AS\n    UMA --\u003e|\"4. Check Policy\"| PDP\n    PDP --\u003e|\"5. Policy Decision\"| UMA\n    UMA --\u003e|\"6. If Permitted\"| ADES\n    ADES --\u003e|\"7. Response\"| UMA\n    UMA --\u003e|\"8. Response\"| Client\n    \n    PEP --\u003e|\"Register Resources\"| AS\n    PEP --\u003e|\"Store State\"| PVC\n    PEP -.-\u003e|\"Policy Sync\"| PDP\n```\n\n**Diagram: Resource Guard Component Architecture**\n\nThe `pep-engine` runs as a separate pod that registers protected resources during startup and maintains policy state in a PersistentVolumeClaim. The `uma-user-agent` handles incoming requests, performs UMA authentication, validates policies, and proxies permitted requests to the backend service.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:28-83](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:28-86]()\n\n---\n\n## Deployment Pattern\n\nResource guards are deployed as HelmRelease resources that install the `resource-guard` chart from the EOEPCA Helm repository. A typical deployment structure:\n\n```mermaid\ngraph LR\n    HelmRepo[\"HelmRepository\u003cbr/\u003ename: eoepca\u003cbr/\u003enamespace: common\"]\n    HR[\"HelmRelease\u003cbr/\u003eproc-ades-guard\u003cbr/\u003enamespace: proc\"]\n    Chart[\"Chart: resource-guard\u003cbr/\u003eversion: 1.3.2\"]\n    \n    HR --\u003e|\"references\"| HelmRepo\n    HR --\u003e|\"deploys\"| Chart\n    Chart --\u003e|\"creates\"| PEPPod[\"pep-engine Pod\"]\n    Chart --\u003e|\"creates\"| UMAPod[\"uma-user-agent Pod\"]\n    Chart --\u003e|\"creates\"| Svc[\"Service\"]\n    Chart --\u003e|\"creates\"| Ingress[\"Ingress\"]\n```\n\n**Diagram: Resource Guard Deployment Resources**\n\nThe HelmRelease specifies:\n- `global.context`: The service context name (used in hostnames and resource URIs)\n- `global.domain`: The base domain for ingress\n- `pep-engine` configuration: AS/PDP endpoints, default resources, volume claims\n- `uma-user-agent` configuration: Nginx integration, backend routing, OIDC credentials\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-14](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:15-24]()\n\n---\n\n## PEP Engine Configuration\n\nThe `pep-engine` component is configured to register protected resources and communicate with identity services:\n\n### Basic Configuration\n\n```yaml\npep-engine:\n  configMap:\n    asHostname: auth\n    pdpHostname: auth\n  volumeClaim:\n    name: eoepca-proc-pvc\n    create: false\n  nginxIntegration:\n    enabled: false\n```\n\n**Configuration Details:**\n\n| Parameter | Purpose |\n|-----------|---------|\n| `asHostname` | Hostname of the Authorization Server (Keycloak) for UMA resource registration |\n| `pdpHostname` | Hostname of the Policy Decision Point for policy evaluation |\n| `volumeClaim.name` | PVC name for persisting registered resource state |\n| `nginxIntegration.enabled` | Whether to deploy nginx integration (typically `false` when using separate `uma-user-agent`) |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:28-57]()\n\n### Resource Registration\n\nProtected resources are pre-registered during PEP engine startup. Two types of resource definitions exist:\n\n1. **defaultResources**: Standard protected paths\n2. **customDefaultResources**: User-specific or custom-scoped resources\n\nExample from ADES guard:\n\n```yaml\ncustomDefaultResources:\n  - name: \"ADES Service for user 'eric'\"\n    description: \"Protected Access for eric to his space in the ADES\"\n    resource_uri: \"/eric\"\n    scopes: []\n    default_owner: \"fad43ef3-23ef-48b0-86f0-1cf29d97908e\"\n  - name: \"ADES Service for user 'bob'\"\n    description: \"Protected Access for bob to his space in the ADES\"\n    resource_uri: \"/bob\"\n    scopes: []\n    default_owner: \"f0a19e32-5651-404e-8acd-128c2c284300\"\n```\n\nEach resource definition specifies:\n- `name`: Human-readable resource identifier\n- `description`: Resource purpose\n- `resource_uri`: Path pattern to protect (e.g., `/eric`, `/bob`)\n- `scopes`: Optional scopes for fine-grained permissions (empty means default permissions)\n- `default_owner`: User ID (from Identity Service) who owns this resource\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:34-49](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:35-53]()\n\n---\n\n## UMA User Agent Configuration\n\nThe `uma-user-agent` component handles the UMA authentication flow and routes requests to backend services:\n\n### Nginx Integration\n\n```yaml\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: ades\n        paths:\n          - path: /(.*)\n            service:\n              name: proc-ades\n              port: 80\n    annotations:\n      nginx.ingress.kubernetes.io/proxy-read-timeout: \"600\"\n      nginx.ingress.kubernetes.io/enable-cors: \"true\"\n      nginx.ingress.kubernetes.io/rewrite-target: /$1\n```\n\n**Routing Configuration:**\n\n| Field | Purpose |\n|-------|---------|\n| `hosts[].host` | Ingress hostname (combined with `global.domain` to form FQDN) |\n| `paths[].path` | Request path pattern with capture group for rewriting |\n| `paths[].service.name` | Backend Kubernetes service name |\n| `paths[].service.port` | Backend service port |\n| `annotations` | Nginx ingress annotations for timeout, CORS, path rewriting |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:61-77]()\n\n### Authentication Settings\n\n```yaml\numa-user-agent:\n  client:\n    credentialsSecretName: \"proc-uma-user-agent\"\n  logging:\n    level: \"info\"\n  unauthorizedResponse: 'Bearer realm=\"https://auth.develop.eoepca.org/oxauth/auth/passport/passportlogin.htm\"'\n  openAccess: false\n```\n\n**Authentication Parameters:**\n\n| Parameter | Purpose |\n|-----------|---------|\n| `credentialsSecretName` | Kubernetes Secret containing OIDC client ID and secret for token validation |\n| `unauthorizedResponse` | WWW-Authenticate header returned on 401 (directs user to login) |\n| `openAccess` | If `true`, allows unauthenticated access (used for public services) |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:78-83](), [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:89-94]()\n\n---\n\n## Request Flow Through Resource Guard\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Ingress as \"Nginx Ingress\u003cbr/\u003eades.develop.eoepca.org\"\n    participant UMA as \"uma-user-agent\"\n    participant AS as \"Authorization Server\u003cbr/\u003eauth.develop.eoepca.org\"\n    participant PDP as \"Policy Decision Point\"\n    participant Backend as \"proc-ades\u003cbr/\u003eService\"\n    \n    Client-\u003e\u003eIngress: GET /eric/jobs\n    Ingress-\u003e\u003eUMA: Forward request\n    \n    alt No Authorization Header\n        UMA-\u003e\u003eAS: Request UMA ticket for /eric\n        AS--\u003e\u003eUMA: Return ticket\n        UMA--\u003e\u003eClient: 401 Unauthorized\u003cbr/\u003eWWW-Authenticate: UMA ticket=xyz\n        Client-\u003e\u003eAS: Authenticate with credentials\n        AS--\u003e\u003eClient: ID Token\n        Client-\u003e\u003eIngress: Retry with Authorization: Bearer \u003cid_token\u003e\n        Ingress-\u003e\u003eUMA: Forward with token\n    end\n    \n    UMA-\u003e\u003eAS: Exchange ticket + ID token for RPT\n    AS-\u003e\u003eAS: Validate ID token\n    AS--\u003e\u003eUMA: Return RPT (access token)\n    \n    UMA-\u003e\u003ePDP: Check policy(user=eric, resource=/eric, action=GET)\n    PDP-\u003e\u003eAS: Validate RPT\n    AS--\u003e\u003ePDP: RPT valid\n    PDP-\u003e\u003ePDP: Evaluate policies\n    PDP--\u003e\u003eUMA: Decision: PERMIT\n    \n    UMA-\u003e\u003eBackend: GET /eric/jobs\n    Backend--\u003e\u003eUMA: Response data\n    UMA--\u003e\u003eClient: 200 OK + data\n```\n\n**Diagram: UMA Authentication and Authorization Flow**\n\n1. Client attempts to access protected resource without token\n2. `uma-user-agent` requests UMA ticket from Authorization Server\n3. Client receives 401 with WWW-Authenticate header containing ticket\n4. Client authenticates with Authorization Server, receives ID token\n5. Client retries request with ID token\n6. `uma-user-agent` exchanges ticket + ID token for RPT (Requesting Party Token)\n7. `uma-user-agent` validates RPT with PDP, checking if user can access resource\n8. If policy permits, request is forwarded to backend service\n9. Response is returned to client\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:61-83]()\n\n---\n\n## Multi-Backend Routing\n\nSome resource guards protect multiple backend services with different routing rules. The combined resource management guard demonstrates this pattern:\n\n```yaml\numa-user-agent:\n  nginxIntegration:\n    enabled: true\n    hosts:\n      - host: resource-catalogue\n        paths:\n          - path: /(.*)\n            service:\n              name: resource-catalogue-service\n              port: 80\n      - host: data-access\n        paths:\n          - path: /(ows.*)\n            service:\n              name: data-access-renderer\n              port: 80\n          - path: /(opensearch.*)\n            service:\n              name: data-access-renderer\n              port: 80\n          - path: /cache/(.*)\n            service:\n              name: data-access-cache\n              port: 80\n          - path: /(.*)\n            service:\n              name: data-access-client\n              port: 80\n```\n\n**Routing Logic:**\n\n- Multiple hosts can be defined, each with its own subdomain\n- Paths are evaluated in order, with more specific patterns first\n- Capture groups `(.*)` are used with `rewrite-target: /$1` to strip prefix\n- Different paths can route to different backend services\n\n**Sources:** [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:50-88]()\n\n---\n\n## Workspace-Specific Resource Guards\n\nFor multi-tenant workspaces, resource guards are dynamically instantiated from templates. The Workspace API creates per-user namespaces with dedicated resource guards.\n\n### Template Structure\n\n```yaml\napiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\n  name: resource-guard\nspec:\n  values:\n    global:\n      context: \"{{ workspace_name }}\"\n      domain: develop.eoepca.org\n      default_owner: {{ default_owner }}\n    pep-engine:\n      volumeClaim:\n        name: \"{{ workspace_name }}-pep-pvc\"\n        create: \"true\"\n      defaultResources:\n        - name: \"Workspace {{ workspace_name }} Root\"\n          description: \"Root URL of a users workspace\"\n          resource_uri: \"/\"\n          scopes: []\n          default_owner: {{ default_owner }}\n```\n\n**Template Variables:**\n\n| Variable | Substituted By | Example Value |\n|----------|----------------|---------------|\n| `{{ workspace_name }}` | Workspace API | `eric-workspace` |\n| `{{ default_owner }}` | Workspace API | `fad43ef3-23ef-48b0-86f0-1cf29d97908e` |\n\nWhen a user requests a workspace, the Workspace API:\n1. Creates a dedicated Kubernetes namespace (e.g., `eric-workspace`)\n2. Substitutes template variables with user-specific values\n3. Creates a HelmRelease from the template in the user's namespace\n4. The resource guard protects all services in that workspace\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:1-98]()\n\n### Workspace Guard Routing\n\nWorkspace resource guards route to multiple services within the workspace namespace:\n\n```mermaid\ngraph TB\n    Client[\"Client Request\"]\n    Ingress[\"Ingress\u003cbr/\u003eresource-catalogue.eric-workspace\u003cbr/\u003edata-access.eric-workspace\"]\n    UMA[\"uma-user-agent\u003cbr/\u003ein eric-workspace ns\"]\n    \n    subgraph \"eric-workspace namespace\"\n        RC[\"resource-catalogue-service:80\"]\n        Renderer[\"vs-renderer:80\"]\n        Cache[\"vs-cache:80\"]\n        Client2[\"vs-client:80\"]\n    end\n    \n    Client --\u003e|\"HTTPS\"| Ingress\n    Ingress --\u003e UMA\n    UMA --\u003e|\"/(.*)\"| RC\n    UMA --\u003e|\"/(ows.*)\"| Renderer\n    UMA --\u003e|\"/(opensearch.*)\"| Renderer\n    UMA --\u003e|\"/cache/(.*)\"| Cache\n    UMA --\u003e|\"/(.*) fallback\"| Client2\n```\n\n**Diagram: Workspace Resource Guard Routing**\n\nThe workspace guard creates two ingress hosts:\n- `resource-catalogue.{{ workspace_name }}.develop.eoepca.org`\n- `data-access.{{ workspace_name }}.develop.eoepca.org`\n\nEach host routes to different services based on path patterns, with all requests protected by the same PEP/UMA authentication.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:49-98]()\n\n---\n\n## ADES Integration with Resource Guards\n\nThe ADES service can optionally use a resource guard for access control. The integration is configured in the ADES HelmRelease:\n\n```yaml\nwps:\n  pepBaseUrl: \"http://ades-pep:5576\"\n  usePep: \"false\"\n```\n\nWhen `usePep` is set to `\"true\"`, the ADES communicates with the PEP engine to enforce access control on workflow submissions and job management operations. However, in the current deployment, ADES uses an external resource guard via nginx ingress rather than direct PEP integration.\n\nThe ADES guard pre-registers user-specific paths:\n\n| User | Resource URI | Owner ID |\n|------|--------------|----------|\n| eric | `/eric` | `fad43ef3-23ef-48b0-86f0-1cf29d97908e` |\n| bob | `/bob` | `f0a19e32-5651-404e-8acd-128c2c284300` |\n| alice | `/alice` | `5fa1b608-2b28-4686-b571-46c79ec75b78` |\n\nRequests to `https://ades.develop.eoepca.org/eric/...` are only permitted for user `eric`, enforced by the resource guard checking the user's identity against the resource owner.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades.yaml:127-129](), [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:34-49]()\n\n---\n\n## Public vs. Protected Access\n\nResource guards support both protected and public access modes:\n\n### Protected Access (Default)\n\n```yaml\numa-user-agent:\n  openAccess: false\n  unauthorizedResponse: 'Bearer realm=\"https://portal.develop.eoepca.org/oidc/authenticate/\"'\n```\n\n- All requests require valid authentication\n- Unauthenticated requests receive 401 with WWW-Authenticate header\n- User must complete UMA flow to obtain access token\n\n### Public Access\n\n```yaml\numa-user-agent:\n  openAccess: true\n```\n\n- Requests are allowed without authentication\n- Still supports authenticated requests for user-specific functionality\n- Used for public services like global Resource Catalogue read access\n\nAdditionally, individual resources can be marked as public using scopes:\n\n```yaml\ncustomDefaultResources:\n  - name: \"Workspace API Swagger Docs\"\n    description: \"Public access to workspace API swagger docs\"\n    resource_uri: \"/docs\"\n    scopes:\n      - \"public_access\"\n    default_owner: \"0000000000000\"\n```\n\nResources with the `public_access` scope bypass authentication checks.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:93-94](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:42-53]()\n\n---\n\n## Resource Guard Deployment Examples\n\n### Example 1: ADES Service Protection\n\nFull deployment protecting the ADES service with user-specific paths:\n\n**HelmRelease:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:1-90]()\n\nKey characteristics:\n- Context: `ades`\n- Namespace: `proc`\n- Pre-registered paths: `/eric`, `/bob`, `/alice`\n- Backend service: `proc-ades:80`\n- OIDC secret: `proc-uma-user-agent`\n\n### Example 2: Workspace API Protection\n\nProtecting the Workspace API with base path restricted and public documentation:\n\n**HelmRelease:** [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:1-92]()\n\nKey characteristics:\n- Context: `workspace-api`\n- Namespace: `rm`\n- Root path `/` restricted to operators only\n- Public paths: `/docs`, `/openapi.json`\n- Backend service: `workspace-api:8080`\n- Annotations: Extended proxy timeout (600s) for long-running operations\n\n### Example 3: Combined Resource Management\n\nSingle guard protecting multiple resource management services:\n\n**HelmRelease:** [system/clusters/creodias/resource-management/hr-combined-rm-guard.yaml:1-101]()\n\nKey characteristics:\n- Context: `combined-rm`\n- Multiple hosts: `resource-catalogue`, `data-access`\n- Open access mode enabled for public read operations\n- Complex path routing to different backend services\n\n### Example 4: Workspace-Specific Template\n\nTemplate instantiated by Workspace API for each user workspace:\n\n**Template:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-guard.yaml:1-99]()\n\nKey characteristics:\n- Parameterized with `{{ workspace_name }}` and `{{ default_owner }}`\n- Creates dedicated PVC for policy state\n- Routes to workspace-local services\n- Enforces ownership based on workspace creator\n\n**Sources:** All files referenced above\n\n---\n\n## Configuration Summary\n\n### Essential Global Parameters\n\n| Parameter | Purpose | Example |\n|-----------|---------|---------|\n| `global.context` | Service identifier used in hostnames | `ades`, `workspace-api` |\n| `global.domain` | Base domain for ingress | `develop.eoepca.org` |\n| `global.nginxIp` | IP address for DNS/LB | `185.52.192.231` |\n| `global.certManager.clusterIssuer` | TLS certificate issuer | `letsencrypt` |\n\n### Essential PEP Engine Parameters\n\n| Parameter | Purpose | Example |\n|-----------|---------|---------|\n| `pep-engine.configMap.asHostname` | Authorization Server hostname | `auth` |\n| `pep-engine.configMap.pdpHostname` | Policy Decision Point hostname | `auth` |\n| `pep-engine.volumeClaim.name` | PVC for policy persistence | `eoepca-proc-pvc` |\n| `pep-engine.defaultResources` | Protected resource definitions | List of resource specs |\n\n### Essential UMA User Agent Parameters\n\n| Parameter | Purpose | Example |\n|-----------|---------|---------|\n| `uma-user-agent.nginxIntegration.enabled` | Enable nginx ingress | `true` |\n| `uma-user-agent.nginxIntegration.hosts` | Backend routing configuration | Host/path/service mappings |\n| `uma-user-agent.client.credentialsSecretName` | OIDC client credentials | `proc-uma-user-agent` |\n| `uma-user-agent.unauthorizedResponse` | 401 WWW-Authenticate header | Bearer realm URL |\n| `uma-user-agent.openAccess` | Allow unauthenticated access | `false` |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-ades-guard.yaml:15-86](), [system/clusters/creodias/resource-management/hr-workspace-api-guard.yaml:15-88]()"])</script><script>self.__next_f.push([1,"30:T463a,"])</script><script>self.__next_f.push([1,"# CWL Application Packages\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [release-notes/release-0.3.md](release-notes/release-0.3.md)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page describes the Common Workflow Language (CWL) application packages used in the EOEPCA Processing and Chaining building block. CWL application packages define containerized Earth Observation processing workflows that can be deployed and executed on the ADES (Application Deployment and Execution Service). For information about the ADES service itself, including its OGC API Processes interfaces and job execution capabilities, see [ADES](#6.1). For development environment setup, see [Processor Development Environment](#6.3).\n\nA CWL application package consists of:\n- A CWL workflow definition (`.cwl` file) describing the processing steps\n- Docker container references for execution environments\n- Input/output specifications compatible with EO data formats\n- Resource requirements and execution patterns\n\nSources: [release-notes/release-0.3.md:1-95]()\n\n## CWL Document Structure\n\nCWL application packages in EOEPCA use the `$graph` notation to define both CommandLineTools and Workflows within a single document. The CWL version `v1.0` is used throughout the platform.\n\n### Graph-Based Document Format\n\n```mermaid\ngraph TB\n    subgraph \"CWL Document Structure\"\n        Doc[\"CWL Document\u003cbr/\u003ecwlVersion: v1.0\"]\n        Graph[\"$graph array\"]\n        CLT[\"CommandLineTool\u003cbr/\u003eid: clt\u003cbr/\u003eclass: CommandLineTool\"]\n        WF[\"Workflow\u003cbr/\u003eid: workflow-name\u003cbr/\u003eclass: Workflow\"]\n    end\n    \n    subgraph \"CommandLineTool Components\"\n        BaseCmd[\"baseCommand\"]\n        Inputs[\"inputs:\u003cbr/\u003e- input_reference\u003cbr/\u003e- threshold\u003cbr/\u003e- aoi\"]\n        Outputs[\"outputs:\u003cbr/\u003e- results\"]\n        Docker[\"DockerRequirement\u003cbr/\u003edockerPull\"]\n        Env[\"EnvVarRequirement\u003cbr/\u003eenvDef\"]\n        Res[\"ResourceRequirement\"]\n    end\n    \n    subgraph \"Workflow Components\"\n        WFInputs[\"inputs:\u003cbr/\u003etype: Directory[]\"]\n        Steps[\"steps:\u003cbr/\u003estep_1\"]\n        WFOutputs[\"outputs:\u003cbr/\u003eoutputSource\"]\n        Scatter[\"ScatterFeatureRequirement\"]\n    end\n    \n    Doc --\u003e Graph\n    Graph --\u003e CLT\n    Graph --\u003e WF\n    CLT --\u003e BaseCmd\n    CLT --\u003e Inputs\n    CLT --\u003e Outputs\n    CLT --\u003e Docker\n    CLT --\u003e Env\n    CLT --\u003e Res\n    WF --\u003e WFInputs\n    WF --\u003e Steps\n    WF --\u003e WFOutputs\n    WF --\u003e Scatter\n    Steps -.-\u003e|\"run: '#clt'\"| CLT\n```\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:1-79]()\n\n### CommandLineTool Definition\n\nThe CommandLineTool specifies the executable container and how inputs are bound to command-line arguments:\n\n| Component | Purpose | Example Value |\n|-----------|---------|---------------|\n| `class` | CWL class type | `CommandLineTool` |\n| `id` | Tool identifier | `clt` |\n| `baseCommand` | Executable name | `nhi` |\n| `inputs` | Input parameters | `input_reference`, `threshold`, `aoi` |\n| `outputs` | Output specifications | `results` (type: Directory) |\n\n**Input Binding Example:**\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:8-23]()\n\nInputs are bound to command-line positions and prefixes:\n- `input_reference`  `--input_reference` at position 1 (type: Directory)\n- `threshold`  `--threshold` at position 2 (type: string)\n- `aoi`  `--aoi` at position 3 (type: string?, optional)\n\n**Output Binding:**\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:24-28]()\n\nThe `glob` pattern `.` captures the entire working directory as a Directory output.\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:2-35](), [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml:18-53]()\n\n### Workflow Definition\n\nThe Workflow class orchestrates one or more CommandLineTools and defines the overall processing pipeline:\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:36-72]()\n\n**Key workflow components:**\n\n- **inputs**: Accept arrays of Directories for batch processing (`Directory[]`)\n- **steps**: Define execution steps with input/output mappings\n- **outputs**: Specify what data is returned from the workflow\n- **requirements**: Declare required CWL features (e.g., `ScatterFeatureRequirement`)\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:36-72]()\n\n## Docker Container Integration\n\n### DockerRequirement Hint\n\nCWL application packages specify Docker containers using the `DockerRequirement` hint in the CommandLineTool:\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:4-6]()\n\nThe `dockerPull` field references the container image that will be executed by the Calrissian CWL engine within Kubernetes.\n\n**Container Image Naming Pattern:**\n\n```\nregistry.hub.docker.com/[organization]/[application]:[tag]\n```\n\nExample: `registry.hub.docker.com/eoepcaci/nhi:dev0.0.3`\n\n### Environment Configuration\n\nThe `EnvVarRequirement` sets environment variables in the container execution context:\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:30-32]()\n\nThis ensures the conda environment and other executables are available in the container's PATH.\n\n### Resource Requirements\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:33]()\n\nThe `ResourceRequirement` can specify CPU, memory, and storage constraints for the container execution (currently empty in this example but can be populated with specific values).\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:4-33](), [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml:21-50]()\n\n## Deployment Methods\n\n### Direct CWL Deployment\n\nApplications can be deployed by providing the CWL document directly via URL reference:\n\n[test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json:1-28]()\n\n**Deployment Request Structure:**\n\n```mermaid\ngraph LR\n    DeployReq[\"Deploy Request\u003cbr/\u003ePOST /processes\"]\n    Input[\"inputs array\"]\n    AppPkg[\"applicationPackage input\"]\n    Format[\"format:\u003cbr/\u003emimeType: application/cwl\"]\n    Value[\"value:\u003cbr/\u003ehref: URL\"]\n    Output[\"outputs array\"]\n    DeployResult[\"deployResult output\"]\n    \n    DeployReq --\u003e Input\n    Input --\u003e AppPkg\n    AppPkg --\u003e Format\n    AppPkg --\u003e Value\n    DeployReq --\u003e Output\n    Output --\u003e DeployResult\n```\n\nThe `applicationPackage` input references a raw CWL file hosted externally (e.g., on GitHub).\n\nSources: [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json:1-28]()\n\n### ATOM-Wrapped Deployment\n\nApplications can also be deployed using ATOM XML format where the CWL is embedded within an `owc:offering` element:\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml:16-92]()\n\n**ATOM Deployment Request:**\n\n[test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json:1-36]()\n\nThe `input_reference` in this case points to an OpenSearch query URL that returns the ATOM-wrapped CWL:\n\n```\nhttps://resource-catalogue.develop.eoepca.org/?mode=opensearch\u0026service=CSW\u0026version=3.0.0\u0026request=GetRecords\u0026elementsetname=full\u0026resulttype=results\u0026typenames=csw:Record\u0026recordids=S2B_MSIL1C_20210402T095029_N0300_R079_T33SVB_20210402T121737.SAFE\n```\n\n**ATOM Structure:**\n\n| Element | Purpose |\n|---------|---------|\n| `\u003cfeed\u003e` | ATOM feed container |\n| `\u003centry\u003e` | Application entry |\n| `\u003cowc:offering code=\"http://www.opengis.net/eoc/applicationContext/cwl\"\u003e` | CWL offering |\n| `\u003cowc:content type=\"application/cwl\"\u003e` | Embedded CWL document |\n\nSources: [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json:1-36](), [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml:1-111]()\n\n## Input and Output Specifications\n\n### Input Types for EO Processing\n\nCWL application packages typically use the following input types:\n\n| Type | Purpose | Example |\n|------|---------|---------|\n| `Directory` | Single EO product (e.g., Sentinel-2 SAFE) | `input_reference: Directory` |\n| `Directory[]` | Multiple EO products for batch processing | `input_reference: Directory[]` |\n| `string` | Processing parameters | `threshold: string` |\n| `string?` | Optional parameters | `aoi: string?` |\n\n**Directory Inputs:**\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:40-43]()\n\nThe ADES stage-in process resolves OpenSearch catalogue references to actual S3 paths and mounts them as Directory inputs.\n\n### Output Specifications\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:53-59]()\n\nOutputs use the `outputSource` to link to the results from processing steps. The output type `{ items: Directory, type: array }` indicates multiple result directories.\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:40-59]()\n\n## Execution Patterns\n\n### Scatter Processing\n\nThe `ScatterFeatureRequirement` enables parallel processing of multiple inputs:\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:60-61]()\n\n**Scatter Configuration:**\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:63-72]()\n\n```mermaid\ngraph TB\n    subgraph \"Scatter Execution Pattern\"\n        Inputs[\"inputs:\u003cbr/\u003einput_reference: Directory[]\u003cbr/\u003e[Product1, Product2, Product3]\"]\n        Step[\"step_1\u003cbr/\u003escatter: input_reference\u003cbr/\u003escatterMethod: dotproduct\"]\n        \n        subgraph \"Parallel Execution\"\n            Pod1[\"Kubernetes Pod 1\u003cbr/\u003eProcess Product1\"]\n            Pod2[\"Kubernetes Pod 2\u003cbr/\u003eProcess Product2\"]\n            Pod3[\"Kubernetes Pod 3\u003cbr/\u003eProcess Product3\"]\n        end\n        \n        Results[\"outputs:\u003cbr/\u003eresults: Directory[]\u003cbr/\u003e[Result1, Result2, Result3]\"]\n    end\n    \n    Inputs --\u003e Step\n    Step --\u003e Pod1\n    Step --\u003e Pod2\n    Step --\u003e Pod3\n    Pod1 --\u003e Results\n    Pod2 --\u003e Results\n    Pod3 --\u003e Results\n```\n\n**Scatter Methods:**\n\n- `dotproduct`: Pairs elements from multiple scattered inputs\n- `nested_crossproduct`: Creates all combinations of scattered inputs\n- `flat_crossproduct`: Similar to nested but flattens the result\n\nThe Calrissian executor creates separate Kubernetes pods for each scattered item, enabling true parallel processing.\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:60-72]()\n\n## Stage-in and Stage-out with STAC\n\n### Stage-in Process\n\nWhen a job is executed, the ADES performs stage-in to resolve input references:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant ADES\n    participant ResourceCatalogue[\"Resource Catalogue\"]\n    participant S3[\"S3 Storage\u003cbr/\u003e(eodata)\"]\n    participant Calrissian\n    \n    User-\u003e\u003eADES: Execute Job with OpenSearch URL\n    ADES-\u003e\u003eResourceCatalogue: Query Product Metadata\n    ResourceCatalogue--\u003e\u003eADES: Return Product Location\n    ADES-\u003e\u003eADES: Create STAC Manifest\n    ADES-\u003e\u003eS3: Stage-in Data\n    ADES-\u003e\u003eCalrissian: Execute CWL with Directory Inputs\n    Calrissian-\u003e\u003eS3: Read Input Data\n    Calrissian-\u003e\u003eS3: Write Output Data\n    Calrissian--\u003e\u003eADES: Job Complete\n    ADES-\u003e\u003eADES: Create Output STAC Manifest\n    ADES--\u003e\u003eUser: Job Results with STAC\n```\n\n**Stage-in Components:**\n\n- **Input**: OpenSearch catalogue reference URL\n- **Process**: ADES queries Resource Catalogue for product locations\n- **STAC Manifest**: Generated to describe input data locations and metadata\n- **Mount**: S3 paths mounted as CWL Directory inputs\n\n### Stage-out Process\n\nProcessing results are staged-out to user workspace S3 buckets:\n\n**Stage-out Flow:**\n\n1. CWL outputs written to ephemeral pod storage\n2. ADES copies outputs to S3 workspace bucket\n3. STAC manifest generated describing output products\n4. Output STAC includes geospatial metadata, temporal coverage, and asset links\n\n**STAC Output Structure:**\n\n- **type**: FeatureCollection\n- **features**: Array of STAC items for each output product\n- **assets**: Links to S3 objects containing results\n- **properties**: Metadata including datetime, spatial extent, and custom properties\n\nSources: [release-notes/release-0.3.md:36-89]()\n\n## Integration with ADES Execution\n\n### Job Execution Flow\n\n```mermaid\ngraph TB\n    subgraph \"CWL Application Package Lifecycle\"\n        Deploy[\"1. Deploy CWL Package\u003cbr/\u003ePOST /processes\"]\n        ListProc[\"2. List Available Processes\u003cbr/\u003eGET /processes\"]\n        Execute[\"3. Execute Job\u003cbr/\u003ePOST /processes/{id}/execution\"]\n        Status[\"4. Monitor Job Status\u003cbr/\u003eGET /jobs/{jobId}\"]\n        Results[\"5. Retrieve Results\u003cbr/\u003eGET /jobs/{jobId}/results\"]\n    end\n    \n    subgraph \"ADES Backend Processing\"\n        ValidateCWL[\"Validate CWL Syntax\"]\n        RegisterProc[\"Register Process Definition\"]\n        CreateJob[\"Create Job Record\"]\n        StageIn[\"Stage-in via stars-t2\"]\n        ExecuteCWL[\"Execute via Calrissian\"]\n        K8sPods[\"Create Kubernetes Pods\"]\n        StageOut[\"Stage-out to S3\"]\n        GenerateSTAC[\"Generate STAC Manifest\"]\n    end\n    \n    Deploy --\u003e ValidateCWL\n    ValidateCWL --\u003e RegisterProc\n    RegisterProc --\u003e ListProc\n    Execute --\u003e CreateJob\n    CreateJob --\u003e StageIn\n    StageIn --\u003e ExecuteCWL\n    ExecuteCWL --\u003e K8sPods\n    K8sPods --\u003e StageOut\n    StageOut --\u003e GenerateSTAC\n    GenerateSTAC --\u003e Results\n    Status -.-\u003e CreateJob\n```\n\n**Container Images Used:**\n\n| Component | Image | Purpose |\n|-----------|-------|---------|\n| ADES Core | `eoepca/proc-ades:dev0.3.1` | OGC API Processes implementation |\n| Stage-in | `terradue/stars-t2:0.6.17.0` | Data staging from catalogue/S3 |\n| Stage-out | `terradue/stars-t2:0.6.17.0` | Result staging to S3 with STAC |\n| CWL Executor | Calrissian (embedded) | Kubernetes-native CWL runner |\n\nSources: [release-notes/release-0.3.md:222-249]()\n\n## Example Application Packages\n\n### Normalized Hotspot Indices (NHI)\n\nThe NHI application demonstrates a complete CWL package structure:\n\n**Application Metadata:**\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:73-78]()\n\n- **cwlVersion**: v1.0\n- **s:softwareVersion**: 0.0.3 (using schema.org namespace)\n- **schemas**: References schema.org for semantic metadata\n\n**Processing Logic:**\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:2-7]()\n\nThe `baseCommand: nhi` executes the main processing script within the container, with inputs bound as command-line arguments.\n\n### S-Expression Application\n\nThe s-expression application shows an alternative processing pattern:\n\n[test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml:8-95]()\n\n**Differences from NHI:**\n\n- **Input Type**: Single `Directory` instead of `Directory[]` (no scatter)\n- **Expression Parameter**: `s_expression` for custom band math expressions\n- **Band Selection**: `cbn` (common band name) parameter for specific band processing\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:1-79](), [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml:1-111]()\n\n## Best Practices\n\n### CWL Document Guidelines\n\n1. **Use $graph notation** for combining CommandLineTool and Workflow definitions\n2. **Specify explicit types** for all inputs and outputs (avoid generic `Any` types)\n3. **Include documentation** in `doc` and `label` fields for user-facing process descriptions\n4. **Set cwlVersion** to `v1.0` for compatibility with Calrissian\n5. **Use semantic versioning** in `s:softwareVersion` annotations\n\n### Docker Container Guidelines\n\n1. **Pin specific tags** instead of using `latest` (e.g., `dev0.0.3` not `latest`)\n2. **Include conda environments** in PATH via `EnvVarRequirement`\n3. **Minimize container size** for faster pod startup\n4. **Use registry.hub.docker.com** or Harbor registry for EOEPCA deployments\n5. **Test containers locally** before deploying to ADES\n\n### Input/Output Guidelines\n\n1. **Use Directory type** for EO products (not File)\n2. **Use Directory[]** with scatter for batch processing\n3. **Make optional parameters explicit** with `?` suffix (e.g., `string?`)\n4. **Output complete directories** rather than individual files\n5. **Use glob patterns** to capture all output artifacts\n\n### Execution Pattern Guidelines\n\n1. **Enable ScatterFeatureRequirement** for parallel processing of multiple products\n2. **Use dotproduct scatterMethod** for independent processing of array inputs\n3. **Set appropriate ResourceRequirement** values for CPU/memory intensive processing\n4. **Configure stdout/stderr** redirection for debugging\n\nSources: [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:1-79](), [release-notes/release-0.3.md:1-95]()"])</script><script>self.__next_f.push([1,"31:T5e24,"])</script><script>self.__next_f.push([1,"# Storage and Persistence\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n- [system/clusters/creodias/system/demo/hr-django-portal.yaml](system/clusters/creodias/system/demo/hr-django-portal.yaml)\n- [system/clusters/creodias/system/storage/hr-storage.yaml](system/clusters/creodias/system/storage/hr-storage.yaml)\n- [system/clusters/creodias/system/test/hr-cheese.yaml](system/clusters/creodias/system/test/hr-cheese.yaml)\n- [system/clusters/creodias/user-management/um-login-service.yaml](system/clusters/creodias/user-management/um-login-service.yaml)\n- [system/clusters/creodias/user-management/um-pdp-engine.yaml](system/clusters/creodias/user-management/um-pdp-engine.yaml)\n- [system/clusters/creodias/user-management/um-user-profile.yaml](system/clusters/creodias/user-management/um-user-profile.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides an overview of the storage and persistence architecture within the EOEPCA platform. It covers the three primary storage layers: S3 object storage for Earth Observation data and processing outputs, database systems for metadata and queuing, and NFS-backed persistent volumes for stateful application data.\n\nFor detailed information on specific storage subsystems, see:\n- S3 storage tiers and bucket provisioning: [S3 Storage Architecture](#7.1)\n- PostgreSQL and Redis configurations: [Database Systems](#7.2)\n- NFS server setup and storage classes: [NFS and Persistent Volumes](#7.3)\n\nFor information on how workspaces provision isolated storage per user, see [Multi-Tenant Workspaces](#5.5).\n\n---\n\n## Storage Architecture Overview\n\nThe EOEPCA platform employs a three-tier storage architecture to handle different data lifecycle requirements and access patterns.\n\n**Storage Tiers in EOEPCA Platform**\n\n```mermaid\ngraph TB\n    subgraph \"S3 Object Storage Tier\"\n        CF[\"CloudFerro eodata Bucket\u003cbr/\u003edata.cloudferro.com\u003cbr/\u003e(Read-only EO Data)\"]\n        MinioGlobal[\"MinIO Global\u003cbr/\u003eminio.develop.eoepca.org\u003cbr/\u003e(Platform Storage)\"]\n        MinioCache[\"S3 Cache Bucket\u003cbr/\u003ecf2.cloudferro.com:8080/cache-bucket\u003cbr/\u003e(Rendered Tiles)\"]\n        MinioWorkspace[\"Workspace Buckets\u003cbr/\u003ee.g. eric-workspace\u003cbr/\u003e(User Outputs)\"]\n    end\n    \n    subgraph \"Database Tier\"\n        PG1[\"resource-catalogue-db\u003cbr/\u003ePostgreSQL\u003cbr/\u003e(ISO 19115 Metadata)\"]\n        PG2[\"Workspace PostgreSQL\u003cbr/\u003epycsw Databases\u003cbr/\u003e(User Metadata)\"]\n        Redis1[\"data-access-redis-master\u003cbr/\u003eRedis\u003cbr/\u003e(Registration Queues)\"]\n        Redis2[\"Workspace Redis\u003cbr/\u003evs-redis-master\u003cbr/\u003e(User Queues)\"]\n    end\n    \n    subgraph \"NFS Persistent Volume Tier\"\n        NFS[\"NFS Server\u003cbr/\u003e192.168.123.14\"]\n        PVC1[\"eoepca-userman-pvc\u003cbr/\u003e(User Management State)\"]\n        PVC2[\"managed-nfs-storage\u003cbr/\u003e(Dynamic Workspace PVCs)\"]\n        PVC3[\"managed-nfs-storage-retain\u003cbr/\u003e(Retained Workspace Data)\"]\n    end\n    \n    subgraph \"Consumers\"\n        DA[\"Data Access\u003cbr/\u003eRenderer\"]\n        RC[\"Resource Catalogue\u003cbr/\u003epycsw\"]\n        WS[\"Workspace API\u003cbr/\u003eProvisioner\"]\n        UM[\"User Management\u003cbr/\u003eGluu/Keycloak\"]\n    end\n    \n    CF --\u003e|\"Read EO Products\"| DA\n    MinioGlobal --\u003e|\"Store/Retrieve\"| WS\n    MinioCache --\u003e|\"Cache Tiles\"| DA\n    MinioWorkspace --\u003e|\"User Data\"| DA\n    \n    PG1 --\u003e|\"Query Metadata\"| RC\n    PG2 --\u003e|\"User Metadata\"| RC\n    Redis1 --\u003e|\"Process Jobs\"| DA\n    Redis2 --\u003e|\"User Jobs\"| DA\n    \n    NFS --\u003e|\"Mount\"| PVC1\n    NFS --\u003e|\"Provision\"| PVC2\n    NFS --\u003e|\"Provision\"| PVC3\n    PVC1 --\u003e|\"State Files\"| UM\n    PVC2 --\u003e|\"Database Files\"| PG2\n    PVC2 --\u003e|\"Redis Snapshots\"| Redis2\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64]()\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:36-49]()\n- [system/clusters/creodias/system/storage/hr-storage.yaml:16-28]()\n\n---\n\n## S3 Object Storage\n\nThe platform uses S3-compatible object storage for high-volume Earth Observation data and processing artifacts. Three distinct S3 storage tiers serve different purposes in the data lifecycle.\n\n### S3 Storage Configuration\n\n| Storage Type | Endpoint | Purpose | Access Pattern | Configured In |\n|--------------|----------|---------|----------------|---------------|\n| CloudFerro eodata | `http://data.cloudferro.com` | Source EO data (Sentinel, Landsat) | Read-only | Data Access |\n| MinIO Platform | `https://minio.develop.eoepca.org` | Workspace buckets, outputs | Read/Write | Workspace API |\n| Cache Bucket | `https://cf2.cloudferro.com:8080/cache-bucket` | Rendered WMS/WMTS tiles | Read/Write | Data Access |\n\n### Global Data Access S3 Configuration\n\nThe global Data Access service reads from the CloudFerro eodata bucket for source EO products and writes rendered tiles to a cache bucket.\n\n```yaml\n# From hr-data-access.yaml\nstorage:\n  data:\n    data:\n      type: S3\n      endpoint_url: http://data.cloudferro.com\n      access_key_id: access\n      secret_access_key: access\n      region_name: RegionOne\n      validate_bucket_name: false\n  cache:\n    type: S3\n    endpoint_url: \"https://cf2.cloudferro.com:8080/cache-bucket\"\n    host: \"cf2.cloudferro.com:8080\"\n    region_name: RegionOne\n    bucket: cache-bucket\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64]()\n\n### Workspace-Specific S3 Configuration\n\nEach workspace receives an isolated S3 bucket provisioned through MinIO. The Workspace API creates these buckets and injects credentials into workspace service configurations.\n\n```yaml\n# From hr-workspace-api.yaml\ns3Endpoint: \"https://minio.develop.eoepca.org\"\ns3Region: \"RegionOne\"\nbucketEndpointUrl: \"http://minio-bucket-api:8080/bucket\"\n```\n\nThe template configuration for workspace data access services receives S3 credentials as template parameters:\n\n```yaml\n# From template-hr-data-access.yaml\nstorage:\n  data:\n    data:\n      type: \"S3\"\n      endpoint_url: https://minio.develop.eoepca.org\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      bucket: {{ bucket }}\n      region_name: RegionOne\n      validate_bucket_name: false\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:38-47]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()\n\n### S3 Environment Variables\n\nThe Data Access renderer uses GDAL's Virtual File System (VSI) interface to read from S3. Configuration is passed via environment variables:\n\n```yaml\n# From hr-data-access.yaml\nglobal:\n  env:\n    CPL_VSIL_CURL_ALLOWED_EXTENSIONS: .TIF,.TIFF,.tif,.tiff,.xml,.jp2,.jpg,.jpeg,.png,.nc\n    AWS_ENDPOINT_URL_S3: https://minio.develop.eoepca.org\n    AWS_HTTPS: \"FALSE\"\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:27-31]()\n\n---\n\n## Database Systems\n\nPostgreSQL and Redis provide structured data persistence for metadata cataloging and asynchronous job processing.\n\n### PostgreSQL for Metadata\n\nThe Resource Catalogue uses PostgreSQL to store ISO 19115-compliant metadata records through the pycsw OGC CSW implementation.\n\n**PostgreSQL Database Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Global Resource Management\"\n        RCDB[\"resource-catalogue-db\u003cbr/\u003ePostgreSQL Pod\"]\n        RCPVC[\"PVC: 5Gi\"]\n    end\n    \n    subgraph \"Workspace eric-workspace\"\n        WSDB[\"vs-database\u003cbr/\u003ePostgreSQL Pod\"]\n        WSPVC[\"PVC: 100Gi\u003cbr/\u003emanaged-nfs-storage\"]\n    end\n    \n    subgraph \"Registrar Services\"\n        RegGlobal[\"registrar\u003cbr/\u003e(Global)\"]\n        RegWS[\"registrar\u003cbr/\u003e(Workspace)\"]\n    end\n    \n    subgraph \"Catalogue Services\"\n        PyCSW[\"resource-catalogue\u003cbr/\u003epycsw\"]\n        WSPyCSW[\"rm-resource-catalogue\u003cbr/\u003epycsw\"]\n    end\n    \n    RCPVC --\u003e|\"Mounts\"| RCDB\n    WSPVC --\u003e|\"Mounts\"| WSDB\n    \n    RegGlobal --\u003e|\"postgresql://postgres:mypass@\u003cbr/\u003eresource-catalogue-db/pycsw\"| RCDB\n    RegWS --\u003e|\"postgresql://postgres:mypass@\u003cbr/\u003eresource-catalogue-db/pycsw\"| WSDB\n    \n    PyCSW --\u003e|\"Query Metadata\"| RCDB\n    WSPyCSW --\u003e|\"Query Metadata\"| WSDB\n```\n\n#### Global Resource Catalogue Database Configuration\n\nThe global Resource Catalogue database is configured with performance tuning for metadata queries:\n\n```yaml\n# From hr-resource-catalogue.yaml\ndb:\n  volume_size: 5Gi\n  config:\n    enabled: true\n    shared_buffers: 2GB\n    effective_cache_size: 6GB\n    maintenance_work_mem: 512MB\n    checkpoint_completion_target: 0.9\n    wal_buffers: 16MB\n    default_statistics_target: 100\n    random_page_cost: 4\n    work_mem: 4MB\n    cpu_tuple_cost: 0.4\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:20-30]()\n\n#### Workspace Database Configuration\n\nEach workspace provisions a dedicated PostgreSQL instance for user-specific metadata:\n\n```yaml\n# From template-hr-data-access.yaml\nvs:\n  database:\n    persistence:\n      enabled: true\n      existingClaim: \"\"  # generated dynamically\n      storageClass: managed-nfs-storage\n      size: \"100Gi\"\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-228]()\n\n#### Database Connection Strings\n\nRegistrar backends connect to PostgreSQL using standardized connection URIs:\n\n```yaml\n# From hr-data-access.yaml (example)\nbackends:\n  - path: registrar_pycsw.backend.ItemBackend\n    kwargs:\n      repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n      ows_url: https://data-access.develop.eoepca.org/ows\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:889-892]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:110-112]()\n\n### Redis for Queuing\n\nRedis provides message queuing for asynchronous data registration and processing workflows.\n\n**Redis Queue Architecture**\n\n```mermaid\ngraph LR\n    subgraph \"Data Sources\"\n        Harvester[\"harvester\u003cbr/\u003e(OpenSearch Polling)\"]\n        Manual[\"Registration API\u003cbr/\u003e(Manual Submit)\"]\n    end\n    \n    subgraph \"Redis Queues\"\n        RedisMaster[\"data-access-redis-master\u003cbr/\u003ePort 6379\"]\n        Queues[\"Queues:\u003cbr/\u003eregister_queue\u003cbr/\u003eregister_collection_queue\u003cbr/\u003eregister_ades_queue\u003cbr/\u003eregister_application_queue\u003cbr/\u003eseed_queue\"]\n    end\n    \n    subgraph \"Consumers\"\n        Registrar[\"registrar\u003cbr/\u003e(Multiple Routes)\"]\n        Seeder[\"seeder\u003cbr/\u003e(Tile Generation)\"]\n    end\n    \n    Harvester --\u003e|\"Enqueue STAC Items\"| RedisMaster\n    Manual --\u003e|\"Enqueue Metadata\"| RedisMaster\n    RedisMaster --\u003e|\"Contains\"| Queues\n    Queues --\u003e|\"Consume\"| Registrar\n    Queues --\u003e|\"Consume\"| Seeder\n```\n\n#### Global Redis Configuration\n\n```yaml\n# From hr-data-access.yaml (harvester config)\nharvester:\n  config:\n    redis:\n      host: data-access-redis-master\n      port: 6379\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:958-960]()\n\n#### Workspace Redis Configuration\n\nEach workspace deploys a dedicated Redis instance for isolated queue processing:\n\n```yaml\n# From template-hr-data-access.yaml\nvs:\n  redis:\n    usePassword: false\n    persistence:\n      existingClaim: \"\"  # generated dynamically\n    master:\n      persistence:\n        existingClaim: \"\"  # generated dynamically\n        storageClass: managed-nfs-storage\n        size: \"1Gi\"\n    cluster:\n      enabled: false\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:230-240]()\n\n#### Queue Routing Configuration\n\nThe registrar service defines multiple queue routes for different data types:\n\n| Queue Name | Route Class | Backend | Purpose |\n|------------|-------------|---------|---------|\n| `register_queue` | `registrar.route.stac.ItemRoute` | `ItemBackend` | STAC items (products) |\n| `register_collection_queue` | `registrar.route.stac.CollectionRoute` | `CollectionBackend` | STAC collections |\n| `register_ades_queue` | `registrar.route.json.JSONRoute` | `ADESBackend` | ADES service metadata |\n| `register_application_queue` | `registrar.route.json.JSONRoute` | `CWLBackend` | CWL application packages |\n| `register_catalogue_queue` | `registrar.route.json.JSONRoute` | `CatalogueBackend` | Catalogue endpoints |\n| `seed_queue` | N/A | Seeder | Tile seeding requests |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:894-947]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:95-164]()\n\n---\n\n## NFS and Persistent Volumes\n\nNFS provides shared filesystem storage for stateful Kubernetes workloads. A central NFS server provisions PersistentVolumeClaims (PVCs) through dynamically-managed StorageClasses.\n\n### NFS Server Configuration\n\nThe platform deploys an NFS server and configures StorageClasses for different retention policies:\n\n```yaml\n# From hr-storage.yaml\nstorage:\n  nfs:\n    server:\n      address: \"192.168.123.14\"\n  domain:\n    resman:\n      storageClass: eoepca-nfs\n    proc:\n      enabled: true\n      storageClass: eoepca-nfs\n    userman:\n      storageClass: eoepca-nfs\n```\n\n**Sources:**\n- [system/clusters/creodias/system/storage/hr-storage.yaml:16-28]()\n\n### Storage Classes\n\nTwo NFS-backed StorageClasses support different data lifecycle requirements:\n\n| StorageClass | Reclaim Policy | Use Case | Provisioner |\n|--------------|----------------|----------|-------------|\n| `managed-nfs-storage` | Delete | Ephemeral workspace data | NFS Provisioner |\n| `managed-nfs-storage-retain` | Retain | Long-term workspace data | NFS Provisioner |\n\n#### Workspace Storage Class Usage\n\nWorkspace components specify `managed-nfs-storage` to dynamically provision PVCs:\n\n```yaml\n# From template-hr-data-access.yaml\nvs:\n  database:\n    persistence:\n      storageClass: managed-nfs-storage\n      size: \"100Gi\"\n  redis:\n    master:\n      persistence:\n        storageClass: managed-nfs-storage\n        size: \"1Gi\"\n```\n\nFor database storage requiring data retention beyond workspace lifecycle:\n\n```yaml\n# From template-hr-resource-catalogue.yaml\ndb:\n  volume_storage_type: managed-nfs-storage-retain\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:223-238]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:19]()\n\n### User Management Persistent Volume\n\nThe User Management building block (Gluu/Keycloak) uses a shared PVC for state persistence across multiple components:\n\n**User Management PVC Architecture**\n\n```mermaid\ngraph TB\n    PVC[\"eoepca-userman-pvc\u003cbr/\u003eStorageClass: eoepca-nfs\"]\n    NFS[\"NFS Server\u003cbr/\u003e192.168.123.14\"]\n    \n    subgraph \"User Management Pods\"\n        OpenDJ[\"opendj\u003cbr/\u003e(LDAP Directory)\"]\n        OxAuth[\"oxauth\u003cbr/\u003e(OAuth2/OIDC)\"]\n        OxTrust[\"oxtrust\u003cbr/\u003e(Admin UI)\"]\n        PDP[\"pdp-engine\u003cbr/\u003e(Policy Engine)\"]\n        Profile[\"user-profile\u003cbr/\u003e(User Data)\"]\n    end\n    \n    NFS --\u003e|\"Provisions\"| PVC\n    PVC --\u003e|\"Mount /opt/opendj/config\u003cbr/\u003e/opt/opendj/ldif\u003cbr/\u003e/opt/opendj/logs\"| OpenDJ\n    PVC --\u003e|\"Mount /opt/gluu/jetty/oxauth\"| OxAuth\n    PVC --\u003e|\"Mount /opt/gluu/jetty/identity\"| OxTrust\n    PVC --\u003e|\"Mount PDP State\"| PDP\n    PVC --\u003e|\"Mount User Profiles\"| Profile\n```\n\n#### PVC Configuration in User Management\n\nAll User Management components reference the same PVC:\n\n```yaml\n# From um-login-service.yaml\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\n\nopendj:\n  volumeClaim:\n    name: eoepca-userman-pvc\noxauth:\n  volumeClaim:\n    name: eoepca-userman-pvc\noxtrust:\n  volumeClaim:\n    name: eoepca-userman-pvc\n```\n\n**Sources:**\n- [system/clusters/creodias/user-management/um-login-service.yaml:16-43]()\n- [system/clusters/creodias/user-management/um-pdp-engine.yaml:22-24]()\n- [system/clusters/creodias/user-management/um-user-profile.yaml:19-21]()\n\n---\n\n## Storage Integration in Key Components\n\nThis section illustrates how major platform components integrate the three storage tiers.\n\n### Data Access Service Storage Integration\n\n**Data Access Storage Flow**\n\n```mermaid\ngraph LR\n    subgraph \"Inputs\"\n        User[\"User Request\u003cbr/\u003e(WMS/WCS)\"]\n        CF[\"CloudFerro eodata\u003cbr/\u003e(Source Data)\"]\n        MinioWS[\"MinIO Workspace\u003cbr/\u003e(User Data)\"]\n    end\n    \n    subgraph \"Data Access Components\"\n        Renderer[\"renderer\u003cbr/\u003e(4 replicas)\"]\n        Registrar[\"registrar\u003cbr/\u003e(Registration)\"]\n        Harvester[\"harvester\u003cbr/\u003e(Data Discovery)\"]\n    end\n    \n    subgraph \"Storage Backends\"\n        PG[\"resource-catalogue-db\u003cbr/\u003e(Metadata)\"]\n        Redis[\"data-access-redis-master\u003cbr/\u003e(Queues)\"]\n        Cache[\"S3 Cache Bucket\u003cbr/\u003e(Rendered Tiles)\"]\n    end\n    \n    User --\u003e|\"GetMap Request\"| Renderer\n    Renderer --\u003e|\"Read Products\"| CF\n    Renderer --\u003e|\"Read Products\"| MinioWS\n    Renderer --\u003e|\"Query Metadata\"| PG\n    Renderer --\u003e|\"Cache Tiles\"| Cache\n    \n    Harvester --\u003e|\"Enqueue Metadata\"| Redis\n    Redis --\u003e|\"Consume Jobs\"| Registrar\n    Registrar --\u003e|\"Store Metadata\"| PG\n```\n\nConfiguration binding these components:\n\n```yaml\n# Registrar connects to PostgreSQL and Redis\nregistrar:\n  config:\n    defaultBackends:\n      - path: registrar_pycsw.backend.ItemBackend\n        kwargs:\n          repository_database_uri: postgresql://postgres:mypass@resource-catalogue-db/pycsw\n    defaultSuccessQueue: seed_queue\n\n# Harvester connects to Redis\nharvester:\n  config:\n    redis:\n      host: data-access-redis-master\n      port: 6379\n\n# Renderer reads from S3\nglobal:\n  storage:\n    data:\n      data:\n        type: S3\n        endpoint_url: http://data.cloudferro.com\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:878-960]()\n\n### Workspace API Storage Provisioning\n\nThe Workspace API orchestrates storage provisioning across all three tiers when creating a user workspace.\n\n**Workspace Storage Provisioning Sequence**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant WSAPI as \"workspace-api\"\n    participant MinioAPI as \"minio-bucket-api\u003cbr/\u003e:8080/bucket\"\n    participant Flux as \"Flux CD\"\n    participant K8s as \"Kubernetes API\"\n    participant NFS as \"NFS Server\u003cbr/\u003e192.168.123.14\"\n    \n    User-\u003e\u003eWSAPI: \"POST /workspaces\u003cbr/\u003e{name: eric-workspace}\"\n    \n    WSAPI-\u003e\u003eMinioAPI: \"POST /bucket\u003cbr/\u003e{name: eric-workspace}\"\n    MinioAPI-\u003e\u003eMinioAPI: \"Create S3 Bucket\"\n    MinioAPI--\u003e\u003eWSAPI: \"Credentials:\u003cbr/\u003eaccess_key_id,\u003cbr/\u003esecret_access_key\"\n    \n    WSAPI-\u003e\u003eFlux: \"Create HelmRelease\u003cbr/\u003edata-access\u003cbr/\u003e(from template)\"\n    WSAPI-\u003e\u003eFlux: \"Create HelmRelease\u003cbr/\u003eresource-catalogue\u003cbr/\u003e(from template)\"\n    \n    Flux-\u003e\u003eK8s: \"Create Namespace\u003cbr/\u003eeric-workspace\"\n    Flux-\u003e\u003eK8s: \"Deploy PostgreSQL\u003cbr/\u003e(with PVC request)\"\n    Flux-\u003e\u003eK8s: \"Deploy Redis\u003cbr/\u003e(with PVC request)\"\n    \n    K8s-\u003e\u003eNFS: \"Provision PVC\u003cbr/\u003emanaged-nfs-storage\"\n    NFS--\u003e\u003eK8s: \"PV Created\"\n    K8s-\u003e\u003eK8s: \"Bind PVC to PV\"\n    K8s-\u003e\u003eK8s: \"Mount PVC to Pods\"\n    \n    K8s--\u003e\u003eWSAPI: \"Workspace Ready\"\n    WSAPI--\u003e\u003eUser: \"Workspace Created\u003cbr/\u003eeric-workspace\"\n```\n\nConfiguration parameters injected into workspace templates:\n\n```yaml\n# Template parameters filled by Workspace API\nstorage:\n  data:\n    data:\n      access_key_id: {{ access_key_id }}      # From MinIO API\n      secret_access_key: {{ secret_access_key }} # From MinIO API\n      bucket: {{ bucket }}                     # From MinIO API\n\nvs:\n  database:\n    persistence:\n      storageClass: managed-nfs-storage       # NFS provisioner\n      size: \"100Gi\"\n  redis:\n    master:\n      persistence:\n        storageClass: managed-nfs-storage     # NFS provisioner\n        size: \"1Gi\"\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml:35-49]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-240]()\n\n---\n\n## Storage Resource Requirements\n\nThe following table summarizes typical storage allocations for platform components:\n\n| Component | Storage Type | Size | Reclaim Policy | Purpose |\n|-----------|--------------|------|----------------|---------|\n| `resource-catalogue-db` | PostgreSQL on NFS | 5Gi | Delete | Global metadata catalogue |\n| `eoepca-userman-pvc` | NFS PVC | Variable | Retain | User management state (LDAP, configs) |\n| `data-access-redis-master` | Redis on NFS | Variable | Delete | Global registration queues |\n| Workspace PostgreSQL | PostgreSQL on NFS | 100Gi | Retain (optional) | User metadata catalogue |\n| Workspace Redis | Redis on NFS | 1Gi | Delete | User-specific queues |\n| Workspace S3 Bucket | MinIO Object Storage | Unlimited | N/A | User outputs and processing results |\n| Cache Bucket | S3 Object Storage | Unlimited | N/A | Rendered WMS/WMTS tiles |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:20]()\n- [system/clusters/creodias/user-management/um-login-service.yaml:17]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:224]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:238]()\n\n---\n\n## Storage Access Patterns\n\nDifferent platform components exhibit distinct storage access patterns optimized for their workloads:\n\n| Component | Read Pattern | Write Pattern | Consistency | Latency Sensitivity |\n|-----------|--------------|---------------|-------------|---------------------|\n| Data Access Renderer | High-volume sequential (EO products from S3) | Low-volume (cache tiles) | Eventual | High (user-facing) |\n| Resource Catalogue | Random access (metadata queries) | Low-volume (registration) | Strong (ACID) | Medium |\n| Registrar | Queue consumption (sequential) | Batch inserts (metadata) | Strong (transactional) | Low |\n| Harvester | N/A | Queue writes (sequential) | At-least-once | Low |\n| Workspace Database | Random access (user queries) | Low-volume (user data) | Strong (ACID) | Medium |\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:864-876]() (renderer replicas and resources)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:20-30]() (database tuning)\n\n---\n\n## Summary\n\nThe EOEPCA storage architecture provides:\n\n1. **S3 Object Storage**: High-throughput access to Earth Observation products via CloudFerro and MinIO, with per-workspace bucket isolation\n2. **PostgreSQL Databases**: ACID-compliant metadata storage for OGC CSW catalogues, with performance tuning for geospatial queries\n3. **Redis Queues**: Asynchronous job processing for data registration and tile seeding workflows\n4. **NFS Persistent Volumes**: Shared filesystem storage for stateful application data with configurable retention policies\n\nThis multi-tier approach separates concerns based on data characteristics: object storage for large binary assets, relational databases for structured metadata, message queues for asynchronous workflows, and shared filesystems for application state. Each tier is provisioned either globally for platform-wide resources or per-workspace for user isolation."])</script><script>self.__next_f.push([1,"32:T5320,"])</script><script>self.__next_f.push([1,"# S3 Storage Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the S3-compatible object storage architecture used throughout the EOEPCA platform. It covers the three-tier storage model, bucket provisioning mechanisms, and how different services interact with S3 endpoints for data access, caching, and workspace storage.\n\nFor information about NFS persistent volumes used by services, see [NFS and Persistent Volumes](#7.3). For database storage architecture, see [Database Systems](#7.2). For workspace provisioning logic, see [Workspace API](#5.3).\n\n---\n\n## Storage Tier Overview\n\nThe EOEPCA platform employs a three-tier S3 storage architecture to separate concerns between input data, performance optimization, and user outputs:\n\n```mermaid\ngraph TB\n    subgraph \"External Data Sources\"\n        Sentinel[\"Sentinel-2/1/3\u003cbr/\u003eLandsat-8\"]\n    end\n    \n    subgraph \"Tier 1: Input Data Storage\"\n        CloudFerroData[\"CloudFerro eodata\u003cbr/\u003ehttp://data.cloudferro.com\u003cbr/\u003eRegion: RegionOne\"]\n    end\n    \n    subgraph \"Tier 2: Cache Storage\"\n        CloudFerroCache[\"CloudFerro cache-bucket\u003cbr/\u003ecf2.cloudferro.com:8080\u003cbr/\u003eRegion: RegionOne\"]\n    end\n    \n    subgraph \"Tier 3: Workspace Storage\"\n        MinIO[\"MinIO\u003cbr/\u003eminio.develop.eoepca.org\u003cbr/\u003eRegion: RegionOne\"]\n        BucketAPI[\"minio-bucket-api\u003cbr/\u003e:8080/bucket\"]\n    end\n    \n    subgraph \"Services\"\n        DataAccess[\"data-access\u003cbr/\u003eRenderer/WMS/WCS\"]\n        ADES[\"ADES\u003cbr/\u003eProcessing Jobs\"]\n        WorkspaceAPI[\"workspace-api\u003cbr/\u003eOrchestrator\"]\n    end\n    \n    subgraph \"User Workspaces\"\n        UserBucket1[\"Bucket: develop-user-eric-workspace\"]\n        UserBucket2[\"Bucket: develop-user-alice-workspace\"]\n    end\n    \n    Sentinel --\u003e|\"Harvest \u0026 Register\"| CloudFerroData\n    \n    DataAccess --\u003e|\"Read Raster Data\"| CloudFerroData\n    DataAccess --\u003e|\"Write/Read Tiles\"| CloudFerroCache\n    \n    ADES --\u003e|\"Stage-in Input Data\"| CloudFerroData\n    ADES --\u003e|\"Stage-out Results\"| MinIO\n    \n    WorkspaceAPI --\u003e|\"Create Buckets\"| BucketAPI\n    BucketAPI --\u003e|\"Provision\"| MinIO\n    \n    MinIO --\u003e UserBucket1\n    MinIO --\u003e UserBucket2\n    \n    DataAccess --\u003e|\"Read User Data\"| MinIO\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:38-40]()\n\n---\n\n## Tier 1: CloudFerro Input Data Storage\n\n### Purpose and Configuration\n\nThe CloudFerro `eodata` bucket serves as the primary read-only source for Earth Observation data harvested from external providers. This tier provides access to registered Sentinel and Landsat datasets stored on the CREODIAS infrastructure.\n\n| Parameter | Value |\n|-----------|-------|\n| **Endpoint** | `http://data.cloudferro.com` |\n| **Region** | `RegionOne` |\n| **Access Type** | Read-only |\n| **Credentials** | `access_key_id: access`\u003cbr/\u003e`secret_access_key: access` |\n| **Bucket Name Validation** | Disabled |\n\n**Configuration Location:** [system/clusters/creodias/resource-management/hr-data-access.yaml:49-57]()\n\n### Data Access Integration\n\nThe Data Access service configures this storage tier for its `renderer` component to read raster data:\n\n```yaml\nstorage:\n  data:\n    data:\n      type: S3\n      endpoint_url: http://data.cloudferro.com\n      access_key_id: access\n      secret_access_key: access\n      region_name: RegionOne\n      validate_bucket_name: false\n```\n\nThe `CPL_VSIL_CURL_ALLOWED_EXTENSIONS` environment variable restricts which file types can be accessed via VSIL (Virtual File System) to ensure security:\n\n**Allowed Extensions:** `.TIF`, `.TIFF`, `.tif`, `.tiff`, `.xml`, `.jp2`, `.jpg`, `.jpeg`, `.png`, `.nc`\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:29](), [system/clusters/creodias/resource-management/hr-data-access.yaml:49-57]()\n\n---\n\n## Tier 2: CloudFerro Cache Storage\n\n### Purpose and Configuration\n\nThe CloudFerro `cache-bucket` provides S3 storage for caching rendered tiles, improving performance for repeated WMS/WMTS requests by avoiding redundant raster processing.\n\n| Parameter | Value |\n|-----------|-------|\n| **Endpoint** | `https://cf2.cloudferro.com:8080/cache-bucket` |\n| **Host** | `cf2.cloudferro.com:8080` |\n| **Region** | `RegionOne` |\n| **Bucket** | `cache-bucket` |\n| **Access Type** | Read/Write |\n\n**Configuration Location:** [system/clusters/creodias/resource-management/hr-data-access.yaml:58-64]()\n\n### Cache Service Integration\n\nThe cache is integrated with the Data Access service's `seeder` and `cache` components. The seeder pre-generates tiles at configured zoom levels, while the cache component serves and stores tiles on-demand:\n\n```yaml\nstorage:\n  cache:\n    type: S3\n    endpoint_url: \"https://cf2.cloudferro.com:8080/cache-bucket\"\n    host: \"cf2.cloudferro.com:8080\"\n    region_name: RegionOne\n    region: RegionOne\n    bucket: cache-bucket\n```\n\nThe seeder is configured with zoom level constraints to manage storage usage:\n\n```yaml\nseeder:\n  config:\n    minzoom: 0\n    maxzoom: 6  # restrict to only 6 for testing\n```\n\n**Note:** The cache and seeder components are disabled in workspace-specific deployments (replica count set to 0) to conserve resources in multi-tenant environments.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:58-64](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:259-269]()\n\n---\n\n## Tier 3: MinIO Workspace Storage\n\n### Architecture Overview\n\nMinIO provides the workspace storage tier, offering isolated S3-compatible buckets for each user workspace. This tier handles:\n\n- Processing job outputs from ADES\n- User-uploaded data\n- STAC catalogs describing workspace contents\n- Application packages and results\n\n```mermaid\ngraph TB\n    subgraph \"Bucket Provisioning Flow\"\n        WorkspaceAPI[\"workspace-api\u003cbr/\u003erm namespace\"]\n        BucketAPI[\"minio-bucket-api\u003cbr/\u003e:8080/bucket\u003cbr/\u003erm namespace\"]\n        MinIOServer[\"MinIO Server\u003cbr/\u003eminio.develop.eoepca.org\"]\n    end\n    \n    subgraph \"User Request\"\n        User[\"User: eric\u003cbr/\u003eID Token\"]\n    end\n    \n    subgraph \"Provisioned Resources\"\n        Namespace[\"K8s Namespace:\u003cbr/\u003edevelop-user-eric-workspace\"]\n        Bucket[\"S3 Bucket:\u003cbr/\u003edevelop-user-eric-workspace\"]\n        Secret[\"K8s Secret: bucket\u003cbr/\u003eaccess_key_id\u003cbr/\u003esecret_access_key\"]\n    end\n    \n    subgraph \"Workspace Services\"\n        DataAccessWS[\"data-access\u003cbr/\u003eReads workspace data\"]\n        ResourceCatWS[\"resource-catalogue\u003cbr/\u003eMetadata storage\"]\n        Harvester[\"harvester\u003cbr/\u003eSTAC catalog reader\"]\n    end\n    \n    User --\u003e|\"POST /workspaces\"| WorkspaceAPI\n    WorkspaceAPI --\u003e|\"1. Create Namespace\"| Namespace\n    WorkspaceAPI --\u003e|\"2. POST /bucket\"| BucketAPI\n    BucketAPI --\u003e|\"3. Create Bucket\"| MinIOServer\n    MinIOServer --\u003e|\"4. Return Credentials\"| BucketAPI\n    BucketAPI --\u003e|\"5. Return Credentials\"| WorkspaceAPI\n    WorkspaceAPI --\u003e|\"6. Create Secret\"| Secret\n    WorkspaceAPI --\u003e|\"7. Deploy HelmReleases\"| DataAccessWS\n    \n    Secret --\u003e|\"Mount Credentials\"| DataAccessWS\n    Secret --\u003e|\"Mount Credentials\"| Harvester\n    \n    DataAccessWS --\u003e|\"Read/Write\"| Bucket\n    Harvester --\u003e|\"Read catalog.json\"| Bucket\n    ResourceCatWS --\u003e|\"Store Metadata\"| DataAccessWS\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:38-48]()\n\n### MinIO Configuration\n\nThe Workspace API manages MinIO integration through the following configuration parameters:\n\n| Parameter | Configuration Value | Purpose |\n|-----------|---------------------|---------|\n| `s3Endpoint` | `https://minio.develop.eoepca.org` | MinIO API endpoint |\n| `s3Region` | `RegionOne` | AWS region identifier |\n| `bucketEndpointUrl` | `http://minio-bucket-api:8080/bucket` | Internal bucket provisioning API |\n| `workspaceSecretName` | `bucket` | K8s secret name for S3 credentials |\n| `namespaceForBucketResource` | `rm` | Namespace hosting bucket operator |\n\n**Configuration Location:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:36-47]()\n\n### Bucket Naming Convention\n\nBuckets follow a deterministic naming pattern based on the workspace name:\n\n```\nBucket Name = {{ prefixForName }}-{{ username }}-workspace\nExample: develop-user-eric-workspace\n```\n\nThe `prefixForName` is configurable per deployment environment (e.g., `develop-user`, `production-user`).\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:35]()\n\n---\n\n## Per-Workspace Storage Configuration\n\n### Data Access Service Storage Binding\n\nEach workspace receives a dedicated Data Access service instance configured to access its specific S3 bucket. The template system injects bucket credentials and configuration at deployment time:\n\n```yaml\nstorage:\n  data:\n    data:\n      type: \"S3\"\n      endpoint_url: https://minio.develop.eoepca.org\n      access_key_id: {{ access_key_id }}\n      secret_access_key: {{ secret_access_key }}\n      bucket: {{ bucket }}\n      region_name: RegionOne\n      validate_bucket_name: false\n```\n\nThese placeholders are substituted during workspace provisioning:\n\n- `{{ access_key_id }}` - Unique access key for the workspace bucket\n- `{{ secret_access_key }}` - Secret access key for authentication\n- `{{ bucket }}` - The workspace-specific bucket name\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()\n\n### Harvester S3 Filesystem Configuration\n\nThe workspace harvester component is configured to read STAC catalogs stored in the workspace bucket. It uses an S3 filesystem definition with the injected credentials:\n\n```yaml\nharvester:\n  config:\n    harvesters:\n      harvest-bucket-catalog:\n        queue: \"register_queue\"\n        resource:\n          type: \"STACCatalog\"\n          staccatalog:\n            filesystem: s3bucket\n            root_path: \"/home/catalog.json\"\n    filesystems:\n      s3bucket:\n        type: s3\n        s3:\n          access_key_id: {{ access_key_id }}\n          secret_access_key: {{ secret_access_key }}\n          endpoint_url: https://minio.develop.eoepca.org\n          region: RegionOne\n          public: False\n```\n\nThis allows the harvester to automatically discover and register STAC items stored in `catalog.json` at the bucket root.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:166-195]()\n\n---\n\n## Service-to-Storage Mapping\n\n### Global Data Access Service\n\nThe global Data Access service (in the `rm` namespace) can read from all three storage tiers:\n\n| Storage Tier | Purpose | Access Pattern |\n|--------------|---------|----------------|\n| CloudFerro eodata | Source raster data | Read-only via GDAL VSIL |\n| CloudFerro cache | Rendered tile cache | Read/Write for tile seeding |\n| MinIO (all buckets) | User workspace data | Read-only for visualization |\n\nThe global service's renderer component scales to 4 replicas to handle concurrent tile rendering requests:\n\n```yaml\nvs:\n  renderer:\n    replicaCount: 4\n    resources:\n      limits:\n        cpu: 1.5\n        memory: 3Gi\n      requests:\n        cpu: 0.5\n        memory: 1Gi\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:865-876]()\n\n### Workspace Data Access Service\n\nPer-workspace Data Access services have constrained storage access:\n\n| Storage Tier | Purpose | Access Pattern |\n|--------------|---------|----------------|\n| MinIO (workspace bucket only) | User's private data | Read/Write via registrar |\n\nResource allocation is reduced for workspace instances to support multi-tenancy:\n\n```yaml\nvs:\n  renderer:\n    replicaCount: 1\n    resources:\n      requests:\n        cpu: 100m\n        memory: 300Mi\n      limits:\n        memory: 3Gi\n```\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:70-82]()\n\n### ADES Processing Service\n\nThe ADES service orchestrates data movement across storage tiers during job execution:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant ADES\n    participant RC as resource-catalogue\n    participant CloudFerro as CloudFerro eodata\n    participant MinIO as MinIO Workspace\n    \n    User-\u003e\u003eADES: Execute Job (CWL)\n    ADES-\u003e\u003eRC: Query Input Locations\n    RC--\u003e\u003eADES: STAC Items with S3 hrefs\n    \n    ADES-\u003e\u003eCloudFerro: Stage-in Input Data\n    CloudFerro--\u003e\u003eADES: Raster Files\n    \n    ADES-\u003e\u003eADES: Execute CWL Workflow\u003cbr/\u003e(Calrissian)\n    \n    ADES-\u003e\u003eMinIO: Stage-out Results\n    ADES-\u003e\u003eMinIO: Write STAC Manifest\n    MinIO--\u003e\u003eADES: Success\n    \n    ADES-\u003e\u003eRC: Register Output STAC\n    ADES--\u003e\u003eUser: Job Complete\n```\n\n**Sources:** Referenced from broader system architecture; ADES details in [ADES (Application Deployment and Execution Service)](#6.1)\n\n---\n\n## S3 Endpoint Configuration Summary\n\n### Environment Variables\n\nMultiple services reference the MinIO endpoint through environment variables:\n\n**Data Access Services:**\n```yaml\nenv:\n  AWS_ENDPOINT_URL_S3: https://minio.develop.eoepca.org\n  AWS_HTTPS: \"FALSE\"\n```\n\n**Purpose:** These variables configure GDAL's VSI subsystem to use the MinIO endpoint for S3 access, with HTTP used for internal cluster communication despite the HTTPS external endpoint.\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:30-31](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:20]()\n\n### Public URL Configuration\n\nThe registrar component constructs public URLs for STAC assets using the configured public S3 URL:\n\n```yaml\nregistrar:\n  config:\n    routes:\n      items:\n        backends:\n          - path: \"registrar_pycsw.backend.ItemBackend\"\n            kwargs:\n              public_s3_url: \"https://minio.develop.eoepca.org/{{ bucket }}\"\n```\n\nThis ensures that STAC `href` fields in the Resource Catalogue point to publicly accessible S3 URLs.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:108-112]()\n\n---\n\n## Storage Isolation and Security\n\n### Multi-Tenancy Isolation Mechanisms\n\nEach workspace receives isolated storage through:\n\n1. **Unique S3 Buckets** - One bucket per workspace with non-overlapping names\n2. **Unique Credentials** - Separate access key pairs stored in namespace-scoped K8s secrets\n3. **Namespace Isolation** - Services deployed in separate K8s namespaces per user\n4. **PEP Protection** - Resource Guards enforce ownership policies (see [Resource Guards and Access Control](#6.4))\n\n```mermaid\ngraph TB\n    subgraph \"User Eric's Workspace\"\n        NS1[\"Namespace: develop-user-eric-workspace\"]\n        Secret1[\"Secret: bucket\u003cbr/\u003eaccess_key: eric_key\u003cbr/\u003esecret_key: eric_secret\"]\n        Bucket1[\"Bucket: develop-user-eric-workspace\"]\n        DA1[\"data-access\"]\n    end\n    \n    subgraph \"User Alice's Workspace\"\n        NS2[\"Namespace: develop-user-alice-workspace\"]\n        Secret2[\"Secret: bucket\u003cbr/\u003eaccess_key: alice_key\u003cbr/\u003esecret_key: alice_secret\"]\n        Bucket2[\"Bucket: develop-user-alice-workspace\"]\n        DA2[\"data-access\"]\n    end\n    \n    subgraph \"MinIO Server\"\n        MinIO[\"MinIO\u003cbr/\u003eminio.develop.eoepca.org\"]\n    end\n    \n    Secret1 -.-\u003e|\"Credentials\"| DA1\n    DA1 --\u003e|\"Authenticated Access\"| MinIO\n    MinIO --\u003e Bucket1\n    \n    Secret2 -.-\u003e|\"Credentials\"| DA2\n    DA2 --\u003e|\"Authenticated Access\"| MinIO\n    MinIO --\u003e Bucket2\n    \n    DA1 -.-\u003e|\"Denied\"| Bucket2\n    DA2 -.-\u003e|\"Denied\"| Bucket1\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:36-49](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()\n\n### Credential Management\n\nS3 credentials are stored as Kubernetes Secrets and mounted into pods:\n\n**Secret Structure (`bucket` secret):**\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: bucket\n  namespace: {{ workspace_name }}\ntype: Opaque\ndata:\n  access_key_id: \u003cbase64-encoded\u003e\n  secret_access_key: \u003cbase64-encoded\u003e\n  bucket: \u003cbase64-encoded\u003e\n```\n\nThe `workspaceSecretName` parameter defines which secret name to use (default: `bucket`).\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:36]()\n\n---\n\n## Bucket Operator and Provisioning API\n\n### API Endpoint\n\nThe `minio-bucket-api` service provides a REST API for programmatic bucket creation:\n\n**Endpoint:** `http://minio-bucket-api:8080/bucket` (internal cluster DNS)\n\n**Purpose:** Abstracts MinIO bucket creation and credential generation, allowing the Workspace API to provision buckets without direct MinIO admin credentials.\n\n### Provisioning Flow\n\n```mermaid\nsequenceDiagram\n    participant WS as workspace-api\n    participant BucketAPI as minio-bucket-api\u003cbr/\u003e:8080/bucket\n    participant MinIO as MinIO Server\n    participant K8s as Kubernetes API\n    \n    WS-\u003e\u003eBucketAPI: POST /bucket\u003cbr/\u003e{name: \"develop-user-eric-workspace\"}\n    BucketAPI-\u003e\u003eMinIO: Create Bucket\n    MinIO--\u003e\u003eBucketAPI: Bucket Created\n    \n    BucketAPI-\u003e\u003eMinIO: Generate Access Keys\n    MinIO--\u003e\u003eBucketAPI: {access_key_id, secret_access_key}\n    \n    BucketAPI--\u003e\u003eWS: {bucket, access_key_id, secret_access_key}\n    \n    WS-\u003e\u003eK8s: Create Secret \"bucket\"\u003cbr/\u003ein namespace develop-user-eric-workspace\n    K8s--\u003e\u003eWS: Secret Created\n    \n    WS-\u003e\u003eK8s: Deploy HelmReleases\u003cbr/\u003e(data-access, resource-catalogue)\n```\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:47]()\n\n### Workspace API Integration\n\nThe Workspace API orchestrates bucket provisioning as part of workspace creation:\n\n**Configuration Parameters:**\n- `namespaceForBucketResource: \"rm\"` - Namespace where bucket operator runs\n- `bucketEndpointUrl: \"http://minio-bucket-api:8080/bucket\"` - Internal API endpoint\n- `s3Endpoint: \"https://minio.develop.eoepca.org\"` - Public MinIO endpoint\n- `s3Region: \"RegionOne\"` - AWS region identifier\n\n**Sources:** [system/clusters/creodias/resource-management/hr-workspace-api.yaml:37-40]()\n\n---\n\n## Persistent Volume Claims for S3 Metadata\n\n### Database Storage\n\nWhile S3 provides object storage, metadata services require persistent volumes:\n\n**Resource Catalogue PostgreSQL:**\n```yaml\ndatabase:\n  persistence:\n    enabled: true\n    existingClaim: \"\"  # Auto-generated PVC\n    storageClass: managed-nfs-storage\n    size: \"100Gi\"\n```\n\nThe Resource Catalogue stores ISO 19115 metadata records in PostgreSQL, which requires persistent storage separate from S3.\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-228]()\n\n### Redis Queue Storage\n\nRedis queues for the registrar and harvester also use persistent volumes:\n\n```yaml\nredis:\n  usePassword: false\n  persistence:\n    existingClaim: \"\"  # Auto-generated PVC\n  master:\n    persistence:\n      existingClaim: \"\"  # Auto-generated PVC\n      storageClass: managed-nfs-storage\n      size: \"1Gi\"\n```\n\nThis ensures queue durability across pod restarts. For more on NFS storage architecture, see [NFS and Persistent Volumes](#7.3).\n\n**Sources:** [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:230-240]()\n\n---\n\n## Summary Table: Storage Tiers by Use Case\n\n| Use Case | Storage Tier | Endpoint | Access Type | Services |\n|----------|--------------|----------|-------------|----------|\n| EO Data Ingestion | CloudFerro eodata | `data.cloudferro.com` | Read-only | Data Access (Global), ADES |\n| Tile Caching | CloudFerro cache | `cf2.cloudferro.com:8080` | Read/Write | Data Access (Global) |\n| User Workspaces | MinIO | `minio.develop.eoepca.org` | Read/Write | Data Access (Workspace), ADES, Harvester |\n| Processing Outputs | MinIO | `minio.develop.eoepca.org` | Write | ADES Stage-out |\n| STAC Catalogs | MinIO | `minio.develop.eoepca.org` | Read/Write | Harvester, Registrar |\n| Metadata | PostgreSQL PVC | N/A (NFS-backed) | Read/Write | Resource Catalogue |\n| Queue Persistence | Redis PVC | N/A (NFS-backed) | Read/Write | Registrar, Harvester |\n\n**Sources:** [system/clusters/creodias/resource-management/hr-data-access.yaml:49-64](), [system/clusters/creodias/resource-management/hr-workspace-api.yaml:38-40](), [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:33-44]()"])</script><script>self.__next_f.push([1,"33:T4c07,"])</script><script>self.__next_f.push([1,"# Database Systems\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/resource-management/hr-data-access.yaml](system/clusters/creodias/resource-management/hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml](system/clusters/creodias/resource-management/hr-registration-api.yaml)\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/hr-resource-catalogue.yaml)\n- [system/clusters/creodias/resource-management/hr-workspace-api.yaml](system/clusters/creodias/resource-management/hr-workspace-api.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml)\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml](system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml)\n- [system/clusters/creodias/system/demo/hr-django-portal.yaml](system/clusters/creodias/system/demo/hr-django-portal.yaml)\n- [system/clusters/creodias/system/storage/hr-storage.yaml](system/clusters/creodias/system/storage/hr-storage.yaml)\n- [system/clusters/creodias/system/test/hr-cheese.yaml](system/clusters/creodias/system/test/hr-cheese.yaml)\n- [system/clusters/creodias/user-management/um-login-service.yaml](system/clusters/creodias/user-management/um-login-service.yaml)\n- [system/clusters/creodias/user-management/um-pdp-engine.yaml](system/clusters/creodias/user-management/um-pdp-engine.yaml)\n- [system/clusters/creodias/user-management/um-user-profile.yaml](system/clusters/creodias/user-management/um-user-profile.yaml)\n\n\u003c/details\u003e\n\n\n\nThis document describes the database systems deployed within the EOEPCA platform, including PostgreSQL for metadata storage and Redis for message queuing. These databases support the Resource Management building block, providing persistent storage for catalogue metadata and asynchronous processing pipelines.\n\nFor information about object storage (S3/MinIO) and NFS file systems, see [S3 Storage Architecture](#7.1) and [NFS and Persistent Volumes](#7.3). For details on how databases are used within specific services, see [Resource Catalogue](#5.2) and [Data Access Services](#5.1).\n\n## Database Architecture Overview\n\nThe EOEPCA platform deploys two primary database technologies:\n\n| Database Type | Technology | Primary Use Cases | Deployment Pattern |\n|---------------|------------|-------------------|-------------------|\n| Relational Database | PostgreSQL | ISO 19115 metadata storage, pycsw catalogue backend | Global instance + per-workspace instances |\n| In-Memory Database | Redis | Message queuing, registration pipelines, cache coordination | Global instance + per-workspace instances |\n\n```mermaid\ngraph TB\n    subgraph \"Global Resource Management (rm namespace)\"\n        DataAccess[\"Data Access Service\u003cbr/\u003evs-renderer, vs-registrar\"]\n        ResourceCat[\"Resource Catalogue\u003cbr/\u003epycsw\"]\n        Registrar[\"Registrar Service\u003cbr/\u003eMultiple Routes\"]\n        Harvester[\"Data Harvester\u003cbr/\u003eOpenSearch Poller\"]\n        \n        GlobalRedis[(\"Redis\u003cbr/\u003edata-access-redis-master\u003cbr/\u003ePort: 6379\")]\n        GlobalPostgres[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\u003cbr/\u003eDatabase: pycsw\")]\n    end\n    \n    subgraph \"User Workspace (e.g., eric-workspace)\"\n        WSDataAccess[\"Workspace Data Access\u003cbr/\u003evs-renderer, vs-registrar\"]\n        WSResourceCat[\"Workspace Resource Catalogue\u003cbr/\u003epycsw\"]\n        \n        WSRedis[(\"Redis\u003cbr/\u003evs-redis-master\u003cbr/\u003ePort: 6379\")]\n        WSPostgres[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\u003cbr/\u003eDatabase: pycsw\")]\n    end\n    \n    Harvester --\u003e|\"Enqueue Items\"| GlobalRedis\n    GlobalRedis --\u003e|\"Consume: register_queue\u003cbr/\u003eregister_collection_queue\u003cbr/\u003eregister_ades_queue\"| Registrar\n    Registrar --\u003e|\"Write Metadata\u003cbr/\u003erepository_database_uri\"| GlobalPostgres\n    ResourceCat --\u003e|\"Query Metadata\"| GlobalPostgres\n    DataAccess --\u003e|\"Seeding Queue\u003cbr/\u003eseed_queue\"| GlobalRedis\n    \n    WSDataAccess --\u003e|\"Register Items\"| WSRedis\n    WSRedis --\u003e|\"Consume: register_queue\"| WSDataAccess\n    WSDataAccess --\u003e|\"Write Metadata\"| WSPostgres\n    WSResourceCat --\u003e|\"Query Metadata\"| WSPostgres\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:886-948]()\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:19-27]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:93-164]()\n\n## PostgreSQL Databases\n\n### Resource Catalogue Database\n\nThe primary PostgreSQL deployment is `resource-catalogue-db`, which serves as the backend for pycsw-based catalogue services. This database stores ISO 19115/19139 metadata records for Earth Observation products, collections, applications, and services.\n\n**Database Connection Pattern:**\n\n```mermaid\ngraph LR\n    subgraph \"Service Connections\"\n        Registrar[\"Registrar Service\u003cbr/\u003eItemBackend\u003cbr/\u003eCollectionBackend\u003cbr/\u003eADESBackend\u003cbr/\u003eCWLBackend\"]\n        PyCSW[\"pycsw Service\u003cbr/\u003eOGC CSW/OpenSearch\"]\n    end\n    \n    PostgresDB[(\"PostgreSQL\u003cbr/\u003eHost: resource-catalogue-db\u003cbr/\u003ePort: 5432\u003cbr/\u003eDatabase: pycsw\u003cbr/\u003eUser: postgres\")]\n    \n    Registrar --\u003e|\"postgresql://postgres:mypass@\u003cbr/\u003eresource-catalogue-db/pycsw\"| PostgresDB\n    PyCSW --\u003e|\"Query/Insert Metadata\"| PostgresDB\n```\n\n**Connection String Format:**\n\nThe standard connection URI used throughout the platform follows this pattern:\n\n```\npostgresql://postgres:mypass@resource-catalogue-db/pycsw\n```\n\nThis connection string appears in multiple registrar backend configurations for different metadata types.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:891-902]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:110-120]()\n\n### Global Database Configuration\n\nThe global resource catalogue database is configured with performance tuning parameters suitable for metadata-heavy workloads:\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `volume_size` | 5Gi | Persistent volume size |\n| `shared_buffers` | 2GB | Memory for caching data |\n| `effective_cache_size` | 6GB | Planner estimate of kernel cache |\n| `maintenance_work_mem` | 512MB | Memory for maintenance operations |\n| `checkpoint_completion_target` | 0.9 | Spread checkpoint writes |\n| `wal_buffers` | 16MB | Write-ahead log buffer |\n| `work_mem` | 4MB | Memory per query operation |\n| `default_statistics_target` | 100 | Statistics sampling target |\n\nThese parameters optimize PostgreSQL for read-heavy catalogue queries while maintaining efficient write performance for metadata ingestion.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:19-31]()\n\n### Workspace Database Instances\n\nEach user workspace provisions its own isolated PostgreSQL database instance. The workspace-specific databases use identical connection patterns but are deployed within the workspace namespace:\n\n**Workspace Database Deployment:**\n\n```mermaid\ngraph TB\n    subgraph \"Workspace: eric-workspace\"\n        WSRegistrar[\"Registrar Routes:\u003cbr/\u003e- items\u003cbr/\u003e- collections\u003cbr/\u003e- ades\u003cbr/\u003e- application\u003cbr/\u003e- catalogue\"]\n        WSPyCSW[\"pycsw Service\"]\n        \n        WSPG[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\u003cbr/\u003eDatabase: pycsw\u003cbr/\u003eNamespace: eric-workspace\")]\n        \n        WSPVC[\"PersistentVolumeClaim\u003cbr/\u003eStorageClass: managed-nfs-storage-retain\"]\n    end\n    \n    WSRegistrar --\u003e|\"Write Metadata\"| WSPG\n    WSPyCSW --\u003e|\"Query CSW/OpenSearch\"| WSPG\n    WSPG --\u003e|\"Persist Data\"| WSPVC\n```\n\nWorkspace databases use the `managed-nfs-storage-retain` storage class to ensure data persistence even after workspace deletion.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:17-19]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-228]()\n\n### Registrar Backend Routes\n\nThe Registrar service connects to PostgreSQL through multiple specialized backend classes, each handling different metadata types:\n\n| Backend Class | Queue Name | Metadata Type | Purpose |\n|---------------|------------|---------------|---------|\n| `registrar_pycsw.backend.ItemBackend` | `register_queue` | STAC Items | EO product metadata |\n| `registrar_pycsw.backend.CollectionBackend` | `register_collection_queue` | STAC Collections | Collection-level metadata |\n| `registrar_pycsw.backend.ADESBackend` | `register_ades_queue` | ADES Metadata | Processing service registration |\n| `registrar_pycsw.backend.CWLBackend` | `register_application_queue` | CWL Applications | Application package metadata |\n| `registrar_pycsw.backend.CatalogueBackend` | `register_catalogue_queue` | Catalogue References | Federated catalogue links |\n| `registrar_pycsw.backend.JSONBackend` | `register_json_queue` | Generic JSON | Custom metadata records |\n| `registrar_pycsw.backend.XMLBackend` | `register_xml_queue` | Generic XML | ISO 19139 XML records |\n\nEach backend receives the `repository_database_uri` parameter pointing to the PostgreSQL database.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:888-947]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:100-164]()\n\n## Redis Instances\n\n### Message Queue Architecture\n\nRedis serves as the message broker for asynchronous metadata registration and data processing workflows. The platform deploys Redis in a master configuration without clustering.\n\n```mermaid\ngraph TB\n    subgraph \"Registration Pipeline\"\n        Harvester[\"Data Harvester\u003cbr/\u003eOpenSearch Client\"]\n        RegistrationAPI[\"Registration API\u003cbr/\u003eHTTP Endpoint\"]\n        \n        Redis[(\"Redis Master\u003cbr/\u003eHost: data-access-redis-master\u003cbr/\u003ePort: 6379\u003cbr/\u003ePassword: disabled\")]\n        \n        Registrar[\"Registrar Workers\u003cbr/\u003eConsume Queues\"]\n    end\n    \n    subgraph \"Queue Types\"\n        RegQueue[\"register_queue\u003cbr/\u003e(STAC Items)\"]\n        CollQueue[\"register_collection_queue\u003cbr/\u003e(Collections)\"]\n        AdesQueue[\"register_ades_queue\u003cbr/\u003e(ADES Services)\"]\n        AppQueue[\"register_application_queue\u003cbr/\u003e(Applications)\"]\n        CatQueue[\"register_catalogue_queue\u003cbr/\u003e(Catalogues)\"]\n        SeedQueue[\"seed_queue\u003cbr/\u003e(Cache Seeding)\"]\n        JSONQueue[\"register_json_queue\u003cbr/\u003e(JSON Metadata)\"]\n        XMLQueue[\"register_xml_queue\u003cbr/\u003e(XML Metadata)\"]\n    end\n    \n    Harvester --\u003e|\"Enqueue Discovered Items\"| Redis\n    RegistrationAPI --\u003e|\"Enqueue User Submissions\"| Redis\n    \n    Redis --\u003e RegQueue\n    Redis --\u003e CollQueue\n    Redis --\u003e AdesQueue\n    Redis --\u003e AppQueue\n    Redis --\u003e CatQueue\n    Redis --\u003e SeedQueue\n    Redis --\u003e JSONQueue\n    Redis --\u003e XMLQueue\n    \n    RegQueue --\u003e Registrar\n    CollQueue --\u003e Registrar\n    AdesQueue --\u003e Registrar\n    AppQueue --\u003e Registrar\n    CatQueue --\u003e Registrar\n    JSONQueue --\u003e Registrar\n    XMLQueue --\u003e Registrar\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:959-960]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:893-947]()\n\n### Global Redis Configuration\n\nThe global Redis instance is deployed as `data-access-redis-master` in the `rm` namespace:\n\n**Connection Parameters:**\n- **Host:** `data-access-redis-master`\n- **Port:** `6379`\n- **Authentication:** Disabled (`usePassword: false`)\n- **Persistence:** Enabled with NFS-backed PersistentVolumeClaim\n\nThe harvester service connects to this Redis instance to enqueue metadata discovered from external OpenSearch endpoints:\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:958-960]()\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml:36]()\n\n### Workspace Redis Instances\n\nEach workspace deploys an isolated Redis instance named `vs-redis-master` within the workspace namespace:\n\n| Configuration | Global Instance | Workspace Instance |\n|---------------|-----------------|-------------------|\n| Service Name | `data-access-redis-master` | `vs-redis-master` |\n| Namespace | `rm` | `\u003cworkspace-name\u003e` |\n| Authentication | Disabled | Disabled |\n| Clustering | Disabled | Disabled |\n| Persistence | Enabled | Enabled |\n| Storage Class | N/A | `managed-nfs-storage` |\n| Volume Size | Default | 1Gi |\n\nWorkspace Redis instances handle metadata registration for user-uploaded data and processing results within the workspace boundary.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:175-177]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:230-240]()\n\n### Queue-Based Registration Flow\n\nThe registration pipeline uses Redis queues to decouple metadata production from consumption:\n\n```mermaid\nsequenceDiagram\n    participant Harvester as Data Harvester\n    participant Redis as Redis Queue\n    participant Registrar as Registrar Worker\n    participant Postgres as PostgreSQL\n    participant PyCSW as pycsw Service\n    \n    Harvester-\u003e\u003eHarvester: Poll OpenSearch Endpoint\n    Harvester-\u003e\u003eHarvester: Post-process Metadata\n    Harvester-\u003e\u003eRedis: LPUSH register_queue\u003cbr/\u003e(STAC Item JSON)\n    \n    loop Worker Poll\n        Registrar-\u003e\u003eRedis: BRPOP register_queue\n        Redis--\u003e\u003eRegistrar: Pop STAC Item\n    end\n    \n    Registrar-\u003e\u003eRegistrar: Route to ItemBackend\n    Registrar-\u003e\u003ePostgres: INSERT INTO records\u003cbr/\u003e(ISO 19115 XML)\n    Postgres--\u003e\u003eRegistrar: Success\n    \n    opt Success Queue Configured\n        Registrar-\u003e\u003eRedis: LPUSH seed_queue\n    end\n    \n    PyCSW-\u003e\u003ePostgres: SELECT * FROM records\u003cbr/\u003eWHERE ...\n    Postgres--\u003e\u003ePyCSW: Metadata Records\n```\n\nThe `defaultSuccessQueue` configuration determines which queue receives successfully processed items for downstream operations like cache seeding.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:893-903]()\n\n## Database Persistence and Storage\n\n### Persistent Volume Configuration\n\nBoth PostgreSQL and Redis databases use PersistentVolumeClaims to ensure data durability:\n\n**Global Resource Catalogue Database:**\n\nThe global PostgreSQL instance uses a configurable volume size (default 5Gi) and can be backed by various storage classes.\n\n**Workspace Databases:**\n\n```mermaid\ngraph LR\n    subgraph \"Workspace Persistence\"\n        PostgresPVC[\"PostgreSQL PVC\u003cbr/\u003eStorageClass: managed-nfs-storage\u003cbr/\u003eSize: 100Gi\"]\n        RedisPVC[\"Redis PVC\u003cbr/\u003eStorageClass: managed-nfs-storage\u003cbr/\u003eSize: 1Gi\"]\n        \n        NFSStorage[\"NFS Server\u003cbr/\u003e192.168.123.14\"]\n    end\n    \n    PostgresPVC --\u003e NFSStorage\n    RedisPVC --\u003e NFSStorage\n```\n\nThe workspace databases use the `managed-nfs-storage` storage class, which provisions volumes from the centralized NFS server. This ensures data survives pod restarts and enables workspace data portability.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-resource-catalogue.yaml:20]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:219-240]()\n- [system/clusters/creodias/system/storage/hr-storage.yaml:19-20]()\n\n### Database Resource Limits\n\nServices connecting to databases have defined resource requests and limits:\n\n**Registrar Service Resources:**\n\n| Resource | Request | Limit |\n|----------|---------|-------|\n| CPU | 100m | Not specified |\n| Memory | 100Mi | Not specified |\n\n**Scheduler Service Resources:**\n\n| Resource | Request | Limit |\n|----------|---------|-------|\n| CPU | 100m | Not specified |\n| Memory | 100Mi | Not specified |\n\nThese conservative resource allocations reflect the lightweight nature of the queue-based registration pipeline.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:883-886]()\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:1140-1143]()\n\n## Connection String Patterns\n\n### PostgreSQL Connection URIs\n\nAll registrar backends use consistent PostgreSQL connection strings with the following pattern:\n\n```\npostgresql://\u003cusername\u003e:\u003cpassword\u003e@\u003chost\u003e/\u003cdatabase\u003e\n```\n\n**Example Configurations:**\n\n| Context | Connection String |\n|---------|-------------------|\n| Global Registrar | `postgresql://postgres:mypass@resource-catalogue-db/pycsw` |\n| Workspace Registrar | `postgresql://postgres:mypass@resource-catalogue-db/pycsw` |\n\nNote: The host `resource-catalogue-db` resolves differently depending on the Kubernetes namespace context. In the global `rm` namespace, it points to the global database service. In a workspace namespace, it resolves to the workspace-specific database.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:891]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:110]()\n\n### Redis Connection Configuration\n\nRedis connections use host/port configuration rather than URIs:\n\n```yaml\nredis:\n  host: data-access-redis-master  # or vs-redis-master for workspaces\n  port: 6379\n```\n\nThe Registration API service also connects to the global Redis instance by referencing the service name:\n\n**Registration API Redis Configuration:**\n\n```yaml\nredisServiceName: \"data-access-redis-master\"\n```\n\n**Sources:**\n- [system/clusters/creodias/resource-management/hr-data-access.yaml:958-960]()\n- [system/clusters/creodias/resource-management/hr-registration-api.yaml:36]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:175-177]()\n\n## Multi-Tenant Database Isolation\n\nThe platform implements database-level multi-tenancy by deploying separate database instances per workspace:\n\n```mermaid\ngraph TB\n    subgraph \"Global Namespace: rm\"\n        GlobalRedis[(\"Redis\u003cbr/\u003edata-access-redis-master\")]\n        GlobalPG[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\")]\n    end\n    \n    subgraph \"Workspace: alice-workspace\"\n        AliceRedis[(\"Redis\u003cbr/\u003evs-redis-master\")]\n        AlicePG[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\")]\n    end\n    \n    subgraph \"Workspace: bob-workspace\"\n        BobRedis[(\"Redis\u003cbr/\u003evs-redis-master\")]\n        BobPG[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\")]\n    end\n    \n    subgraph \"Workspace: charlie-workspace\"\n        CharlieRedis[(\"Redis\u003cbr/\u003evs-redis-master\")]\n        CharliePG[(\"PostgreSQL\u003cbr/\u003eresource-catalogue-db\")]\n    end\n    \n    style GlobalRedis fill:#f9f9f9\n    style GlobalPG fill:#f9f9f9\n    style AliceRedis fill:#e8f5e9\n    style AlicePG fill:#e8f5e9\n    style BobRedis fill:#e8f5e9\n    style BobPG fill:#e8f5e9\n    style CharlieRedis fill:#e8f5e9\n    style CharliePG fill:#e8f5e9\n```\n\nThis isolation ensures:\n- **Data Privacy:** Each user's metadata is stored in a dedicated database instance\n- **Resource Isolation:** Database load from one workspace does not impact others\n- **Independent Scaling:** Workspace databases can be sized according to user needs\n- **Namespace-Level Access Control:** Kubernetes RBAC restricts database access to workspace namespace\n\nThe Workspace API orchestrates database provisioning by instantiating HelmRelease templates with user-specific parameters.\n\n**Sources:**\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-data-access.yaml:1-269]()\n- [system/clusters/creodias/resource-management/rm-workspace-charts/template-hr-resource-catalogue.yaml:1-68]()"])</script><script>self.__next_f.push([1,"34:T3fdf,"])</script><script>self.__next_f.push([1,"# NFS and Persistent Volumes\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [bin/install-kubeseal.sh](bin/install-kubeseal.sh)\n- [bin/install-rke.sh](bin/install-rke.sh)\n- [creodias/.gitignore](creodias/.gitignore)\n- [creodias/.terraform/modules/modules.json](creodias/.terraform/modules/modules.json)\n- [creodias/README.md](creodias/README.md)\n- [creodias/deployCREODIAS.sh](creodias/deployCREODIAS.sh)\n- [creodias/eoepca.tf](creodias/eoepca.tf)\n- [creodias/eoepca.tfvars](creodias/eoepca.tfvars)\n- [creodias/modules/compute/main.tf](creodias/modules/compute/main.tf)\n- [creodias/modules/compute/nfs-setup.sh](creodias/modules/compute/nfs-setup.sh)\n- [creodias/modules/compute/nfs.tf](creodias/modules/compute/nfs.tf)\n- [creodias/modules/compute/outputs.tf](creodias/modules/compute/outputs.tf)\n- [creodias/modules/compute/variables.tf](creodias/modules/compute/variables.tf)\n- [creodias/modules/loadbalancer/main.tf](creodias/modules/loadbalancer/main.tf)\n- [creodias/terraform.tfstate](creodias/terraform.tfstate)\n- [creodias/terraform.tfstate.backup](creodias/terraform.tfstate.backup)\n- [creodias/variables.tf](creodias/variables.tf)\n- [kubernetes/cluster.7z](kubernetes/cluster.7z)\n- [kubernetes/create-cluster-config.sh](kubernetes/create-cluster-config.sh)\n- [system/clusters/creodias/system/demo/hr-django-portal.yaml](system/clusters/creodias/system/demo/hr-django-portal.yaml)\n- [system/clusters/creodias/system/storage/hr-storage.yaml](system/clusters/creodias/system/storage/hr-storage.yaml)\n- [system/clusters/creodias/system/test/hr-cheese.yaml](system/clusters/creodias/system/test/hr-cheese.yaml)\n- [system/clusters/creodias/user-management/um-login-service.yaml](system/clusters/creodias/user-management/um-login-service.yaml)\n- [system/clusters/creodias/user-management/um-pdp-engine.yaml](system/clusters/creodias/user-management/um-pdp-engine.yaml)\n- [system/clusters/creodias/user-management/um-user-profile.yaml](system/clusters/creodias/user-management/um-user-profile.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the Network File System (NFS) server infrastructure and PersistentVolume (PV) architecture used in the EOEPCA platform. It covers the provisioning of the NFS server via Terraform, the configuration of Kubernetes storage classes, and the usage patterns of PersistentVolumeClaims (PVCs) across platform services.\n\nFor information about S3-based object storage used for data access and workspace buckets, see [S3 Storage Architecture](#7.1). For database persistence mechanisms, see [Database Systems](#7.2).\n\n## NFS Server Overview\n\nThe EOEPCA platform deploys a dedicated NFS server as a centralized shared storage solution for services requiring ReadWriteMany access patterns. The NFS server is provisioned as a separate VM in the OpenStack infrastructure and provides persistent storage for user management, resource management, and processing services.\n\nThe NFS server is distinct from other storage backends in the platform:\n- **NFS**: Shared filesystem for configuration and metadata requiring multi-pod access\n- **S3/MinIO**: Object storage for large-scale EO data and workspace outputs\n- **PostgreSQL**: Structured metadata storage for catalogues and identity services\n\nSources: [system/clusters/creodias/system/storage/hr-storage.yaml:1-30](), [creodias/modules/compute/nfs.tf:1-70]()\n\n## Infrastructure Provisioning\n\n### NFS VM Deployment\n\nThe NFS server is provisioned through Terraform as part of the infrastructure deployment. The configuration creates an OpenStack compute instance with an attached block storage volume.\n\n```mermaid\ngraph TB\n    subgraph \"Terraform Configuration\"\n        TFVars[\"eoepca.tfvars\u003cbr/\u003enfs_disk_size=1024\"]\n        Variables[\"variables.tf\u003cbr/\u003enfs_disk_size default=500\"]\n        MainTF[\"eoepca.tf\u003cbr/\u003ecompute module\"]\n    end\n    \n    subgraph \"Compute Module\"\n        NFSTerraform[\"nfs.tf\u003cbr/\u003eopenstack_compute_instance_v2.eoepca_nfs\"]\n        NFSVolume[\"openstack_blockstorage_volume_v2.nfs_expansion\"]\n        NFSAttach[\"openstack_compute_volume_attach_v2.volume_attachment_nfs\"]\n    end\n    \n    subgraph \"OpenStack Resources\"\n        NFSVM[\"develop-nfs VM\u003cbr/\u003eIP: 192.168.123.14\u003cbr/\u003eFlavor: eo2.large\"]\n        BlockVolume[\"SSD Volume\u003cbr/\u003e1024 GB\u003cbr/\u003e/dev/sdb\"]\n    end\n    \n    subgraph \"Provisioning Script\"\n        SetupScript[\"nfs-setup.sh\u003cbr/\u003eat now + 2 minute\"]\n        Partition[\"fdisk /dev/sdb\u003cbr/\u003emkfs -t ext4 /dev/sdb1\"]\n        Mount[\"mount /dev/sdb1 /data\"]\n        NFSExports[\"/etc/exports configuration\"]\n    end\n    \n    TFVars --\u003e|defines| MainTF\n    Variables --\u003e|default value| MainTF\n    MainTF --\u003e|instantiates| NFSTerraform\n    NFSTerraform --\u003e|creates| NFSVM\n    NFSTerraform --\u003e|provisions| NFSVolume\n    NFSVolume --\u003e|creates| BlockVolume\n    NFSAttach --\u003e|attaches /dev/sdb| BlockVolume\n    NFSVM --\u003e|uploads| SetupScript\n    SetupScript --\u003e|partitions| Partition\n    Partition --\u003e|mounts| Mount\n    Mount --\u003e|configures| NFSExports\n```\n\nThe NFS instance is created with metadata indicating its role and dependencies:\n- `kubespray_groups`: \"nfs,no-floating\"\n- `ssh_user`: \"eouser\"\n- `depends_on`: references the block volume ID\n\nSources: [creodias/modules/compute/nfs.tf:1-70](), [creodias/eoepca.tfvars:56-56](), [creodias/variables.tf:231-233]()\n\n### Volume Configuration\n\nThe NFS server uses a two-tier storage approach:\n\n| Storage Tier | Purpose | Size | Type | Device |\n|--------------|---------|------|------|--------|\n| Root Volume | OS and system files | Flavor default | Image-based | /dev/sda |\n| Expansion Volume | NFS export data | 1024 GB (configurable) | SSD | /dev/sdb |\n\nThe expansion volume is defined in [creodias/modules/compute/nfs.tf:52-62]() as:\n\n```\nresource \"openstack_blockstorage_volume_v2\" \"nfs_expansion\"\n  name: \"${var.cluster_name}-nfs-expansion\"\n  size: \"${var.nfs_disk_size}\"\n  volume_type: \"SSD\"\n```\n\nSources: [creodias/modules/compute/nfs.tf:52-70](), [creodias/terraform.tfstate:220-259]()\n\n### NFS Server Setup Script\n\nThe NFS server is configured automatically via the `nfs-setup.sh` script, which is uploaded and executed via Terraform provisioners. The script runs as a delayed job (`at now + 2 minute`) to ensure the volume attachment is complete.\n\n**NFS Export Directory Structure:**\n\n```mermaid\ngraph TB\n    Root[\"/data\u003cbr/\u003e(mounted from /dev/sdb1)\"]\n    Userman[\"/data/userman\u003cbr/\u003enobody:nogroup 777\"]\n    Proc[\"/data/proc\u003cbr/\u003enobody:nogroup 777\"]\n    Resman[\"/data/resman\u003cbr/\u003enobody:nogroup 777\"]\n    Dynamic[\"/data/dynamic\u003cbr/\u003enobody:nogroup 777\"]\n    \n    Root --\u003e Userman\n    Root --\u003e Proc\n    Root --\u003e Resman\n    Root --\u003e Dynamic\n    \n    Userman --\u003e|\"NFS Export\"| UserPVC[\"eoepca-userman-pvc\u003cbr/\u003eStorageClass: eoepca-nfs\"]\n    Proc --\u003e|\"NFS Export\"| ProcPVC[\"Processing PVCs\u003cbr/\u003eStorageClass: eoepca-nfs\"]\n    Resman --\u003e|\"NFS Export\"| ResmanPVC[\"Resource Management PVCs\u003cbr/\u003eStorageClass: eoepca-nfs\"]\n    Dynamic --\u003e|\"NFS Export\"| WorkspacePVC[\"Workspace PVCs\u003cbr/\u003eStorageClass: eoepca-nfs\"]\n```\n\nEach directory is exported with NFS options: `rw,no_root_squash,no_subtree_check`\n\nThe setup script performs the following operations:\n1. Installs `nfs-kernel-server` package\n2. Creates partition on `/dev/sdb` using `fdisk`\n3. Formats partition as ext4 filesystem\n4. Mounts `/dev/sdb1` to `/data`\n5. Adds mount to `/etc/fstab` for persistence\n6. Creates export subdirectories: userman, proc, resman, dynamic\n7. Configures `/etc/exports` with appropriate NFS export rules\n8. Restarts `nfs-kernel-server` service\n\nSources: [creodias/modules/compute/nfs-setup.sh:1-44](), [creodias/modules/compute/nfs.tf:34-44]()\n\n## Kubernetes Storage Configuration\n\n### Storage HelmRelease\n\nThe NFS integration with Kubernetes is configured through the storage HelmRelease, which deploys the necessary StorageClasses and NFS provisioners.\n\n**Storage Configuration Structure:**\n\n```mermaid\ngraph LR\n    subgraph \"HelmRelease\"\n        StorageChart[\"storage chart v1.0.1\"]\n        NFSConfig[\"nfs.server.address:\u003cbr/\u003e192.168.123.14\"]\n    end\n    \n    subgraph \"StorageClasses\"\n        UserManSC[\"userman domain\u003cbr/\u003estorageClass: eoepca-nfs\"]\n        ProcSC[\"proc domain\u003cbr/\u003estorageClass: eoepca-nfs\u003cbr/\u003eenabled: true\"]\n        ResmanSC[\"resman domain\u003cbr/\u003estorageClass: eoepca-nfs\"]\n    end\n    \n    subgraph \"NFS Exports\"\n        UserExport[\"192.168.123.14:/data/userman\"]\n        ProcExport[\"192.168.123.14:/data/proc\"]\n        ResmanExport[\"192.168.123.14:/data/resman\"]\n    end\n    \n    StorageChart --\u003e|defines| NFSConfig\n    NFSConfig --\u003e|creates| UserManSC\n    NFSConfig --\u003e|creates| ProcSC\n    NFSConfig --\u003e|creates| ResmanSC\n    UserManSC --\u003e|provisions from| UserExport\n    ProcSC --\u003e|provisions from| ProcExport\n    ResmanSC --\u003e|provisions from| ResmanExport\n```\n\nThe storage configuration is defined in [system/clusters/creodias/system/storage/hr-storage.yaml:15-29]():\n\n```yaml\nvalues:\n  host:\n    enabled: false\n  nfs:\n    server:\n      address: \"192.168.123.14\"\n  domain:\n    resman:\n      storageClass: eoepca-nfs\n    proc:\n      enabled: true\n      storageClass: eoepca-nfs\n    userman:\n      storageClass: eoepca-nfs\n```\n\nThe NFS server address matches the IP output from Terraform: [creodias/terraform.tfstate:79-82]()\n\nSources: [system/clusters/creodias/system/storage/hr-storage.yaml:1-30](), [creodias/eoepca.tf:143-145]()\n\n### StorageClass Provisioning\n\nThe `eoepca-nfs` StorageClass is created for each domain (userman, proc, resman) and enables dynamic provisioning of PersistentVolumes. When a PersistentVolumeClaim is created with this StorageClass, the NFS provisioner automatically creates a subdirectory on the NFS export and binds it to the claim.\n\n**StorageClass Characteristics:**\n- **Provisioner**: NFS CSI driver or external provisioner\n- **Access Modes**: ReadWriteMany (RWX) - multiple pods can mount concurrently\n- **Reclaim Policy**: Typically Retain or Delete based on configuration\n- **Volume Binding Mode**: Immediate or WaitForFirstConsumer\n\nSources: [system/clusters/creodias/system/storage/hr-storage.yaml:15-29]()\n\n## PersistentVolumeClaim Usage Patterns\n\n### User Management Services\n\nThe User Management building block uses a shared PVC named `eoepca-userman-pvc` across multiple components. This PVC is provisioned from the `eoepca-nfs` StorageClass and mounted by all Gluu-based services.\n\n**User Management PVC Topology:**\n\n```mermaid\ngraph TB\n    subgraph \"NFS Server\"\n        NFSExport[\"/data/userman\u003cbr/\u003e192.168.123.14\"]\n    end\n    \n    subgraph \"Kubernetes Storage\"\n        StorageClass[\"StorageClass: eoepca-nfs\"]\n        PVC[\"PersistentVolumeClaim\u003cbr/\u003eeoepca-userman-pvc\u003cbr/\u003ecreate: false\"]\n        PV[\"PersistentVolume\u003cbr/\u003enfs: 192.168.123.14:/data/userman\"]\n    end\n    \n    subgraph \"Login Service Components\"\n        OpenDJ[\"opendj\u003cbr/\u003evolumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n        OxAuth[\"oxauth\u003cbr/\u003evolumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n        OxTrust[\"oxtrust\u003cbr/\u003evolumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n        Config[\"config\u003cbr/\u003evolumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n    end\n    \n    subgraph \"Other UM Services\"\n        PDPEngine[\"pdp-engine\u003cbr/\u003evolumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n        UserProfile[\"user-profile\u003cbr/\u003evolumeClaim.name:\u003cbr/\u003eeoepca-userman-pvc\"]\n    end\n    \n    NFSExport --\u003e|exported via NFS| PV\n    StorageClass --\u003e|provisions| PV\n    PV --\u003e|bound to| PVC\n    PVC --\u003e|mounted by| OpenDJ\n    PVC --\u003e|mounted by| OxAuth\n    PVC --\u003e|mounted by| OxTrust\n    PVC --\u003e|mounted by| Config\n    PVC --\u003e|mounted by| PDPEngine\n    PVC --\u003e|mounted by| UserProfile\n```\n\nThe PVC configuration shows `create: false` in the HelmRelease values, indicating the PVC is pre-created rather than managed by individual service Helm charts. This ensures all User Management services share the same underlying storage.\n\nExample from [system/clusters/creodias/user-management/um-login-service.yaml:16-29]():\n\n```yaml\nvolumeClaim:\n  name: eoepca-userman-pvc\n  create: false\nconfig:\n  volumeClaim:\n    name: eoepca-userman-pvc\nopendj:\n  volumeClaim:\n    name: eoepca-userman-pvc\noxauth:\n  volumeClaim:\n    name: eoepca-userman-pvc\noxtrust:\n  volumeClaim:\n    name: eoepca-userman-pvc\n```\n\nSources: [system/clusters/creodias/user-management/um-login-service.yaml:16-47](), [system/clusters/creodias/user-management/um-pdp-engine.yaml:22-24](), [system/clusters/creodias/user-management/um-user-profile.yaml:19-21]()\n\n### Resource Management Services\n\nResource Management services use the `eoepca-nfs` StorageClass backed by the `/data/resman` NFS export. This storage is used for:\n- Resource catalogue metadata persistence\n- Data access service cache\n- Workspace API configuration\n- Template storage for workspace provisioning\n\n### Processing Services\n\nThe Processing domain (`proc`) uses NFS storage for:\n- ADES workflow state and job metadata\n- Application Hub user notebooks and environment state\n- PDE shared development resources\n\nThe processing storage is explicitly enabled in the storage configuration: [system/clusters/creodias/system/storage/hr-storage.yaml:24-26]()\n\nSources: [system/clusters/creodias/system/storage/hr-storage.yaml:15-29]()\n\n## Access and Connectivity\n\n### Network Access Requirements\n\nThe NFS server must be accessible from all Kubernetes worker nodes within the internal network. The NFS instance is configured with:\n- **Internal IP**: 192.168.123.14 (no floating IP)\n- **Security Group**: `develop-k8s` allowing internal cluster communication\n- **NFS Ports**: TCP/UDP 2049, 111 (rpcbind), 20048 (mountd)\n\nThe NFS VM metadata includes `no-floating` to indicate it should not be assigned a public IP address:\n[creodias/modules/compute/nfs.tf:21-21]()\n\n### Bastion Access for Administration\n\nDirect SSH access to the NFS server requires connection through the bastion host. The Terraform provisioner demonstrates this pattern:\n\n```\nconnection {\n  type         = \"ssh\"\n  user         = \"${var.ssh_user}\"\n  host         = \"${self.access_ip_v4}\"\n  bastion_host = var.bastion_fips[0]\n}\n```\n\nFor operational procedures related to bastion access, see [Monitoring and Troubleshooting](#11.2).\n\nSources: [creodias/modules/compute/nfs.tf:26-32](), [creodias/terraform.tfstate:450-515]()\n\n## Storage Capacity and Scaling\n\n### Initial Capacity\n\nThe default NFS expansion volume size is configurable through the `nfs_disk_size` Terraform variable:\n- **Default**: 500 GB ([creodias/variables.tf:231-233]())\n- **Current Deployment**: 1024 GB ([creodias/eoepca.tfvars:56-56]())\n\n### Volume Expansion\n\nTo increase NFS storage capacity:\n\n1. Use OpenStack block storage volume expansion API\n2. Extend the filesystem using SSH access through bastion\n3. Update Terraform state to reflect new size\n\nThe volume is attached as `/dev/sdb` and can be expanded without VM restart if the OpenStack environment supports online volume expansion.\n\nSources: [creodias/modules/compute/nfs.tf:52-62](), [creodias/eoepca.tfvars:56-56](), [creodias/variables.tf:231-233]()\n\n## Disaster Recovery Considerations\n\n### Backup Strategy\n\nThe NFS exports contain critical platform state including:\n- User identity and authorization data (`/data/userman`)\n- Resource catalogue metadata (`/data/resman`)\n- Processing workflow state (`/data/proc`)\n\nBackup procedures should:\n1. Use OpenStack volume snapshots for point-in-time recovery\n2. Implement regular rsync or NFS-level replication\n3. Maintain metadata exports separately from large data objects (which use S3)\n\n### PVC Reclaim Policy\n\nThe PVC reclaim policy determines what happens to PersistentVolumes when their claims are deleted. Common configurations:\n- **Retain**: PV is not deleted, requires manual cleanup (safer for production)\n- **Delete**: PV and underlying storage are automatically deleted (used for dynamic workspaces)\n\nSources: [creodias/modules/compute/nfs-setup.sh:1-44]()\n\n## Terraform Output Reference\n\nThe NFS server IP address is exposed as a Terraform output for consumption by downstream configuration:\n\n[creodias/eoepca.tf:143-145]()\n```hcl\noutput \"nfs_ip_address\" {\n  value = \"${module.compute.nfs_ip_address}\"\n}\n```\n\nThis output is referenced in the storage HelmRelease configuration and can be queried via:\n```bash\nterraform output -state=creodias/terraform.tfstate nfs_ip_address\n```\n\nSources: [creodias/eoepca.tf:143-145](), [creodias/terraform.tfstate:79-82]()"])</script><script>self.__next_f.push([1,"35:T4c6f,"])</script><script>self.__next_f.push([1,"# Infrastructure\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [bin/install-kubeseal.sh](bin/install-kubeseal.sh)\n- [bin/install-rke.sh](bin/install-rke.sh)\n- [creodias/.gitignore](creodias/.gitignore)\n- [creodias/.terraform/modules/modules.json](creodias/.terraform/modules/modules.json)\n- [creodias/README.md](creodias/README.md)\n- [creodias/deployCREODIAS.sh](creodias/deployCREODIAS.sh)\n- [creodias/eoepca.tf](creodias/eoepca.tf)\n- [creodias/eoepca.tfvars](creodias/eoepca.tfvars)\n- [creodias/modules/compute/main.tf](creodias/modules/compute/main.tf)\n- [creodias/modules/compute/nfs-setup.sh](creodias/modules/compute/nfs-setup.sh)\n- [creodias/modules/compute/nfs.tf](creodias/modules/compute/nfs.tf)\n- [creodias/modules/compute/outputs.tf](creodias/modules/compute/outputs.tf)\n- [creodias/modules/compute/variables.tf](creodias/modules/compute/variables.tf)\n- [creodias/modules/loadbalancer/main.tf](creodias/modules/loadbalancer/main.tf)\n- [creodias/terraform.tfstate](creodias/terraform.tfstate)\n- [creodias/terraform.tfstate.backup](creodias/terraform.tfstate.backup)\n- [creodias/variables.tf](creodias/variables.tf)\n- [kubernetes/cluster.7z](kubernetes/cluster.7z)\n- [kubernetes/create-cluster-config.sh](kubernetes/create-cluster-config.sh)\n- [minikube/README.md](minikube/README.md)\n- [system/clusters/README.md](system/clusters/README.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the underlying infrastructure layer that supports the EOEPCA platform deployment. It covers the physical and virtual resources, networking topology, and orchestration mechanisms required to run the Kubernetes cluster and associated services.\n\nThe infrastructure can be provisioned in two primary deployment models: cloud-based deployment on OpenStack CREODIAS infrastructure (for production and development environments), or local deployment using Minikube/k3s (for testing and development). Both approaches leverage Infrastructure-as-Code principles to ensure reproducible, version-controlled deployments.\n\nFor detailed procedures on provisioning Kubernetes clusters, see [Kubernetes Cluster Setup](#8.1). For Terraform module documentation, see [Terraform Infrastructure as Code](#8.2). For network topology details, see [Network Architecture](#8.3).\n\n---\n\n## Infrastructure Deployment Models\n\nEOEPCA supports two infrastructure deployment patterns:\n\n| Deployment Model | Use Case | Orchestration | Platform |\n|-----------------|----------|---------------|----------|\n| **Cloud (Production)** | Production, development clusters | Terraform + RKE | OpenStack CREODIAS |\n| **Local (Development)** | Testing, local development | Setup scripts | Minikube or k3s |\n\nThe cloud deployment creates a full multi-node cluster with dedicated networking, load balancing, and persistent storage. The local deployment provides a single-node or minimal cluster for rapid iteration and testing.\n\n**Sources:** [README.md:73-96](), [minikube/README.md:1-52](), [creodias/README.md:1-129]()\n\n---\n\n## Infrastructure-as-Code Architecture\n\n### Terraform Module Structure\n\nThe EOEPCA infrastructure is defined using Terraform with a modular architecture. The root configuration orchestrates five specialized modules:\n\n```mermaid\ngraph TB\n    root[\"eoepca.tf\u003cbr/\u003eRoot Configuration\"]\n    \n    root --\u003e network[\"module.network\u003cbr/\u003emodules/network\"]\n    root --\u003e ips[\"module.ips\u003cbr/\u003emodules/ips\"]\n    root --\u003e compute[\"module.compute\u003cbr/\u003emodules/compute\"]\n    root --\u003e loadbalancer[\"module.loadbalancer\u003cbr/\u003emodules/loadbalancer\"]\n    \n    network --\u003e|\"router_id\"| ips\n    network --\u003e|\"network_id\"| compute\n    network --\u003e|\"network_id\"| loadbalancer\n    \n    ips --\u003e|\"floating_ips\"| compute\n    \n    compute --\u003e|\"k8s_node_ips\"| loadbalancer\n    compute --\u003e|\"k8s_secgroup_id\"| loadbalancer\n    \n    subgraph \"Terraform State Outputs\"\n        outputs[\"bastion_fips\u003cbr/\u003ek8s_master_ips\u003cbr/\u003ek8s_node_ips\u003cbr/\u003eloadbalancer_fips\u003cbr/\u003enfs_ip_address\"]\n    end\n    \n    root --\u003e outputs\n```\n\n**Terraform Module Structure**\n\nThe root module at [creodias/eoepca.tf:1-146]() coordinates infrastructure provisioning by declaring module dependencies and passing outputs between modules. Configuration values are defined in [creodias/eoepca.tfvars:1-57]() and [creodias/variables.tf:1-233]().\n\n**Sources:** [creodias/eoepca.tf:6-93](), [creodias/.terraform/modules/modules.json:1]()\n\n### OpenStack Provider Configuration\n\nTerraform uses the OpenStack provider configured through `clouds.yaml` file. The provider is initialized in [creodias/eoepca.tf:1-4]() with Octavia load balancing support enabled. The deployment script [creodias/deployCREODIAS.sh:1-52]() requires the `OS_CLOUD` environment variable to specify which cloud configuration from `clouds.yaml` to use.\n\n**Sources:** [creodias/eoepca.tf:1-4](), [creodias/deployCREODIAS.sh:14-52](), [creodias/README.md:29-53]()\n\n---\n\n## Core Infrastructure Components\n\n### Compute Resources\n\nThe compute module provisions multiple VM types with specific roles:\n\n```mermaid\ngraph TB\n    subgraph \"Bastion Host\"\n        bastion[\"openstack_compute_instance_v2.bastion\u003cbr/\u003eFlavor: eo1.xsmall\u003cbr/\u003eImage: Ubuntu 18.04 LTS\"]\n    end\n    \n    subgraph \"Kubernetes Control Plane\"\n        master[\"openstack_compute_instance_v2.k8s_master_no_floating_ip\u003cbr/\u003eFlavor: eo2.large\u003cbr/\u003eRoles: etcd,kube-master,k8s-cluster,vault\"]\n    end\n    \n    subgraph \"Kubernetes Worker Nodes\"\n        nodes[\"openstack_compute_instance_v2.k8s_node_no_floating_ip\u003cbr/\u003eCount: 6\u003cbr/\u003eFlavor: eo2.xlarge\u003cbr/\u003eRoles: kube-node,k8s-cluster\"]\n    end\n    \n    subgraph \"Shared Storage\"\n        nfs[\"openstack_compute_instance_v2.eoepca_nfs\u003cbr/\u003eFlavor: eo2.large\u003cbr/\u003eVolume: openstack_blockstorage_volume_v2.nfs_expansion\"]\n        nfs_vol[\"SSD Volume: 1024 GB\u003cbr/\u003eMount: /dev/sdb -\u003e /data\"]\n        nfs --\u003e nfs_vol\n    end\n    \n    bastion --\u003e|\"SSH Proxy\"| master\n    bastion --\u003e|\"SSH Proxy\"| nodes\n    bastion --\u003e|\"SSH Proxy\"| nfs\n```\n\n**Virtual Machine Deployment**\n\nThe compute module defines VM resources in [creodias/modules/compute/main.tf:1-967]():\n\n- **Bastion host** (`bastion`): Single point of SSH access with floating IP, configured at [creodias/modules/compute/main.tf:116-141]()\n- **Master nodes** (`k8s_master_no_floating_ip`): Control plane nodes without floating IPs, defined at [creodias/modules/compute/main.tf:384-428]()\n- **Worker nodes** (`k8s_node_no_floating_ip`): Compute nodes for workload execution, defined at [creodias/modules/compute/main.tf:687-731]()\n- **NFS server** (`eoepca_nfs`): Dedicated storage server with attached block volume, configured at [creodias/modules/compute/nfs.tf:1-70]()\n\nInstance counts and flavors are configured via variables in [creodias/eoepca.tfvars:18-48]():\n- `number_of_bastions = 1`\n- `number_of_k8s_masters_no_floating_ip = 1`\n- `number_of_k8s_nodes_no_floating_ip = 6`\n- `nfs_disk_size = 1024`\n\n**Sources:** [creodias/modules/compute/main.tf:116-141](), [creodias/modules/compute/main.tf:384-428](), [creodias/modules/compute/main.tf:687-731](), [creodias/modules/compute/nfs.tf:1-70](), [creodias/eoepca.tfvars:18-57]()\n\n### NFS Storage Provisioning\n\nThe NFS server is automatically configured through a provisioning script that:\n\n1. Installs `nfs-kernel-server` package\n2. Creates partition on attached `/dev/sdb` volume\n3. Formats and mounts the partition at `/data`\n4. Exports subdirectories: `userman`, `proc`, `resman`, `dynamic`\n\nThe provisioning logic is implemented in [creodias/modules/compute/nfs-setup.sh:1-44]() and executed via remote-exec provisioner in [creodias/modules/compute/nfs.tf:26-44](). The NFS instance definition includes a connection block at [creodias/modules/compute/nfs.tf:26-32]() that uses the bastion host as a jump server.\n\n**Sources:** [creodias/modules/compute/nfs.tf:1-70](), [creodias/modules/compute/nfs-setup.sh:1-44]()\n\n---\n\n## Network Architecture\n\n### Network Topology\n\n```mermaid\ngraph TB\n    internet[\"Internet\u003cbr/\u003e0.0.0.0/0\"]\n    \n    subgraph \"External Network\"\n        external[\"external3\u003cbr/\u003eFloating IP Pool\u003cbr/\u003eUUID: 31d7e67a-b30a\"]\n        bastion_fip[\"Bastion Floating IP\u003cbr/\u003e185.52.192.185\"]\n        lb_fip[\"LoadBalancer Floating IP\u003cbr/\u003e185.52.192.231\"]\n    end\n    \n    subgraph \"OpenStack Router\"\n        router[\"openstack_networking_router_v2.k8s\u003cbr/\u003eName: develop\"]\n    end\n    \n    subgraph \"Internal Network: 192.168.123.0/24\"\n        subnet[\"openstack_networking_subnet_v2\u003cbr/\u003eCIDR: 192.168.123.0/24\u003cbr/\u003eDNS: 8.8.8.8, 8.8.4.4\"]\n        \n        bastion_int[\"Bastion\u003cbr/\u003e192.168.123.24\"]\n        master[\"Master\u003cbr/\u003e192.168.123.15\"]\n        workers[\"Workers\u003cbr/\u003e192.168.123.16-22\"]\n        nfs[\"NFS\u003cbr/\u003e192.168.123.14\"]\n        lb_vip[\"LB VIP\u003cbr/\u003e192.168.123.x\"]\n    end\n    \n    internet \u003c--\u003e external\n    external \u003c--\u003e bastion_fip\n    external \u003c--\u003e lb_fip\n    \n    bastion_fip \u003c--\u003e router\n    lb_fip \u003c--\u003e router\n    router \u003c--\u003e subnet\n    \n    subnet --- bastion_int\n    subnet --- master\n    subnet --- workers\n    subnet --- nfs\n    subnet --- lb_vip\n    \n    bastion_int -.SSH Proxy.-\u003e master\n    bastion_int -.SSH Proxy.-\u003e workers\n    bastion_int -.SSH Proxy.-\u003e nfs\n```\n\n**Network Infrastructure**\n\nThe network module creates:\n- **Internal network** (`develop`): Private network with CIDR `192.168.123.0/24` [creodias/eoepca.tfvars:8-11]()\n- **Router**: Connects internal network to external network, provides NAT gateway\n- **Floating IP Pool** (`external3`): Source of public IPs for bastion and load balancer\n\n**Sources:** [creodias/eoepca.tfvars:3-11](), [creodias/terraform.tfstate:91-94]()\n\n### Security Groups\n\nThe compute module defines security groups to control traffic:\n\n| Security Group | Resource | Rules |\n|----------------|----------|-------|\n| `${cluster_name}-k8s-master` | `openstack_networking_secgroup_v2.k8s_master` | Port 6443 (Kubernetes API) from allowed IPs |\n| `${cluster_name}-bastion` | `openstack_networking_secgroup_v2.bastion` | Port 22 (SSH) from `0.0.0.0/0` |\n| `${cluster_name}-k8s` | `openstack_networking_secgroup_v2.k8s` | Intra-cluster traffic, SSH from allowed IPs |\n| `${cluster_name}-k8s-worker` | `openstack_networking_secgroup_v2.worker` | Ports 30000-32767 (NodePort range) |\n| `${cluster_name}-lb` | `openstack_networking_secgroup_v2.lb` | Ports 80, 443 (HTTP/HTTPS) |\n\nSecurity group definitions are in [creodias/modules/compute/main.tf:14-96]() and [creodias/modules/loadbalancer/main.tf:2-36]().\n\n**Sources:** [creodias/modules/compute/main.tf:14-96](), [creodias/modules/loadbalancer/main.tf:2-36]()\n\n### Load Balancer Configuration\n\nThe load balancer module creates an Octavia load balancer with the following resources:\n\n```mermaid\ngraph TB\n    fip[\"Floating IP\u003cbr/\u003e185.52.192.231\"]\n    \n    lb[\"openstack_lb_loadbalancer_v2.k8s\u003cbr/\u003eVIP Network: develop\"]\n    \n    listener_https[\"openstack_lb_listener_v2.https\u003cbr/\u003ePort: 443\u003cbr/\u003eProtocol: HTTPS\"]\n    listener_http[\"openstack_lb_listener_v2.http\u003cbr/\u003ePort: 80\u003cbr/\u003eProtocol: HTTP\"]\n    \n    pool_https[\"openstack_lb_pool_v2.https\u003cbr/\u003eMethod: ROUND_ROBIN\u003cbr/\u003ePersistence: SOURCE_IP\"]\n    pool_http[\"openstack_lb_pool_v2.http\u003cbr/\u003eMethod: ROUND_ROBIN\u003cbr/\u003ePersistence: SOURCE_IP\"]\n    \n    members_https[\"openstack_lb_members_v2.https\u003cbr/\u003eBackend Port: 31443\"]\n    members_http[\"openstack_lb_members_v2.http\u003cbr/\u003eBackend Port: 31080\"]\n    \n    workers[\"Worker Nodes\u003cbr/\u003e192.168.123.16\u003cbr/\u003e192.168.123.6\u003cbr/\u003e192.168.123.19\u003cbr/\u003e192.168.123.13\u003cbr/\u003e192.168.123.8\u003cbr/\u003e192.168.123.22\"]\n    \n    fip --\u003e lb\n    lb --\u003e listener_https\n    lb --\u003e listener_http\n    listener_https --\u003e pool_https\n    listener_http --\u003e pool_http\n    pool_https --\u003e members_https\n    pool_http --\u003e members_http\n    members_https --\u003e workers\n    members_http --\u003e workers\n```\n\n**Load Balancer Resources**\n\nThe load balancer forwards public traffic to Kubernetes ingress NodePorts:\n- **HTTPS (443)**  Worker nodes port **31443** [creodias/modules/loadbalancer/main.tf:71-82]()\n- **HTTP (80)**  Worker nodes port **31080** [creodias/modules/loadbalancer/main.tf:83-93]()\n\nMembers are dynamically generated from worker node IPs using `for_each` loops [creodias/modules/loadbalancer/main.tf:95-122]() and [creodias/modules/loadbalancer/main.tf:124-151]().\n\n**Sources:** [creodias/modules/loadbalancer/main.tf:38-184](), [creodias/README.md:76-97]()\n\n---\n\n## Infrastructure Provisioning Workflow\n\n### Deployment Process\n\n```mermaid\nsequenceDiagram\n    actor Operator\n    participant Script as deployCREODIAS.sh\n    participant Terraform\n    participant OpenStack as OpenStack API\n    participant Bastion\n    participant NFS\n    \n    Operator-\u003e\u003eScript: ./deployCREODIAS.sh\n    Script-\u003e\u003eScript: Check OS_CLOUD env var\n    Script-\u003e\u003eTerraform: terraform init\n    \n    Note over Script,Terraform: Phase 1: Keypair Creation\n    Script-\u003e\u003eTerraform: terraform apply\u003cbr/\u003e-target=module.compute.openstack_compute_keypair_v2.k8s\n    Terraform-\u003e\u003eOpenStack: Create keypair kubernetes-{cluster_name}\n    OpenStack--\u003e\u003eTerraform: Keypair created\n    Script-\u003e\u003eScript: Poll for keypair readiness\n    \n    Note over Script,Terraform: Phase 2: Full Infrastructure\n    Script-\u003e\u003eTerraform: terraform apply\u003cbr/\u003e-var-file=eoepca.tfvars\n    \n    Terraform-\u003e\u003eOpenStack: Create network resources\n    Terraform-\u003e\u003eOpenStack: Create security groups\n    Terraform-\u003e\u003eOpenStack: Create floating IPs\n    Terraform-\u003e\u003eOpenStack: Create VMs (bastion, master, workers)\n    Terraform-\u003e\u003eOpenStack: Create NFS volume\n    Terraform-\u003e\u003eOpenStack: Attach volume to NFS instance\n    Terraform-\u003e\u003eOpenStack: Create load balancer\n    Terraform-\u003e\u003eOpenStack: Configure LB listeners and pools\n    \n    OpenStack--\u003e\u003eTerraform: Infrastructure provisioned\n    \n    Terraform-\u003e\u003eBastion: Remote-exec provisioner (if configured)\n    Terraform-\u003e\u003eNFS: Transfer nfs-setup.sh\n    Terraform-\u003e\u003eNFS: Schedule nfs-setup.sh execution\n    \n    NFS-\u003e\u003eNFS: Install nfs-kernel-server\n    NFS-\u003e\u003eNFS: Partition and format /dev/sdb\n    NFS-\u003e\u003eNFS: Mount volume to /data\n    NFS-\u003e\u003eNFS: Create export directories\n    NFS-\u003e\u003eNFS: Configure NFS exports\n    NFS-\u003e\u003eNFS: Restart NFS service\n    \n    Terraform--\u003e\u003eScript: Output infrastructure details\n    Script--\u003e\u003eOperator: Display outputs:\u003cbr/\u003ebastion_fips, k8s_master_ips,\u003cbr/\u003ek8s_node_ips, loadbalancer_fips\n```\n\n**Provisioning Phases**\n\nThe [creodias/deployCREODIAS.sh:1-52]() script orchestrates a two-phase deployment:\n\n1. **Keypair Phase** [creodias/deployCREODIAS.sh:30-45](): Creates SSH keypair first to ensure it's available before VM provisioning. Uses targeted apply: `terraform apply -target=module.compute.openstack_compute_keypair_v2.k8s`\n\n2. **Full Infrastructure Phase** [creodias/deployCREODIAS.sh:47-51](): Provisions all remaining resources with `terraform apply -var-file=eoepca.tfvars`\n\nThe script uses `TF_LOG_PATH` environment variable to separate debug logs for each phase [creodias/deployCREODIAS.sh:30-49]().\n\n**Sources:** [creodias/deployCREODIAS.sh:1-52](), [creodias/README.md:58-68]()\n\n### Terraform State Management\n\nTerraform maintains infrastructure state in `terraform.tfstate` [creodias/terraform.tfstate:1-100](). The state captures:\n- Resource IDs and metadata\n- IP address allocations\n- Resource dependencies\n- Module outputs\n\nState outputs are consumed by subsequent deployment steps, particularly Kubernetes cluster configuration via [kubernetes/create-cluster-config.sh:1-117](). This script queries Terraform outputs to generate RKE cluster configuration:\n\n```bash\nmaster_nodes=$(terraform output -state=../creodias/terraform.tfstate -json | jq -r '.k8s_master_ips.value[]')\nworker_nodes=$(terraform output -state=../creodias/terraform.tfstate -json | jq -r '.k8s_node_ips.value[]')\nbastion=$(terraform output -state=../creodias/terraform.tfstate -json | jq -r '.bastion_fips.value[]')\n```\n\n**Sources:** [creodias/terraform.tfstate:1-95](), [kubernetes/create-cluster-config.sh:20-80]()\n\n---\n\n## Accessing the Infrastructure\n\n### Bastion Host Access Pattern\n\nAll administrative access to cluster VMs is routed through the bastion host, which serves as the sole entry point with a public floating IP. The bastion provides two access methods:\n\n**Direct SSH Jump**\n```bash\nssh -J eouser@\u003cbastion-fip\u003e eouser@\u003cnode-internal-ip\u003e\n```\n\n**VPN Tunnel via sshuttle**\n\nThe helper script [bin/bastion-vpn.sh]() establishes a VPN tunnel using `sshuttle`, routing local traffic to the internal network CIDR. This enables direct interaction with cluster services using `kubectl` without repeated SSH jumps.\n\nThe script extracts bastion IP and subnet CIDR from Terraform outputs:\n```bash\nsshuttle -r eouser@$(terraform output bastion_fips) $(terraform output subnet_cidr)\n```\n\n**Sources:** [creodias/README.md:98-120](), [README.md:100-119]()\n\n---\n\n## Local Development Infrastructure\n\n### Minikube Deployment\n\nFor local development and testing, [minikube/setup-minikube.sh]() provides a streamlined Kubernetes cluster setup. Minikube supports two driver modes:\n\n| Driver | Deployment | Command |\n|--------|------------|---------|\n| `docker` (default) | Containerized cluster | `./setup-minikube.sh` |\n| `none` (native) | Direct on host VM | `./setup-minikube.sh native` |\n\nThe `docker` driver is preferred but may have performance constraints in nested virtualization scenarios. The `none` driver runs Kubernetes components directly on the host for better performance in VMs.\n\n**Sources:** [minikube/README.md:1-52](), [README.md:89-96]()\n\n### k3s Alternative\n\nAs a lightweight alternative to Minikube, [minikube/setup-k3s.sh]() installs Rancher's k3s distribution. k3s provides faster startup and lower resource consumption, making it suitable for constrained environments. The setup script configures k3s with the Docker container runtime (instead of default containerd) to support ADES Argo requirements.\n\n**Sources:** [minikube/README.md:38-49]()\n\n---\n\n## Infrastructure Configuration Reference\n\n### Key Configuration Variables\n\nThe following table summarizes the primary infrastructure configuration parameters defined in [creodias/eoepca.tfvars:1-57]():\n\n| Variable | Value | Purpose |\n|----------|-------|---------|\n| `cluster_name` | `develop` | Cluster identifier, prefixes all resources |\n| `subnet_cidr` | `192.168.123.0/24` | Internal network address range |\n| `number_of_k8s_masters_no_floating_ip` | `1` | Control plane nodes without public IPs |\n| `number_of_k8s_nodes_no_floating_ip` | `6` | Worker nodes without public IPs |\n| `flavor_k8s_master` | `20` (eo2.large) | Master node flavor (4 vCPU, 8 GB RAM) |\n| `flavor_k8s_node` | `21` (eo2.xlarge) | Worker node flavor (8 vCPU, 16 GB RAM) |\n| `nfs_disk_size` | `1024` | NFS volume size in GB |\n| `image` | `Ubuntu 18.04 LTS` | Base OS image |\n| `dns_nameservers` | `[\"8.8.8.8\", \"8.8.4.4\"]` | DNS servers for internal network |\n\n**Sources:** [creodias/eoepca.tfvars:1-57](), [creodias/variables.tf:1-233]()\n\n### Required Tools\n\nInfrastructure provisioning requires the following tools:\n\n- **Terraform** (v0.12+): Install via [bin/install-terraform.sh]() [README.md:13-16]()\n- **OpenStack CLI**: `pip install openstackclient` [creodias/README.md:20-26]()\n- **kubectl**: Install via [bin/install-kubectl.sh]() [minikube/README.md:7-14]()\n- **RKE**: Install via [bin/install-rke.sh]() [bin/install-rke.sh:1-14]()\n- **kubeseal** (for secrets): Install via [bin/install-kubeseal.sh]() [bin/install-kubeseal.sh:1-23]()\n- **jq** (for helper scripts): Command-line JSON processor [creodias/README.md:5-7]()\n\n**Sources:** [creodias/README.md:9-25](), [README.md:73-94](), [bin/install-rke.sh:1-14](), [bin/install-kubeseal.sh:1-23]()"])</script><script>self.__next_f.push([1,"36:T4c90,"])</script><script>self.__next_f.push([1,"# Kubernetes Cluster Setup\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [bin/install-kubeseal.sh](bin/install-kubeseal.sh)\n- [bin/install-rke.sh](bin/install-rke.sh)\n- [creodias/.gitignore](creodias/.gitignore)\n- [creodias/.terraform/modules/modules.json](creodias/.terraform/modules/modules.json)\n- [creodias/README.md](creodias/README.md)\n- [creodias/deployCREODIAS.sh](creodias/deployCREODIAS.sh)\n- [creodias/eoepca.tf](creodias/eoepca.tf)\n- [creodias/eoepca.tfvars](creodias/eoepca.tfvars)\n- [creodias/modules/compute/main.tf](creodias/modules/compute/main.tf)\n- [creodias/modules/compute/nfs-setup.sh](creodias/modules/compute/nfs-setup.sh)\n- [creodias/modules/compute/nfs.tf](creodias/modules/compute/nfs.tf)\n- [creodias/modules/compute/outputs.tf](creodias/modules/compute/outputs.tf)\n- [creodias/modules/compute/variables.tf](creodias/modules/compute/variables.tf)\n- [creodias/modules/loadbalancer/main.tf](creodias/modules/loadbalancer/main.tf)\n- [creodias/terraform.tfstate](creodias/terraform.tfstate)\n- [creodias/terraform.tfstate.backup](creodias/terraform.tfstate.backup)\n- [creodias/variables.tf](creodias/variables.tf)\n- [kubernetes/cluster.7z](kubernetes/cluster.7z)\n- [kubernetes/create-cluster-config.sh](kubernetes/create-cluster-config.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the process of provisioning a Kubernetes cluster using RKE (Rancher Kubernetes Engine) on the infrastructure created by Terraform on OpenStack CREODIAS. This page assumes that the underlying infrastructure (VMs, networks, security groups) has already been provisioned using Terraform. For information about the Terraform infrastructure provisioning, see [Infrastructure Provisioning](#2.2) and [Terraform Infrastructure as Code](#8.2). For network topology details, see [Network Architecture](#8.3).\n\nThe cluster setup involves:\n- Installing the RKE command-line tool\n- Generating a cluster configuration file from Terraform outputs\n- Deploying Kubernetes using RKE\n- Configuring node access via bastion host\n\n---\n\n## Infrastructure Overview\n\nThe Kubernetes cluster is deployed on infrastructure provisioned by Terraform, consisting of the following components:\n\n```mermaid\ngraph TB\n    subgraph \"External Access\"\n        Bastion[\"Bastion Host\u003cbr/\u003e185.52.192.185\u003cbr/\u003e(Floating IP)\"]\n        LB[\"Load Balancer\u003cbr/\u003e185.52.192.231\u003cbr/\u003e(Floating IP)\"]\n    end\n    \n    subgraph \"Private Network: 192.168.123.0/24\"\n        Master[\"Master Node\u003cbr/\u003e192.168.123.15\u003cbr/\u003edevelop-k8s-master-nf-1\u003cbr/\u003econtrolplane + etcd\"]\n        \n        subgraph \"Worker Nodes\"\n            Worker1[\"Worker 1\u003cbr/\u003e192.168.123.16\u003cbr/\u003edevelop-k8s-node-nf-1\"]\n            Worker2[\"Worker 2\u003cbr/\u003e192.168.123.6\u003cbr/\u003edevelop-k8s-node-nf-2\"]\n            Worker3[\"Worker 3\u003cbr/\u003e192.168.123.19\u003cbr/\u003edevelop-k8s-node-nf-3\"]\n            Worker4[\"Worker 4\u003cbr/\u003e192.168.123.13\u003cbr/\u003edevelop-k8s-node-nf-4\"]\n            Worker5[\"Worker 5\u003cbr/\u003e192.168.123.8\u003cbr/\u003edevelop-k8s-node-nf-5\"]\n            Worker6[\"Worker 6\u003cbr/\u003e192.168.123.22\u003cbr/\u003edevelop-k8s-node-nf-6\"]\n        end\n        \n        NFS[\"NFS Server\u003cbr/\u003e192.168.123.14\u003cbr/\u003edevelop-nfs\u003cbr/\u003e1024GB SSD Volume\"]\n    end\n    \n    subgraph \"External Networks\"\n        Internet[\"Internet\"]\n        EOData[\"EOData Network\u003cbr/\u003e10.111.0.0/16\"]\n    end\n    \n    Internet --\u003e|SSH:22| Bastion\n    Internet --\u003e|HTTP:80\u003cbr/\u003eHTTPS:443| LB\n    \n    Bastion -.-\u003e|SSH Access| Master\n    Bastion -.-\u003e|SSH Access| Worker1\n    Bastion -.-\u003e|SSH Access| NFS\n    \n    LB --\u003e|31080:HTTP\u003cbr/\u003e31443:HTTPS| Worker1\n    LB --\u003e|31080:HTTP\u003cbr/\u003e31443:HTTPS| Worker2\n    LB --\u003e|31080:HTTP\u003cbr/\u003e31443:HTTPS| Worker3\n    \n    Master --\u003e|NFS Mount| NFS\n    Worker1 --\u003e|NFS Mount| NFS\n    Worker2 --\u003e|NFS Mount| NFS\n    \n    Worker1 -.-\u003e|Data Access| EOData\n    Worker2 -.-\u003e|Data Access| EOData\n```\n\n**Infrastructure Components from Terraform State**\n\nSources: [creodias/terraform.tfstate:1-150](), [creodias/eoepca.tfvars:1-57]()\n\n---\n\n## RKE Installation\n\nRKE is the tool used to provision the Kubernetes cluster. The installation script downloads the RKE binary and places it in the user's local bin directory.\n\n```mermaid\ngraph LR\n    Script[\"bin/install-rke.sh\"]\n    GitHub[\"GitHub Release\u003cbr/\u003erancher/rke\u003cbr/\u003ev1.3.13\"]\n    LocalBin[\"$HOME/.local/bin/rke\"]\n    \n    Script --\u003e|\"curl download\"| GitHub\n    GitHub --\u003e|\"rke_linux-amd64\"| LocalBin\n    LocalBin --\u003e|\"chmod +x\"| LocalBin\n```\n\n**Installation Command:**\n```bash\n$ bin/install-rke.sh\n```\n\nThe script downloads RKE version 1.3.13 from the official GitHub releases.\n\nSources: [bin/install-rke.sh:1-14]()\n\n---\n\n## Cluster Configuration Generation\n\nThe `create-cluster-config.sh` script generates the RKE cluster configuration file by reading Terraform state outputs to obtain node IP addresses.\n\n```mermaid\ngraph TB\n    TFState[\"Terraform State\u003cbr/\u003ecreodias/terraform.tfstate\"]\n    Script[\"kubernetes/create-cluster-config.sh\"]\n    ClusterYML[\"cluster.yml\u003cbr/\u003eRKE Configuration\"]\n    \n    TFState --\u003e|\"terraform output -json\"| Script\n    Script --\u003e|\"Parse k8s_master_ips\"| ClusterYML\n    Script --\u003e|\"Parse k8s_node_ips\"| ClusterYML\n    Script --\u003e|\"Parse bastion_fips\"| ClusterYML\n    \n    subgraph \"Configuration Components\"\n        MasterNodes[\"Master Nodes\u003cbr/\u003econtrolplane + etcd\"]\n        WorkerNodes[\"Worker Nodes\u003cbr/\u003eworker role\"]\n        BastionConfig[\"Bastion Host\u003cbr/\u003eSSH Jump Host\"]\n        K8sVersion[\"kubernetes_version\u003cbr/\u003ev1.22.11-rancher1-1\"]\n    end\n    \n    ClusterYML --\u003e MasterNodes\n    ClusterYML --\u003e WorkerNodes\n    ClusterYML --\u003e BastionConfig\n    ClusterYML --\u003e K8sVersion\n```\n\n**Script Execution:**\n```bash\n$ cd kubernetes\n$ ./create-cluster-config.sh develop cluster.yml\n```\n\nThe script performs the following operations:\n\n| Function | Purpose | Terraform Output Used |\n|----------|---------|----------------------|\n| `master_nodes()` | Generate master node configuration | `k8s_master_ips.value[]` |\n| `worker_nodes()` | Generate worker node configuration | `k8s_node_ips.value[]` |\n| `bastion_host()` | Configure SSH bastion access | `bastion_fips.value[]` |\n| `docker_registry()` | Configure private registry credentials | `$DOCKER_USER`, `$DOCKER_PASSWORD` |\n\n**Generated cluster.yml Structure:**\n\n```yaml\ncluster_name: develop\nkubernetes_version: \"v1.22.11-rancher1-1\"\nnodes:\n  - address: 192.168.123.15\n    user: eouser\n    role:\n      - controlplane\n      - etcd\n  - address: 192.168.123.16\n    user: eouser\n    role:\n      - worker\n  # ... additional worker nodes\n\ningress:\n  provider: none\n\nbastion_host:\n  address: 185.52.192.185\n  user: eouser\n```\n\nSources: [kubernetes/create-cluster-config.sh:1-117](), [creodias/terraform.tfstate:22-67]()\n\n---\n\n## Node Configuration and Roles\n\nThe Kubernetes nodes are configured with specific roles and metadata during Terraform provisioning.\n\n### Master Node Configuration\n\nMaster nodes run the Kubernetes control plane and etcd:\n\n```mermaid\ngraph LR\n    TFResource[\"openstack_compute_instance_v2\u003cbr/\u003ek8s_master_no_floating_ip\"]\n    \n    subgraph \"Node Metadata\"\n        Groups[\"kubespray_groups:\u003cbr/\u003eetcd,kube-master,\u003cbr/\u003ek8s-cluster,vault,\u003cbr/\u003eno-floating\"]\n        SSHUser[\"ssh_user: eouser\"]\n        AccessIP[\"use_access_ip: 1\"]\n    end\n    \n    subgraph \"Node Specifications\"\n        Flavor[\"flavor_id: 20\u003cbr/\u003eeo2.large\"]\n        Image[\"image_name:\u003cbr/\u003eUbuntu 18.04 LTS\"]\n        Network[\"network: develop\u003cbr/\u003e192.168.123.0/24\"]\n    end\n    \n    TFResource --\u003e Groups\n    TFResource --\u003e SSHUser\n    TFResource --\u003e AccessIP\n    TFResource --\u003e Flavor\n    TFResource --\u003e Image\n    TFResource --\u003e Network\n```\n\nSources: [creodias/modules/compute/main.tf:384-437](), [creodias/terraform.tfstate:594-669]()\n\n### Worker Node Configuration\n\nWorker nodes execute application workloads and are connected to both the internal network and the EOData network for accessing Earth Observation data:\n\n| Configuration | Value | Purpose |\n|--------------|-------|---------|\n| Flavor | `eo2.xlarge` (flavor_id: 21) | Larger resources for workload execution |\n| Primary Network | `develop` (192.168.123.0/24) | Cluster communication |\n| Secondary Network | `eodata` (10.111.0.0/16) | Direct access to satellite data |\n| Security Groups | `develop-k8s`, `develop-k8s-worker` | Network access control |\n| Kubespray Groups | `kube-node,k8s-cluster,no-floating` | Node role assignment |\n\nSources: [creodias/modules/compute/main.tf:720-804](), [creodias/eoepca.tfvars:30-33](), [creodias/eoepca.tfvars:47]()\n\n---\n\n## Node Provisioning Process\n\nEach node is automatically provisioned with the necessary software when created by Terraform.\n\n```mermaid\nsequenceDiagram\n    participant TF as Terraform\n    participant Node as Compute Instance\n    participant Bastion as Bastion Host\n    participant Script as rke-node-setup.sh\n    \n    TF-\u003e\u003eNode: Create openstack_compute_instance_v2\n    TF-\u003e\u003eNode: Establish SSH connection via bastion\n    Note over TF,Bastion: Connection:\u003cbr/\u003ebastion_host = bastion_fips[0]\u003cbr/\u003euser = ssh_user\n    TF-\u003e\u003eNode: Upload rke-node-setup.sh to /tmp\n    TF-\u003e\u003eNode: chmod +x /tmp/rke-node-setup.sh\n    TF-\u003e\u003eNode: Execute setup script with ssh_user\n    Node-\u003e\u003eScript: Run node setup\n    Script--\u003e\u003eNode: Configure Docker and node requirements\n    Node--\u003e\u003eTF: Provisioning complete\n```\n\n**Master Node Provisioning:**\n- Connection established through bastion host SSH jump\n- Private key authentication using the key pair's private key\n- Setup script executed with `eouser` parameter\n- Script installs Docker and configures the node for RKE\n\nSources: [creodias/modules/compute/main.tf:418-436](), [creodias/modules/compute/main.tf:598-668]()\n\n---\n\n## NFS Server Setup\n\nThe cluster includes a dedicated NFS server for shared persistent storage across all nodes.\n\n```mermaid\ngraph TB\n    TFResource[\"openstack_compute_instance_v2\u003cbr/\u003eeoepca_nfs\"]\n    Volume[\"openstack_blockstorage_volume_v2\u003cbr/\u003enfs_expansion\u003cbr/\u003e1024GB SSD\"]\n    Setup[\"nfs-setup.sh\"]\n    \n    subgraph \"NFS Export Structure\"\n        Root[\"/data\"]\n        UM[\"/data/userman\"]\n        Proc[\"/data/proc\"]\n        RM[\"/data/resman\"]\n        Dyn[\"/data/dynamic\"]\n    end\n    \n    TFResource --\u003e|\"Attach /dev/sdb\"| Volume\n    TFResource --\u003e|\"Provision after 2 min\"| Setup\n    Setup --\u003e|\"fdisk + mkfs.ext4\"| Volume\n    Setup --\u003e|\"mount /dev/sdb1\"| Root\n    Setup --\u003e|\"Create exports\"| UM\n    Setup --\u003e|\"Create exports\"| Proc\n    Setup --\u003e|\"Create exports\"| RM\n    Setup --\u003e|\"Create exports\"| Dyn\n    \n    Root -.-\u003e|\"NFS Export\u003cbr/\u003erw,no_root_squash\"| K8sCluster[\"Kubernetes Cluster\"]\n```\n\n**NFS Server Configuration:**\n\n| Component | Configuration | Details |\n|-----------|--------------|---------|\n| Instance Name | `develop-nfs` | Single NFS server instance |\n| Flavor | `eo2.large` (flavor_id: 20) | 8 vCPU, 32GB RAM |\n| Boot Volume | Standard OS disk | Ubuntu 18.04 LTS |\n| Data Volume | 1024GB SSD (`/dev/sdb`) | Attached block storage |\n| Mount Point | `/data` | Formatted as ext4 |\n| Exports | `/data/{userman,proc,resman,dynamic}` | NFS shares for different subsystems |\n| Export Options | `rw,no_root_squash,no_subtree_check` | Full read-write access |\n\n**NFS Setup Process:**\n\nThe `nfs-setup.sh` script performs the following:\n1. Installs `nfs-kernel-server` package\n2. Partitions and formats the attached SSD volume (`/dev/sdb1`)\n3. Mounts the volume to `/data`\n4. Adds the mount to `/etc/fstab` for persistence\n5. Creates export subdirectories for different EOEPCA subsystems\n6. Configures `/etc/exports` with NFS share definitions\n7. Restarts the NFS service\n\nThe script is scheduled to run 2 minutes after instance creation using the `at` command to allow the volume attachment to complete.\n\nSources: [creodias/modules/compute/nfs.tf:1-70](), [creodias/modules/compute/nfs-setup.sh:1-44](), [creodias/eoepca.tfvars:56]()\n\n---\n\n## Cluster Access via Bastion\n\nAll administrative access to the Kubernetes cluster is routed through the bastion host for security.\n\n```mermaid\ngraph LR\n    Admin[\"Administrator\u003cbr/\u003eLocal Workstation\"]\n    Bastion[\"Bastion Host\u003cbr/\u003e185.52.192.185\u003cbr/\u003edevelop-bastion-1\"]\n    \n    subgraph \"Private Network Nodes\"\n        Master[\"Master\u003cbr/\u003e192.168.123.15\"]\n        Worker[\"Workers\u003cbr/\u003e192.168.123.16\u003cbr/\u003e192.168.123.6\u003cbr/\u003e...\"]\n        NFS[\"NFS\u003cbr/\u003e192.168.123.14\"]\n    end\n    \n    Admin --\u003e|\"SSH Jump\u003cbr/\u003e-J eouser@185.52.192.185\"| Bastion\n    Bastion --\u003e|\"Internal SSH\"| Master\n    Bastion --\u003e|\"Internal SSH\"| Worker\n    Bastion --\u003e|\"Internal SSH\"| NFS\n    \n    Admin -.-\u003e|\"sshuttle VPN\u003cbr/\u003e192.168.123.0/24\"| Bastion\n```\n\n**SSH Access Methods:**\n\n1. **Direct SSH Jump:**\n```bash\nssh -J eouser@185.52.192.185 eouser@192.168.123.15\n```\n\n2. **VPN via sshuttle:**\n```bash\nsshuttle -r eouser@185.52.192.185 192.168.123.0/24\n```\n\nThe sshuttle VPN approach is preferred as it enables direct access to the Kubernetes API and facilitates `kubectl` commands from the local workstation.\n\n**Bastion Host Configuration:**\n\n| Property | Value | Purpose |\n|----------|-------|---------|\n| Flavor | `eo1.xsmall` (flavor_id: 14) | Minimal resources for SSH gateway |\n| Floating IP | `185.52.192.185` | Public internet access |\n| Private IP | `192.168.123.24` | Internal network connectivity |\n| Security Groups | `develop-bastion`, `develop-k8s` | SSH access (port 22) from internet |\n| SSH User | `eouser` | Administrative user account |\n\nSources: [creodias/README.md:98-120](), [creodias/terraform.tfstate:356-429](), [creodias/modules/compute/main.tf:116-141]()\n\n---\n\n## Deployment Workflow\n\nThe complete cluster deployment follows this sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Terraform\n    participant OpenStack\n    participant RKE\n    participant Cluster\n    \n    User-\u003e\u003eTerraform: cd creodias \u0026\u0026 ./deployCREODIAS.sh\n    Terraform-\u003e\u003eOpenStack: Provision VMs, networks, volumes\n    OpenStack--\u003e\u003eTerraform: Infrastructure created\n    Terraform--\u003e\u003eUser: Output IPs and configuration\n    \n    User-\u003e\u003eRKE: bin/install-rke.sh\n    RKE--\u003e\u003eUser: RKE tool installed\n    \n    User-\u003e\u003eUser: cd kubernetes\n    User-\u003e\u003eUser: ./create-cluster-config.sh develop\n    Note over User: Reads terraform.tfstate\u003cbr/\u003eGenerates cluster.yml\n    \n    User-\u003e\u003eRKE: rke up --config cluster.yml\n    RKE-\u003e\u003eCluster: SSH via bastion to each node\n    RKE-\u003e\u003eCluster: Install Kubernetes components\n    RKE-\u003e\u003eCluster: Configure control plane and etcd\n    RKE-\u003e\u003eCluster: Join worker nodes\n    Cluster--\u003e\u003eRKE: Cluster operational\n    RKE--\u003e\u003eUser: Generate kube_config_cluster.yml\n    \n    User-\u003e\u003eUser: export KUBECONFIG=kube_config_cluster.yml\n    User-\u003e\u003eCluster: kubectl get nodes\n    Cluster--\u003e\u003eUser: Node status\n```\n\n**Deployment Steps:**\n\n1. **Infrastructure Provisioning** (see [Infrastructure Provisioning](#2.2))\n   ```bash\n   cd creodias\n   export OS_CLOUD=eoepca\n   ./deployCREODIAS.sh\n   ```\n\n2. **Install RKE Tool**\n   ```bash\n   bin/install-rke.sh\n   ```\n\n3. **Generate Cluster Configuration**\n   ```bash\n   cd kubernetes\n   ./create-cluster-config.sh develop cluster.yml\n   ```\n\n4. **Deploy Kubernetes Cluster**\n   ```bash\n   rke up --config cluster.yml\n   ```\n\n5. **Configure kubectl Access**\n   ```bash\n   export KUBECONFIG=$PWD/kube_config_cluster.yml\n   kubectl get nodes\n   ```\n\nSources: [creodias/deployCREODIAS.sh:1-52](), [kubernetes/create-cluster-config.sh:9-116](), [bin/install-rke.sh:1-14]()\n\n---\n\n## Cluster Configuration Details\n\nThe generated `cluster.yml` file contains the complete RKE cluster specification.\n\n**Key Configuration Parameters:**\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `cluster_name` | `develop` | Cluster identifier |\n| `kubernetes_version` | `v1.22.11-rancher1-1` | Specific Kubernetes release |\n| `ingress.provider` | `none` | Ingress disabled (managed separately) |\n| `nodes[].user` | `eouser` | SSH user for node access |\n| `nodes[].role` | `[controlplane, etcd]` or `[worker]` | Node role assignment |\n| `bastion_host.address` | Bastion floating IP | SSH jump host |\n\n**Node Role Definitions:**\n\n- **controlplane**: Runs kube-apiserver, kube-controller-manager, kube-scheduler\n- **etcd**: Runs etcd distributed key-value store\n- **worker**: Runs kubelet and kube-proxy for workload execution\n\nThe cluster uses a single combined control plane and etcd node for the default deployment, with multiple worker nodes for workload distribution.\n\nSources: [kubernetes/create-cluster-config.sh:92-109](), [kubernetes/create-cluster-config.sh:11]()\n\n---\n\n## Security Configuration\n\nThe cluster security is enforced through OpenStack security groups and network isolation.\n\n```mermaid\ngraph TB\n    subgraph \"Security Groups\"\n        SGBastion[\"develop-bastion\u003cbr/\u003ePort 22 from 0.0.0.0/0\"]\n        SGMaster[\"develop-k8s-master\u003cbr/\u003ePort 6443 from 0.0.0.0/0\"]\n        SGK8s[\"develop-k8s\u003cbr/\u003eIntra-cluster traffic\"]\n        SGWorker[\"develop-k8s-worker\u003cbr/\u003eNodePort range 30000-32767\"]\n    end\n    \n    subgraph \"Network Isolation\"\n        External[\"External Network\u003cbr/\u003eInternet\"]\n        Internal[\"Internal Network\u003cbr/\u003e192.168.123.0/24\"]\n        EOData[\"EOData Network\u003cbr/\u003e10.111.0.0/16\"]\n    end\n    \n    External --\u003e|SSH:22| SGBastion\n    External --\u003e|API:6443| SGMaster\n    External --\u003e|NodePort| SGWorker\n    \n    SGK8s -.-\u003e|\"All traffic allowed\u003cbr/\u003ewithin security group\"| SGK8s\n    \n    Internal -.-\u003e SGK8s\n    EOData -.-\u003e SGWorker\n```\n\n**Security Group Rules:**\n\n1. **Bastion Security Group** (`develop-bastion`)\n   - Ingress: TCP port 22 from configurable CIDR blocks\n   - Default: Allows SSH from anywhere (0.0.0.0/0)\n\n2. **Master Security Group** (`develop-k8s-master`)\n   - Ingress: TCP port 6443 (Kubernetes API)\n   - Allows API access from specified IP ranges\n\n3. **K8s Security Group** (`develop-k8s`)\n   - Ingress: All traffic from members of the same security group\n   - Allows internal cluster communication\n   - SSH access from specified IP ranges\n\n4. **Worker Security Group** (`develop-k8s-worker`)\n   - Ingress: TCP ports 30000-32767 (NodePort range)\n   - Allows external access to exposed services\n\nSources: [creodias/modules/compute/main.tf:14-96](), [creodias/eoepca.tfvars:53](), [creodias/variables.tf:210-221]()\n\n---\n\n## Post-Deployment Verification\n\nAfter RKE completes the cluster deployment, verify the cluster state:\n\n```bash\n# Set kubeconfig\nexport KUBECONFIG=$PWD/kube_config_cluster.yml\n\n# Verify node status\nkubectl get nodes\n\n# Expected output:\n# NAME              STATUS   ROLES               AGE   VERSION\n# 192.168.123.15    Ready    controlplane,etcd   5m    v1.22.11\n# 192.168.123.16    Ready    worker              5m    v1.22.11\n# 192.168.123.6     Ready    worker              5m    v1.22.11\n# ...\n\n# Verify system pods\nkubectl get pods -n kube-system\n\n# Check cluster info\nkubectl cluster-info\n```\n\nRKE generates the `kube_config_cluster.yml` file containing the necessary credentials and configuration to access the cluster via kubectl. This kubeconfig includes the cluster CA certificate, client certificate, and API server endpoint configured to work through the bastion host.\n\nSources: [kubernetes/create-cluster-config.sh:1-117]()\n\n---\n\n## Next Steps\n\nWith the Kubernetes cluster operational, the next phase is to deploy the EOEPCA platform using GitOps with Flux CD. See [Deployment Guide](#2.1) for instructions on bootstrapping Flux and deploying the complete platform stack.\n\nThe cluster is now ready for:\n- Flux CD installation and GitOps configuration\n- Ingress controller deployment (nginx-ingress)\n- Certificate management (cert-manager)\n- EOEPCA building block deployments (User Management, Resource Management, Processing \u0026 Chaining)\n\nSources: [creodias/README.md:126-129]()"])</script><script>self.__next_f.push([1,"37:T5cbb,"])</script><script>self.__next_f.push([1,"# Terraform Infrastructure as Code\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [bin/install-kubeseal.sh](bin/install-kubeseal.sh)\n- [bin/install-rke.sh](bin/install-rke.sh)\n- [creodias/.gitignore](creodias/.gitignore)\n- [creodias/.terraform/modules/modules.json](creodias/.terraform/modules/modules.json)\n- [creodias/README.md](creodias/README.md)\n- [creodias/deployCREODIAS.sh](creodias/deployCREODIAS.sh)\n- [creodias/eoepca.tf](creodias/eoepca.tf)\n- [creodias/eoepca.tfvars](creodias/eoepca.tfvars)\n- [creodias/modules/compute/main.tf](creodias/modules/compute/main.tf)\n- [creodias/modules/compute/nfs-setup.sh](creodias/modules/compute/nfs-setup.sh)\n- [creodias/modules/compute/nfs.tf](creodias/modules/compute/nfs.tf)\n- [creodias/modules/compute/outputs.tf](creodias/modules/compute/outputs.tf)\n- [creodias/modules/compute/variables.tf](creodias/modules/compute/variables.tf)\n- [creodias/modules/loadbalancer/main.tf](creodias/modules/loadbalancer/main.tf)\n- [creodias/terraform.tfstate](creodias/terraform.tfstate)\n- [creodias/terraform.tfstate.backup](creodias/terraform.tfstate.backup)\n- [creodias/variables.tf](creodias/variables.tf)\n- [kubernetes/cluster.7z](kubernetes/cluster.7z)\n- [kubernetes/create-cluster-config.sh](kubernetes/create-cluster-config.sh)\n\n\u003c/details\u003e\n\n\n\n## Overview\n\nThis document describes the Terraform modules and configurations used to provision the EOEPCA infrastructure on OpenStack (CREODIAS/Cloudferro platform). The Terraform code creates all virtual machines, networking, security groups, storage volumes, and load balancers required for a Kubernetes cluster deployment.\n\nFor information about setting up the Kubernetes cluster after infrastructure provisioning, see [Kubernetes Cluster Setup](#8.1). For details about the network architecture and security groups, see [Network Architecture](#8.3).\n\n## Terraform Module Structure\n\nThe Terraform configuration is organized into modular components under [creodias/]():\n\n```mermaid\ngraph TB\n    subgraph \"Root Module\"\n        Main[\"eoepca.tf\u003cbr/\u003eRoot Configuration\"]\n        Vars[\"variables.tf\u003cbr/\u003eVariable Definitions\"]\n        TVars[\"eoepca.tfvars\u003cbr/\u003eDeployment Values\"]\n    end\n    \n    subgraph \"Module: network\"\n        NetMod[\"modules/network/\u003cbr/\u003eNetwork Infrastructure\"]\n    end\n    \n    subgraph \"Module: ips\"\n        IpsMod[\"modules/ips/\u003cbr/\u003eFloating IP Allocation\"]\n    end\n    \n    subgraph \"Module: compute\"\n        CompMod[\"modules/compute/main.tf\u003cbr/\u003eVM Instances \u0026 Security\"]\n        NfsMod[\"modules/compute/nfs.tf\u003cbr/\u003eNFS Server\"]\n        CompVars[\"modules/compute/variables.tf\"]\n        CompOut[\"modules/compute/outputs.tf\"]\n    end\n    \n    subgraph \"Module: loadbalancer\"\n        LbMod[\"modules/loadbalancer/main.tf\u003cbr/\u003eLoad Balancer\"]\n    end\n    \n    Main --\u003e|uses| NetMod\n    Main --\u003e|uses| IpsMod\n    Main --\u003e|uses| CompMod\n    Main --\u003e|uses| LbMod\n    \n    IpsMod --\u003e|depends on| NetMod\n    CompMod --\u003e|depends on| IpsMod\n    LbMod --\u003e|depends on| CompMod\n    LbMod --\u003e|depends on| NetMod\n    \n    TVars --\u003e|provides values| Vars\n    Vars --\u003e|defines| Main\n```\n\n**Sources:** [creodias/eoepca.tf:1-146](), [creodias/.terraform/modules/modules.json:1]()\n\n## Infrastructure Components\n\n### Virtual Machine Types\n\nThe Terraform configuration provisions multiple categories of VMs with specific roles:\n\n| VM Type | Resource Name | Count Variable | Purpose |\n|---------|---------------|----------------|---------|\n| Bastion | `openstack_compute_instance_v2.bastion` | `number_of_bastions` | SSH gateway to cluster |\n| K8s Master | `openstack_compute_instance_v2.k8s_master_no_floating_ip` | `number_of_k8s_masters_no_floating_ip` | Control plane nodes |\n| K8s Workers | `openstack_compute_instance_v2.k8s_node_no_floating_ip` | `number_of_k8s_nodes_no_floating_ip` | Worker nodes |\n| ETCD | `openstack_compute_instance_v2.etcd` | `number_of_etcd` | Dedicated etcd nodes |\n| NFS Server | `openstack_compute_instance_v2.eoepca_nfs` | 1 | Shared storage |\n| GlusterFS | `openstack_compute_instance_v2.glusterfs_node_no_floating_ip` | `number_of_gfs_nodes_no_floating_ip` | Distributed storage (optional) |\n\n**Sources:** [creodias/modules/compute/main.tf:116-437](), [creodias/modules/compute/nfs.tf:1-70]()\n\n### Deployment Topology Diagram\n\n```mermaid\ngraph TB\n    subgraph \"External Network\"\n        Internet[\"Internet\u003cbr/\u003e(external3)\"]\n    end\n    \n    subgraph \"OpenStack CREODIAS\"\n        subgraph \"Floating IPs\"\n            BastionFIP[\"bastion_fips\u003cbr/\u003e185.52.192.185\"]\n            LBFIP[\"loadbalancer_fips\u003cbr/\u003e185.52.192.231\"]\n        end\n        \n        subgraph \"Private Network: 192.168.123.0/24\"\n            Router[\"openstack_networking_router_v2.k8s\"]\n            \n            subgraph \"Security Groups\"\n                SGBastion[\"develop-bastion\u003cbr/\u003eSSH access\"]\n                SGMaster[\"develop-k8s-master\u003cbr/\u003eAPI:6443\"]\n                SGK8s[\"develop-k8s\u003cbr/\u003eInternal cluster\"]\n                SGWorker[\"develop-k8s-worker\u003cbr/\u003eNodePort 30000-32767\"]\n                SGLB[\"develop-lb\u003cbr/\u003eHTTP/HTTPS\"]\n            end\n            \n            subgraph \"Compute Instances\"\n                Bastion[\"bastion-1\u003cbr/\u003e192.168.123.24\u003cbr/\u003eflavor: eo1.xsmall\"]\n                Master[\"k8s-master-nf-1\u003cbr/\u003e192.168.123.15\u003cbr/\u003eflavor: eo2.large\"]\n                Worker1[\"k8s-node-nf-1\u003cbr/\u003e192.168.123.16\u003cbr/\u003eflavor: eo2.xlarge\"]\n                WorkerN[\"k8s-node-nf-N\u003cbr/\u003e...5 more workers\u003cbr/\u003eflavor: eo2.xlarge\"]\n                NFS[\"nfs\u003cbr/\u003e192.168.123.14\u003cbr/\u003eflavor: eo2.large\"]\n            end\n            \n            subgraph \"Load Balancer\"\n                LB[\"openstack_lb_loadbalancer_v2.k8s\"]\n                HTTPSPool[\"lb_pool_v2.https\u003cbr/\u003e:443:31443\"]\n                HTTPPool[\"lb_pool_v2.http\u003cbr/\u003e:80:31080\"]\n            end\n            \n            subgraph \"Storage Volumes\"\n                NFSVol[\"nfs-expansion\u003cbr/\u003e1024GB SSD\u003cbr/\u003e/dev/sdb\"]\n            end\n        end\n        \n        subgraph \"eodata Network\"\n            EOData[\"eodata\u003cbr/\u003e10.111.0.0/16\u003cbr/\u003eCloudFerro Data\"]\n        end\n    end\n    \n    Internet --\u003e|SSH:22| BastionFIP\n    Internet --\u003e|HTTP/HTTPS| LBFIP\n    \n    BastionFIP --\u003e Bastion\n    LBFIP --\u003e LB\n    \n    Router --\u003e|routes| Internet\n    \n    LB --\u003e HTTPSPool\n    LB --\u003e HTTPPool\n    HTTPSPool --\u003e|31443| Worker1\n    HTTPSPool --\u003e|31443| WorkerN\n    HTTPPool --\u003e|31080| Worker1\n    HTTPPool --\u003e|31080| WorkerN\n    \n    Bastion -.-\u003e|SSH gateway| Master\n    Bastion -.-\u003e|SSH gateway| Worker1\n    Bastion -.-\u003e|SSH gateway| NFS\n    \n    NFSVol --\u003e|attached| NFS\n    Worker1 -.-\u003e|mount NFS| NFS\n    WorkerN -.-\u003e|mount NFS| NFS\n    \n    Worker1 -.-\u003e|secondary network| EOData\n    WorkerN -.-\u003e|secondary network| EOData\n```\n\n**Sources:** [creodias/terraform.tfstate:96-1100](), [creodias/eoepca.tfvars:1-57](), [creodias/README.md:76-97]()\n\n## Module: Network\n\nThe network module creates the private network infrastructure for the cluster:\n\n```mermaid\ngraph LR\n    ExtNet[\"external_net\u003cbr/\u003e31d7e67a-b30a-43f4-8b06-1667c70ba90d\"]\n    \n    Router[\"openstack_networking_router_v2.k8s\u003cbr/\u003erouter_id\"]\n    Network[\"openstack_networking_network_v2\u003cbr/\u003enetwork_name: develop\"]\n    Subnet[\"openstack_networking_subnet_v2\u003cbr/\u003esubnet_cidr: 192.168.123.0/24\u003cbr/\u003edns_nameservers: 8.8.8.8, 8.8.4.4\"]\n    \n    ExtNet --\u003e|external_gateway| Router\n    Router --\u003e|routes to| Subnet\n    Subnet --\u003e|part of| Network\n```\n\n**Configuration:**\n- Network name is set via `cluster_name` variable\n- Subnet CIDR configurable in [eoepca.tfvars:10]()\n- DNS nameservers configurable in [eoepca.tfvars:11]()\n- Router provides NAT to external network\n\n**Sources:** [creodias/eoepca.tf:6-16](), [creodias/eoepca.tfvars:6-11]()\n\n## Module: IPs\n\nThe IPs module allocates floating IPs from the external pool for public-facing resources:\n\n```mermaid\ngraph TB\n    Pool[\"floatingip_pool\u003cbr/\u003eexternal3\"]\n    \n    subgraph \"Floating IP Resources\"\n        BastionFIP[\"openstack_networking_floatingip_v2.bastion\u003cbr/\u003ecount: number_of_bastions\"]\n        MasterFIP[\"openstack_networking_floatingip_v2.k8s_master\u003cbr/\u003ecount: number_of_k8s_masters\"]\n        NodeFIP[\"openstack_networking_floatingip_v2.k8s_node\u003cbr/\u003ecount: number_of_k8s_nodes\"]\n    end\n    \n    subgraph \"Associations\"\n        BastionAssoc[\"openstack_compute_floatingip_associate_v2.bastion\"]\n        MasterAssoc[\"openstack_compute_floatingip_associate_v2.k8s_master\"]\n        NodeAssoc[\"openstack_compute_floatingip_associate_v2.k8s_node\"]\n    end\n    \n    Pool --\u003e|allocates from| BastionFIP\n    Pool --\u003e|allocates from| MasterFIP\n    Pool --\u003e|allocates from| NodeFIP\n    \n    BastionFIP --\u003e|associates to| BastionAssoc\n    MasterFIP --\u003e|associates to| MasterAssoc\n    NodeFIP --\u003e|associates to| NodeAssoc\n```\n\n**Typical Deployment:**\n- Bastion: 1 floating IP (`number_of_bastions = 1`)\n- Masters: 0 floating IPs (uses `number_of_k8s_masters_no_floating_ip` instead)\n- Workers: 0 floating IPs (uses `number_of_k8s_nodes_no_floating_ip` instead)\n- Load Balancer: 1 floating IP (created in loadbalancer module)\n\n**Sources:** [creodias/eoepca.tf:18-29](), [creodias/eoepca.tfvars:18-33]()\n\n## Module: Compute\n\nThe compute module creates all VM instances and security infrastructure.\n\n### Security Groups\n\nSecurity groups control network access to VMs:\n\n```mermaid\ngraph TB\n    subgraph \"Security Group Hierarchy\"\n        SGBastion[\"openstack_networking_secgroup_v2.bastion\u003cbr/\u003eSSH from bastion_allowed_remote_ips\"]\n        SGMaster[\"openstack_networking_secgroup_v2.k8s_master\u003cbr/\u003eTCP:6443 from master_allowed_remote_ips\"]\n        SGK8s[\"openstack_networking_secgroup_v2.k8s\u003cbr/\u003eAll traffic within group\u003cbr/\u003eSSH from k8s_allowed_remote_ips\u003cbr/\u003eEgress to k8s_allowed_egress_ips\"]\n        SGWorker[\"openstack_networking_secgroup_v2.worker\u003cbr/\u003eTCP:30000-32767 from 0.0.0.0/0\"]\n    end\n    \n    subgraph \"Applied To VMs\"\n        Bastion[\"bastion\u003cbr/\u003esecurity_groups:\u003cbr/\u003e- bastion\u003cbr/\u003e- k8s\"]\n        Master[\"k8s-master\u003cbr/\u003esecurity_groups:\u003cbr/\u003e- k8s-master\u003cbr/\u003e- k8s\"]\n        Worker[\"k8s-node\u003cbr/\u003esecurity_groups:\u003cbr/\u003e- k8s-worker\u003cbr/\u003e- k8s\"]\n    end\n    \n    SGBastion --\u003e|applied to| Bastion\n    SGMaster --\u003e|applied to| Master\n    SGK8s --\u003e|applied to| Bastion\n    SGK8s --\u003e|applied to| Master\n    SGK8s --\u003e|applied to| Worker\n    SGWorker --\u003e|applied to| Worker\n```\n\n**Security Group Resources:**\n- `openstack_networking_secgroup_v2.bastion` - [creodias/modules/compute/main.tf:31-36]()\n- `openstack_networking_secgroup_v2.k8s_master` - [creodias/modules/compute/main.tf:14-18]()\n- `openstack_networking_secgroup_v2.k8s` - [creodias/modules/compute/main.tf:49-53]()\n- `openstack_networking_secgroup_v2.worker` - [creodias/modules/compute/main.tf:81-85]()\n\n**Sources:** [creodias/modules/compute/main.tf:14-96]()\n\n### Compute Instance Creation\n\nVM instances are created with metadata for Kubespray configuration:\n\n```mermaid\ngraph TB\n    subgraph \"Instance Creation Flow\"\n        Keypair[\"openstack_compute_keypair_v2.k8s\u003cbr/\u003epublic_key from public_key_path\"]\n        \n        subgraph \"Master Instances\"\n            MasterNoFIP[\"openstack_compute_instance_v2.k8s_master_no_floating_ip\u003cbr/\u003emetadata.kubespray_groups:\u003cbr/\u003e'etcd,kube-master,k8s-cluster,vault,no-floating'\"]\n        end\n        \n        subgraph \"Worker Instances\"\n            WorkerNoFIP[\"openstack_compute_instance_v2.k8s_node_no_floating_ip\u003cbr/\u003emetadata.kubespray_groups:\u003cbr/\u003e'kube-node,k8s-cluster,no-floating'\"]\n        end\n        \n        subgraph \"Provisioning\"\n            RKEScript[\"provisioner: file\u003cbr/\u003erke-node-setup.sh\"]\n            RKEExec[\"provisioner: remote-exec\u003cbr/\u003eExecute setup script\"]\n        end\n    end\n    \n    Keypair --\u003e|key_pair| MasterNoFIP\n    Keypair --\u003e|key_pair| WorkerNoFIP\n    \n    MasterNoFIP --\u003e|connection via bastion| RKEScript\n    RKEScript --\u003e RKEExec\n```\n\n**Key Instance Attributes:**\n- `image_name` - OS image (default: \"Ubuntu 18.04 LTS\")\n- `flavor_id` - VM size (e.g., \"eo2.large\" for masters, \"eo2.xlarge\" for workers)\n- `key_pair` - SSH key for access\n- `network.name` - Attached to private network\n- `security_groups` - List of security groups\n- `metadata.kubespray_groups` - Used by RKE for node role assignment\n\n**Sources:** [creodias/modules/compute/main.tf:384-437](), [creodias/eoepca.tfvars:39-47]()\n\n### NFS Server Provisioning\n\nThe NFS server provides shared persistent storage for the cluster:\n\n```mermaid\ngraph TB\n    subgraph \"NFS Resources\"\n        NFSInstance[\"openstack_compute_instance_v2.eoepca_nfs\u003cbr/\u003ename: develop-nfs\u003cbr/\u003eflavor: eo2.large\u003cbr/\u003eip: 192.168.123.14\"]\n        \n        NFSVolume[\"openstack_blockstorage_volume_v2.nfs_expansion\u003cbr/\u003esize: nfs_disk_size (1024GB)\u003cbr/\u003evolume_type: SSD\"]\n        \n        VolumeAttach[\"openstack_compute_volume_attach_v2.volume_attachment_nfs\u003cbr/\u003edevice: /dev/sdb\"]\n        \n        NFSSetup[\"provisioner: remote-exec\u003cbr/\u003enfs-setup.sh\"]\n    end\n    \n    subgraph \"NFS Setup Script Actions\"\n        InstallNFS[\"apt-get install\u003cbr/\u003enfs-kernel-server\"]\n        CreateRoot[\"mkdir /data\"]\n        Partition[\"fdisk /dev/sdb\u003cbr/\u003emkfs -t ext4 /dev/sdb1\"]\n        Mount[\"mount /dev/sdb1 /data\u003cbr/\u003eadd to /etc/fstab\"]\n        CreateExports[\"mkdir /data/userman\u003cbr/\u003emkdir /data/proc\u003cbr/\u003emkdir /data/resman\u003cbr/\u003emkdir /data/dynamic\"]\n        ExportConfig[\"update /etc/exports\u003cbr/\u003erestart nfs-kernel-server\"]\n    end\n    \n    NFSVolume --\u003e|attached as| VolumeAttach\n    VolumeAttach --\u003e|/dev/sdb on| NFSInstance\n    NFSInstance --\u003e|executes| NFSSetup\n    \n    NFSSetup --\u003e InstallNFS\n    InstallNFS --\u003e CreateRoot\n    CreateRoot --\u003e Partition\n    Partition --\u003e Mount\n    Mount --\u003e CreateExports\n    CreateExports --\u003e ExportConfig\n```\n\n**NFS Export Directories:**\n- `/data/userman` - User management data\n- `/data/proc` - Processing data\n- `/data/resman` - Resource management data\n- `/data/dynamic` - Dynamic workspace data\n\n**Export Configuration:**\n- All exports use `*(rw,no_root_squash,no_subtree_check)`\n- Accessible from all cluster nodes\n\n**Sources:** [creodias/modules/compute/nfs.tf:1-70](), [creodias/modules/compute/nfs-setup.sh:1-44](), [creodias/eoepca.tfvars:56]()\n\n## Module: Load Balancer\n\nThe load balancer module creates an OpenStack Octavia load balancer to route external traffic:\n\n```mermaid\ngraph TB\n    subgraph \"Load Balancer Components\"\n        LB[\"openstack_lb_loadbalancer_v2.k8s\u003cbr/\u003ename: develop-lb\u003cbr/\u003evip_network_id: network_id\"]\n        \n        subgraph \"Listeners\"\n            HTTPSListener[\"openstack_lb_listener_v2.https\u003cbr/\u003eprotocol: HTTPS\u003cbr/\u003eprotocol_port: 443\"]\n            HTTPListener[\"openstack_lb_listener_v2.http\u003cbr/\u003eprotocol: HTTP\u003cbr/\u003eprotocol_port: 80\"]\n        end\n        \n        subgraph \"Pools\"\n            HTTPSPool[\"openstack_lb_pool_v2.https\u003cbr/\u003elb_method: ROUND_ROBIN\u003cbr/\u003epersistence: SOURCE_IP\"]\n            HTTPPool[\"openstack_lb_pool_v2.http\u003cbr/\u003elb_method: ROUND_ROBIN\u003cbr/\u003epersistence: SOURCE_IP\"]\n        end\n        \n        subgraph \"Members\"\n            HTTPSMembers[\"openstack_lb_members_v2.https\u003cbr/\u003ek8s_node_ips  port 31443\"]\n            HTTPMembers[\"openstack_lb_members_v2.http\u003cbr/\u003ek8s_node_ips  port 31080\"]\n        end\n        \n        FIP[\"openstack_networking_floatingip_v2.loadbalancer\u003cbr/\u003epool: floatingip_pool\"]\n    end\n    \n    LB --\u003e HTTPSListener\n    LB --\u003e HTTPListener\n    \n    HTTPSListener --\u003e HTTPSPool\n    HTTPListener --\u003e HTTPPool\n    \n    HTTPSPool --\u003e HTTPSMembers\n    HTTPPool --\u003e HTTPMembers\n    \n    FIP --\u003e|associates to| LB\n```\n\n**Load Balancer Forwarding:**\n- External HTTPS (443)  NodePort 31443 on all worker nodes\n- External HTTP (80)  NodePort 31080 on all worker nodes\n- Connects to `k8s_node_ips`, `k8s_node_hm_ips`, and `k8s_node_ws_ips`\n\n**Security:**\n- Load balancer uses security groups `develop-lb` and `develop-k8s`\n- Allows ingress on ports 80 and 443\n- Egress to all destinations (IPv4 and IPv6)\n\n**Sources:** [creodias/modules/loadbalancer/main.tf:1-184](), [creodias/eoepca.tf:83-93]()\n\n## Deployment Configuration\n\n### Variable Configuration File\n\nThe [eoepca.tfvars]() file contains deployment-specific values:\n\n| Variable | Purpose | Default Value |\n|----------|---------|---------------|\n| `cluster_name` | Deployment identifier | `\"develop\"` |\n| `external_net` | OpenStack external network UUID | `\"31d7e67a-b30a-43f4-8b06-1667c70ba90d\"` |\n| `subnet_cidr` | Internal network CIDR | `\"192.168.123.0/24\"` |\n| `number_of_bastions` | Bastion host count | `1` |\n| `number_of_k8s_masters_no_floating_ip` | Master count | `1` |\n| `number_of_k8s_nodes_no_floating_ip` | Worker count | `6` |\n| `flavor_bastion` | Bastion VM flavor | `\"14\"` (eo1.xsmall) |\n| `flavor_k8s_master` | Master VM flavor | `\"20\"` (eo2.large) |\n| `flavor_k8s_node` | Worker VM flavor | `\"21\"` (eo2.xlarge) |\n| `image` | OS image name | `\"Ubuntu 18.04 LTS\"` |\n| `nfs_disk_size` | NFS volume size in GB | `1024` |\n\n**Sources:** [creodias/eoepca.tfvars:1-57](), [creodias/variables.tf:1-233]()\n\n### OpenStack Authentication\n\nTerraform uses the OpenStack client for authentication. Configuration is loaded from `clouds.yaml`:\n\n**Example clouds.yaml:**\n```yaml\nclouds:\n  eoepca:\n    auth:\n      auth_url: https://cf2.cloudferro.com:5000/v3\n      username: \"user@example.com\"\n      project_name: \"eoepca\"\n      project_id: d86660d4a1a443579c71096771a8104c\n      user_domain_name: \"cloud_xxxxx\"\n      password: \"password\"\n    region_name: \"RegionOne\"\n    interface: \"public\"\n    identity_api_version: 3\n```\n\n**Location:** Must be placed in one of:\n- `./clouds.yaml` (current directory)\n- `$HOME/.config/openstack/clouds.yaml`\n- `/etc/openstack/clouds.yaml`\n\n**Sources:** [creodias/README.md:29-53]()\n\n## Deployment Process\n\n### Deployment Script Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Script as deployCREODIAS.sh\n    participant TF as terraform\n    participant OS as OpenStack API\n    \n    User-\u003e\u003eScript: ./deployCREODIAS.sh [apply|destroy]\n    Script-\u003e\u003eScript: Check OS_CLOUD env var\n    Script-\u003e\u003eTF: terraform init\n    \n    Note over Script,TF: Phase 1: Keypair\n    Script-\u003e\u003eTF: terraform apply\u003cbr/\u003e-target=module.compute.openstack_compute_keypair_v2.k8s\n    TF-\u003e\u003eOS: Create keypair: kubernetes-{cluster_name}\n    OS--\u003e\u003eTF: Keypair created\n    \n    Script-\u003e\u003eOS: Wait for keypair ready\u003cbr/\u003eopenstack keypair show\n    OS--\u003e\u003eScript: Keypair confirmed\n    \n    Note over Script,TF: Phase 2: Full Deployment\n    Script-\u003e\u003eTF: terraform apply\u003cbr/\u003e-var-file=eoepca.tfvars\n    TF-\u003e\u003eOS: Create network resources\n    TF-\u003e\u003eOS: Create compute instances\n    TF-\u003e\u003eOS: Create load balancer\n    TF-\u003e\u003eOS: Attach volumes\n    OS--\u003e\u003eTF: Infrastructure provisioned\n    TF--\u003e\u003eScript: Outputs (IPs, IDs)\n    Script--\u003e\u003eUser: Deployment complete\n```\n\n**Deployment Steps:**\n1. Set `OS_CLOUD` environment variable to match `clouds.yaml` project\n2. Run [deployCREODIAS.sh]():\n   ```bash\n   cd creodias\n   export OS_CLOUD=eoepca\n   ./deployCREODIAS.sh apply\n   ```\n3. Terraform creates keypair first, then remaining resources\n4. Script waits for keypair availability before proceeding\n5. Full deployment completes with all VMs, network, and storage\n\n**Sources:** [creodias/deployCREODIAS.sh:1-52](), [creodias/README.md:58-68]()\n\n## Terraform Outputs\n\nThe Terraform configuration produces outputs used by subsequent deployment steps:\n\n```mermaid\ngraph LR\n    subgraph \"Terraform Outputs\"\n        BastionFIPs[\"bastion_fips\u003cbr/\u003eList of bastion floating IPs\"]\n        LBFIPs[\"loadbalancer_fips\u003cbr/\u003eList of LB floating IPs\"]\n        MasterIPs[\"k8s_master_ips\u003cbr/\u003ePrivate IPs of masters\"]\n        NodeIPs[\"k8s_node_ips\u003cbr/\u003ePrivate IPs of workers\"]\n        NFSIP[\"nfs_ip_address\u003cbr/\u003ePrivate IP of NFS\"]\n        SubnetCIDR[\"subnet_cidr\u003cbr/\u003eInternal network CIDR\"]\n    end\n    \n    subgraph \"Consumed By\"\n        RKEConfig[\"create-cluster-config.sh\u003cbr/\u003eGenerates cluster.yml\"]\n        BastionVPN[\"bastion-vpn.sh\u003cbr/\u003eEstablishes sshuttle VPN\"]\n        Kubectl[\"kubectl\u003cbr/\u003eCluster management\"]\n    end\n    \n    BastionFIPs --\u003e|used by| RKEConfig\n    MasterIPs --\u003e|used by| RKEConfig\n    NodeIPs --\u003e|used by| RKEConfig\n    BastionFIPs --\u003e|used by| BastionVPN\n    SubnetCIDR --\u003e|used by| BastionVPN\n    BastionFIPs --\u003e|SSH gateway| Kubectl\n```\n\n**Output Commands:**\n```bash\n# View all outputs as JSON\nterraform output -json\n\n# View specific output\nterraform output bastion_fips\n```\n\n**Output Usage Examples:**\n\nFrom [create-cluster-config.sh:21-45]():\n```bash\n# Extract master nodes from terraform state\nmaster_nodes=$(terraform output -state=../creodias/terraform.tfstate -json | \\\n  jq -r '.k8s_master_ips.value[]')\n\n# Extract worker nodes from terraform state  \nworker_nodes=$(terraform output -state=../creodias/terraform.tfstate -json | \\\n  jq -r '.k8s_node_ips.value[]')\n\n# Extract bastion IP from terraform state\nbastion=$(terraform output -state=../creodias/terraform.tfstate -json | \\\n  jq -r '.bastion_fips.value[]')\n```\n\n**Sources:** [creodias/eoepca.tf:95-146](), [creodias/modules/compute/outputs.tf:1-20](), [kubernetes/create-cluster-config.sh:20-80]()\n\n## Provider Configuration\n\nThe Terraform configuration uses the OpenStack provider with Octavia support:\n\n```hcl\nprovider \"openstack\" {\n  version     = \"~\u003e 1.17\"\n  use_octavia = true\n}\n```\n\n**Provider Features:**\n- Version constraint: `~\u003e 1.17` (1.17.x)\n- `use_octavia = true` - Uses Octavia for load balancer resources\n- Authentication via OpenStack client (`clouds.yaml`)\n\n**Sources:** [creodias/eoepca.tf:1-4]()\n\n## Instance Provisioning with Remote Execution\n\nMaster and worker nodes execute provisioning scripts via SSH through the bastion host:\n\n```mermaid\ngraph LR\n    subgraph \"Provisioning Connection\"\n        Local[\"Local Machine\u003cbr/\u003eTerraform\"]\n        Bastion[\"Bastion Host\u003cbr/\u003ebastion_fips[0]\"]\n        Target[\"Target Instance\u003cbr/\u003eMaster/Worker\"]\n    end\n    \n    subgraph \"Provisioning Steps\"\n        FileProvisioner[\"provisioner 'file'\u003cbr/\u003erke-node-setup.sh  /tmp/\"]\n        RemoteExec[\"provisioner 'remote-exec'\u003cbr/\u003echmod +x\u003cbr/\u003eexecute setup script\"]\n    end\n    \n    Local --\u003e|SSH via| Bastion\n    Bastion --\u003e|SSH to| Target\n    Target --\u003e|upload| FileProvisioner\n    FileProvisioner --\u003e|execute| RemoteExec\n```\n\n**Connection Configuration:**\n```hcl\nconnection {\n  type         = \"ssh\"\n  user         = \"${var.ssh_user}\"\n  private_key  = \"${chomp(file(trimsuffix(var.public_key_path, \".pub\")))}\"\n  host         = \"${self.access_ip_v4}\"\n  bastion_host = var.bastion_fips[0]\n}\n```\n\n**Provisioning Actions:**\n- Uploads `rke-node-setup.sh` to `/tmp/`\n- Makes script executable\n- Executes script to configure Docker and user permissions for RKE\n\n**Sources:** [creodias/modules/compute/main.tf:418-436]()\n\n## State Management\n\nTerraform state is stored in local state files:\n\n| File | Purpose |\n|------|---------|\n| `terraform.tfstate` | Current infrastructure state |\n| `terraform.tfstate.backup` | Previous state backup |\n\n**State Contents:**\n- All resource IDs and attributes\n- Resource dependencies\n- Output values\n- Provider configurations\n\n**State Format:** JSON with versioning (format version 4, Terraform 0.12.29)\n\n**Important:** State files contain sensitive data and should not be committed to version control.\n\n**Sources:** [creodias/terraform.tfstate:1-10](), [creodias/terraform.tfstate.backup:1-10]()\n\n## Next Steps\n\nAfter Terraform provisioning completes:\n\n1. **Create Kubernetes Cluster Configuration:**\n   ```bash\n   cd kubernetes\n   ./create-cluster-config.sh develop cluster.yml\n   ```\n   This generates `cluster.yml` for RKE using Terraform outputs.\n\n2. **Establish Bastion VPN:**\n   ```bash\n   ../bin/bastion-vpn.sh\n   ```\n   Creates sshuttle VPN for cluster access.\n\n3. **Deploy Kubernetes:** See [Kubernetes Cluster Setup](#8.1) for RKE deployment steps.\n\n**Sources:** [creodias/README.md:126-128](), [kubernetes/create-cluster-config.sh:1-117]()"])</script><script>self.__next_f.push([1,"38:T42e2,"])</script><script>self.__next_f.push([1,"# Network Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [bin/install-kubeseal.sh](bin/install-kubeseal.sh)\n- [bin/install-rke.sh](bin/install-rke.sh)\n- [creodias/.gitignore](creodias/.gitignore)\n- [creodias/.terraform/modules/modules.json](creodias/.terraform/modules/modules.json)\n- [creodias/README.md](creodias/README.md)\n- [creodias/deployCREODIAS.sh](creodias/deployCREODIAS.sh)\n- [creodias/eoepca.tf](creodias/eoepca.tf)\n- [creodias/eoepca.tfvars](creodias/eoepca.tfvars)\n- [creodias/modules/compute/main.tf](creodias/modules/compute/main.tf)\n- [creodias/modules/compute/nfs-setup.sh](creodias/modules/compute/nfs-setup.sh)\n- [creodias/modules/compute/nfs.tf](creodias/modules/compute/nfs.tf)\n- [creodias/modules/compute/outputs.tf](creodias/modules/compute/outputs.tf)\n- [creodias/modules/compute/variables.tf](creodias/modules/compute/variables.tf)\n- [creodias/modules/loadbalancer/main.tf](creodias/modules/loadbalancer/main.tf)\n- [creodias/terraform.tfstate](creodias/terraform.tfstate)\n- [creodias/terraform.tfstate.backup](creodias/terraform.tfstate.backup)\n- [creodias/variables.tf](creodias/variables.tf)\n- [kubernetes/cluster.7z](kubernetes/cluster.7z)\n- [kubernetes/create-cluster-config.sh](kubernetes/create-cluster-config.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the network topology, security groups, and access patterns for the EOEPCA deployment on OpenStack (CREODIAS). It covers the private network design, bastion host configuration, load balancer setup, and security group rules that control traffic flow within the Kubernetes cluster infrastructure.\n\nFor information about provisioning this network infrastructure using Terraform, see [Terraform Infrastructure as Code](#8.2). For details on setting up the Kubernetes cluster that runs within this network, see [Kubernetes Cluster Setup](#8.1).\n\n## Network Topology Overview\n\nThe EOEPCA deployment creates an isolated private network with controlled external access through a bastion host and load balancer. The following diagram illustrates the high-level network architecture:\n\n```mermaid\ngraph TB\n    subgraph \"External Network\"\n        Internet[Internet]\n        ExtNet[\"external3 Network\u003cbr/\u003e31d7e67a-b30a-43f4-8b06-1667c70ba90d\"]\n    end\n    \n    subgraph \"Floating IPs\"\n        BastionFIP[\"Bastion Floating IP\u003cbr/\u003e185.52.192.185\"]\n        LBFIP[\"Load Balancer Floating IP\u003cbr/\u003e185.52.192.231\"]\n    end\n    \n    subgraph \"Private Network: develop (192.168.123.0/24)\"\n        Router[\"Router\u003cbr/\u003e3e69d186-59dd-4dcc-998c-db78c72d366f\"]\n        \n        subgraph \"Compute Instances\"\n            Bastion[\"develop-bastion-1\u003cbr/\u003e192.168.123.24\"]\n            Master[\"develop-k8s-master-nf-1\u003cbr/\u003e192.168.123.15\"]\n            Worker1[\"develop-k8s-node-nf-1\u003cbr/\u003e192.168.123.16\"]\n            Worker2[\"develop-k8s-node-nf-2\u003cbr/\u003e192.168.123.6\"]\n            WorkerN[\"develop-k8s-node-nf-6\u003cbr/\u003e192.168.123.22\"]\n            NFS[\"develop-nfs\u003cbr/\u003e192.168.123.14\"]\n        end\n        \n        LB[\"Load Balancer\u003cbr/\u003eVIP: Internal\"]\n    end\n    \n    subgraph \"eodata Network\"\n        EOData[\"eodata\u003cbr/\u003e3b3c7f28-6c13-4850-b82a-2a2e351df74b\"]\n        EODataIF[\"Worker eodata Interface\u003cbr/\u003e10.111.0.64\"]\n    end\n    \n    Internet --\u003e ExtNet\n    ExtNet --\u003e BastionFIP\n    ExtNet --\u003e LBFIP\n    BastionFIP --\u003e Bastion\n    LBFIP --\u003e LB\n    \n    Router --\u003e ExtNet\n    Router --\u003e Bastion\n    Router --\u003e Master\n    Router --\u003e Worker1\n    Router --\u003e Worker2\n    Router --\u003e WorkerN\n    Router --\u003e NFS\n    Router --\u003e LB\n    \n    LB --\u003e Worker1\n    LB --\u003e Worker2\n    LB --\u003e WorkerN\n    \n    Worker1 --\u003e EODataIF\n    EODataIF --\u003e EOData\n```\n\n**Network Topology Diagram**\n\nThe deployment consists of:\n- **Private Network**: An isolated internal network (`192.168.123.0/24`) for cluster communication\n- **Router**: Connects the private network to the external network with NAT\n- **Bastion Host**: Single SSH entry point with a floating IP for administrative access\n- **Load Balancer**: Provides public HTTP/HTTPS access with a floating IP\n- **eodata Network**: Separate network providing access to Earth Observation data storage\n\n**Sources:** [creodias/terraform.tfstate:1-95](), [creodias/eoepca.tfvars:1-57](), [creodias/modules/network/main.tf]()\n\n## Security Groups\n\nThe network security is enforced through OpenStack security groups that control ingress and egress traffic. The following table describes each security group and its purpose:\n\n| Security Group | Purpose | Key Rules |\n|----------------|---------|-----------|\n| `develop-bastion` | Protects bastion host | SSH (port 22) from allowed remote IPs |\n| `develop-k8s-master` | Protects Kubernetes control plane | API server (port 6443) from allowed remote IPs |\n| `develop-k8s` | General cluster communication | All traffic between instances in the same group |\n| `develop-k8s-worker` | Protects worker nodes | NodePort range (30000-32767) from internet |\n| `develop-lb` | Protects load balancer | HTTP (80) and HTTPS (443) from internet |\n\n### Security Group Details\n\n```mermaid\ngraph LR\n    subgraph \"Ingress Rules\"\n        Internet1[Internet] --\u003e Bastion22[\"develop-bastion\u003cbr/\u003ePort 22\"]\n        Internet2[Internet] --\u003e K8SMaster[\"develop-k8s-master\u003cbr/\u003ePort 6443\"]\n        Internet3[Internet] --\u003e Worker[\"develop-k8s-worker\u003cbr/\u003ePorts 30000-32767\"]\n        Internet4[Internet] --\u003e LB[\"develop-lb\u003cbr/\u003ePorts 80, 443\"]\n    end\n    \n    subgraph \"Internal Communication\"\n        K8SGroup[\"develop-k8s\u003cbr/\u003eAll ports within group\"]\n    end\n    \n    Bastion22 --\u003e K8SGroup\n    K8SMaster --\u003e K8SGroup\n    Worker --\u003e K8SGroup\n```\n\n**Security Group Rules Diagram**\n\nThe `develop-k8s` security group allows unrestricted communication between all cluster members, facilitating Kubernetes networking requirements. Each instance is assigned multiple security groups to create layered security.\n\n**Sources:** [creodias/modules/compute/main.tf:14-96](), [creodias/terraform.tfstate:410-413](), [creodias/modules/loadbalancer/main.tf:2-36]()\n\n## IP Address Allocation\n\nThe following table documents the IP address assignments within the deployment:\n\n| Resource Type | Hostname | Internal IP | Floating IP | Network Interface |\n|---------------|----------|-------------|-------------|-------------------|\n| Bastion | `develop-bastion-1` | 192.168.123.24 | 185.52.192.185 | develop |\n| Master | `develop-k8s-master-nf-1` | 192.168.123.15 | - | develop |\n| Worker Node 1 | `develop-k8s-node-nf-1` | 192.168.123.16 | - | develop + eodata |\n| Worker Node 2 | `develop-k8s-node-nf-2` | 192.168.123.6 | - | develop + eodata |\n| Worker Node 3 | `develop-k8s-node-nf-3` | 192.168.123.19 | - | develop + eodata |\n| Worker Node 4 | `develop-k8s-node-nf-4` | 192.168.123.13 | - | develop + eodata |\n| Worker Node 5 | `develop-k8s-node-nf-5` | 192.168.123.8 | - | develop + eodata |\n| Worker Node 6 | `develop-k8s-node-nf-6` | 192.168.123.22 | - | develop + eodata |\n| NFS Server | `develop-nfs` | 192.168.123.14 | - | develop |\n| Load Balancer | `develop-lb` | VIP (internal) | 185.52.192.231 | develop |\n\n### Subnet Configuration\n\nThe private subnet `192.168.123.0/24` provides 254 usable IP addresses for cluster resources. DNS resolution is provided by Google public DNS servers (`8.8.8.8`, `8.8.4.4`).\n\n**Sources:** [creodias/terraform.tfstate:6-94](), [creodias/eoepca.tfvars:6-11]()\n\n## Bastion Host Access Pattern\n\nThe bastion host serves as the sole SSH entry point to the cluster, implementing a security best practice that limits attack surface. Administrative access to internal nodes must route through the bastion.\n\n```mermaid\nsequenceDiagram\n    participant Admin as Administrator\n    participant BastionFIP as Bastion\u003cbr/\u003e185.52.192.185\n    participant BastionInt as Bastion Internal\u003cbr/\u003e192.168.123.24\n    participant Node as Internal Node\u003cbr/\u003e192.168.123.x\n    \n    Admin-\u003e\u003eBastionFIP: SSH (port 22)\n    BastionFIP-\u003e\u003eBastionInt: Mapped to internal IP\n    Note over Admin,BastionInt: Authentication with private key\n    \n    Admin-\u003e\u003eBastionInt: SSH jump to internal node\n    BastionInt-\u003e\u003eNode: SSH forwarded\n    Note over BastionInt,Node: Private network communication\n    Node--\u003e\u003eAdmin: Access granted\n```\n\n**Bastion SSH Access Flow**\n\n### Direct SSH Access\n\nAccess to internal nodes using SSH jump host syntax:\n\n```bash\nssh -J eouser@185.52.192.185 eouser@192.168.123.15\n```\n\nThe `-J` flag establishes a jump connection through the bastion to reach the internal node.\n\n### VPN Access via sshuttle\n\nFor persistent administrative access, a VPN tunnel can be established using `sshuttle`:\n\n```bash\nsshuttle -r eouser@185.52.192.185 192.168.123.0/24\n```\n\nThis command creates a transparent VPN that routes all traffic for the `192.168.123.0/24` subnet through the bastion host, enabling direct access to cluster services via their internal IPs.\n\nThe helper script `bastion-vpn.sh` automates this process by extracting connection parameters from Terraform state.\n\n**Sources:** [creodias/README.md:98-120](), [bin/bastion-vpn.sh](), [creodias/terraform.tfstate:7-10]()\n\n## Load Balancer Configuration\n\nThe load balancer provides external HTTP and HTTPS access to services running in the Kubernetes cluster. It is implemented using OpenStack Octavia (LBaaS v2).\n\n```mermaid\ngraph TB\n    subgraph \"External Access\"\n        Internet[Internet Users]\n    end\n    \n    subgraph \"Load Balancer: 185.52.192.231\"\n        HTTPListener[\"HTTP Listener\u003cbr/\u003ePort 80\"]\n        HTTPSListener[\"HTTPS Listener\u003cbr/\u003ePort 443\"]\n        \n        HTTPPool[\"HTTP Pool\u003cbr/\u003eROUND_ROBIN\u003cbr/\u003eSOURCE_IP persistence\"]\n        HTTPSPool[\"HTTPS Pool\u003cbr/\u003eROUND_ROBIN\u003cbr/\u003eSOURCE_IP persistence\"]\n    end\n    \n    subgraph \"Backend Members\"\n        Worker1[\"192.168.123.16\u003cbr/\u003eNodePort 31080/31443\"]\n        Worker2[\"192.168.123.6\u003cbr/\u003eNodePort 31080/31443\"]\n        Worker3[\"192.168.123.19\u003cbr/\u003eNodePort 31080/31443\"]\n        Worker4[\"192.168.123.13\u003cbr/\u003eNodePort 31080/31443\"]\n        Worker5[\"192.168.123.8\u003cbr/\u003eNodePort 31080/31443\"]\n        Worker6[\"192.168.123.22\u003cbr/\u003eNodePort 31080/31443\"]\n    end\n    \n    Internet --\u003e|HTTP| HTTPListener\n    Internet --\u003e|HTTPS| HTTPSListener\n    \n    HTTPListener --\u003e HTTPPool\n    HTTPSListener --\u003e HTTPSPool\n    \n    HTTPPool --\u003e Worker1\n    HTTPPool --\u003e Worker2\n    HTTPPool --\u003e Worker3\n    HTTPPool --\u003e Worker4\n    HTTPPool --\u003e Worker5\n    HTTPPool --\u003e Worker6\n    \n    HTTPSPool --\u003e Worker1\n    HTTPSPool --\u003e Worker2\n    HTTPSPool --\u003e Worker3\n    HTTPSPool --\u003e Worker4\n    HTTPSPool --\u003e Worker5\n    HTTPSPool --\u003e Worker6\n```\n\n**Load Balancer Architecture Diagram**\n\n### Load Balancer Resources\n\nThe load balancer is created by the Terraform module at [creodias/modules/loadbalancer/main.tf:38-45]() and consists of:\n\n- **LoadBalancer**: `openstack_lb_loadbalancer_v2.k8s` attached to the internal network\n- **HTTP Listener**: `openstack_lb_listener_v2.http` on port 80\n- **HTTPS Listener**: `openstack_lb_listener_v2.https` on port 443\n- **Backend Pools**: `openstack_lb_pool_v2.http` and `openstack_lb_pool_v2.https` with `ROUND_ROBIN` algorithm\n- **Session Persistence**: `SOURCE_IP` based to maintain client sessions\n- **Backend Members**: All worker nodes added via `openstack_lb_members_v2` resources\n\n### Port Mapping\n\nThe load balancer forwards traffic to Kubernetes Ingress Controller NodePorts:\n\n| External Port | Internal NodePort | Protocol |\n|---------------|-------------------|----------|\n| 80 | 31080 | HTTP |\n| 443 | 31443 | HTTPS |\n\nThis mapping is defined in [creodias/modules/loadbalancer/main.tf:95-151]() where each worker node is configured as a backend member.\n\n**Sources:** [creodias/modules/loadbalancer/main.tf:38-183](), [creodias/README.md:89-92](), [creodias/terraform.tfstate:68-77]()\n\n## Data Access Networks\n\n### eodata Network\n\nWorker nodes are connected to a secondary network called `eodata` which provides access to CloudFerro's Earth Observation data repository. This network is separate from the cluster management network.\n\n```mermaid\ngraph LR\n    subgraph \"Worker Node\"\n        Eth0[\"eth0\u003cbr/\u003e192.168.123.16\u003cbr/\u003edevelop network\"]\n        Eth1[\"eth1\u003cbr/\u003e10.111.0.64\u003cbr/\u003eeodata network\"]\n    end\n    \n    subgraph \"Networks\"\n        DevelopNet[\"develop Network\u003cbr/\u003eCluster management\"]\n        EODataNet[\"eodata Network\u003cbr/\u003e3b3c7f28-6c13-4850-b82a-2a2e351df74b\"]\n    end\n    \n    subgraph \"Storage\"\n        EODataStore[\"CloudFerro eodata\u003cbr/\u003eEO data repository\"]\n    end\n    \n    Eth0 --\u003e DevelopNet\n    Eth1 --\u003e EODataNet\n    EODataNet --\u003e EODataStore\n```\n\n**Worker Node Network Interfaces**\n\nWorker nodes are configured with dual network interfaces:\n- Primary interface on the `develop` network for cluster communication\n- Secondary interface on the `eodata` network for data access\n\nThis configuration is specified in the Terraform variables at [creodias/eoepca.tfvars:9]() and implemented in the compute module.\n\n**Sources:** [creodias/terraform.tfstate:768-777](), [creodias/eoepca.tfvars:9](), [creodias/modules/compute/main.tf:777-786]()\n\n### NFS Network Setup\n\nThe NFS server provides shared persistent storage for the Kubernetes cluster. It is attached to an additional block storage volume and exports NFS shares over the private network.\n\nThe NFS instance is created by [creodias/modules/compute/nfs.tf:1-45]() and includes:\n- **Instance**: `openstack_compute_instance_v2.eoepca_nfs` with flavor `eo2.large`\n- **Block Volume**: `openstack_blockstorage_volume_v2.nfs_expansion` providing additional SSD storage\n- **Volume Attachment**: Mounted at `/dev/sdb` and formatted as ext4\n- **Export Path**: `/data` with subdirectories for different services\n\nThe NFS setup script at [creodias/modules/compute/nfs-setup.sh:1-44]() configures:\n- NFS kernel server installation\n- Partition creation and formatting on the attached volume\n- Export directories: `userman`, `proc`, `resman`, `dynamic`\n- NFS exports with `rw,no_root_squash,no_subtree_check` options\n\nAll cluster nodes can mount NFS shares from `192.168.123.14:/data/\u003csubdirectory\u003e` over the private network.\n\n**Sources:** [creodias/modules/compute/nfs.tf:1-70](), [creodias/modules/compute/nfs-setup.sh:1-44](), [creodias/eoepca.tfvars:56](), [creodias/terraform.tfstate:222-258]()\n\n## Network Provisioning Workflow\n\nThe network infrastructure is provisioned using Terraform modules with the following component relationships:\n\n```mermaid\ngraph TB\n    RootModule[\"Root Module\u003cbr/\u003ecreodias/eoepca.tf\"]\n    \n    NetworkModule[\"network Module\u003cbr/\u003emodules/network\"]\n    IPsModule[\"ips Module\u003cbr/\u003emodules/ips\"]\n    ComputeModule[\"compute Module\u003cbr/\u003emodules/compute\"]\n    LBModule[\"loadbalancer Module\u003cbr/\u003emodules/loadbalancer\"]\n    \n    ExtNet[\"External Network\u003cbr/\u003e31d7e67a-...\"]\n    Router[\"openstack_networking_router_v2.k8s\"]\n    Subnet[\"openstack_networking_subnet_v2.k8s\"]\n    \n    FloatingIPs[\"openstack_networking_floatingip_v2\"]\n    Instances[\"openstack_compute_instance_v2\"]\n    SecurityGroups[\"openstack_networking_secgroup_v2\"]\n    LoadBalancer[\"openstack_lb_loadbalancer_v2.k8s\"]\n    \n    RootModule --\u003e NetworkModule\n    RootModule --\u003e IPsModule\n    RootModule --\u003e ComputeModule\n    RootModule --\u003e LBModule\n    \n    NetworkModule --\u003e Router\n    NetworkModule --\u003e Subnet\n    Router --\u003e ExtNet\n    \n    IPsModule --\u003e FloatingIPs\n    IPsModule --\u003e Router\n    \n    ComputeModule --\u003e Instances\n    ComputeModule --\u003e SecurityGroups\n    ComputeModule --\u003e FloatingIPs\n    \n    LBModule --\u003e LoadBalancer\n    LBModule --\u003e Subnet\n    LBModule --\u003e Instances\n```\n\n**Terraform Module Dependencies**\n\nThe provisioning sequence follows these dependencies:\n1. Network module creates the router and subnet\n2. IPs module allocates floating IPs with dependency on router\n3. Compute module creates instances and security groups\n4. LoadBalancer module creates the load balancer with backend members\n\nThe deployment is initiated by [creodias/deployCREODIAS.sh:1-52]() which first provisions the SSH keypair, then deploys all other resources.\n\n**Sources:** [creodias/eoepca.tf:1-146](), [creodias/deployCREODIAS.sh:1-52](), [creodias/modules/modules.json:1]()\n\n## Network Configuration Files\n\nThe network architecture is defined through several key configuration files:\n\n| File | Purpose | Key Configuration |\n|------|---------|-------------------|\n| `creodias/eoepca.tfvars` | Deployment parameters | Subnet CIDR, network names, instance counts |\n| `creodias/eoepca.tf` | Root Terraform module | Module composition and outputs |\n| `creodias/variables.tf` | Variable definitions | Network configuration variables |\n| `modules/network/main.tf` | Network resources | Router, subnet, port creation |\n| `modules/ips/main.tf` | Floating IP allocation | Bastion and worker floating IPs |\n| `modules/compute/main.tf` | Security groups | Security group rules definition |\n| `modules/loadbalancer/main.tf` | Load balancer | Listener, pool, member configuration |\n\nThe Terraform state is persisted in [creodias/terraform.tfstate]() which records all provisioned resource IDs and IP addresses for subsequent operations.\n\n**Sources:** [creodias/eoepca.tfvars:1-57](), [creodias/eoepca.tf:1-146](), [creodias/variables.tf:1-233]()"])</script><script>self.__next_f.push([1,"39:T5602,"])</script><script>self.__next_f.push([1,"# Development and Testing\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot](test/acceptance/02__Processing/01__ADES/01__API_PROC.robot)\n- [test/acceptance/02__Processing/01__ADES/02__WPS.robot](test/acceptance/02__Processing/01__ADES/02__WPS.robot)\n- [test/acceptance/__init__.robot](test/acceptance/__init__.robot)\n- [test/client/.gitignore](test/client/.gitignore)\n- [test/client/DemoClient.py](test/client/DemoClient.py)\n- [test/client/debug/jwt-output-by-pep.json](test/client/debug/jwt-output-by-pep.json)\n- [test/client/main.py](test/client/main.py)\n- [test/client/requirements.txt](test/client/requirements.txt)\n- [test/client/setup.sh](test/client/setup.sh)\n- [travis/acceptanceTest.sh](travis/acceptanceTest.sh)\n- [travis/setupMinikube.sh](travis/setupMinikube.sh)\n- [travis/setupRobot.sh](travis/setupRobot.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the development tools, test frameworks, and local development environments available for testing EOEPCA services. It provides technical guidance for developers who need to:\n\n- Interact with EOEPCA services programmatically using the DemoClient library\n- Run automated acceptance tests using Robot Framework\n- Set up local development environments using Minikube\n- Validate deployments and debug service interactions\n\nFor detailed information on specific testing subsystems, see:\n- DemoClient library implementation: [DemoClient Library](#9.1)\n- Robot Framework acceptance tests: [Acceptance Testing Framework](#9.2)\n- Local development setup: [Local Development with Minikube](#9.3)\n\nFor deployment validation procedures, see [Testing and Validation](#2.3).\n\n**Sources:** [test/client/main.py:1-225](), [test/client/DemoClient.py:1-683](), [test/acceptance/__init__.robot:1-22]()\n\n---\n\n## Testing Architecture Overview\n\nThe EOEPCA testing framework consists of three primary components:\n\n| Component | Technology | Purpose |\n|-----------|-----------|---------|\n| **DemoClient** | Python library | Programmatic client for UMA authentication and service interaction |\n| **Robot Framework Tests** | Robot Framework + Python | Automated acceptance tests for all EOEPCA services |\n| **Local Dev Environment** | Minikube/k3s | Kubernetes environment for local testing and development |\n\n```mermaid\ngraph TB\n    subgraph \"Test Framework Components\"\n        DemoClient[\"DemoClient Library\u003cbr/\u003etest/client/DemoClient.py\"]\n        MainScript[\"Test Client Script\u003cbr/\u003etest/client/main.py\"]\n        RobotTests[\"Robot Framework Tests\u003cbr/\u003etest/acceptance/**/*.robot\"]\n    end\n    \n    subgraph \"Development Environments\"\n        Minikube[\"Local Minikube Cluster\u003cbr/\u003etravis/setupMinikube.sh\"]\n        CREODIAS[\"CREODIAS Deployment\u003cbr/\u003edevelop.eoepca.org\"]\n    end\n    \n    subgraph \"EOEPCA Services Under Test\"\n        IdentityService[\"Identity Service\u003cbr/\u003eauth.domain\"]\n        ADES[\"ADES Service\u003cbr/\u003eades.domain\"]\n        WorkspaceAPI[\"Workspace API\u003cbr/\u003eworkspace-api.domain\"]\n        ResourceCatalogue[\"Resource Catalogue\u003cbr/\u003eresource-catalogue.domain\"]\n        DummyService[\"Dummy Service\u003cbr/\u003edummy-service.domain\"]\n    end\n    \n    MainScript --\u003e|Uses| DemoClient\n    RobotTests --\u003e|Uses| DemoClient\n    \n    DemoClient --\u003e|UMA Auth| IdentityService\n    DemoClient --\u003e|Test Endpoints| ADES\n    DemoClient --\u003e|Test Endpoints| WorkspaceAPI\n    DemoClient --\u003e|Test Endpoints| DummyService\n    \n    Minikube --\u003e|Hosts| IdentityService\n    Minikube --\u003e|Hosts| ADES\n    Minikube --\u003e|Hosts| WorkspaceAPI\n    \n    CREODIAS --\u003e|Hosts| IdentityService\n    CREODIAS --\u003e|Hosts| ADES\n    CREODIAS --\u003e|Hosts| WorkspaceAPI\n    CREODIAS --\u003e|Hosts| ResourceCatalogue\n```\n\n**Testing Components Diagram**\n\n**Sources:** [test/client/DemoClient.py:14-33](), [test/client/main.py:7-33](), [test/acceptance/__init__.robot:1-22]()\n\n---\n\n## DemoClient Library Overview\n\nThe `DemoClient` class provides a comprehensive Python library for interacting with EOEPCA services, handling UMA authentication flows, and managing client state.\n\n### Core Capabilities\n\n```mermaid\ngraph LR\n    subgraph \"DemoClient Class Structure\"\n        Init[\"__init__()\u003cbr/\u003eInitialize session\"]\n        \n        subgraph \"User Management Methods\"\n            GetToken[\"get_token_endpoint()\"]\n            RegClient[\"register_client()\"]\n            GetIDToken[\"get_id_token()\"]\n            RegResource[\"register_protected_resource()\"]\n            GetAccessToken[\"get_access_token_from_ticket()\"]\n        end\n        \n        subgraph \"UMA Flow Methods\"\n            UMARequest[\"uma_http_request()\"]\n            NotUMARequest[\"notuma_http_request()\"]\n        end\n        \n        subgraph \"Service-Specific Methods\"\n            DummyCall[\"dummy_service_call()\"]\n            WSAPICreate[\"wsapi_create()\"]\n            WSAPIGetDetails[\"wsapi_get_details()\"]\n            ProcList[\"proc_list_processes()\"]\n            ProcDeploy[\"proc_deploy_application()\"]\n            ProcExecute[\"proc_execute_application()\"]\n            ProcStatus[\"proc_get_job_status()\"]\n            ProcResult[\"proc_get_job_result()\"]\n            WPSCaps[\"wps_get_capabilities()\"]\n        end\n        \n        subgraph \"Policy Management\"\n            UpdatePolicy[\"update_policy()\"]\n            GetOwnership[\"get_ownership_id()\"]\n            GetResourceByName[\"get_resource_by_name()\"]\n            ResetPolicy[\"reset_resource_policy()\"]\n        end\n        \n        subgraph \"State Management\"\n            LoadState[\"load_state()\"]\n            SaveState[\"save_state()\"]\n        end\n    end\n    \n    Init --\u003e GetToken\n    GetToken --\u003e RegClient\n    RegClient --\u003e GetIDToken\n    GetIDToken --\u003e RegResource\n    RegResource --\u003e UMARequest\n    \n    UMARequest --\u003e DummyCall\n    UMARequest --\u003e WSAPICreate\n    UMARequest --\u003e ProcList\n    UMARequest --\u003e ProcExecute\n    \n    GetAccessToken --\u003e UMARequest\n```\n\n**DemoClient Method Architecture**\n\n**Sources:** [test/client/DemoClient.py:14-683]()\n\n### Key Class Attributes\n\n| Attribute | Type | Purpose |\n|-----------|------|---------|\n| `base_url` | str | Base URL for identity service (e.g., `https://auth.domain`) |\n| `session` | requests.Session | HTTP session with SSL verification disabled |\n| `token_endpoint` | str | OAuth2/UMA token endpoint URL |\n| `scim_client` | EOEPCA_Scim | SCIM client for OIDC registration |\n| `state` | dict | Persisted state (client_id, client_secret, resource_ids) |\n| `trace_flow` | bool | Enable UMA flow debugging output |\n| `trace_requests` | bool | Enable HTTP request logging |\n\n**Sources:** [test/client/DemoClient.py:21-33]()\n\n### UMA Authentication Flow Implementation\n\nThe `uma_http_request` method implements the complete UMA flow:\n\n```mermaid\nsequenceDiagram\n    participant Client as DemoClient\n    participant Method as uma_http_request()\n    participant PEP as Protected Service\n    participant Identity as Identity Service\n    \n    Client-\u003e\u003eMethod: Call with id_token, access_token\n    \n    alt access_token exists\n        Method-\u003e\u003eMethod: Add Authorization header\n    end\n    \n    Method-\u003e\u003ePEP: HTTP Request\n    \n    alt Response 200 OK\n        PEP--\u003e\u003eMethod: Success\n        Method--\u003e\u003eClient: Return response, access_token\n    end\n    \n    alt Response 401 Unauthorized\n        PEP--\u003e\u003eMethod: WWW-Authenticate: ticket=\u003cticket\u003e\n        Method-\u003e\u003eMethod: Extract ticket from header\n        Method-\u003e\u003eIdentity: POST /token\u003cbr/\u003e(ticket + id_token)\n        Identity--\u003e\u003eMethod: Return RPT (access_token)\n        Method-\u003e\u003ePEP: Retry request with RPT\n        PEP--\u003e\u003eMethod: Success\n        Method--\u003e\u003eClient: Return response, new access_token\n    end\n```\n\n**UMA Flow Implementation in DemoClient**\n\n**Sources:** [test/client/DemoClient.py:239-291]()\n\n### Service Interaction Methods\n\nThe DemoClient provides keyword-decorated methods for Robot Framework integration:\n\n| Method | Decorator | Service | Purpose |\n|--------|-----------|---------|---------|\n| `get_id_token()` | `@keyword` | Identity Service | Obtain user ID token via password grant |\n| `register_protected_resource()` | `@keyword` | PEP Resource API | Register resource path with ownership |\n| `wsapi_create()` | `@keyword` | Workspace API | Create user workspace |\n| `wsapi_get_details()` | `@keyword` | Workspace API | Get workspace details |\n| `proc_list_processes()` | `@keyword` | ADES | List deployed applications |\n| `proc_deploy_application()` | `@keyword` | ADES | Deploy CWL application |\n| `proc_execute_application()` | `@keyword` | ADES | Execute application job |\n| `proc_poll_job_completion()` | `@keyword` | ADES | Poll job status until completion |\n| `proc_get_job_result()` | `@keyword` | ADES | Get STAC catalog result |\n| `wps_get_capabilities()` | `@keyword` | ADES WPS | Get WPS capabilities document |\n\n**Sources:** [test/client/DemoClient.py:113-536]()\n\n---\n\n## Robot Framework Integration\n\nThe DemoClient is decorated with Robot Framework keywords, enabling declarative test syntax:\n\n```mermaid\ngraph TB\n    subgraph \"Robot Framework Test Structure\"\n        TestSuite[\"Test Suite\u003cbr/\u003e01__API_PROC.robot\"]\n        Settings[\"*** Settings ***\u003cbr/\u003eLibrary imports\"]\n        Variables[\"*** Variables ***\u003cbr/\u003eTest configuration\"]\n        TestCases[\"*** Test Cases ***\u003cbr/\u003eTest scenarios\"]\n        Keywords[\"*** Keywords ***\u003cbr/\u003eReusable test logic\"]\n    end\n    \n    subgraph \"DemoClient Integration\"\n        DemoClientLib[\"DemoClient.py\u003cbr/\u003e@library decorator\"]\n        KeywordMethods[\"@keyword decorated methods\"]\n    end\n    \n    subgraph \"Test Execution Flow\"\n        SuiteSetup[\"Suite Setup:\u003cbr/\u003eInit ID Token\"]\n        TestCase1[\"Deploy Application\"]\n        TestCase2[\"Execute Application\"]\n        TestCase3[\"Get Job Status\"]\n        TestCase4[\"Undeploy Application\"]\n        SuiteTeardown[\"Suite Teardown:\u003cbr/\u003eSave State\"]\n    end\n    \n    Settings --\u003e|Import| DemoClientLib\n    DemoClientLib --\u003e|Provides| KeywordMethods\n    Keywords --\u003e|Call| KeywordMethods\n    TestCases --\u003e|Use| Keywords\n    \n    SuiteSetup --\u003e TestCase1\n    TestCase1 --\u003e TestCase2\n    TestCase2 --\u003e TestCase3\n    TestCase3 --\u003e TestCase4\n    TestCase4 --\u003e SuiteTeardown\n```\n\n**Robot Framework and DemoClient Integration**\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot:1-124](), [test/client/DemoClient.py:12-14]()\n\n### Example Robot Test Structure\n\nThe ADES API test suite demonstrates the Robot Framework pattern:\n\n```\n*** Settings ***\nDocumentation  Tests for the ADES OGC API Processes endpoint\nLibrary  ../../../client/DemoClient.py  ${UM_BASE_URL}\n\n*** Variables ***\n${USERNAME}=  ${USER_A_NAME}\n${ADES_WORKSPACE}=  ${USERNAME}\n${API_PROC_SERVICE_URL}=  ${ADES_BASE_URL}${API_PROC_PATH_PREFIX}\n\n*** Test Cases ***\nDeploy Application\n  Deploy Application  ${CURDIR}${/}data/app-deploy-body-cwl.json\n  Sleep  5  Waiting for process deploy process to complete\n  Process Is Deployed  ${PROCESS_NAME}\n\nExecute Application\n  Execute Application Success  ${PROCESS_NAME}  ${CURDIR}${/}data/app-execute-body.json\n\n*** Keywords ***\nDeploy Application\n  [Arguments]  ${app_filename}\n  ${resp}  ${access_token} =  Proc Deploy App  ${API_PROC_SERVICE_URL}  ${app_filename}  ${ID_TOKEN}  ${ACCESS_TOKEN}\n  Should Be Equal As Integers  201  ${resp.status_code}\n```\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot:1-124]()\n\n---\n\n## Test Client Execution Pattern\n\nThe `main.py` script demonstrates the complete testing workflow:\n\n```mermaid\ngraph TB\n    Start[\"Start Test Client\u003cbr/\u003emain.py:main()\"]\n    \n    subgraph \"1. Initialize Client\"\n        InitClient[\"Create DemoClient instance\u003cbr/\u003ebase_url = https://auth.domain\"]\n        LoadState[\"Load state.json\u003cbr/\u003e(reuse client credentials)\"]\n    end\n    \n    subgraph \"2. User Management Setup\"\n        GetToken[\"Get token endpoint\u003cbr/\u003e/.well-known/uma2-configuration\"]\n        RegisterClient[\"Register OAuth2 client\u003cbr/\u003e(via SCIM)\"]\n        GetIDToken[\"Get user ID token\u003cbr/\u003e(password grant)\"]\n        RegisterResources[\"Register protected resources\u003cbr/\u003e(ADES, Workspace paths)\"]\n    end\n    \n    subgraph \"3. Workspace Operations\"\n        CreateWorkspace[\"Create workspace\u003cbr/\u003ePOST /workspaces\"]\n        GetWorkspaceDetails[\"Get workspace details\u003cbr/\u003eGET /workspaces/{user}\"]\n        TestAccessControl[\"Test access as different user\u003cbr/\u003e(should fail)\"]\n    end\n    \n    subgraph \"4. Processing Operations\"\n        ListProcesses[\"List processes\u003cbr/\u003eGET /wps3/processes\"]\n        DeployApp[\"Deploy application\u003cbr/\u003ePOST /wps3/processes\"]\n        GetAppDetails[\"Get app details\u003cbr/\u003eGET /wps3/processes/{app}\"]\n        ExecuteApp[\"Execute application\u003cbr/\u003ePOST /wps3/processes/{app}/execution\"]\n        PollStatus[\"Poll job status\u003cbr/\u003eGET /wps3/jobs/{job}\"]\n        GetResult[\"Get job result\u003cbr/\u003eGET /wps3/jobs/{job}/results\"]\n        UndeployApp[\"Undeploy application\u003cbr/\u003eDELETE /wps3/processes/{app}\"]\n    end\n    \n    subgraph \"5. WPS Operations\"\n        WPSCaps[\"Get WPS capabilities\u003cbr/\u003eGET /zoo?request=GetCapabilities\"]\n    end\n    \n    SaveState[\"Save state.json\u003cbr/\u003e(persist tokens, resources)\"]\n    \n    Start --\u003e InitClient\n    InitClient --\u003e LoadState\n    LoadState --\u003e GetToken\n    GetToken --\u003e RegisterClient\n    RegisterClient --\u003e GetIDToken\n    GetIDToken --\u003e RegisterResources\n    \n    RegisterResources --\u003e CreateWorkspace\n    CreateWorkspace --\u003e GetWorkspaceDetails\n    GetWorkspaceDetails --\u003e TestAccessControl\n    \n    TestAccessControl --\u003e ListProcesses\n    ListProcesses --\u003e DeployApp\n    DeployApp --\u003e GetAppDetails\n    GetAppDetails --\u003e ExecuteApp\n    ExecuteApp --\u003e PollStatus\n    PollStatus --\u003e GetResult\n    GetResult --\u003e UndeployApp\n    \n    UndeployApp --\u003e WPSCaps\n    WPSCaps --\u003e SaveState\n```\n\n**Test Client Execution Flow**\n\n**Sources:** [test/client/main.py:7-224]()\n\n### Test Client Configuration\n\nThe test client is configured at the top of `main.py`:\n\n| Configuration | Default Value | Purpose |\n|--------------|---------------|---------|\n| `USER_NAME` | `\"eric\"` | Test user for authentication |\n| `USER_PASSWORD` | `\"defaultPWD\"` | Test user password |\n| `domain` | `\"develop.eoepca.org\"` | Target deployment domain |\n| `base_url` | `\"https://auth.{domain}\"` | Identity service base URL |\n| `ades_url` | `\"https://ades.{domain}\"` | ADES service URL |\n| `wsapi_url` | `\"https://workspace-api.{domain}\"` | Workspace API URL |\n\n**Sources:** [test/client/main.py:9-28]()\n\n---\n\n## State Persistence\n\nThe DemoClient maintains persistent state in `state.json` to avoid repeated client registration:\n\n```json\n{\n  \"client_id\": \"963e4c2a-9924-4c4d-b999-bebe79c96a5e\",\n  \"client_secret\": \"\u003csecret\u003e\",\n  \"resources\": {\n    \"http://ades-pepapi.domain\": {\n      \"/eric\": \"6c58a5e5-95b7-44ca-ab49-b1192e0db198\"\n    },\n    \"http://workspace-api-pepapi.domain\": {\n      \"/workspaces/develop-user-eric\": \"b7cade5c-372f-4c87-be82-a795649e8729\"\n    }\n  }\n}\n```\n\nThe state management methods:\n- `load_state()` - Called in `__init__`, loads from `state.json` if it exists\n- `save_state()` - Persists current state to `state.json`\n\n**Sources:** [test/client/DemoClient.py:35-55](), [test/client/.gitignore:4]()\n\n---\n\n## Local Development Environment Setup\n\n### Minikube Installation and Configuration\n\nThe `setupMinikube.sh` script provisions a local Kubernetes cluster:\n\n```mermaid\ngraph TB\n    Start[\"Start Setup\u003cbr/\u003esetupMinikube.sh\"]\n    \n    subgraph \"1. Install Components\"\n        InstallKubectl[\"Install kubectl v1.13.0\"]\n        InstallMinikube[\"Install minikube v1.12.1\"]\n        InstallHelm[\"Install Helm 3\"]\n    end\n    \n    subgraph \"2. Cluster Setup\"\n        MountRoot[\"Mount root as rshared\u003cbr/\u003e(fix kube-dns)\"]\n        DeleteCluster[\"Delete existing cluster\u003cbr/\u003eminikube delete --purge\"]\n        StartCluster[\"Start minikube\u003cbr/\u003e--vm-driver=none\u003cbr/\u003e--kubernetes-version=v1.13.0\"]\n        EnableIngress[\"Enable ingress addon\u003cbr/\u003eminikube addons enable ingress\"]\n        UpdateContext[\"Update kubectl context\"]\n    end\n    \n    subgraph \"3. Wait for Ready\"\n        WaitNodes[\"Wait for nodes Ready=True\"]\n        ShowInfo[\"kubectl cluster-info\"]\n    end\n    \n    subgraph \"4. Configure Access\"\n        FixPerms[\"Fix permissions\u003cbr/\u003echmod ~/.kube, ~/.minikube\"]\n        ChownFiles[\"chown to user\"]\n    end\n    \n    subgraph \"5. Install Tools\"\n        InstallJQ[\"apt-get install jq\"]\n        InstallConntrack[\"apt-get install conntrack\"]\n        InstallSocat[\"apt-get install socat\"]\n        InstallPython[\"apt-get install python3-venv\"]\n    end\n    \n    subgraph \"6. Create Paths\"\n        CreateDataPath[\"mkdir -p /data/config/db\"]\n    end\n    \n    Start --\u003e InstallKubectl\n    InstallKubectl --\u003e InstallMinikube\n    InstallMinikube --\u003e InstallHelm\n    \n    InstallHelm --\u003e MountRoot\n    MountRoot --\u003e DeleteCluster\n    DeleteCluster --\u003e StartCluster\n    StartCluster --\u003e EnableIngress\n    EnableIngress --\u003e UpdateContext\n    \n    UpdateContext --\u003e WaitNodes\n    WaitNodes --\u003e ShowInfo\n    \n    ShowInfo --\u003e FixPerms\n    FixPerms --\u003e ChownFiles\n    \n    ChownFiles --\u003e InstallJQ\n    InstallJQ --\u003e InstallConntrack\n    InstallConntrack --\u003e InstallSocat\n    InstallSocat --\u003e InstallPython\n    \n    InstallPython --\u003e CreateDataPath\n```\n\n**Minikube Setup Workflow**\n\n**Sources:** [travis/setupMinikube.sh:1-51]()\n\n### Key Setup Parameters\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `K8S_VER` | `v1.13.0` | Kubernetes version |\n| `MINIKUBE_VER` | `v1.12.1` | Minikube version |\n| `--vm-driver` | `none` | Run directly on host (no VM) |\n| `--bootstrapper` | `kubeadm` | Use kubeadm for cluster init |\n| `--extra-config` | `apiserver.authorization-mode=RBAC` | Enable RBAC |\n\n**Sources:** [travis/setupMinikube.sh:2-21]()\n\n---\n\n## Acceptance Test Execution\n\n### Test Runner Script\n\nThe `acceptanceTest.sh` script executes Robot Framework tests:\n\n```bash\n# Get Kubernetes cluster IP\nkubeIP=$(kubectl cluster-info | grep 'master' | grep -oE '[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}')\n\n# Run Robot Framework tests\n~/.local/bin/robot test/acceptance\n```\n\n**Sources:** [travis/acceptanceTest.sh:20-31]()\n\n### Global Test Variables\n\nThe `__init__.robot` file sets up global variables for all tests:\n\n| Variable | Value | Purpose |\n|----------|-------|---------|\n| `${UM_BASE_URL}` | `https://auth.${PUBLIC_HOSTNAME}` | Identity service endpoint |\n| `${ADES_BASE_URL}` | `http://ades.${PUBLIC_HOSTNAME}` | ADES service endpoint |\n| `${ADES_RESOURCES_API_URL}` | `http://ades-pepapi.${PUBLIC_HOSTNAME}` | ADES PEP resource API |\n| `${CATALOGUE_BASE_URL}` | `https://resource-catalogue.${PUBLIC_HOSTNAME}` | Catalogue endpoint |\n| `${USER_A_NAME}` | `eric` | Primary test user |\n| `${USER_A_PASSWORD}` | `defaultPWD` | Test user password |\n| `${USER_PREFIX}` | `develop-user` | Workspace prefix for user resources |\n\n**Sources:** [test/acceptance/__init__.robot:6-21]()\n\n---\n\n## Test Dependencies\n\n### Python Requirements\n\nThe test client requires the following Python packages:\n\n| Package | Version | Purpose |\n|---------|---------|---------|\n| `eoepca-scim` | 2.8.1 | SCIM client for OIDC registration |\n| `PyJWT` | 2.6.0 | JWT token parsing and validation |\n| `requests` | 2.26.0 | HTTP client for service calls |\n| `robotframework` | 4.1 | Test framework and keywords |\n| `pyjwkest` | 1.4.2 | JWT key management |\n| `WellKnownHandler` | 0.2.0 | OpenID Connect discovery |\n\n**Sources:** [test/client/requirements.txt:1-7]()\n\n### Robot Framework Dependencies\n\nThe Robot Framework installation includes:\n\n```bash\npip install -U \\\n  robotframework \\\n  robotframework-requests \\\n  robotframework-seleniumlibrary \\\n  robotframework-sshlibrary\n```\n\n**Sources:** [travis/setupRobot.sh:2-8]()\n\n---\n\n## Test Client Setup\n\n### Installation Steps\n\n```bash\n# Navigate to test client directory\ncd test/client\n\n# Run setup script\n./setup.sh\n```\n\nThe setup script:\n1. Creates Python virtual environment: `python3 -m venv venv`\n2. Activates virtual environment: `source venv/bin/activate`\n3. Upgrades pip: `python -m pip install -U pip`\n4. Installs dependencies: `pip install -U -r requirements.txt`\n\n**Sources:** [test/client/setup.sh:1-14]()\n\n### Running the Test Client\n\n```bash\n# Activate virtual environment\nsource venv/bin/activate\n\n# Run test client\npython main.py\n```\n\n**Sources:** [test/client/main.py:1-3]()\n\n---\n\n## JWT Token Structure\n\nDuring testing, the PEP injects JWT tokens with the following structure:\n\n```json\n{\n  \"header\": {\n    \"alg\": \"RS256\",\n    \"kid\": \"RSA1\"\n  },\n  \"payload\": {\n    \"active\": true,\n    \"permissions\": [\n      {\n        \"resource_id\": \"c90141d2-223a-4e51-ab00-99e31898e77b\",\n        \"resource_scopes\": [\"Authenticated\", \"user_name\", \"openid\"],\n        \"exp\": 1604406738\n      }\n    ],\n    \"client_id\": \"cbc4f5c1-f444-4b0c-8ed2-6949f0f88476\",\n    \"pct_claims\": {\n      \"sub\": [\"24243808-b7a1-4c28-9b47-c0e5c84e7882\"],\n      \"user_name\": [\"demo\"],\n      \"iss\": [\"https://test.192.168.49.2.nip.io\"]\n    }\n  }\n}\n```\n\nThe `pct_claims` section contains user identity information extracted from the original ID token.\n\n**Sources:** [test/client/debug/jwt-output-by-pep.json:1-52]()\n\n---\n\n## Testing Best Practices\n\n### State Management\n\n- Always call `save_state()` at test completion to persist client credentials\n- Check for existing `state.json` before re-registering clients\n- Delete `state.json` when testing against a fresh deployment\n\n### UMA Flow Testing\n\n- Enable `trace_flow = True` to debug authentication issues\n- Reuse `access_token` across requests to the same service\n- Handle 401 responses gracefully with automatic ticket exchange\n\n### Resource Registration\n\n- Register base paths (e.g., `/eric`) before accessing sub-resources\n- Use meaningful resource names for debugging\n- Include appropriate scopes (`protected_access`, `public_access`)\n\n### Job Status Polling\n\n- Use `proc_poll_job_completion()` with appropriate intervals (30-60 seconds)\n- Implement error count limits to prevent infinite loops\n- Disable request tracing during polling to reduce log noise\n\n**Sources:** [test/client/DemoClient.py:473-500](), [test/client/main.py:54-65](), [test/client/main.py:187-195]()"])</script><script>self.__next_f.push([1,"3a:T558a,"])</script><script>self.__next_f.push([1,"# DemoClient Library\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot](test/acceptance/02__Processing/01__ADES/01__API_PROC.robot)\n- [test/acceptance/02__Processing/01__ADES/02__WPS.robot](test/acceptance/02__Processing/01__ADES/02__WPS.robot)\n- [test/acceptance/__init__.robot](test/acceptance/__init__.robot)\n- [test/client/.gitignore](test/client/.gitignore)\n- [test/client/DemoClient.py](test/client/DemoClient.py)\n- [test/client/debug/jwt-output-by-pep.json](test/client/debug/jwt-output-by-pep.json)\n- [test/client/main.py](test/client/main.py)\n- [test/client/requirements.txt](test/client/requirements.txt)\n- [test/client/setup.sh](test/client/setup.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe DemoClient library is a Python client for interacting with EOEPCA platform services through User-Managed Access (UMA) authentication flows. This library provides programmatic access to protected resources including ADES (Application Deployment and Execution Service), Workspace API, and other EOEPCA services with built-in UMA ticket-based authorization handling.\n\nFor information about the UMA authentication flow from a system architecture perspective, see [UMA Authentication Flow](#4.4). For details on the acceptance testing framework that uses this client, see [Acceptance Testing Framework](#9.2). For local development scenarios, see [Local Development with Minikube](#9.3).\n\n**Sources:** [test/client/DemoClient.py:1-20]()\n\n## Library Overview\n\nThe `DemoClient` class, implemented in [test/client/DemoClient.py](), serves as a comprehensive client library with the following capabilities:\n\n| Capability | Description |\n|------------|-------------|\n| **Client Registration** | Dynamic registration with the Identity Service using OIDC |\n| **User Authentication** | Password-based authentication to obtain ID tokens |\n| **Resource Registration** | Registration of protected resources with PEP APIs |\n| **UMA Flow Handling** | Automatic ticket-to-RPT exchange for accessing protected resources |\n| **State Persistence** | JSON-based state management for client credentials and resource IDs |\n| **Robot Framework Integration** | Keyword decorators for test automation |\n| **Multi-Service Support** | ADES, Workspace API, WPS, and custom services |\n\nThe library is initialized with a base URL and maintains a persistent session with disabled SSL verification for development environments.\n\n**Sources:** [test/client/DemoClient.py:14-33](), [test/client/main.py:1-40]()\n\n## Architecture and Key Components\n\n### DemoClient Class Structure\n\n```mermaid\ngraph TB\n    DemoClient[\"DemoClient\u003cbr/\u003eclass\"]\n    \n    subgraph \"Session Management\"\n        session[\"requests.Session\u003cbr/\u003eself.session\"]\n        base_url[\"base_url\u003cbr/\u003estr\"]\n        state[\"state\u003cbr/\u003edict\"]\n    end\n    \n    subgraph \"Authentication Components\"\n        scim_client[\"EOEPCA_Scim\u003cbr/\u003eself.scim_client\"]\n        token_endpoint[\"token_endpoint\u003cbr/\u003estr\"]\n        client_creds[\"client_id/secret\u003cbr/\u003ein state\"]\n    end\n    \n    subgraph \"Configuration\"\n        trace_flow[\"trace_flow\u003cbr/\u003ebool\"]\n        trace_requests[\"trace_requests\u003cbr/\u003ebool\"]\n    end\n    \n    subgraph \"Core Methods\"\n        register_client[\"register_client()\"]\n        get_id_token[\"get_id_token()\"]\n        register_protected_resource[\"register_protected_resource()\"]\n        uma_http_request[\"uma_http_request()\"]\n    end\n    \n    subgraph \"Service Methods\"\n        ades_methods[\"proc_list_processes()\u003cbr/\u003eproc_deploy_application()\u003cbr/\u003eproc_execute_application()\u003cbr/\u003eproc_get_job_status()\u003cbr/\u003eproc_poll_job_completion()\"]\n        ws_methods[\"wsapi_create()\u003cbr/\u003ewsapi_get_details()\"]\n        wps_methods[\"wps_get_capabilities()\"]\n    end\n    \n    subgraph \"Policy Methods\"\n        policy_methods[\"update_policy()\u003cbr/\u003eget_ownership_id()\u003cbr/\u003eget_resource_by_name()\"]\n    end\n    \n    DemoClient --\u003e session\n    DemoClient --\u003e base_url\n    DemoClient --\u003e state\n    DemoClient --\u003e scim_client\n    DemoClient --\u003e token_endpoint\n    DemoClient --\u003e client_creds\n    DemoClient --\u003e trace_flow\n    DemoClient --\u003e trace_requests\n    \n    DemoClient --\u003e register_client\n    DemoClient --\u003e get_id_token\n    DemoClient --\u003e register_protected_resource\n    DemoClient --\u003e uma_http_request\n    \n    DemoClient --\u003e ades_methods\n    DemoClient --\u003e ws_methods\n    DemoClient --\u003e wps_methods\n    DemoClient --\u003e policy_methods\n```\n\n**Sources:** [test/client/DemoClient.py:14-56]()\n\n### UMA Flow Implementation\n\nThe library implements the complete UMA authentication flow through the `uma_http_request()` method:\n\n```mermaid\nsequenceDiagram\n    participant Client as \"DemoClient\u003cbr/\u003eClient Code\"\n    participant uma_http_request as \"uma_http_request()\"\n    participant http_request as \"http_request()\"\n    participant PEP as \"Protected Resource\u003cbr/\u003e(via PEP)\"\n    participant get_access_token_from_ticket as \"get_access_token_from_ticket()\"\n    participant Token as \"Token Endpoint\"\n    \n    Client-\u003e\u003euma_http_request: method, url, id_token\n    \n    alt \"First Attempt (no access_token)\"\n        uma_http_request-\u003e\u003euma_http_request: Set X-User-Id header\n        uma_http_request-\u003e\u003ehttp_request: Request without Authorization\n        http_request-\u003e\u003ePEP: GET/POST resource\n        PEP--\u003e\u003ehttp_request: 401 + WWW-Authenticate: ticket\n        http_request--\u003e\u003euma_http_request: Response with ticket\n    end\n    \n    uma_http_request-\u003e\u003euma_http_request: Extract ticket from WWW-Authenticate\n    uma_http_request-\u003e\u003eget_access_token_from_ticket: ticket, id_token\n    get_access_token_from_ticket-\u003e\u003eToken: POST grant_type=uma-ticket\n    Token--\u003e\u003eget_access_token_from_ticket: access_token (RPT)\n    get_access_token_from_ticket--\u003e\u003euma_http_request: RPT\n    \n    alt \"Second Attempt (with access_token)\"\n        uma_http_request-\u003e\u003euma_http_request: Set Authorization: Bearer RPT\n        uma_http_request-\u003e\u003ehttp_request: Retry request\n        http_request-\u003e\u003ePEP: GET/POST with RPT\n        PEP--\u003e\u003ehttp_request: 200 OK + Response\n        http_request--\u003e\u003euma_http_request: Success response\n    end\n    \n    uma_http_request--\u003e\u003eClient: response, access_token\n```\n\n**Sources:** [test/client/DemoClient.py:239-291](), [test/client/DemoClient.py:185-212]()\n\n## Authentication and Authorization Methods\n\n### Client Registration and Token Management\n\nThe library provides methods for managing OIDC client registration and obtaining authentication tokens:\n\n| Method | Purpose | Returns |\n|--------|---------|---------|\n| `get_token_endpoint()` | Discovers the token endpoint from UMA2 configuration | Token endpoint URL |\n| `register_client()` | Registers a new OIDC client with the Identity Service | Stores `client_id` and `client_secret` in state |\n| `get_client_credentials()` | Returns existing or newly registered client credentials | `(client_id, client_secret)` tuple |\n| `get_id_token(username, password)` | Obtains an ID token using password grant | ID token string |\n| `get_access_token_from_ticket(ticket, id_token)` | Exchanges UMA ticket + ID token for RPT | Access token (RPT) |\n| `get_access_token_from_password(username, password)` | Obtains access token via password grant | Access token |\n\nThe client registration process uses the `EOEPCA_Scim` library and supports multiple grant types including `client_credentials`, `password`, and `urn:ietf:params:oauth:grant-type:uma-ticket`.\n\n**Sources:** [test/client/DemoClient.py:74-140](), [test/client/DemoClient.py:185-229]()\n\n### Resource Registration and Protection\n\nProtected resources are registered with the PEP through the `register_protected_resource()` method:\n\n```mermaid\ngraph LR\n    subgraph \"Registration Flow\"\n        method[\"register_protected_resource()\"]\n        check_state[\"Check state.json\u003cbr/\u003efor existing resource_id\"]\n        post_api[\"POST {resource_api_url}/resources\"]\n        persist[\"Store resource_id\u003cbr/\u003ein state\"]\n    end\n    \n    method --\u003e check_state\n    check_state --\u003e|\"Not Found\"| post_api\n    check_state --\u003e|\"Found\"| persist\n    post_api --\u003e persist\n    \n    subgraph \"Parameters\"\n        resource_api_url[\"resource_api_url\u003cbr/\u003ePEP API endpoint\"]\n        uri[\"uri\u003cbr/\u003eResource path\"]\n        id_token[\"id_token\u003cbr/\u003eAuthorization\"]\n        name[\"name\u003cbr/\u003eResource name\"]\n        scopes[\"scopes\u003cbr/\u003ee.g., ['public_access']\"]\n        ownershipId[\"ownershipId (optional)\u003cbr/\u003eOverride owner\"]\n    end\n```\n\nThe method constructs a registration request with the following payload structure:\n\n```json\n{\n  \"resource_scopes\": [\"scope1\", \"scope2\"],\n  \"icon_uri\": \"/resource/path\",\n  \"name\": \"Resource Display Name\",\n  \"uuid\": \"optional-ownership-id\"\n}\n```\n\n**Sources:** [test/client/DemoClient.py:141-183](), [test/client/main.py:54-65]()\n\n## ADES Processing Methods\n\nThe library provides comprehensive methods for interacting with the ADES service through both OGC API Processes and WPS interfaces:\n\n### API Processes Methods\n\n| Method | Endpoint Pattern | Purpose |\n|--------|------------------|---------|\n| `proc_list_processes()` | `GET {base_url}/processes` | List deployed applications |\n| `proc_deploy_application()` | `POST {base_url}/processes` | Deploy CWL application from file |\n| `proc_get_app_details()` | `GET {base_url}/processes/{app_name}` | Get application details |\n| `proc_execute_application()` | `POST {base_url}/processes/{app_name}/execution` | Execute application with inputs |\n| `proc_get_job_status()` | `GET {base_url}{job_location}` | Check job execution status |\n| `proc_poll_job_completion()` | Polling loop | Poll until job completes |\n| `proc_get_job_result()` | `GET {base_url}{job_location}/results` | Retrieve STAC result URI |\n| `proc_list_jobs()` | `GET {base_url}/jobs` | List all jobs |\n| `proc_undeploy_application()` | `DELETE {base_url}/processes/{app_name}` | Remove deployed application |\n\n**Sources:** [test/client/DemoClient.py:383-536]()\n\n### WPS Methods\n\nThe `wps_get_capabilities()` method provides access to the OGC WPS GetCapabilities endpoint:\n\n```\nGET {service_base_url}/?service=WPS\u0026version=1.0.0\u0026request=GetCapabilities\n```\n\n**Sources:** [test/client/DemoClient.py:370-377]()\n\n### ADES Usage Pattern\n\n```mermaid\nsequenceDiagram\n    participant Client as \"Client Code\"\n    participant DemoClient\n    participant ADES as \"ADES Service\"\n    participant Job as \"Job Execution\"\n    \n    Client-\u003e\u003eDemoClient: proc_list_processes()\n    DemoClient-\u003e\u003eADES: GET /processes (via UMA)\n    ADES--\u003e\u003eDemoClient: Process list\n    DemoClient--\u003e\u003eClient: response, access_token, process_ids\n    \n    Client-\u003e\u003eDemoClient: proc_deploy_application(cwl_file)\n    DemoClient-\u003e\u003eDemoClient: Load JSON from file\n    DemoClient-\u003e\u003eADES: POST /processes (via UMA)\n    ADES--\u003e\u003eDemoClient: 201 Created\n    DemoClient--\u003e\u003eClient: response, access_token\n    \n    Client-\u003e\u003eClient: sleep(5) for async deploy\n    \n    Client-\u003e\u003eDemoClient: proc_execute_application(app_name, inputs_file)\n    DemoClient-\u003e\u003eDemoClient: Load JSON from file\n    DemoClient-\u003e\u003eADES: POST /processes/{app_name}/execution\n    ADES--\u003e\u003eDemoClient: 201 Created + Location header\n    DemoClient--\u003e\u003eClient: response, access_token, job_location\n    \n    Client-\u003e\u003eDemoClient: proc_poll_job_completion(job_location)\n    loop \"Until status != 'running'\"\n        DemoClient-\u003e\u003eADES: GET {job_location}\n        ADES--\u003e\u003eDemoClient: Job status\n        DemoClient-\u003e\u003eDemoClient: sleep(interval)\n    end\n    DemoClient--\u003e\u003eClient: response, access_token, status\n    \n    Client-\u003e\u003eDemoClient: proc_get_job_result(job_location)\n    DemoClient-\u003e\u003eADES: GET {job_location}/results\n    ADES--\u003e\u003eDemoClient: STAC catalog URI\n    DemoClient--\u003e\u003eClient: response, access_token, stacCatalogUri\n    \n    Client-\u003e\u003eDemoClient: proc_undeploy_application(app_name)\n    DemoClient-\u003e\u003eADES: DELETE /processes/{app_name}\n    ADES--\u003e\u003eDemoClient: 200 OK\n    DemoClient--\u003e\u003eClient: response, access_token\n```\n\n**Sources:** [test/client/main.py:147-207](), [test/client/DemoClient.py:383-536]()\n\n## Workspace API Methods\n\nThe library provides methods for interacting with the Workspace API:\n\n| Method | Endpoint | Purpose |\n|--------|----------|---------|\n| `wsapi_create()` | `POST {base_url}/workspaces` | Create a new workspace for a user |\n| `wsapi_get_details()` | `GET {base_url}` | Retrieve workspace details |\n| `workspace_get_details()` | `GET {base_url}/workspaces/{name}` | Get details for a named workspace |\n| `workspace_register_application()` | `POST {base_url}/workspaces/{name}/register` | Register an application in a workspace |\n\nThe `wsapi_create()` method constructs the request body with the following structure:\n\n```json\n{\n  \"preferred_name\": \"username\",\n  \"default_owner\": \"user-sub-claim-from-id-token\"\n}\n```\n\nIf no explicit `owner` parameter is provided, the method extracts the `sub` claim from the ID token to set the `default_owner`.\n\n**Sources:** [test/client/DemoClient.py:310-365](), [test/client/main.py:103-128]()\n\n## Policy Management Methods\n\nThe library includes methods for managing authorization policies through the PDP:\n\n### Policy Update\n\nThe `update_policy()` method supports two modes:\n\n1. **Policy ID mode**: Update a specific policy by ID (with ownership validation)\n2. **Resource ID mode**: Find and update policies matching a resource ID and action\n\n```mermaid\ngraph TB\n    update_policy[\"update_policy(pdp_base_url,\u003cbr/\u003epolicy_cfg, resource_id,\u003cbr/\u003eid_token, policy_id, action)\"]\n    \n    has_policy_id{\"policy_id\u003cbr/\u003eprovided?\"}\n    update_by_id[\"PUT /policy/{policy_id}\"]\n    \n    get_policies[\"GET /policy/\u003cbr/\u003ewith resource_id filter\"]\n    match_action{\"Match resource_id\u003cbr/\u003eand action?\"}\n    update_matched[\"PUT /policy/{matched_id}\"]\n    \n    update_policy --\u003e has_policy_id\n    has_policy_id --\u003e|\"Yes\"| update_by_id\n    has_policy_id --\u003e|\"No\"| get_policies\n    get_policies --\u003e match_action\n    match_action --\u003e|\"Yes\"| update_matched\n```\n\n### Resource Lookup Methods\n\n| Method | Purpose |\n|--------|---------|\n| `get_ownership_id(id_token)` | Extracts the `sub` claim from an ID token to determine ownership |\n| `get_resource_by_name(pep_base_url, name, id_token)` | Finds a resource ID by matching the `_name` field |\n| `get_resource_by_uri(pep_base_url, relative_url, id_token)` | Finds a resource ID by matching the `_reverse_match_url` field |\n| `reset_resource_policy()` | Resets a resource's policy to allow only the owner |\n\n**Sources:** [test/client/DemoClient.py:538-683]()\n\n## State Management\n\nThe DemoClient library maintains persistent state in a JSON file to avoid re-registering clients and resources across multiple executions.\n\n### State Structure\n\n```json\n{\n  \"client_id\": \"registered-oidc-client-id\",\n  \"client_secret\": \"registered-oidc-client-secret\",\n  \"resources\": {\n    \"http://ades-pepapi.example.org\": {\n      \"/eric\": \"resource-id-1\",\n      \"/test424\": \"resource-id-2\"\n    },\n    \"http://workspace-api-pepapi.example.org\": {\n      \"/workspaces/develop-user-eric\": \"resource-id-3\"\n    }\n  }\n}\n```\n\n### State Lifecycle\n\n```mermaid\ngraph LR\n    subgraph \"Initialization\"\n        init[\"__init__()\"]\n        load_state[\"load_state()\"]\n        read_file[\"Read state.json\"]\n    end\n    \n    subgraph \"During Execution\"\n        register[\"register_client()\"]\n        check_state{\"client_id\u003cbr/\u003ein state?\"}\n        perform_registration[\"scim_client.registerClient()\"]\n        save_credentials[\"state['client_id']\u003cbr/\u003estate['client_secret']\"]\n        \n        register_resource[\"register_protected_resource()\"]\n        check_resource{\"resource_id\u003cbr/\u003ein state?\"}\n        post_pep[\"POST PEP API\"]\n        save_resource_id[\"state['resources'][url][uri]\"]\n    end\n    \n    subgraph \"Completion\"\n        save_state[\"save_state()\"]\n        write_file[\"Write state.json\"]\n    end\n    \n    init --\u003e load_state\n    load_state --\u003e read_file\n    \n    register --\u003e check_state\n    check_state --\u003e|\"No\"| perform_registration\n    perform_registration --\u003e save_credentials\n    \n    register_resource --\u003e check_resource\n    check_resource --\u003e|\"No\"| post_pep\n    post_pep --\u003e save_resource_id\n    \n    save_state --\u003e write_file\n```\n\nThe state file is excluded from version control through [test/client/.gitignore:4]().\n\n**Sources:** [test/client/DemoClient.py:35-56](), [test/client/DemoClient.py:87-112](), [test/client/DemoClient.py:141-183]()\n\n## Robot Framework Integration\n\nThe DemoClient library is designed as a Robot Framework test library with the `@library` and `@keyword` decorators:\n\n### Library Declaration\n\n```python\n@library\nclass DemoClient:\n    ROBOT_LIBRARY_SCOPE = 'GLOBAL'\n    ROBOT_LIBRARY_VERSION = '0.1'\n```\n\nThis makes the entire class available as a Robot Framework library with global scope, meaning a single instance is shared across all test cases.\n\n### Keyword Methods\n\nMethods decorated with `@keyword` are exposed as Robot Framework keywords:\n\n| Keyword Name | Method | Purpose |\n|--------------|--------|---------|\n| `Get Client Credentials` | `get_client_credentials()` | Returns client credentials |\n| `Get ID Token` | `get_id_token()` | Obtains user ID token |\n| `Register Protected Resource` | `register_protected_resource()` | Registers a resource with PEP |\n| `Dummy Service Call` | `dummy_service_call()` | Test endpoint call |\n| `Workspace API Create` | `wsapi_create()` | Create workspace |\n| `Workspace API Get Details` | `wsapi_get_details()` | Get workspace details |\n| `WPS Get Capabilities` | `wps_get_capabilities()` | WPS GetCapabilities |\n| `Proc List Processes` | `proc_list_processes()` | List ADES processes |\n| `Proc Deploy App` | `proc_deploy_application()` | Deploy ADES application |\n| `Proc App Details` | `proc_get_app_details()` | Get app details |\n| `Proc Execute App` | `proc_execute_application()` | Execute ADES application |\n| `Proc Job Status` | `proc_get_job_status()` | Get job status |\n| `Proc Poll Job Completion` | `proc_poll_job_completion()` | Poll until job completes |\n| `Proc Job Result` | `proc_get_job_result()` | Get job results |\n| `Proc Undeploy App` | `proc_undeploy_application()` | Undeploy application |\n| `Update Policy` | `update_policy()` | Update PDP policy |\n| `Get Ownership Id` | `get_ownership_id()` | Extract ownership from token |\n| `Client Save State` | `save_state()` | Save state to file |\n\n### Usage in Robot Tests\n\n```robot\n*** Settings ***\nLibrary  ../../../client/DemoClient.py  ${UM_BASE_URL}\n\n*** Test Cases ***\nDeploy Application\n  ${resp}  ${access_token} =  Proc Deploy App  ${API_PROC_SERVICE_URL}  ${app_filename}  ${ID_TOKEN}  ${ACCESS_TOKEN}\n  Should Be Equal As Integers  201  ${resp.status_code}\n```\n\n**Sources:** [test/client/DemoClient.py:12-19](), [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot:1-10](), [test/acceptance/02__Processing/01__ADES/01__API_PROC.robot:76-81]()\n\n## Example Usage\n\n### Complete Workflow Example\n\nThe [test/client/main.py]() script demonstrates a complete workflow:\n\n```mermaid\ngraph TB\n    start[\"Start\"]\n    \n    subgraph \"Setup\"\n        init[\"Initialize DemoClient\"]\n        discover[\"Get Token Endpoint\"]\n        register[\"Register Client\"]\n        id_token[\"Get User ID Token\"]\n    end\n    \n    subgraph \"Resource Registration\"\n        reg_ades[\"Register ADES Base Path\"]\n        reg_ws[\"Register Workspace Path\"]\n    end\n    \n    subgraph \"Workspace Operations\"\n        ws_create[\"Create Workspace\"]\n        ws_details[\"Get Workspace Details\"]\n    end\n    \n    subgraph \"ADES Processing\"\n        list_proc[\"List Processes\"]\n        deploy[\"Deploy Application\"]\n        details[\"Get App Details\"]\n        execute[\"Execute Application\"]\n        poll[\"Poll Job Completion\"]\n        results[\"Get Job Results\"]\n        undeploy[\"Undeploy Application\"]\n    end\n    \n    subgraph \"Completion\"\n        save[\"Save State\"]\n    end\n    \n    start --\u003e init\n    init --\u003e discover\n    discover --\u003e register\n    register --\u003e id_token\n    \n    id_token --\u003e reg_ades\n    id_token --\u003e reg_ws\n    \n    reg_ws --\u003e ws_create\n    ws_create --\u003e ws_details\n    \n    reg_ades --\u003e list_proc\n    list_proc --\u003e deploy\n    deploy --\u003e details\n    details --\u003e execute\n    execute --\u003e poll\n    poll --\u003e results\n    results --\u003e undeploy\n    \n    undeploy --\u003e save\n```\n\n### Key Configuration Points\n\nThe example script configures:\n\n1. **Domain**: Target EOEPCA deployment (e.g., `develop.eoepca.org`)\n2. **User credentials**: Username and password for authentication\n3. **Service endpoints**:\n   - Base URL: `https://auth.{domain}`\n   - ADES: `https://ades.{domain}` with resource API at `http://ades-pepapi.{domain}`\n   - Workspace API: `https://workspace-api.{domain}` with resource API at `http://workspace-api-pepapi.{domain}`\n\n### Trace Flow Debugging\n\nThe library supports trace logging for debugging UMA flows:\n\n```python\ndemo.trace_flow = True\nresponse, access_token = demo.wsapi_create(wsapi_url, wsapi_user, \n                                           id_token=user_id_token, \n                                           access_token=wsapi_access_token)\ndemo.trace_flow = False\n```\n\nThis outputs detailed logs prefixed with `[UMA]` showing:\n- Ticket acquisition\n- Token exchange\n- Authorization attempts\n- Access token reuse\n\n**Sources:** [test/client/main.py:7-221](), [test/client/DemoClient.py:57-68]()\n\n## Dependencies\n\nThe library requires the following Python packages:\n\n| Package | Version | Purpose |\n|---------|---------|---------|\n| `eoepca-scim` | 2.8.1 | SCIM client for OIDC registration |\n| `pycrypto` | 2.6.1 | Cryptographic operations |\n| `pyjwkest` | 1.4.2 | JWT key management |\n| `PyJWT` | 2.6.0 | JWT token handling |\n| `requests` | 2.26.0 | HTTP client library |\n| `robotframework` | 4.1 | Robot Framework test library support |\n| `WellKnownHandler` | 0.2.0 | OIDC well-known endpoint handling |\n\nInstallation is automated through [test/client/setup.sh](), which creates a Python virtual environment and installs all dependencies.\n\n**Sources:** [test/client/requirements.txt:1-8](), [test/client/setup.sh:1-14]()"])</script><script>self.__next_f.push([1,"3b:T64e7,"])</script><script>self.__next_f.push([1,"# Acceptance Testing Framework\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot](test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot)\n- [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot](test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot)\n- [test/acceptance/01__UserManagement/03__PDP_Engine/getOwnership.py](test/acceptance/01__UserManagement/03__PDP_Engine/getOwnership.py)\n- [test/acceptance/01__UserManagement/03__PDP_Engine/tkn.sh](test/acceptance/01__UserManagement/03__PDP_Engine/tkn.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Acceptance Testing Framework validates the functionality and integration of all EOEPCA building blocks through automated end-to-end tests. This framework uses Robot Framework to execute test scenarios that verify authentication flows, policy enforcement, data access, catalogue operations, and processing capabilities across the deployed system.\n\nFor information about the underlying Python client library used by these tests, see [DemoClient Library](#9.1). For setting up a local test environment, see [Local Development with Minikube](#9.3).\n\n## Test Framework Architecture\n\nThe acceptance testing framework is built on Robot Framework and organized into test suites that correspond to the major EOEPCA building blocks. The framework executes tests against a deployed EOEPCA instance, validating both individual services and cross-service workflows.\n\n**Test Framework Architecture**\n\n```mermaid\ngraph TB\n    subgraph \"Test Runner\"\n        RobotFW[\"Robot Framework\u003cbr/\u003eTest Executor\"]\n    end\n    \n    subgraph \"Test Suites\"\n        UM[\"01__UserManagement/\u003cbr/\u003eTest Suites\"]\n        PROC[\"02__Processing/\u003cbr/\u003eTest Suites\"]\n        RC[\"03__ResourceCatalogue/\u003cbr/\u003eTest Suites\"]\n    end\n    \n    subgraph \"Helper Libraries\"\n        DemoClient[\"DemoClient.py\u003cbr/\u003eUMA Authentication\u003cbr/\u003eToken Management\"]\n        CSW[\"CatalogueServiceWeb.py\u003cbr/\u003eOWS Library Wrapper\"]\n        SeleniumLib[\"SeleniumLibrary\u003cbr/\u003eBrowser Automation\"]\n        RequestsLib[\"RequestsLibrary\u003cbr/\u003eHTTP Requests\"]\n    end\n    \n    subgraph \"External Libraries\"\n        owslib[\"owslib.csw\u003cbr/\u003eowslib.wms\"]\n        pyops[\"pyops\u003cbr/\u003eOpenSearch Client\"]\n        requests[\"requests\u003cbr/\u003eHTTP Library\"]\n        selenium[\"selenium\u003cbr/\u003eWebDriver\"]\n    end\n    \n    subgraph \"Target System\"\n        LoginSvc[\"login-service\"]\n        IdentitySvc[\"identity-service\"]\n        PDPEngine[\"pdp-engine\"]\n        ADES[\"ades\"]\n        ResCat[\"resource-catalogue\"]\n        DataAccess[\"data-access\"]\n    end\n    \n    RobotFW --\u003e UM\n    RobotFW --\u003e PROC\n    RobotFW --\u003e RC\n    \n    UM --\u003e DemoClient\n    UM --\u003e SeleniumLib\n    UM --\u003e RequestsLib\n    \n    PROC --\u003e DemoClient\n    PROC --\u003e RequestsLib\n    \n    RC --\u003e CSW\n    RC --\u003e DemoClient\n    RC --\u003e RequestsLib\n    \n    DemoClient --\u003e requests\n    CSW --\u003e owslib\n    CSW --\u003e pyops\n    SeleniumLib --\u003e selenium\n    \n    DemoClient --\u003e LoginSvc\n    DemoClient --\u003e IdentitySvc\n    \n    UM --\u003e PDPEngine\n    UM --\u003e LoginSvc\n    \n    PROC --\u003e ADES\n    \n    RC --\u003e ResCat\n    RC --\u003e DataAccess\n```\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:1-5](), [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:1-6](), [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:1-6](), [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:1-17]()\n\n## Test Suite Organization\n\nTest suites are organized hierarchically by building block, with each major component having its own directory structure. The test files use the `.robot` extension and follow a naming convention that indicates test scope and purpose.\n\n**Test Suite File Structure**\n\n| Directory | Test Suite | Purpose |\n|-----------|-----------|---------|\n| `01__UserManagement/02__UserProfile/` | `LoginServiceInteraction.robot` | Browser-based login flow validation |\n| `01__UserManagement/03__PDP_Engine/` | `PDP_Engine.robot` | Policy decision point validation |\n| `02__Processing/01__ADES/` | ADES test suites | Application deployment and execution |\n| `03__ResourceCatalogue/` | `Resource_catalogue.robot` | CSW, OpenSearch, and WMS operations |\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:1](), [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:1](), [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:1]()\n\n## User Management Test Suite\n\nThe User Management test suite validates authentication, authorization, and policy enforcement across the identity subsystem. Tests verify UMA flows, PDP policy decisions, and browser-based login interactions.\n\n### PDP Policy Validation Tests\n\nThe PDP test suite validates policy enforcement by testing access control decisions for different users, resources, and actions. Tests exercise both positive (permit) and negative (deny) scenarios.\n\n**PDP Validation Test Flow**\n\n```mermaid\nsequenceDiagram\n    participant Test as \"PDP_Engine.robot\"\n    participant DemoClient as \"DemoClient.py\"\n    participant Identity as \"identity-service\"\n    participant PDP as \"pdp-engine\"\n    participant DummyService as \"dummy-service-pep\"\n    \n    Test-\u003e\u003eDemoClient: \"Get Id Token(eric, defaultPWD)\"\n    DemoClient-\u003e\u003eIdentity: \"Password grant request\"\n    Identity--\u003e\u003eDemoClient: \"Return ID token\"\n    DemoClient--\u003e\u003eTest: \"eric_id_token\"\n    \n    Test-\u003e\u003eDemoClient: \"Get Resource By URI(/ericspace)\"\n    DemoClient-\u003e\u003eDummyService: \"Query resource API\"\n    DummyService--\u003e\u003eDemoClient: \"resource_id\"\n    DemoClient--\u003e\u003eTest: \"ericspace_id\"\n    \n    Test-\u003e\u003eTest: \"PDP Validate Resource Access Permit\"\n    Test-\u003e\u003ePDP: \"POST /pdp/policy/validate\u003cbr/\u003e{user: eric, action: get, resource: ericspace_id}\"\n    PDP-\u003e\u003ePDP: \"Evaluate policy rules\"\n    PDP--\u003e\u003eTest: \"200 OK {Decision: Permit}\"\n    \n    Test-\u003e\u003eTest: \"Should Be Equal: Permit\"\n```\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:13-31]()\n\n### PDP Test Cases\n\nThe PDP test suite includes comprehensive validation of access control policies:\n\n**Access Permission Tests** - Validate that users can access their own resources and are denied access to other users' resources:\n- `PDP Access Permitted eric -\u003e /ericspace` - [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:13-16]()\n- `PDP Access Denied bob -\u003e /ericspace` - [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:18-21]()\n- `PDP Access Permitted bob -\u003e /bobspace` - [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:23-26]()\n\n**HTTP Method Tests** - Verify policy enforcement for different HTTP actions:\n- `PDP Access Permitted GET/POST/PUT/DELETE/HEAD` - [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:28-51]()\n\n**Policy Validation Edge Cases**:\n- `PDP Policy Validate Bad Action` - [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:53-56]()\n- `PDP Policy Validate Bad Resource` - [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:58-61]()\n\n**Policy Management Tests** - Validate authorization for policy modifications:\n- `User bob Unauthorized Policy Change` - Verifies non-owners cannot modify policies [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:63-71]()\n- `User eric Authorized Policy Change` - Verifies owners can update policies [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:73-82]()\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:11-99]()\n\n### PDP Test Keywords\n\nThe test suite defines reusable Robot Framework keywords for policy validation:\n\n**`PDP Validate Resource Access`** - Core keyword that sends policy validation requests to the PDP endpoint:\n```\n[Arguments]  ${user_name}  ${action}  ${resource_id}  ${expected_status}\n```\nConstructs XACML-style request with `AccessSubject`, `Action`, and `Resource` attributes, sends to `/pdp/policy/validate`, and returns the decision. [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:102-112]()\n\n**`PDP Validate Resource Access Permit`** - Wrapper that expects `200` status and `Permit` decision [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:114-117]()\n\n**`PDP Validate Resource Access Denied`** - Wrapper that expects `401` status and `Deny` decision [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:119-122]()\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:100-123]()\n\n### Login Service Browser Tests\n\nThe Login Service test suite uses Selenium WebDriver to validate browser-based authentication flows through the Gluu-based login service.\n\n**Test Cases**:\n- `Log in to the User Profile through the Login Service` - Validates OAuth2 authorization code flow with browser redirection [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:20-37]()\n- `Add Two Users to Gluu` - Automated user creation through Gluu admin interface [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:39-59]()\n\n**Browser Configuration** - Tests use headless Chrome with security options:\n```robot\n${chrome_options} =  Evaluate  sys.modules['selenium.webdriver'].ChromeOptions()\nCall Method  ${chrome_options}  add_argument  headless\nCall Method  ${chrome_options}  add_argument  ignore-certificate-errors\n```\n[test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:21-28]()\n\n**Sources:** [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:18-127]()\n\n## Processing Test Suite\n\nThe Processing test suite validates ADES (Application Deployment and Execution Service) functionality, including CWL application deployment and execution workflows.\n\n### ADES Test Data\n\nThe test suite includes structured test data for different application deployment scenarios:\n\n**CWL Application Package Deployment** - JSON request body for deploying CWL-based applications via the ADES `/processes/deployment` endpoint:\n```json\n{\n  \"inputs\": [{\n    \"id\": \"applicationPackage\",\n    \"input\": {\n      \"format\": {\"mimeType\": \"application/cwl\"},\n      \"value\": {\"href\": \"https://raw.githubusercontent.com/EOEPCA/app-snuggs/main/app-package.cwl\"}\n    }\n  }]\n}\n```\n[test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json:1-28]()\n\n**ATOM Feed Application Deployment** - Alternative deployment format using OGC OWS Context ATOM encoding [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json:1-35]()\n\n**CWL Application Package Definition** - Complete CWL workflow specification with:\n- `CommandLineTool` definition with Docker container reference [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:2-35]()\n- `Workflow` definition with scatter-gather pattern [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:36-72]()\n\n**Sources:** [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json:1-28](), [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl:1-79](), [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json:1-35]()\n\n## Resource Catalogue Test Suite\n\nThe Resource Catalogue test suite validates metadata cataloguing, search, and data access services through OGC CSW, OpenSearch, and WMS interfaces.\n\n### CatalogueServiceWeb Helper Library\n\nThe `CatalogueServiceWeb.py` library wraps `owslib` and `pyops` to provide Robot Framework keywords for catalogue interaction.\n\n**Library Initialization**:\n```python\ndef __init__(self, base_url):\n    self.system_catalogue_endpoint = base_url\n    self.csw = CSW(self.system_catalogue_endpoint, timeout=30)\n```\n[test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:17-22]()\n\n**Key Components**:\n- `owslib.csw.CatalogueServiceWeb` - OGC CSW client [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:3]()\n- `owslib.wms.WebMapService` - OGC WMS client [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:5]()\n- `pyops.Client` - OpenSearch client [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:6]()\n\n**Sources:** [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:10-156]()\n\n### Catalogue Test Keywords\n\nThe library provides Robot Framework keywords for catalogue operations:\n\n**CSW Operations**:\n- `Get Operations` - Returns available CSW operations [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:45-49]()\n- `Get Constraints` - Returns operation constraints [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:51-55]()\n- `Get Results` - Retrieves records with optional limit [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:58-64]()\n- `Get Records Filtered` - Queries with spatial/temporal/property filters [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:74-98]()\n\n**Filter Construction** - Uses `owslib.fes` for OGC Filter Encoding:\n```python\nbbox_query = BBox([37, 13.9, 37.9, 15.1])\nbegin = PropertyIsGreaterThanOrEqualTo(propertyname='apiso:TempExtent_begin', literal='2021-04-02 00:00')\nend = PropertyIsLessThanOrEqualTo(propertyname='apiso:TempExtent_end', literal='2021-04-03 00:00')\ncloud = PropertyIsLessThanOrEqualTo(propertyname='apiso:CloudCover', literal='20')\nfilter_list = [And([bbox_query, begin, end, cloud])]\n```\n[test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:81-93]()\n\n**WMS Integration**:\n- `Get Record Link` - Extracts WMS endpoint from catalogue record [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:100-111]()\n- `Get Map` - Initializes WMS client and retrieves layer contents [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:113-116]()\n\n**OpenSearch Support**:\n- `Load Opensearch` - Initializes pyops client from description document [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:139-142]()\n- `Open Search` - Executes OpenSearch query with parameters [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:144-147]()\n\n**Sources:** [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:45-156]()\n\n### Catalogue Test Cases\n\n**CSW Operation Validation** - Verifies catalogue service capabilities:\n```robot\nGet Operations Information\n  Get Csw Operations\n```\nValidates that the catalogue exposes 6 operations and `GetRecords` has 3 constraints [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:21-22]()\n\n**Record Query and Filtering** - Tests spatial/temporal queries:\n```robot\nGet Records Information\n  Get Csw Records Filtered\n```\nQueries for Sentinel-2 records within bounding box and date range, verifies WMS endpoint availability [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:24-25]()\n\n**OpenSearch Queries** - Validates OpenSearch interface:\n```robot\nOpensearch\n  Load Opensearch  ${CATALOGUE_BASE_URL}/opensearch\n  Search Opensearch\n  Search Requests\n```\nTests both pyops client and direct HTTP requests to OpenSearch endpoint [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:32-35]()\n\n**Test Variables**:\n- `${CATALOGUE_BASE_URL}` - Global resource catalogue endpoint\n- `${WORKSPACE_CATALOGUE}` - User-specific workspace catalogue URL pattern: `https://resource-catalogue.${USER_PREFIX}-${USERNAME}.${PUBLIC_HOSTNAME}` [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:18]()\n\n**Sources:** [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:20-96]()\n\n## Test Execution Environment\n\n### Environment Variables\n\nTests expect the following environment variables to be set:\n\n| Variable | Purpose | Example |\n|----------|---------|---------|\n| `${UM_BASE_URL}` | User Management base URL | `https://auth.develop.eoepca.org` |\n| `${CATALOGUE_BASE_URL}` | Resource Catalogue endpoint | `https://resource-catalogue.develop.eoepca.org` |\n| `${PUBLIC_HOSTNAME}` | Public domain name | `develop.eoepca.org` |\n| `${USER_PREFIX}` | Workspace prefix | `ws` |\n| `${USER_A_NAME}` | Test user A username | `eric` |\n| `${USER_A_PASSWORD}` | Test user A password | `defaultPWD` |\n| `${USER_B_NAME}` | Test user B username | `bob` |\n| `${USER_B_PASSWORD}` | Test user B password | `defaultPWD` |\n| `${DUMMY_SERVICE_RESOURCES_API_URL}` | Test service endpoint | Used for PDP tests |\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/PDP_Engine.robot:7-9](), [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:12-18]()\n\n### Test Data Registration\n\nThe acceptance tests require baseline data to be registered in the catalogue. Shell scripts automate data registration by pushing product paths to the Redis registration queue:\n\n**Sentinel-2 L2A Data Registration**:\n```bash\nkubectl -n rm exec --stdin --tty data-access-redis-master-0 -- \\\n  redis-cli lpush register_queue \\\n  EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_20200902T090559_N0214_R050_T34SFH_20200902T113910.SAFE/ \\\n  EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_20200902T090559_N0214_R050_T35SLB_20200902T113910.SAFE/ \\\n  ...\n```\n[system/clusters/data/register-S2-L2A-data.sh:9]()\n\n**Sentinel-2 L1C Data Registration**:\n```bash\nkubectl -n rm exec --stdin --tty data-access-redis-master-0 -- \\\n  redis-cli lpush register_queue \\\n  EODATA/Sentinel-2/MSI/L1C/2020/09/30/S2A_MSIL1C_20200930T092031_N0209_R093_T34SDF_20200930T105731.SAFE \\\n  ...\n```\n[system/clusters/data/register-S2-L1C-data.sh:9]()\n\nThese scripts directly enqueue product paths into the `register_queue`, which the registrar service consumes to create catalogue metadata entries.\n\n**Sources:** [system/clusters/data/register-S2-L2A-data.sh:1-10](), [system/clusters/data/register-S2-L1C-data.sh:1-10]()\n\n## Policy Management Utilities\n\nThe testing framework includes utility scripts for managing and debugging PEP/PDP policy configurations.\n\n### Policy Dumping Script\n\nThe `dump-policy.sh` script extracts policy configurations from all PEP and PDP deployments for inspection:\n\n**Script Operation**:\n```bash\n#!/usr/bin/env bash\ndumpDeployment() {\n  namespace=\"${1}\"\n  deployment=\"${2}\"\n  kubectl -n \"${namespace}\" exec -it deploy/\"${deployment}\" -c \"${deployment}\" -- \\\n    management_tools list --all | jq \u003e \"${deployment}.json\"\n}\n```\n[bin/dump-policy.sh:30-40]()\n\n**Deployments Monitored**:\n- `proc/ades-pep` - ADES Policy Enforcement Point\n- `rm/combined-rm-pep` - Combined Resource Management PEP\n- `rm/workspace-api-pep` - Workspace API PEP\n- `test/dummy-service-pep` - Test service PEP\n- `um/pdp-engine` - Policy Decision Point (no JSON formatting)\n\n[bin/dump-policy.sh:21-27]()\n\n**Sources:** [bin/dump-policy.sh:1-43]()\n\n### Resource Unregistration Script\n\nThe `unregister-resource.sh` script removes resources from all PEP/PDP policy stores:\n\n**Usage**:\n```bash\n./unregister-resource.sh \u003cresource-id\u003e\n```\n\n**Execution Flow**:\n```bash\nkubectl -n rm exec -it svc/combined-rm-pep -c combined-rm-pep -- \\\n  management_tools remove -r ${resourceId}\n\nkubectl -n proc exec -it svc/ades-pep -c ades-pep -- \\\n  management_tools remove -r ${resourceId}\n\nkubectl -n rm exec -it svc/workspace-api-pep -c workspace-api-pep -- \\\n  management_tools remove -r ${resourceId}\n\nkubectl -n test exec -it svc/dummy-service-pep -c dummy-service-pep -- \\\n  management_tools remove -r ${resourceId}\n\nkubectl -n um exec -it svc/pdp-engine -c pdp-engine -- \\\n  management_tools remove -r ${resourceId}\n```\n[bin/unregister-resource.sh:22-54]()\n\nThis ensures resources are cleanly removed from all policy enforcement points and the central PDP.\n\n**Sources:** [bin/unregister-resource.sh:1-55]()\n\n## Test Execution Workflow\n\n**Test Execution Process**\n\n```mermaid\ngraph TB\n    Start[\"Test Execution Start\"]\n    EnvCheck[\"Verify Environment Variables\u003cbr/\u003e${UM_BASE_URL}, ${CATALOGUE_BASE_URL}\"]\n    DataReg[\"Register Test Data\u003cbr/\u003eregister-S2-L2A-data.sh\u003cbr/\u003eregister-S2-L1C-data.sh\"]\n    \n    subgraph \"Suite Setup\"\n        InitToken[\"Get ID Token\u003cbr/\u003eDemoClient.Get_Id_Token()\"]\n        LoadState[\"Load State from state.json\"]\n    end\n    \n    subgraph \"Test Execution\"\n        UMTests[\"User Management Tests\u003cbr/\u003ePDP_Engine.robot\u003cbr/\u003eLoginServiceInteraction.robot\"]\n        ProcTests[\"Processing Tests\u003cbr/\u003eADES test suites\"]\n        RCTests[\"Resource Catalogue Tests\u003cbr/\u003eResource_catalogue.robot\"]\n    end\n    \n    subgraph \"Suite Teardown\"\n        SaveState[\"Save State to state.json\u003cbr/\u003eClient Save State\u003cbr/\u003eCatalogue Save State\"]\n    end\n    \n    Results[\"Test Results\u003cbr/\u003eRobot Framework Report\"]\n    \n    Start --\u003e EnvCheck\n    EnvCheck --\u003e DataReg\n    DataReg --\u003e InitToken\n    InitToken --\u003e LoadState\n    LoadState --\u003e UMTests\n    UMTests --\u003e ProcTests\n    ProcTests --\u003e RCTests\n    RCTests --\u003e SaveState\n    SaveState --\u003e Results\n```\n\n**Sources:** [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:8-9](), [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:38-43](), [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:23-34]()\n\n## Token Management and Authentication\n\n### Token Acquisition Script\n\nThe `tkn.sh` script provides command-line token acquisition for manual testing and debugging:\n\n**Usage**:\n```bash\ntkn.sh -t \u003ctoken_endpoint\u003e -i \u003cclient_id\u003e -s \u003cclient_secret\u003e \\\n       -u \u003cuser_name\u003e -p \u003cuser_password\u003e -f \u003coutput-file\u003e\n```\n\n**Token Request**:\n```bash\ncurl -k -v -XPOST \"$TOKEN_ENDPOINT\" -H 'cache-control: no-cache' \\\n  -d \"grant_type=password\u0026client_id=$CLIENT_ID\u0026client_secret=$CLIENT_SECRET\u0026username=${USER_NAME}\u0026password=${USER_PASSWORD}\u0026scope=openid\" \\\n  \u003e \"${OUTPUT_FILE}\"\n```\n[test/acceptance/01__UserManagement/03__PDP_Engine/tkn.sh:33]()\n\nThis script performs OAuth2 Resource Owner Password Credentials grant to obtain ID tokens for testing.\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/tkn.sh:1-34]()\n\n### Ownership ID Extraction\n\nThe `getOwnership.py` script extracts the ownership ID (subject claim) from JWT tokens:\n\n**Script Operation**:\n```python\npayload = str(sys.argv[1]).split(\".\")[1]\npaddedPayload = payload + '=' * (4 - len(payload) % 4)\ndecoded = base64.b64decode(paddedPayload)\njwt_decoded = json.loads(decoded)\nf.write(jwt_decoded[\"sub\"])\n```\n[test/acceptance/01__UserManagement/03__PDP_Engine/getOwnership.py:8-14]()\n\nThis extracts the `sub` claim from JWT tokens, which is used as the ownership identifier in policy rules.\n\n**Sources:** [test/acceptance/01__UserManagement/03__PDP_Engine/getOwnership.py:1-15]()\n\n## Test Configuration and Dependencies\n\n### Python Library Dependencies\n\nThe test framework requires the following Python packages:\n\n| Library | Purpose | Import Location |\n|---------|---------|-----------------|\n| `robotframework` | Test execution framework | Implicit |\n| `robotframework-seleniumlibrary` | Browser automation | [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:3]() |\n| `robotframework-requests` | HTTP testing | [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:3]() |\n| `owslib` | OGC service clients | [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:3-5]() |\n| `pyops` | OpenSearch client | [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:6]() |\n| `requests` | HTTP library | [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:7]() |\n| `beautifulsoup4` | XML/HTML parsing | [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:8]() |\n\n**Sources:** [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:1-8](), [test/acceptance/01__UserManagement/02__UserProfile/LoginServiceInteraction.robot:3-6]()\n\n### State Persistence\n\nThe `CatalogueServiceWeb` library maintains test state across executions using a JSON state file:\n\n**State Loading**:\n```python\ndef load_state(self):\n    self.state = {}\n    try:\n        with open(\"state.json\") as state_file:\n            self.state = json.loads(state_file.read())\n    except FileNotFoundError:\n        pass\n```\n[test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:23-32]()\n\n**State Saving**:\n```python\n@keyword(name='Catalogue Save State')\ndef save_state(self):\n    with open(\"state.json\", \"w\") as state_file:\n        state_file.write(json.dumps(self.state, sort_keys=True, indent=2))\n```\n[test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:36-43]()\n\nThis allows tests to maintain context between runs, such as created resource IDs or authentication tokens.\n\n**Sources:** [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:23-43]()"])</script><script>self.__next_f.push([1,"3c:T507e,"])</script><script>self.__next_f.push([1,"# Local Development with Minikube\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [minikube/README.md](minikube/README.md)\n- [system/clusters/README.md](system/clusters/README.md)\n- [travis/acceptanceTest.sh](travis/acceptanceTest.sh)\n- [travis/setupMinikube.sh](travis/setupMinikube.sh)\n- [travis/setupRobot.sh](travis/setupRobot.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides instructions for setting up a local Kubernetes development environment using Minikube or k3s. This local environment enables developers to test EOEPCA system components and building blocks on a single-node cluster without requiring cloud infrastructure. For full system deployment to cloud infrastructure, see [Infrastructure Provisioning](#2.2). For GitOps-based deployment of the EOEPCA system after the cluster is ready, see [GitOps and Flux CD](#3.2). For running tests against a deployed system, see [Testing and Validation](#2.3).\n\n## Overview\n\nThe EOEPCA system can be deployed locally for development and testing purposes using either Minikube or k3s as the Kubernetes platform. While cloud deployments use RKE on OpenStack, local development environments provide a lightweight alternative that runs on a developer's workstation or VM. As noted in [README.md:96](), with release v1.4 the system has expanded in size, making full deployments more challenging in resource-constrained environments, but single-node deployments remain feasible with sufficient resources (8 CPU, 32GB RAM).\n\n### Deployment Comparison\n\n| Aspect | Cloud (OpenStack) | Local Developer |\n|--------|-------------------|-----------------|\n| Infrastructure | [CREODIAS](./creodias/README.md) | Local workstation/VM |\n| Kubernetes Cluster | [RKE](./kubernetes/README.md) | Minikube or k3s |\n| System Deployment | [EOEPCA GitOps](./system/clusters/README.md) | [EOEPCA GitOps](./system/clusters/README.md) |\n| Deployment Guide | [Deployment Guide](https://deployment-guide.docs.eoepca.org/) | [Deployment Guide](https://deployment-guide.docs.eoepca.org/) |\n| Acceptance Tests | [Run Test Suite](./test/acceptance/README.md) | [Run Test Suite](./test/acceptance/README.md) |\n\n**Sources:** [README.md:88-94]()\n\n## Local Development Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Developer Workstation or VM\"\n        subgraph \"Kubernetes Platform Options\"\n            Minikube[\"Minikube\u003cbr/\u003esetup-minikube.sh\"]\n            K3s[\"k3s (alternative)\u003cbr/\u003esetup-k3s.sh\"]\n        end\n        \n        subgraph \"Cluster Configuration\"\n            DockerDriver[\"Docker Driver\u003cbr/\u003e(default, preferred)\"]\n            NoneDriver[\"None Driver\u003cbr/\u003e(native in VM)\"]\n            ContainerdRuntime[\"containerd runtime\u003cbr/\u003e(k3s default)\"]\n            DockerRuntime[\"docker runtime\u003cbr/\u003e(required for ADES/Argo)\"]\n        end\n        \n        subgraph \"Deployment Tools\"\n            Kubectl[\"kubectl\u003cbr/\u003einstall-kubectl.sh\"]\n            Flux[\"Flux CD\u003cbr/\u003edeployCluster.sh\"]\n            Helm[\"Helm 3\"]\n        end\n        \n        subgraph \"EOEPCA System\"\n            FluxSystem[\"flux-system namespace\"]\n            UMNamespace[\"um namespace\u003cbr/\u003e(User Management)\"]\n            RMNamespace[\"rm namespace\u003cbr/\u003e(Resource Management)\"]\n            ProcNamespace[\"proc namespace\u003cbr/\u003e(Processing)\"]\n        end\n        \n        subgraph \"Testing\"\n            Robot[\"Robot Framework\u003cbr/\u003esetupRobot.sh\"]\n            AcceptanceTests[\"Acceptance Tests\u003cbr/\u003eacceptanceTest.sh\"]\n        end\n    end\n    \n    Minikube --\u003e|docker driver| DockerDriver\n    Minikube --\u003e|none driver| NoneDriver\n    K3s --\u003e|default| ContainerdRuntime\n    K3s --\u003e|configured| DockerRuntime\n    \n    Kubectl --\u003e Minikube\n    Kubectl --\u003e K3s\n    Flux --\u003e Kubectl\n    Helm --\u003e Kubectl\n    \n    Flux --\u003e FluxSystem\n    FluxSystem --\u003e UMNamespace\n    FluxSystem --\u003e RMNamespace\n    FluxSystem --\u003e ProcNamespace\n    \n    Robot --\u003e AcceptanceTests\n    AcceptanceTests --\u003e UMNamespace\n    AcceptanceTests --\u003e RMNamespace\n    AcceptanceTests --\u003e ProcNamespace\n```\n\n**Diagram: Local Development Environment Architecture**\n\nThis diagram illustrates the layered architecture of local EOEPCA development. The Kubernetes platform (Minikube or k3s) provides the foundation, with different driver/runtime options. Deployment tools (kubectl, Flux, Helm) orchestrate the installation, and the EOEPCA system deploys across multiple namespaces. Robot Framework provides the testing layer.\n\n**Sources:** [minikube/README.md:1-52](), [README.md:88-98]()\n\n## Prerequisites\n\n### kubectl Installation\n\nThe `kubectl` command-line tool is required for Kubernetes cluster administration. Install kubectl by following the [official Kubernetes documentation](https://kubernetes.io/docs/tasks/tools/install-kubectl/) or use the provided helper script:\n\n```bash\n$ bin/install-kubectl.sh\n```\n\nThis script downloads and installs the appropriate kubectl binary for your platform.\n\n**Sources:** [minikube/README.md:7-14]()\n\n### System Requirements\n\nFor EOEPCA v1.4 full system deployment:\n- **CPU:** 8 cores minimum\n- **Memory:** 32GB RAM minimum\n- **Disk:** Sufficient space for container images and persistent volumes\n- **OS:** Linux, macOS, or Windows with WSL2\n\n**Sources:** [README.md:96-98]()\n\n## Minikube Setup\n\n### Basic Installation\n\nMinikube creates a single-node Kubernetes cluster running in a container or VM. Install using the official instructions or the provided setup script:\n\n```bash\n$ minikube/setup-minikube.sh\n```\n\nThe `setup-minikube.sh` script performs the following operations:\n1. Downloads the latest stable Minikube release\n2. Installs the Minikube binary to system path\n3. Configures the default Docker driver\n4. Starts the cluster\n5. Enables required addons (ingress controller)\n\n**Sources:** [minikube/README.md:16-23]()\n\n### Minikube Driver Options\n\n```mermaid\ngraph LR\n    subgraph \"Minikube Deployment Models\"\n        Default[\"Default: Docker Driver\u003cbr/\u003esetup-minikube.sh\"]\n        Native[\"Native: None Driver\u003cbr/\u003esetup-minikube.sh native\"]\n    end\n    \n    subgraph \"Docker Driver Characteristics\"\n        DD1[\"Runs in Docker container\"]\n        DD2[\"Preferred by minikube team\"]\n        DD3[\"Better tested\"]\n        DD4[\"Isolated from host\"]\n    end\n    \n    subgraph \"None Driver Characteristics\"\n        ND1[\"Runs natively in VM\"]\n        ND2[\"Better performance in VMs\"]\n        ND3[\"No container overhead\"]\n        ND4[\"Some limitations\"]\n    end\n    \n    Default --\u003e DD1\n    Default --\u003e DD2\n    Default --\u003e DD3\n    Default --\u003e DD4\n    \n    Native --\u003e ND1\n    Native --\u003e ND2\n    Native --\u003e ND3\n    Native --\u003e ND4\n```\n\n**Diagram: Minikube Driver Selection**\n\nThe script supports two deployment modes:\n\n1. **Docker Driver (Default)**: `setup-minikube.sh`\n   - Runs Minikube as a Docker container\n   - Recommended by the Minikube project\n   - Better tested and more portable\n   - Provides better isolation from host system\n\n2. **None Driver (Native)**: `setup-minikube.sh native`\n   - Runs Kubernetes components directly in the VM\n   - Better performance in resource-constrained VMs\n   - Avoids nested virtualization overhead\n   - Has some limitations noted by Minikube team\n\n**Sources:** [minikube/README.md:25-34]()\n\n### Travis CI Minikube Configuration\n\nThe Travis CI setup provides a reference implementation for automated testing environments:\n\n```bash\n# Key configuration from setupMinikube.sh\nK8S_VER=v1.13.0\nMINIKUBE_VER=v1.12.1\n\n# Start Minikube with specific configuration\nexport CHANGE_MINIKUBE_NONE_USER=true\nminikube start \\\n  --vm-driver=none \\\n  --bootstrapper=kubeadm \\\n  --kubernetes-version=${K8S_VER} \\\n  --extra-config=apiserver.authorization-mode=RBAC\n```\n\nKey steps in [travis/setupMinikube.sh:1-51]():\n1. Configure shared root mount for kube-dns [line 7]()\n2. Install kubectl with specific version [lines 10-12]()\n3. Install Minikube binary [lines 14-16]()\n4. Delete any existing cluster [line 20]()\n5. Start fresh cluster with RBAC enabled [line 21]()\n6. Enable ingress addon [line 23]()\n7. Wait for cluster readiness [line 30]()\n8. Install Helm 3 [lines 38-41]()\n9. Install supporting tools: jq, conntrack, socat, python3-venv [lines 44-48]()\n\n**Sources:** [travis/setupMinikube.sh:1-51]()\n\n## k3s as an Alternative\n\n### k3s Advantages\n\n[k3s](https://k3s.io/) provides a lightweight Kubernetes distribution from Rancher that offers several advantages for local development:\n\n- **Faster Installation**: Starts more quickly than Minikube\n- **Lightweight**: Lower resource overhead, suitable for constrained VMs\n- **Production Alignment**: Same vendor (Rancher) as RKE used in cloud deployments\n- **Simplified Architecture**: Single binary, minimal dependencies\n\n### k3s Installation\n\n```bash\n$ minikube/setup-k3s.sh\n```\n\nThe `setup-k3s.sh` script configures k3s with the Docker container runtime instead of the default containerd. This is a critical requirement because the ADES building block uses Argo workflows, which require Docker runtime for container execution.\n\n**Configuration Detail:**\n```bash\n# k3s installation with docker runtime\ncurl -sfL https://get.k3s.io | sh -s - \\\n  --docker \\\n  --write-kubeconfig-mode 644\n```\n\n**Sources:** [minikube/README.md:38-48]()\n\n### Minikube vs k3s Comparison\n\n| Feature | Minikube | k3s |\n|---------|----------|-----|\n| **Installation Speed** | Slower | Faster |\n| **Resource Usage** | Higher | Lower |\n| **VM Compatibility** | Can struggle in VMs | Better VM performance |\n| **Maturity** | More mature for development | Newer, production-focused |\n| **Container Runtime** | Docker (default) | containerd (default), Docker (configured) |\n| **Vendor** | Kubernetes project | Rancher |\n| **Cloud Alignment** | Generic | Aligned with RKE |\n\n**Sources:** [minikube/README.md:38-48]()\n\n## DNS and Hostname Configuration\n\n### nip.io Dynamic DNS\n\nEOEPCA deployments use `nip.io` for dynamic DNS resolution, embedding IP addresses directly in hostnames to avoid DNS server configuration:\n\n```\nService hostname format: \u003cservice-name\u003e.\u003cip-with-dashes\u003e.nip.io\nExample: workspace.192-168-49-2.nip.io\n```\n\nFor Minikube, obtain the cluster IP using:\n```bash\n$ minikube ip\n192.168.49.2\n\n# Services would be accessed as:\n# workspace.192-168-49-2.nip.io\n# resource-catalogue.192-168-49-2.nip.io\n# ades.192-168-49-2.nip.io\n```\n\nThe nip.io service supports both dots and dashes for IP delimiters, but dashes are preferred for better compatibility with LetsEncrypt rate limits.\n\n**Sources:** [README.md:100-111]()\n\n### Ingress Configuration\n\nKubernetes Ingress resources route traffic based on hostnames. The deployment configuration must be updated with the correct public IP address:\n\n1. Get Minikube IP: `minikube ip`\n2. Search and replace in deployment files:\n   - Find references to example IPs like `185.52.193.87`\n   - Replace with your Minikube IP in nip.io format\n3. Update Ingress resources in `system/clusters/minikube/`\n\n**Sources:** [README.md:100-111](), [system/clusters/README.md:45-49]()\n\n## GitOps Deployment with Flux\n\n### Flux Installation and Configuration\n\n```mermaid\nsequenceDiagram\n    participant Dev as \"Developer\"\n    participant Script as \"deployCluster.sh\"\n    participant Flux as \"Flux CLI\"\n    participant GitHub as \"GitHub Repo\"\n    participant K8s as \"Kubernetes Cluster\"\n    participant Controllers as \"Flux Controllers\"\n    \n    Dev-\u003e\u003eScript: \"./system/clusters/deployCluster.sh\"\n    Script-\u003e\u003eScript: \"Set BRANCH and TARGET env vars\"\n    Script-\u003e\u003eFlux: \"flux bootstrap github\"\n    Flux-\u003e\u003eGitHub: \"Create/update flux-system\"\n    Flux-\u003e\u003eK8s: \"Install flux controllers\"\n    K8s--\u003e\u003eControllers: \"source-controller\u003cbr/\u003ekustomize-controller\u003cbr/\u003ehelm-controller\u003cbr/\u003enotification-controller\"\n    Controllers-\u003e\u003eGitHub: \"Poll every 1 minute\"\n    GitHub--\u003e\u003eControllers: \"HelmRelease specs\"\n    Controllers-\u003e\u003eK8s: \"Deploy EOEPCA components\"\n    K8s--\u003e\u003eDev: \"System ready\"\n```\n\n**Diagram: GitOps Deployment Flow with Flux**\n\n### Flux Prerequisites\n\nBefore deploying EOEPCA with Flux, install the Flux CLI:\n\n```bash\n$ curl -s https://toolkit.fluxcd.io/install.sh | sudo bash\n```\n\nThis installs Flux to `/usr/local/bin/flux`.\n\nVerify prerequisites:\n```bash\n$ flux check --pre\n```\n\n**Sources:** [system/clusters/README.md:9-29]()\n\n### GitHub Authentication\n\nConfigure GitHub credentials for Flux:\n\n```bash\nexport GITHUB_USER=\u003cyour-username\u003e\nexport GITHUB_TOKEN=\u003cyour-token\u003e\n```\n\nThe `GITHUB_TOKEN` must be a Personal Access Token with all 'repo' scopes enabled. Create tokens at: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\n\n**Sources:** [system/clusters/README.md:31-43]()\n\n### Deployment Target Configuration\n\nThe `system/clusters/minikube/` directory contains the deployment configuration for local Minikube environments. Before deployment:\n\n1. **Copy reference configuration** (optional):\n   ```bash\n   $ cp -r system/clusters/develop system/clusters/my-minikube\n   ```\n\n2. **Update IP addresses** throughout the configuration:\n   - Search for `185.52.193.87` (example IP)\n   - Replace with your Minikube IP\n   - Update all `*.nip.io` hostname references\n\n3. **Adjust resource limits** for constrained environments:\n   - Reduce replica counts\n   - Lower memory/CPU requests\n   - Disable non-essential components\n\n**Sources:** [system/clusters/README.md:45-49]()\n\n### Bootstrap EOEPCA Deployment\n\nExecute the deployment script:\n\n```bash\n$ ./system/clusters/deployCluster.sh\n```\n\n**Environment Variables:**\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `BRANCH` | Git branch to deploy from | Current working branch |\n| `TARGET` | Deployment target directory | `minikube` |\n\nThe script performs `flux bootstrap` configured for the EOEPCA GitHub organization. For personal repositories, modify the bootstrap command:\n\n```bash\nflux bootstrap github \\\n  --owner=$GITHUB_USER \\\n  --repository=eoepca \\\n  --branch=\"${BRANCH}\" \\\n  --path=\"system/clusters/${TARGET}/system\" \\\n  --personal\n```\n\n**Sources:** [system/clusters/README.md:51-77]()\n\n### Flux Reconciliation\n\n```mermaid\ngraph TB\n    subgraph \"Git Repository\"\n        GitRepo[\"eoepca repository\u003cbr/\u003esystem/clusters/minikube\"]\n        HelmReleases[\"HelmRelease CRDs\u003cbr/\u003erm-workspace-api.yaml\u003cbr/\u003erm-resource-catalogue.yaml\u003cbr/\u003eproc-ades.yaml\"]\n    end\n    \n    subgraph \"Flux Controllers\"\n        SourceCtrl[\"source-controller\u003cbr/\u003ePolls every 1 minute\"]\n        KustomCtrl[\"kustomize-controller\u003cbr/\u003eApplies Kustomizations\"]\n        HelmCtrl[\"helm-controller\u003cbr/\u003eManages HelmReleases\"]\n        NotifyCtrl[\"notification-controller\u003cbr/\u003eSends alerts\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        Namespace_UM[\"um namespace\"]\n        Namespace_RM[\"rm namespace\"]\n        Namespace_Proc[\"proc namespace\"]\n    end\n    \n    subgraph \"Version Tracking\"\n        ChartVersion[\"Chart version must increment\u003cbr/\u003efor GitRepository sources\"]\n    end\n    \n    GitRepo --\u003e SourceCtrl\n    SourceCtrl --\u003e KustomCtrl\n    SourceCtrl --\u003e HelmCtrl\n    HelmReleases --\u003e HelmCtrl\n    \n    KustomCtrl --\u003e Namespace_UM\n    KustomCtrl --\u003e Namespace_RM\n    KustomCtrl --\u003e Namespace_Proc\n    \n    HelmCtrl --\u003e Namespace_UM\n    HelmCtrl --\u003e Namespace_RM\n    HelmCtrl --\u003e Namespace_Proc\n    \n    HelmCtrl -.-\u003e ChartVersion\n```\n\n**Diagram: Flux Continuous Reconciliation**\n\nFlux continuously monitors the Git repository and reconciles cluster state:\n\n1. **source-controller** fetches manifests from GitHub every minute\n2. **kustomize-controller** applies Kustomization overlays\n3. **helm-controller** manages HelmRelease resources\n4. **notification-controller** sends alerts on failures\n\n**Important Note:** When using `GitRepository` as a Helm source, the chart version must be incremented for Flux to detect updates. This is documented in [system/clusters/README.md:85]().\n\n**Sources:** [system/clusters/README.md:79-86]()\n\n## Acceptance Testing\n\n### Robot Framework Setup\n\nInstall Robot Framework and its dependencies:\n\n```bash\n$ pip install -U \\\n    robotframework \\\n    robotframework-requests \\\n    docutils \\\n    robotframework-seleniumlibrary \\\n    robotframework-sshlibrary \\\n    webdrivermanager\n```\n\nOr use the provided script:\n```bash\n$ travis/setupRobot.sh\n```\n\n**Sources:** [travis/setupRobot.sh:1-8]()\n\n### Running Acceptance Tests\n\nThe acceptance test suite validates the deployed EOEPCA system:\n\n```bash\n$ ~/.local/bin/robot test/acceptance\n```\n\nTest execution steps from [travis/acceptanceTest.sh:1-37]():\n\n1. **Obtain cluster IP**:\n   ```bash\n   kubeIP=$(kubectl cluster-info | grep 'master' | \\\n     grep -oE '[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}')\n   ```\n\n2. **Verify connectivity** to Kubernetes services\n\n3. **Execute Robot test suite**:\n   ```bash\n   ~/.local/bin/robot test/acceptance\n   ```\n\nThe test suite validates:\n- PDP policy enforcement\n- ADES workflow execution\n- Resource catalogue queries\n- Data access services\n- Workspace provisioning\n- Authentication/authorization flows\n\n**Sources:** [travis/acceptanceTest.sh:1-37](), [README.md:94]()\n\n### Debug Commands\n\nFor troubleshooting, the acceptance test script provides debug commands (disabled by default in [travis/acceptanceTest.sh:6]()):\n\n```bash\n# View cluster configuration\nkubectl config view\n\n# List all resources\nkubectl get nodes\nkubectl get namespaces\nkubectl get pods --all-namespaces\nkubectl get deployments --all\nkubectl get services --all\n\n# Detailed resource information\nkubectl describe deployments --all\nkubectl describe services --all\n```\n\n**Sources:** [travis/acceptanceTest.sh:3-17]()\n\n## Undeploy and Cleanup\n\n### Remove Flux\n\nTo uninstall Flux from the cluster:\n\n```bash\n$ ./system/clusters/undeployCluster.sh\n```\n\n**Important:** This script only removes Flux controllers. It does **not** uninstall EOEPCA components that were deployed by Flux. To fully clean the environment, manually delete EOEPCA namespaces and resources.\n\n**Sources:** [system/clusters/README.md:88-94]()\n\n### Delete Minikube Cluster\n\nTo completely remove the Minikube cluster:\n\n```bash\n$ minikube delete --purge --all\n```\n\nThis command:\n- Stops the cluster\n- Deletes all cluster resources\n- Removes cluster configuration\n- Purges cached images and data\n\n**Sources:** [travis/setupMinikube.sh:20]()\n\n### Delete k3s Installation\n\nTo uninstall k3s:\n\n```bash\n$ /usr/local/bin/k3s-uninstall.sh\n```\n\nThis script is created during k3s installation and removes all k3s components and data.\n\n## Common Issues and Troubleshooting\n\n### Resource Constraints\n\n**Problem:** System components fail to start due to insufficient resources.\n\n**Solutions:**\n1. Increase Minikube resources:\n   ```bash\n   minikube start --cpus=8 --memory=32768\n   ```\n\n2. Deploy a subset of components by modifying Flux Kustomizations\n\n3. Use k3s instead of Minikube for lower overhead\n\n**Sources:** [README.md:96]()\n\n### KUBECONFIG Path Issues\n\n**Problem:** Flux commands fail with kubeconfig errors.\n\n**Solution:** Flux requires a single file path, not a colon-separated list:\n```bash\n# Instead of:\nexport KUBECONFIG=/path/one:/path/two\n\n# Use:\nexport KUBECONFIG=/path/to/config\n```\n\nThe EOEPCA deploy/undeploy scripts handle this automatically.\n\n**Sources:** [system/clusters/README.md:17-21]()\n\n### Hostname Resolution\n\n**Problem:** Services cannot be accessed via nip.io hostnames.\n\n**Solutions:**\n1. Verify Minikube IP: `minikube ip`\n2. Check ingress controller: `kubectl get pods -n ingress-nginx`\n3. Verify DNS resolution: `nslookup \u003cservice\u003e.192-168-49-2.nip.io`\n4. Check ingress resources: `kubectl get ingress --all-namespaces`\n\n**Sources:** [README.md:100-107]()\n\n### Argo/Calrissian Issues\n\n**Problem:** ADES workflows fail to execute.\n\n**Solution:** Ensure Docker container runtime is configured:\n- For Minikube: Docker driver is default\n- For k3s: Must explicitly configure `--docker` flag\n\nThe ADES building block requires Docker runtime for Calrissian/Argo workflow execution.\n\n**Sources:** [minikube/README.md:48]()\n\n## Next Steps\n\nOnce the Kubernetes cluster is provisioned and validated:\n\n1. **Deploy EOEPCA System**: Follow [EOEPCA GitOps](#3.2) deployment guide\n2. **Alternative Deployment**: Use [Deployment Guide](https://deployment-guide.docs.eoepca.org/) for component-by-component deployment with Helm\n3. **Run Acceptance Tests**: Execute [test suite](#2.3) to validate deployment\n4. **Explore Building Blocks**: Deploy individual components for development:\n   - [ADES](#6.1) for workflow execution\n   - [Workspace API](#5.3) for multi-tenant storage\n   - [Resource Catalogue](#5.2) for metadata management\n   - [Data Access](#5.1) for OGC services\n\n**Sources:** [minikube/README.md:50-52](), [README.md:88-98]()"])</script><script>self.__next_f.push([1,"3d:T4651,"])</script><script>self.__next_f.push([1,"# Configuration and Secrets Management\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh)\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml](system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml)\n- [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml](system/clusters/creodias/system/demo/hr-eoepca-portal.yaml)\n- [system/clusters/creodias/system/demo/ss-django-secrets-create.sh](system/clusters/creodias/system/demo/ss-django-secrets-create.sh)\n- [system/clusters/creodias/system/demo/ss-django-secrets.yaml](system/clusters/creodias/system/demo/ss-django-secrets.yaml)\n- [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml](system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml)\n- [system/clusters/creodias/user-management/um-identity-service.yaml](system/clusters/creodias/user-management/um-identity-service.yaml)\n\n\u003c/details\u003e\n\n\n\nThis page documents the configuration and secrets management patterns used throughout the EOEPCA platform. It covers the SealedSecrets system for storing encrypted credentials in Git, OIDC client configuration, TLS certificate management via cert-manager, and the workflows for creating and referencing secrets in service deployments.\n\nFor information about OIDC client registration and OAuth2 flows, see [OIDC Client Configuration](#10.2). For policy management and resource protection configuration, see [Policy Enforcement (PEP/PDP)](#4.3).\n\n## Purpose and Scope\n\nEOEPCA uses a GitOps deployment model where all configuration is stored in Git repositories. This creates a challenge for managing sensitive data like OIDC client secrets, database passwords, and API keys. The platform addresses this using the Bitnami SealedSecrets controller, which enables storing encrypted secrets in Git that can only be decrypted by the Kubernetes cluster. This page covers:\n\n- The SealedSecrets encryption/decryption workflow\n- OIDC client credential management for OAuth2 authentication\n- TLS certificate provisioning via cert-manager and Let's Encrypt\n- Patterns for referencing secrets in HelmRelease configurations\n- Scripts and procedures for creating and updating secrets\n\n## Secrets Management Architecture\n\nThe EOEPCA platform uses a layered approach to secrets management, with encryption at rest in Git and decryption only within the Kubernetes cluster.\n\n```mermaid\ngraph TB\n    Developer[\"Developer\u003cbr/\u003e(Local Machine)\"]\n    Script[\"Secret Creation Script\u003cbr/\u003e*-create.sh\"]\n    Kubeseal[\"kubeseal CLI\"]\n    Controller[\"eoepca-sealed-secrets\u003cbr/\u003e(infra namespace)\"]\n    Git[\"Git Repository\u003cbr/\u003eEOEPCA/eoepca\"]\n    Flux[\"Flux CD\"]\n    \n    Developer --\u003e|\"1. Run with credentials\"| Script\n    Script --\u003e|\"2. Generate Secret YAML\"| Kubeseal\n    Kubeseal --\u003e|\"3. Encrypt to SealedSecret\"| Controller\n    Controller --\u003e|\"4. Return encrypted YAML\"| Kubeseal\n    Kubeseal --\u003e|\"5. Write ss-*.yaml\"| Git\n    Developer --\u003e|\"6. Commit \u0026 Push\"| Git\n    Git --\u003e|\"7. Poll \u0026 Sync\"| Flux\n    Flux --\u003e|\"8. Apply SealedSecret\"| Controller\n    Controller --\u003e|\"9. Decrypt to Secret\"| K8sSecret[\"Kubernetes Secret\u003cbr/\u003e(target namespace)\"]\n    \n    Service[\"Service Pod\u003cbr/\u003e(e.g., application-hub)\"] --\u003e|\"10. Mount/Read\"| K8sSecret\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36](), [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33]()\n\n## SealedSecrets System\n\n### Controller Configuration\n\nThe SealedSecrets controller is deployed in the `infra` namespace with the name `eoepca-sealed-secrets`. This controller holds the private key used to decrypt SealedSecret resources into standard Kubernetes Secrets.\n\n| Component | Value |\n|-----------|-------|\n| Controller Name | `eoepca-sealed-secrets` |\n| Controller Namespace | `infra` |\n| API Version | `bitnami.com/v1alpha1` |\n| Kind | `SealedSecret` |\n\n### SealedSecret Structure\n\nSealedSecrets are custom Kubernetes resources that contain encrypted data. The controller watches for these resources and automatically creates corresponding Secrets in the target namespace.\n\n```mermaid\ngraph LR\n    subgraph \"Git Repository\"\n        SS[\"SealedSecret\u003cbr/\u003ess-django-secrets.yaml\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        SSRes[\"SealedSecret Resource\u003cbr/\u003ename: django-secrets\u003cbr/\u003enamespace: demo\"]\n        Controller[\"eoepca-sealed-secrets\u003cbr/\u003econtroller\"]\n        Secret[\"Secret\u003cbr/\u003ename: django-secrets\u003cbr/\u003enamespace: demo\"]\n    end\n    \n    SS --\u003e|\"Applied by Flux\"| SSRes\n    SSRes --\u003e|\"Watched by\"| Controller\n    Controller --\u003e|\"Creates\"| Secret\n    \n    Secret --\u003e|\"Keys:\u003cbr/\u003eOIDC_RP_CLIENT_ID\u003cbr/\u003eOIDC_RP_CLIENT_SECRET\u003cbr/\u003eDJANGO_SECRET\"| Secret\n```\n\nThe SealedSecret manifest structure follows this pattern:\n\n[system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17]()\n\nKey elements:\n- `metadata.name`: Name of the Secret to be created\n- `metadata.namespace`: Target namespace for the Secret\n- `spec.encryptedData`: Map of key-value pairs with encrypted values\n- `spec.template.metadata`: Template for the created Secret\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-17]()\n\n## Secret Creation Workflow\n\n### Creation Script Pattern\n\nEach SealedSecret has an associated creation script that follows a standard pattern. These scripts are typically named `ss-\u003csecret-name\u003e-create.sh` and use `kubectl` and `kubeseal` to generate encrypted manifests.\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant Script as *-create.sh\n    participant Kubectl as kubectl\n    participant Kubeseal as kubeseal\n    participant File as ss-*.yaml\n    \n    Dev-\u003e\u003eScript: Execute with credentials\n    Script-\u003e\u003eScript: Set SECRET_NAME, NAMESPACE\n    Script-\u003e\u003eKubectl: create secret generic --dry-run=client\n    Kubectl--\u003e\u003eScript: Plain Secret YAML\n    Script-\u003e\u003eKubeseal: Pipe to kubeseal -o yaml\n    Kubeseal-\u003e\u003eKubeseal: Encrypt with controller's public key\n    Kubeseal--\u003e\u003eScript: SealedSecret YAML\n    Script-\u003e\u003eFile: Write encrypted manifest\n    Dev-\u003e\u003eFile: Commit to Git\n```\n\n### Application Hub Secret Creation\n\nThe Application Hub requires three secrets for OAuth2 authentication with the Login Service:\n\n[system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36]()\n\nKey steps in the script:\n1. Define secret values (lines 3-5): `JUPYTERHUB_CRYPT_KEY`, `OAUTH_CLIENT_ID`, `OAUTH_CLIENT_SECRET`\n2. Create a temporary Secret manifest using `kubectl create secret generic --dry-run=client` (lines 26-31)\n3. Pipe to `kubeseal` with controller specification (line 35)\n4. Output encrypted SealedSecret to `application-hub-sealed-secrets.yaml`\n\nThe `kubeseal` command parameters:\n- `--controller-name eoepca-sealed-secrets`: Specifies the controller instance\n- `--controller-namespace infra`: Namespace where the controller is deployed\n- `-o yaml`: Output format\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36]()\n\n### Django Portal Secret Creation\n\nThe EOEPCA Portal (Django-based) requires OIDC credentials for both the Login Service (Gluu) and Identity Service (Keycloak):\n\n[system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33]()\n\nThis script accepts five parameters:\n1. `client_id`: OIDC client ID for Gluu\n2. `client_secret`: OIDC client secret for Gluu\n3. `django_secret`: Django SECRET_KEY for session management\n4. `keycloak_client_id`: OIDC client ID for Keycloak\n5. `keycloak_client_secret`: OIDC client secret for Keycloak\n\nThe `secretYaml()` function (lines 22-30) creates a Secret with all five literal values, then pipes to `kubeseal` for encryption.\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33]()\n\n## Secret Reference Patterns\n\n### ExistingSecret Pattern\n\nServices reference SealedSecrets (after they're decrypted to Secrets) using the `existingSecret` Helm value pattern:\n\n[system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:34]()\n\nThis tells the JupyterHub chart to use the `application-hub-secrets` Secret instead of generating one. The hub component then references individual keys within that Secret.\n\n### ValueFrom SecretKeyRef Pattern\n\nFor environment variables, secrets are injected using the `valueFrom.secretKeyRef` pattern:\n\n[system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:47-63]()\n\nThis pattern specifies:\n- `name`: The Secret name (e.g., `application-hub-secrets`)\n- `key`: The specific key within the Secret (e.g., `JUPYTERHUB_CRYPT_KEY`)\n\nThe Kubernetes kubelet retrieves the value from the Secret and injects it into the pod's environment.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:34-63]()\n\n## OIDC Client Secrets\n\n### Application Hub OAuth2 Configuration\n\nThe Application Hub uses OAuth2 to authenticate users via the Login Service. The OAuth2 flow requires three pieces of configuration:\n\n| Secret Key | Purpose | Usage |\n|------------|---------|-------|\n| `JUPYTERHUB_CRYPT_KEY` | Encrypts JupyterHub cookies and tokens | Generated via `openssl rand -hex 32` |\n| `OAUTH_CLIENT_ID` | OIDC client identifier | Registered in Login Service |\n| `OAUTH_CLIENT_SECRET` | OIDC client secret | Obtained from Login Service |\n\nThese values are referenced in the HelmRelease configuration:\n\n[system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:38-41]()\n\nThe OAuth2 URLs point to the Login Service endpoints:\n- Authorization: `https://auth.develop.eoepca.org/oxauth/restv1/authorize`\n- Token: `https://auth.develop.eoepca.org/oxauth/restv1/token`\n- UserInfo: `https://auth.develop.eoepca.org/oxauth/restv1/userinfo`\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:38-63](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:8-11]()\n\n### Django Portal Multi-Provider Configuration\n\nThe EOEPCA Portal supports authentication against both the Login Service (Gluu) and Identity Service (Keycloak), requiring separate OIDC credentials for each:\n\n```mermaid\ngraph LR\n    Portal[\"EOEPCA Portal\u003cbr/\u003e(Django)\"]\n    Secret[\"django-secrets\"]\n    Gluu[\"Login Service\u003cbr/\u003e(Gluu oxAuth)\"]\n    Keycloak[\"Identity Service\u003cbr/\u003e(Keycloak)\"]\n    \n    Secret --\u003e|\"OIDC_RP_CLIENT_ID\u003cbr/\u003eOIDC_RP_CLIENT_SECRET\"| Portal\n    Secret --\u003e|\"KEYCLOAK_OIDC_RP_CLIENT_ID\u003cbr/\u003eKEYCLOAK_OIDC_RP_CLIENT_SECRET\"| Portal\n    Secret --\u003e|\"DJANGO_SECRET\"| Portal\n    \n    Portal --\u003e|\"Authenticate users\"| Gluu\n    Portal --\u003e|\"Alternative auth\"| Keycloak\n```\n\nThe SealedSecret contains five keys:\n- `OIDC_RP_CLIENT_ID` / `OIDC_RP_CLIENT_SECRET`: Gluu credentials\n- `KEYCLOAK_OIDC_RP_CLIENT_ID` / `KEYCLOAK_OIDC_RP_CLIENT_SECRET`: Keycloak credentials\n- `DJANGO_SECRET`: Django application secret key\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets.yaml:8-11](), [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:23-28]()\n\n## TLS Certificate Management\n\n### Cert-Manager and Let's Encrypt\n\nEOEPCA uses cert-manager to automatically provision and renew TLS certificates from Let's Encrypt. Services declare certificate requirements via Ingress annotations.\n\n```mermaid\ngraph TB\n    Ingress[\"Ingress Resource\u003cbr/\u003eidentity.keycloak.develop.eoepca.org\"]\n    Annotation[\"cert-manager.io/cluster-issuer:\u003cbr/\u003eletsencrypt\"]\n    CertManager[\"cert-manager\"]\n    ClusterIssuer[\"ClusterIssuer:\u003cbr/\u003eletsencrypt\"]\n    LetsEncrypt[\"Let's Encrypt\u003cbr/\u003eACME Server\"]\n    Secret[\"Secret:\u003cbr/\u003eidentity-keycloak-tls-certificate\"]\n    \n    Ingress --\u003e|\"Has annotation\"| Annotation\n    Annotation --\u003e|\"Triggers\"| CertManager\n    CertManager --\u003e|\"Uses\"| ClusterIssuer\n    ClusterIssuer --\u003e|\"ACME Challenge\"| LetsEncrypt\n    LetsEncrypt --\u003e|\"Issue certificate\"| CertManager\n    CertManager --\u003e|\"Store in\"| Secret\n    Ingress --\u003e|\"References in spec.tls\"| Secret\n```\n\n### Identity Service TLS Configuration\n\nThe Identity Service components each have dedicated TLS certificates:\n\n[system/clusters/creodias/user-management/um-identity-service.yaml:23-34]()\n\nPattern for each component:\n1. **Annotation**: `cert-manager.io/cluster-issuer: letsencrypt` triggers certificate issuance\n2. **Hosts**: Define the FQDN(s) for the certificate\n3. **TLS Section**: References the Secret name where the certificate will be stored\n\nThe same pattern is repeated for:\n- `identity.api.develop.eoepca.org`  `identity-api-tls-certificate`\n- `identity.manager.develop.eoepca.org`  `identity-manager-tls-certificate`\n- `identity.gatekeeper.develop.eoepca.org`  `identity-gatekeeper-tls-certificate`\n\n**Sources:** [system/clusters/creodias/user-management/um-identity-service.yaml:23-76]()\n\n### Application Hub TLS Configuration\n\nThe Application Hub uses a similar pattern:\n\n[system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:24-27]()\n\nThe TLS secret `applicationhub-tls` is automatically created and managed by cert-manager for the host `applicationhub.develop.eoepca.org`.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:16-27]()\n\n### Portal TLS Configuration\n\nThe EOEPCA Portal demonstrates the full annotation and TLS specification pattern:\n\n[system/clusters/creodias/system/demo/hr-eoepca-portal.yaml:22-35]()\n\nKey elements:\n- `kubernetes.io/ingress.class: nginx`: Specifies the ingress controller\n- `cert-manager.io/cluster-issuer: letsencrypt`: Requests automatic certificate provisioning\n- `spec.tls.secretName`: Destination for the certificate Secret\n- `spec.tls.hosts`: Must match the hosts in `spec.rules`\n\n**Sources:** [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml:22-35]()\n\n## Nginx Authentication Integration\n\n### Identity Service Integration\n\nSome services use Nginx ingress configuration to integrate with the Identity Service's gatekeeper component for authentication:\n\n[system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31]()\n\nThis configuration demonstrates two key patterns:\n\n1. **Authentication Snippet** (lines 9-17): Configures `auth_request /auth` to delegate authentication to the Identity Service gatekeeper, with CORS headers for browser access.\n\n2. **Server Snippet** (lines 18-31): Defines the internal `/auth` location that proxies to `identity-gatekeeper.um.svc.cluster.local:3000`, forwarding request metadata for validation.\n\nThe gatekeeper validates the request and returns HTTP 200 (authorized) or 401/403 (unauthorized), controlling access to the backend service.\n\n**Sources:** [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31]()\n\n## Configuration Management Best Practices\n\n### Secret Lifecycle\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Created: Developer runs create script\n    Created --\u003e Encrypted: kubeseal encrypts\n    Encrypted --\u003e Committed: Commit to Git\n    Committed --\u003e Applied: Flux syncs\n    Applied --\u003e Decrypted: Controller decrypts\n    Decrypted --\u003e Mounted: Pod mounts Secret\n    Mounted --\u003e Rotated: Update required\n    Rotated --\u003e Created: Re-run create script\n```\n\n### Secret Rotation Procedure\n\nTo rotate a secret:\n\n1. Generate new credentials from the identity provider\n2. Update values in the `*-create.sh` script\n3. Re-run the script to generate a new encrypted SealedSecret\n4. Commit the updated `ss-*.yaml` file to Git\n5. Flux will detect the change and apply it\n6. The SealedSecrets controller will update the Secret\n7. Restart affected pods to pick up the new values\n\n### Separation of Concerns\n\n| Secret Type | Scope | Examples |\n|-------------|-------|----------|\n| OIDC Credentials | Service-specific | `OAUTH_CLIENT_ID`, `OAUTH_CLIENT_SECRET` |\n| Application Keys | Service-specific | `JUPYTERHUB_CRYPT_KEY`, `DJANGO_SECRET` |\n| TLS Certificates | Ingress/Domain | `*-tls-certificate` secrets |\n| Database Credentials | Shared infrastructure | PostgreSQL passwords |\n\nOIDC credentials are unique per service and registered separately in the identity provider. Application-specific secrets like `JUPYTERHUB_CRYPT_KEY` are generated randomly and used only by that service. TLS certificates are scoped to domain names and automatically managed by cert-manager.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36](), [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33]()\n\n## Troubleshooting\n\n### Common Issues\n\n| Issue | Symptom | Solution |\n|-------|---------|----------|\n| Secret not decrypted | SealedSecret exists but Secret doesn't | Check controller logs in `infra` namespace |\n| Wrong controller specified | Secret not created | Verify `--controller-name` and `--controller-namespace` in kubeseal command |\n| Pod can't read secret | Permission denied | Check RBAC permissions for service account |\n| Certificate not issued | Ingress has no TLS | Check cert-manager logs, verify ACME challenge |\n\n### Verification Commands\n\nCheck if a SealedSecret was decrypted:\n```bash\nkubectl get secret \u003csecret-name\u003e -n \u003cnamespace\u003e\n```\n\nView certificate status:\n```bash\nkubectl get certificate -n \u003cnamespace\u003e\nkubectl describe certificate \u003ccert-name\u003e -n \u003cnamespace\u003e\n```\n\nCheck SealedSecrets controller logs:\n```bash\nkubectl logs -n infra deployment/eoepca-sealed-secrets\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-17](), [system/clusters/creodias/user-management/um-identity-service.yaml:23-76]()"])</script><script>self.__next_f.push([1,"3e:T4587,"])</script><script>self.__next_f.push([1,"# SealedSecrets\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh)\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml](system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml)\n- [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml](system/clusters/creodias/system/demo/hr-eoepca-portal.yaml)\n- [system/clusters/creodias/system/demo/ss-django-secrets-create.sh](system/clusters/creodias/system/demo/ss-django-secrets-create.sh)\n- [system/clusters/creodias/system/demo/ss-django-secrets.yaml](system/clusters/creodias/system/demo/ss-django-secrets.yaml)\n- [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml](system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml)\n- [system/clusters/creodias/user-management/um-identity-service.yaml](system/clusters/creodias/user-management/um-identity-service.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains the SealedSecrets pattern used in EOEPCA to securely store sensitive credentials in Git repositories. SealedSecrets enables GitOps-based deployments while maintaining security for OIDC client credentials, API keys, and other sensitive configuration data. For general configuration management, see [Configuration and Secrets Management](#10). For OIDC-specific client configuration, see [OIDC Client Configuration](#10.2).\n\n## Overview\n\nSealedSecrets is a Kubernetes controller and CLI tool that encrypts standard Kubernetes Secrets into SealedSecret custom resources. Unlike regular Secrets, SealedSecrets can be safely committed to Git repositories because they are encrypted with a public key. The sealed-secrets controller running in the cluster holds the private key and automatically decrypts SealedSecrets into regular Secrets at runtime.\n\nEOEPCA uses SealedSecrets to store:\n- OIDC client credentials for OAuth2/OpenID Connect authentication flows\n- JupyterHub encryption keys\n- Django application secrets\n- Service-to-service authentication credentials\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-17]()\n\n## Architecture and Components\n\n```mermaid\ngraph TB\n    subgraph \"Developer Workstation\"\n        PlainSecret[\"Plain Secret Values\u003cbr/\u003e(OAUTH_CLIENT_ID, etc)\"]\n        KubectlCmd[\"kubectl create secret\u003cbr/\u003e--dry-run=client\"]\n        KubesealCmd[\"kubeseal CLI\u003cbr/\u003e--controller-name eoepca-sealed-secrets\u003cbr/\u003e--controller-namespace infra\"]\n    end\n    \n    subgraph \"Git Repository\"\n        SealedSecretYAML[\"SealedSecret YAML\u003cbr/\u003ess-django-secrets.yaml\u003cbr/\u003ess-application-hub-secrets.yaml\"]\n    end\n    \n    subgraph \"Kubernetes Cluster - infra namespace\"\n        SSController[\"sealed-secrets-controller\u003cbr/\u003eHolds Private Key\"]\n    end\n    \n    subgraph \"Kubernetes Cluster - Application Namespaces\"\n        SealedSecretCR[\"SealedSecret CR\u003cbr/\u003ekind: SealedSecret\u003cbr/\u003eapiVersion: bitnami.com/v1alpha1\"]\n        DecryptedSecret[\"Kubernetes Secret\u003cbr/\u003ename: application-hub-secrets\u003cbr/\u003ename: django-secrets\"]\n        Pod[\"Application Pod\u003cbr/\u003eJupyterHub, Django Portal\"]\n    end\n    \n    subgraph \"Flux CD\"\n        FluxSync[\"Flux Kustomization\u003cbr/\u003eApplies SealedSecret\"]\n    end\n    \n    PlainSecret --\u003e KubectlCmd\n    KubectlCmd --\u003e KubesealCmd\n    KubesealCmd --\u003e|\"Encrypts with\u003cbr/\u003ePublic Key\"| SealedSecretYAML\n    SealedSecretYAML --\u003e FluxSync\n    FluxSync --\u003e|\"kubectl apply\"| SealedSecretCR\n    SealedSecretCR --\u003e SSController\n    SSController --\u003e|\"Decrypts with\u003cbr/\u003ePrivate Key\"| DecryptedSecret\n    DecryptedSecret --\u003e Pod\n```\n\n**Diagram: SealedSecrets Architecture in EOEPCA**\n\nThe sealed-secrets controller (`eoepca-sealed-secrets`) is deployed in the `infra` namespace and manages all SealedSecret resources across the cluster. When a SealedSecret is applied, the controller watches for it, decrypts the encrypted data using its private key, and creates or updates the corresponding Secret.\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:32-33](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:34-35]()\n\n## SealedSecrets Workflow\n\n```mermaid\nsequenceDiagram\n    participant Dev as Developer\n    participant Script as Creation Script\u003cbr/\u003e(ss-*-create.sh)\n    participant Kubectl as kubectl CLI\n    participant Kubeseal as kubeseal CLI\n    participant Git as Git Repository\n    participant Flux as Flux CD\n    participant Controller as sealed-secrets-controller\u003cbr/\u003e(infra namespace)\n    participant K8s as Kubernetes API\n    participant App as Application Pod\n    \n    Dev-\u003e\u003eScript: Run creation script with credentials\n    Script-\u003e\u003eKubectl: kubectl create secret --dry-run=client -o yaml\n    Kubectl--\u003e\u003eScript: YAML manifest (not applied)\n    Script-\u003e\u003eKubeseal: Pipe manifest to kubeseal\n    Note over Kubeseal: Fetches public key from controller\u003cbr/\u003eeoepca-sealed-secrets/infra\n    Kubeseal--\u003e\u003eScript: Encrypted SealedSecret YAML\n    Script-\u003e\u003eGit: Write ss-*.yaml file\n    Dev-\u003e\u003eGit: Commit and push\n    \n    Flux-\u003e\u003eGit: Poll for changes (1m interval)\n    Git--\u003e\u003eFlux: New SealedSecret manifest\n    Flux-\u003e\u003eK8s: Apply SealedSecret CR\n    K8s-\u003e\u003eController: SealedSecret created event\n    Controller-\u003e\u003eController: Decrypt with private key\n    Controller-\u003e\u003eK8s: Create/Update Secret\n    App-\u003e\u003eK8s: Mount Secret as environment variables\n    K8s--\u003e\u003eApp: Provide decrypted values\n```\n\n**Diagram: SealedSecrets Creation and Deployment Flow**\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36]()\n\n## Creating SealedSecrets\n\n### Creation Script Pattern\n\nEOEPCA uses shell scripts with the naming convention `ss-\u003csecret-name\u003e-create.sh` to generate SealedSecrets. These scripts follow a standard pattern:\n\n| Script Element | Purpose | Example Value |\n|----------------|---------|---------------|\n| `SECRET_NAME` | Name of the Kubernetes Secret | `application-hub-secrets` |\n| `NAMESPACE` | Target namespace | `proc`, `demo` |\n| Input parameters | Sensitive values as arguments | `OAUTH_CLIENT_ID`, `OAUTH_CLIENT_SECRET` |\n| `secretYaml()` function | Generates plain Secret YAML | Uses `kubectl create secret --dry-run=client` |\n| `kubeseal` command | Encrypts Secret into SealedSecret | Targets `eoepca-sealed-secrets` controller |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:17-35]()\n\n### Application Hub SealedSecret Creation\n\nThe Application Hub (JupyterHub) requires three encrypted values:\n\n```bash\n#!/usr/bin/env bash\n\nSECRET_NAME=\"application-hub-secrets\"\nNAMESPACE=\"proc\"\n\nsecretYaml() {\n  kubectl -n \"${NAMESPACE}\" create secret generic \"${SECRET_NAME}\" \\\n    --from-literal=\"JUPYTERHUB_CRYPT_KEY=${JUPYTERHUB_CRYPT_KEY}\" \\\n    --from-literal=\"OAUTH_CLIENT_ID=${OAUTH_CLIENT_ID}\" \\\n    --from-literal=\"OAUTH_CLIENT_SECRET=${OAUTH_CLIENT_SECRET}\" \\\n    --dry-run=client -o yaml\n}\n\nsecretYaml | kubeseal -o yaml --controller-name eoepca-sealed-secrets --controller-namespace infra \u003e application-hub-sealed-secrets.yaml\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:17-35]()\n\nThe `--dry-run=client` flag ensures the Secret is not actually created in the cluster, only output as YAML. The `kubeseal` command then:\n1. Fetches the public key from the `eoepca-sealed-secrets` controller in the `infra` namespace\n2. Encrypts each `--from-literal` field separately\n3. Outputs a SealedSecret manifest with encrypted data\n\n### Django Portal SealedSecret Creation\n\nThe EOEPCA portal uses a similar pattern with additional OIDC credentials:\n\n```bash\nSECRET_NAME=\"django-secrets\"\nNAMESPACE=\"demo\"\n\nclient_id=\"${1:-set-client-id-here}\"\nclient_secret=\"${2:-set-client-secret-here}\"\ndjango_secret=\"${3:-set-django-secret-here}\"\n\nsecretYaml() {\n  kubectl -n \"${NAMESPACE}\" create secret generic \"${SECRET_NAME}\" \\\n    --from-literal=OIDC_RP_CLIENT_ID=\"${client_id}\" \\\n    --from-literal=OIDC_RP_CLIENT_SECRET=\"${client_secret}\" \\\n    --from-literal=DJANGO_SECRET=\"${django_secret}\" \\\n    --dry-run=client -o yaml\n}\n\nsecretYaml | kubeseal -o yaml --controller-name eoepca-sealed-secrets --controller-namespace infra \u003e ss-${SECRET_NAME}.yaml\n```\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:13-33]()\n\n## SealedSecret Manifest Structure\n\n### Manifest Components\n\nA SealedSecret manifest consists of:\n\n```mermaid\ngraph TB\n    SealedSecret[\"SealedSecret Resource\"]\n    Metadata[\"metadata\u003cbr/\u003ename: application-hub-secrets\u003cbr/\u003enamespace: proc\"]\n    Spec[\"spec\"]\n    EncryptedData[\"encryptedData\u003cbr/\u003e(base64-encoded encrypted values)\"]\n    Template[\"template\u003cbr/\u003e(target Secret metadata)\"]\n    \n    SealedSecret --\u003e Metadata\n    SealedSecret --\u003e Spec\n    Spec --\u003e EncryptedData\n    Spec --\u003e Template\n    \n    EncryptedData --\u003e Key1[\"JUPYTERHUB_CRYPT_KEY: AgAASKm5g1W...\"]\n    EncryptedData --\u003e Key2[\"OAUTH_CLIENT_ID: AgAxKnBsdtK...\"]\n    EncryptedData --\u003e Key3[\"OAUTH_CLIENT_SECRET: AgBc0P5B1ue...\"]\n    \n    Template --\u003e TargetMeta[\"metadata\u003cbr/\u003ename: application-hub-secrets\u003cbr/\u003enamespace: proc\"]\n```\n\n**Diagram: SealedSecret Manifest Structure**\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-17]()\n\n### Application Hub SealedSecret Example\n\nThe Application Hub SealedSecret manifest at [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-17]() demonstrates the structure:\n\n| Field | Value |\n|-------|-------|\n| `apiVersion` | `bitnami.com/v1alpha1` |\n| `kind` | `SealedSecret` |\n| `metadata.name` | `application-hub-secrets` |\n| `metadata.namespace` | `proc` |\n| `spec.encryptedData.JUPYTERHUB_CRYPT_KEY` | Encrypted 32-byte hex key for JupyterHub |\n| `spec.encryptedData.OAUTH_CLIENT_ID` | Encrypted OIDC client identifier |\n| `spec.encryptedData.OAUTH_CLIENT_SECRET` | Encrypted OIDC client secret |\n| `spec.template.metadata.name` | `application-hub-secrets` (target Secret name) |\n\nThe encrypted values in `spec.encryptedData` are prefixed with `AgA` (AES-GCM encryption algorithm indicator) followed by base64-encoded ciphertext. Each field is encrypted independently, allowing the controller to decrypt them into separate Secret keys.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:8-16]()\n\n### Django Secrets Example\n\nThe Django portal SealedSecret at [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17]() stores OIDC credentials for the EOEPCA web portal:\n\n| Secret Key | Purpose |\n|------------|---------|\n| `DJANGO_SECRET` | Django framework secret key for session management |\n| `OIDC_RP_CLIENT_ID` | OpenID Connect Relying Party client ID for login service integration |\n| `OIDC_RP_CLIENT_SECRET` | OpenID Connect Relying Party client secret |\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets.yaml:8-11]()\n\n## Integration with Application Deployments\n\n### Application Hub Integration\n\nThe Application Hub HelmRelease references the SealedSecret-created Secret via `existingSecret`:\n\n```mermaid\ngraph LR\n    SealedSecret[\"SealedSecret\u003cbr/\u003eapplication-hub-secrets\u003cbr/\u003e(proc namespace)\"]\n    Controller[\"sealed-secrets-controller\"]\n    Secret[\"Secret\u003cbr/\u003eapplication-hub-secrets\u003cbr/\u003e(proc namespace)\"]\n    HelmRelease[\"HelmRelease\u003cbr/\u003eapplication-hub\"]\n    JupyterHub[\"JupyterHub Pod\"]\n    \n    SealedSecret --\u003e Controller\n    Controller --\u003e|\"Decrypts to\"| Secret\n    HelmRelease --\u003e|\"hub.existingSecret\"| Secret\n    HelmRelease --\u003e|\"Deploys\"| JupyterHub\n    JupyterHub --\u003e|\"Mounts env vars from\"| Secret\n```\n\n**Diagram: SealedSecret Integration with Application Hub**\n\nThe HelmRelease at [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:34]() references the Secret:\n\n```yaml\nhub:\n  existingSecret: application-hub-secrets\n```\n\nIndividual environment variables then reference the Secret keys:\n\n```yaml\nJUPYTERHUB_CRYPT_KEY:\n  valueFrom:\n    secretKeyRef:\n      name: application-hub-secrets\n      key: JUPYTERHUB_CRYPT_KEY\n\nOAUTH_CLIENT_ID:\n  valueFrom:\n    secretKeyRef:\n      name: application-hub-secrets\n      key: OAUTH_CLIENT_ID\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:34](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:47-63]()\n\n### Secret Consumption Pattern\n\nApplications consume SealedSecret-generated Secrets through environment variable injection:\n\n| HelmRelease Field | Purpose | Example Value |\n|-------------------|---------|---------------|\n| `hub.existingSecret` | References pre-created Secret | `application-hub-secrets` |\n| `extraEnv[].valueFrom.secretKeyRef.name` | Secret name to mount from | `application-hub-secrets` |\n| `extraEnv[].valueFrom.secretKeyRef.key` | Specific key within Secret | `OAUTH_CLIENT_ID` |\n\nThe pattern ensures sensitive values never appear in HelmRelease manifests committed to Git, only references to the Secret names and keys.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:47-63]()\n\n## GitOps Integration\n\n### Flux CD and SealedSecrets\n\nSealedSecrets enable GitOps by allowing encrypted secrets in Git repositories that Flux CD can apply:\n\n```mermaid\ngraph TB\n    subgraph \"Git Repository eoepca/eoepca\"\n        SSManifests[\"SealedSecret Manifests\u003cbr/\u003ess-django-secrets.yaml\u003cbr/\u003eapplication-hub-sealed-secrets.yaml\"]\n        HRManifests[\"HelmRelease Manifests\u003cbr/\u003eproc-application-hub.yaml\u003cbr/\u003ehr-eoepca-portal.yaml\"]\n    end\n    \n    subgraph \"Flux CD System\"\n        SourceCtrl[\"source-controller\u003cbr/\u003eFetches from Git\"]\n        KustomizeCtrl[\"kustomize-controller\u003cbr/\u003eApplies manifests\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        SSController[\"sealed-secrets-controller\u003cbr/\u003e(infra namespace)\"]\n        \n        subgraph \"proc namespace\"\n            SS1[\"SealedSecret\u003cbr/\u003eapplication-hub-secrets\"]\n            Secret1[\"Secret\u003cbr/\u003eapplication-hub-secrets\"]\n            Hub[\"JupyterHub\"]\n        end\n        \n        subgraph \"demo namespace\"\n            SS2[\"SealedSecret\u003cbr/\u003edjango-secrets\"]\n            Secret2[\"Secret\u003cbr/\u003edjango-secrets\"]\n            Portal[\"Django Portal\"]\n        end\n    end\n    \n    SSManifests --\u003e SourceCtrl\n    HRManifests --\u003e SourceCtrl\n    SourceCtrl --\u003e KustomizeCtrl\n    \n    KustomizeCtrl --\u003e|\"Apply\"| SS1\n    KustomizeCtrl --\u003e|\"Apply\"| SS2\n    \n    SS1 --\u003e SSController\n    SS2 --\u003e SSController\n    \n    SSController --\u003e|\"Decrypt\"| Secret1\n    SSController --\u003e|\"Decrypt\"| Secret2\n    \n    Secret1 --\u003e Hub\n    Secret2 --\u003e Portal\n    \n    KustomizeCtrl --\u003e|\"Apply\"| Hub\n    KustomizeCtrl --\u003e|\"Apply\"| Portal\n```\n\n**Diagram: SealedSecrets in GitOps Workflow**\n\nFlux CD's kustomize-controller applies SealedSecret manifests from Git. The sealed-secrets-controller watches for these resources and automatically decrypts them into Secrets. Applications can then reference these Secrets in their HelmRelease configurations, completing the GitOps loop without exposing sensitive data.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-17](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:1-99](), [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17]()\n\n## SealedSecrets Controller Configuration\n\nThe sealed-secrets controller is deployed as part of the system infrastructure with the name `eoepca-sealed-secrets` in the `infra` namespace. All creation scripts reference this controller:\n\n```bash\nkubeseal -o yaml --controller-name eoepca-sealed-secrets --controller-namespace infra\n```\n\nThe controller configuration includes:\n- **Controller Name:** `eoepca-sealed-secrets`\n- **Namespace:** `infra`\n- **Scope:** Cluster-wide (manages SealedSecrets in all namespaces)\n- **Encryption:** RSA with AES-GCM for data encryption\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:33](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:35]()\n\n## Security Considerations\n\n### Encryption Scope\n\nEach SealedSecret field is encrypted independently, ensuring:\n- Individual credential rotation without re-encrypting entire secrets\n- Separate encryption contexts for each value\n- Audit trail of which specific credentials changed in Git history\n\n### Controller Access\n\nThe sealed-secrets controller's private key is the critical security component:\n- Only the controller can decrypt SealedSecrets\n- Private key is stored as a Kubernetes Secret in the `infra` namespace\n- Loss of private key prevents decryption of existing SealedSecrets\n- Key rotation requires re-encrypting all SealedSecrets\n\n### Namespace Scoping\n\nSealedSecrets in EOEPCA are namespace-scoped, meaning:\n- A SealedSecret in the `proc` namespace can only create Secrets in `proc`\n- A SealedSecret in the `demo` namespace can only create Secrets in `demo`\n- The controller enforces namespace boundaries during decryption\n\nThis prevents accidental or malicious cross-namespace Secret creation.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:6](), [system/clusters/creodias/system/demo/ss-django-secrets.yaml:6]()"])</script><script>self.__next_f.push([1,"3f:T486e,"])</script><script>self.__next_f.push([1,"# OIDC Client Configuration\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh)\n- [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml](system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml)\n- [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml](system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml)\n- [system/clusters/creodias/system/demo/hr-eoepca-portal.yaml](system/clusters/creodias/system/demo/hr-eoepca-portal.yaml)\n- [system/clusters/creodias/system/demo/ss-django-secrets-create.sh](system/clusters/creodias/system/demo/ss-django-secrets-create.sh)\n- [system/clusters/creodias/system/demo/ss-django-secrets.yaml](system/clusters/creodias/system/demo/ss-django-secrets.yaml)\n- [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml](system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml)\n- [system/clusters/creodias/user-management/um-identity-service.yaml](system/clusters/creodias/user-management/um-identity-service.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the configuration of OpenID Connect (OIDC) clients within the EOEPCA platform. OIDC clients are services that authenticate users through the Identity Service (Keycloak) using the OAuth2/OIDC protocol. This page covers client registration, credential management using SealedSecrets, OAuth2 endpoint configuration, and integration patterns for various EOEPCA services.\n\nFor information about the Identity Service (Keycloak) itself, see [Identity Service (Keycloak)](#4.1). For Policy Enforcement Point configuration, see [Policy Enforcement (PEP/PDP)](#4.3). For the complete UMA authentication flow, see [UMA Authentication Flow](#4.4).\n\n---\n\n## OIDC Client Registration Overview\n\nOIDC clients in EOEPCA must be registered with the Identity Service (Keycloak) before they can authenticate users. Each client receives unique credentials (`client_id` and `client_secret`) that are used during the OAuth2 authorization flow.\n\n### Client Types in EOEPCA\n\nThe platform supports several types of OIDC clients:\n\n| Client Type | Example | Authentication Pattern |\n|------------|---------|----------------------|\n| Interactive Web Applications | Application Hub (JupyterHub) | Authorization Code Flow with PKCE |\n| Web Portals | EOEPCA Portal (Django) | Authorization Code Flow |\n| Protected Services | Identity Dummy Service | Identity Gatekeeper (auth_request) |\n| API Services | Workspace API, ADES | Service Account / Client Credentials |\n\n### OIDC Provider Endpoints\n\nThe Identity Service exposes standard OIDC endpoints that clients must configure:\n\n```mermaid\ngraph LR\n    Client[\"OIDC Client\u003cbr/\u003e(Application Hub)\"]\n    \n    subgraph \"Identity Service (Keycloak)\"\n        AuthEP[\"Authorization Endpoint\u003cbr/\u003e/oxauth/restv1/authorize\"]\n        TokenEP[\"Token Endpoint\u003cbr/\u003e/oxauth/restv1/token\"]\n        UserInfoEP[\"UserInfo Endpoint\u003cbr/\u003e/oxauth/restv1/userinfo\"]\n        JWKSEP[\"JWKS Endpoint\u003cbr/\u003e/oxauth/restv1/jwks\"]\n    end\n    \n    Client --\u003e|\"1. Authorization Request\"| AuthEP\n    AuthEP --\u003e|\"2. Redirect with Code\"| Client\n    Client --\u003e|\"3. Token Exchange\"| TokenEP\n    TokenEP --\u003e|\"4. Access Token + ID Token\"| Client\n    Client --\u003e|\"5. Get User Info\"| UserInfoEP\n    UserInfoEP --\u003e|\"6. User Claims\"| Client\n    Client --\u003e|\"Verify Token Signature\"| JWKSEP\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:39-41]()\n\n---\n\n## Client Credential Management with SealedSecrets\n\nOIDC client credentials are sensitive values that must be encrypted at rest in the Git repository. EOEPCA uses the SealedSecrets pattern to achieve this.\n\n### SealedSecret Creation Workflow\n\n```mermaid\nsequenceDiagram\n    participant Admin as Administrator\n    participant Script as Secret Creation Script\n    participant Kubectl as kubectl\n    participant Kubeseal as kubeseal CLI\n    participant Git as Git Repository\n    participant SealedSecretCtrl as sealed-secrets-controller\n    participant K8s as Kubernetes Secret\n    \n    Admin-\u003e\u003eScript: Run with client credentials\n    Script-\u003e\u003eKubectl: Create Secret (dry-run)\n    Kubectl--\u003e\u003eScript: Secret YAML (not applied)\n    Script-\u003e\u003eKubeseal: Pipe YAML to kubeseal\n    Note over Kubeseal: Encrypts with cluster public key\n    Kubeseal--\u003e\u003eScript: SealedSecret YAML\n    Script-\u003e\u003eGit: Write encrypted YAML file\n    \n    Note over Git,K8s: GitOps Sync (Flux CD)\n    \n    Git-\u003e\u003eSealedSecretCtrl: Apply SealedSecret\n    SealedSecretCtrl-\u003e\u003eSealedSecretCtrl: Decrypt with private key\n    SealedSecretCtrl-\u003e\u003eK8s: Create plaintext Secret\n    K8s--\u003e\u003eAdmin: Secret available to pods\n```\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36]()\n\n### Secret Creation Script Pattern\n\nOIDC client secrets are generated using shell scripts that follow a consistent pattern:\n\n```bash\n# Example structure from application-hub-sealed-secrets-create.sh\nSECRET_NAME=\"application-hub-secrets\"\nNAMESPACE=\"proc\"\n\nsecretYaml() {\n  kubectl -n \"${NAMESPACE}\" create secret generic \"${SECRET_NAME}\" \\\n    --from-literal=\"OAUTH_CLIENT_ID=${OAUTH_CLIENT_ID}\" \\\n    --from-literal=\"OAUTH_CLIENT_SECRET=${OAUTH_CLIENT_SECRET}\" \\\n    --dry-run=client -o yaml\n}\n\nsecretYaml | kubeseal -o yaml \\\n  --controller-name eoepca-sealed-secrets \\\n  --controller-namespace infra \u003e sealed-secret.yaml\n```\n\nThe script creates a Kubernetes Secret in dry-run mode, pipes it to `kubeseal`, which encrypts it using the cluster's public key, and outputs a `SealedSecret` resource that can be committed to Git.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:17-35](), [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:13-33]()\n\n### SealedSecret Resource Structure\n\nThe resulting `SealedSecret` contains encrypted credential data:\n\n```yaml\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  name: application-hub-secrets\n  namespace: proc\nspec:\n  encryptedData:\n    OAUTH_CLIENT_ID: AgAx...base64-encrypted...\n    OAUTH_CLIENT_SECRET: AgBc...base64-encrypted...\n  template:\n    metadata:\n      name: application-hub-secrets\n      namespace: proc\n```\n\nThe `sealed-secrets-controller` running in the cluster decrypts these values and creates a standard Kubernetes Secret that pods can mount or reference.\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-18](), [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17]()\n\n---\n\n## OAuth2 Configuration Parameters\n\n### Standard OAuth2/OIDC Parameters\n\nOIDC clients must configure the following parameters to integrate with the Identity Service:\n\n| Parameter | Purpose | Example Value |\n|-----------|---------|---------------|\n| `OAUTH_CLIENT_ID` | Unique client identifier | `application-hub-client` |\n| `OAUTH_CLIENT_SECRET` | Client authentication secret | `(encrypted value)` |\n| `OAUTH2_AUTHORIZE_URL` | Authorization endpoint | `https://auth.develop.eoepca.org/oxauth/restv1/authorize` |\n| `OAUTH2_TOKEN_URL` | Token exchange endpoint | `https://auth.develop.eoepca.org/oxauth/restv1/token` |\n| `OAUTH2_USERDATA_URL` | UserInfo endpoint | `https://auth.develop.eoepca.org/oxauth/restv1/userinfo` |\n| `OAUTH_CALLBACK_URL` | Redirect URI after authentication | `https://applicationhub.develop.eoepca.org/hub/oauth_callback` |\n| `OAUTH_LOGOUT_REDIRECT_URL` | Redirect after logout | `https://applicationhub.develop.eoepca.org` |\n| `OAUTH2_USERNAME_KEY` | User identifier claim | `user_name` |\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:38-43]()\n\n### Identity Service Domain Configuration\n\nThe Identity Service operates across multiple subdomains, each serving a specific purpose:\n\n```mermaid\ngraph TB\n    subgraph \"Identity Service Domains\"\n        Keycloak[\"identity.keycloak.develop.eoepca.org\u003cbr/\u003eKeycloak Admin Console\"]\n        API[\"identity.api.develop.eoepca.org\u003cbr/\u003eIdentity API\"]\n        Manager[\"identity.manager.develop.eoepca.org\u003cbr/\u003eIdentity Manager UI\"]\n        Gatekeeper[\"identity.gatekeeper.develop.eoepca.org\u003cbr/\u003eAuth Proxy / Gatekeeper\"]\n        Auth[\"auth.develop.eoepca.org\u003cbr/\u003eOAuth2/OIDC Endpoints\"]\n    end\n    \n    Client[\"OIDC Client\"]\n    \n    Client --\u003e|\"OAuth2 Flow\"| Auth\n    Client --\u003e|\"User Management\"| API\n    Client -.-\u003e|\"Protected via\"| Gatekeeper\n    \n    Admin[\"Administrator\"]\n    Admin --\u003e|\"Configure Clients\"| Keycloak\n    Admin --\u003e|\"Manage Users\"| Manager\n```\n\n**Sources:** [system/clusters/creodias/user-management/um-identity-service.yaml:23-76]()\n\n---\n\n## Application Hub OIDC Configuration\n\nThe Application Hub (JupyterHub) demonstrates a complete OIDC client integration pattern.\n\n### Configuration Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Application Hub Deployment\"\n        HelmRelease[\"HelmRelease\u003cbr/\u003eapplication-hub\"]\n        \n        subgraph \"Configuration Sources\"\n            EnvVars[\"extraEnv\u003cbr/\u003eOAuth2 Endpoints\"]\n            Secret[\"SealedSecret\u003cbr/\u003eapplication-hub-secrets\"]\n        end\n        \n        subgraph \"JupyterHub Pod\"\n            Hub[\"JupyterHub Hub\u003cbr/\u003eOAuth2 Client\"]\n            DB[\"SQLite PVC\u003cbr/\u003emanaged-nfs-storage\"]\n        end\n        \n        subgraph \"Credentials\"\n            ClientID[\"OAUTH_CLIENT_ID\"]\n            ClientSecret[\"OAUTH_CLIENT_SECRET\"]\n            CryptKey[\"JUPYTERHUB_CRYPT_KEY\"]\n        end\n    end\n    \n    IdentityService[\"Identity Service\u003cbr/\u003eauth.develop.eoepca.org\"]\n    User[\"End User\"]\n    \n    HelmRelease --\u003e EnvVars\n    HelmRelease --\u003e Secret\n    Secret --\u003e ClientID\n    Secret --\u003e ClientSecret\n    Secret --\u003e CryptKey\n    \n    EnvVars --\u003e Hub\n    ClientID --\u003e Hub\n    ClientSecret --\u003e Hub\n    CryptKey --\u003e Hub\n    \n    Hub --\u003e DB\n    \n    User --\u003e|\"1. Access\"| Hub\n    Hub --\u003e|\"2. Redirect to Login\"| IdentityService\n    IdentityService --\u003e|\"3. Callback\"| Hub\n    Hub --\u003e|\"4. Spawn Notebook\"| User\n```\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:1-99]()\n\n### Hub Configuration Values\n\nThe Application Hub HelmRelease configures OAuth2 parameters as environment variables:\n\n[system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:35-63]()\n\nKey configuration elements:\n\n- **`OAUTH_CALLBACK_URL`**: Must match the redirect URI registered in Keycloak\n- **`OAUTH2_USERNAME_KEY`**: Specifies which claim from the ID token to use as the username (`user_name`)\n- **`JUPYTERHUB_CRYPT_KEY`**: Used to encrypt cookies and internal tokens (generated via `openssl rand -hex 32`)\n- **Credential References**: Client credentials are mounted from the `application-hub-secrets` Secret using `valueFrom.secretKeyRef`\n\n### Secret Management for Application Hub\n\nThe Application Hub creates its SealedSecret using a dedicated script:\n\n[system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:17-35]()\n\nThe script generates three secrets:\n- `JUPYTERHUB_CRYPT_KEY`: 32-byte hex encryption key\n- `OAUTH_CLIENT_ID`: Client identifier registered in Keycloak\n- `OAUTH_CLIENT_SECRET`: Client authentication secret\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:29-63](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36](), [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets.yaml:1-18]()\n\n---\n\n## Django-Based Service OIDC Configuration\n\nServices using Django (such as the EOEPCA Portal) follow a different configuration pattern, storing OIDC parameters with the `OIDC_RP_` prefix.\n\n### Django OIDC Parameter Mapping\n\n```mermaid\ngraph LR\n    subgraph \"Django Application\"\n        Settings[\"Django Settings\u003cbr/\u003emozilla-django-oidc\"]\n    end\n    \n    subgraph \"SealedSecret: django-secrets\"\n        RPID[\"OIDC_RP_CLIENT_ID\"]\n        RPSecret[\"OIDC_RP_CLIENT_SECRET\"]\n        DjangoSecret[\"DJANGO_SECRET\"]\n        KCID[\"KEYCLOAK_OIDC_RP_CLIENT_ID\"]\n        KCSecret[\"KEYCLOAK_OIDC_RP_CLIENT_SECRET\"]\n    end\n    \n    subgraph \"Identity Providers\"\n        Gluu[\"Login Service\u003cbr/\u003e(Gluu)\"]\n        Keycloak[\"Identity Service\u003cbr/\u003e(Keycloak)\"]\n    end\n    \n    RPID --\u003e Settings\n    RPSecret --\u003e Settings\n    DjangoSecret --\u003e Settings\n    \n    Settings --\u003e|\"Primary Auth\"| Gluu\n    \n    KCID --\u003e Settings\n    KCSecret --\u003e Settings\n    Settings --\u003e|\"Alternative Auth\"| Keycloak\n```\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17](), [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33]()\n\n### Django Secret Creation Script\n\nThe Django secret creation script handles multiple identity providers:\n\n[system/clusters/creodias/system/demo/ss-django-secrets-create.sh:16-33]()\n\nThe script creates five secret literals:\n- `OIDC_RP_CLIENT_ID`: Client ID for the primary identity provider (Gluu)\n- `OIDC_RP_CLIENT_SECRET`: Client secret for Gluu\n- `DJANGO_SECRET`: Django application secret key\n- `KEYCLOAK_OIDC_RP_CLIENT_ID`: Alternative client ID for Keycloak\n- `KEYCLOAK_OIDC_RP_CLIENT_SECRET`: Alternative client secret for Keycloak\n\nThis dual-provider configuration allows services to support both the Login Service (Gluu) and the Identity Service (Keycloak) simultaneously, facilitating migration and testing scenarios.\n\n**Sources:** [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33](), [system/clusters/creodias/system/demo/ss-django-secrets.yaml:1-17]()\n\n---\n\n## Identity Gatekeeper Integration Pattern\n\nSome services use Identity Gatekeeper as a reverse proxy for authentication rather than implementing OIDC directly.\n\n### Gatekeeper Authentication Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Ingress as \"Nginx Ingress\"\n    participant Gatekeeper as \"identity-gatekeeper\u003cbr/\u003e(um namespace)\"\n    participant Backend as \"dummy-service\u003cbr/\u003e(test namespace)\"\n    participant Identity as \"Identity Service\"\n    \n    User-\u003e\u003eIngress: GET /resource\n    Note over Ingress: identity.dummy-service.develop.eoepca.org\n    \n    Ingress-\u003e\u003eIngress: Execute auth_request\n    Ingress-\u003e\u003eGatekeeper: Internal auth request\n    Note over Gatekeeper: http://identity-gatekeeper.um.svc.cluster.local:3000\n    \n    alt No Valid Token\n        Gatekeeper--\u003e\u003eIngress: 401 Unauthorized\n        Ingress--\u003e\u003eUser: 401 + Redirect to Login\n        User-\u003e\u003eIdentity: Authenticate\n        Identity--\u003e\u003eUser: ID Token\n        User-\u003e\u003eIngress: Retry with Token\n    end\n    \n    Gatekeeper-\u003e\u003eIdentity: Validate Token\n    Identity--\u003e\u003eGatekeeper: Token Valid\n    Gatekeeper--\u003e\u003eIngress: 200 OK\n    \n    Ingress-\u003e\u003eBackend: Forward Request\n    Backend--\u003e\u003eIngress: Response\n    Ingress--\u003e\u003eUser: 200 OK + Data\n```\n\n**Sources:** [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:1-48]()\n\n### Nginx Ingress Configuration for Gatekeeper\n\nThe Ingress resource configures Nginx to perform authentication checks before forwarding requests:\n\n[system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:9-31]()\n\nKey configuration elements:\n\n- **`auth_request /auth`**: Nginx subrequest to authenticate before forwarding\n- **Internal `/auth` location**: Proxies to `identity-gatekeeper.um.svc.cluster.local:3000`\n- **`proxy_pass_request_body off`**: Only headers are sent to the gatekeeper for validation\n- **CORS headers**: Added for browser-based clients\n- **Proxy buffers**: Sized to handle large JWT tokens\n\nThis pattern enables authentication enforcement without modifying the backend service. The backend service (e.g., `dummy-service`) receives authenticated requests with user context in headers.\n\n**Sources:** [system/clusters/creodias/system/test/identity-dummy-service-ingress.yaml:1-48]()\n\n---\n\n## OIDC Endpoint Reference\n\n### Identity Service Endpoints\n\nThe following table provides the complete set of OIDC endpoints exposed by the Identity Service:\n\n| Endpoint Type | URL Path | Purpose |\n|--------------|----------|---------|\n| Authorization | `/oxauth/restv1/authorize` | Initiates OAuth2 authorization flow |\n| Token | `/oxauth/restv1/token` | Exchanges authorization code for tokens |\n| UserInfo | `/oxauth/restv1/userinfo` | Retrieves authenticated user claims |\n| JWKS | `/oxauth/restv1/jwks` | Public keys for token signature verification |\n| End Session | `/oxauth/restv1/end_session` | Logout endpoint |\n| Discovery | `/.well-known/openid-configuration` | OIDC metadata document |\n\n### Domain Configuration\n\nThe Identity Service is accessible through the `auth.develop.eoepca.org` domain for OAuth2/OIDC flows. Administrative interfaces use separate subdomains:\n\n- **Keycloak Admin**: `identity.keycloak.develop.eoepca.org`\n- **Identity API**: `identity.api.develop.eoepca.org`\n- **Identity Manager**: `identity.manager.develop.eoepca.org`\n- **Identity Gatekeeper**: `identity.gatekeeper.develop.eoepca.org`\n\nEach subdomain has dedicated ingress configuration with TLS certificates managed by cert-manager using the `letsencrypt` ClusterIssuer.\n\n**Sources:** [system/clusters/creodias/user-management/um-identity-service.yaml:23-76](), [system/clusters/creodias/processing-and-chaining/proc-application-hub.yaml:38-43]()\n\n---\n\n## Client Registration Checklist\n\nWhen registering a new OIDC client in the EOEPCA platform:\n\n1. **Register client in Keycloak**:\n   - Access Keycloak Admin Console at `identity.keycloak.develop.eoepca.org`\n   - Create new client with appropriate redirect URIs\n   - Note the generated `client_id` and `client_secret`\n\n2. **Create SealedSecret**:\n   - Use or adapt one of the secret creation scripts\n   - Generate encrypted SealedSecret YAML\n   - Commit to Git repository\n\n3. **Configure service deployment**:\n   - Set OAuth2 endpoint URLs (authorize, token, userinfo)\n   - Reference SealedSecret for credentials\n   - Configure callback URL matching registered URI\n   - Set username claim mapping\n\n4. **Validate integration**:\n   - Test authentication flow\n   - Verify token exchange\n   - Confirm user identity propagation\n\n**Sources:** [system/clusters/creodias/processing-and-chaining/application-hub-sealed-secrets-create.sh:1-36](), [system/clusters/creodias/system/demo/ss-django-secrets-create.sh:1-33]()"])</script><script>self.__next_f.push([1,"40:T6821,"])</script><script>self.__next_f.push([1,"# Operations and Maintenance\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml](system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system/flux-system-patch.yaml](system/clusters/creodias/system/flux-system/flux-system-patch.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-components.yaml](system/clusters/creodias/system/flux-system/gotk-components.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-sync.yaml](system/clusters/creodias/system/flux-system/gotk-sync.yaml)\n- [system/clusters/creodias/system/flux-system/kustomization.yaml](system/clusters/creodias/system/flux-system/kustomization.yaml)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides guidance for operational procedures required to manage and maintain a deployed EOEPCA system. It covers day-to-day administrative tasks including policy management, resource registration and deregistration, monitoring GitOps reconciliation, troubleshooting deployments, and data ingestion workflows.\n\nFor initial deployment procedures, see [Deployment Guide](#2.1). For configuration management including secrets and OIDC setup, see [Configuration and Secrets Management](#10). For detailed policy management procedures, see [Policy Management Tools](#11.1). For troubleshooting guidance, see [Monitoring and Troubleshooting](#11.2).\n\n## Operational Architecture\n\nThe EOEPCA system is deployed across multiple Kubernetes namespaces with GitOps-managed components. Understanding this architecture is essential for operational tasks.\n\n```mermaid\ngraph TB\n    subgraph \"Operator Tools\"\n        kubectl[\"kubectl CLI\"]\n        flux[\"flux CLI\"]\n        scripts[\"Operational Scripts\u003cbr/\u003ebin/dump-policy.sh\u003cbr/\u003ebin/unregister-resource.sh\"]\n    end\n    \n    subgraph \"flux-system namespace\"\n        source[\"source-controller\"]\n        kustomize[\"kustomize-controller\"]\n        helm[\"helm-controller\"]\n        notify[\"notification-controller\"]\n        gitrepo[\"GitRepository CR\u003cbr/\u003eflux-system\"]\n    end\n    \n    subgraph \"um namespace\"\n        identity[\"identity-service\"]\n        login[\"login-service\"]\n        pdp[\"pdp-engine\"]\n        profile[\"user-profile\"]\n    end\n    \n    subgraph \"rm namespace\"\n        workspace[\"workspace-api\"]\n        catalogue[\"resource-catalogue\"]\n        dataaccess[\"data-access\"]\n        redis[\"data-access-redis-master-0\"]\n        combinedpep[\"combined-rm-pep\"]\n        workspacepep[\"workspace-api-pep\"]\n    end\n    \n    subgraph \"proc namespace\"\n        ades[\"ades\"]\n        apphub[\"application-hub\"]\n        adespep[\"ades-pep\"]\n    end\n    \n    subgraph \"test namespace\"\n        dummy[\"dummy-service\"]\n        dummypep[\"dummy-service-pep\"]\n    end\n    \n    kubectl --\u003e|exec/logs| um\n    kubectl --\u003e|exec/logs| rm\n    kubectl --\u003e|exec/logs| proc\n    kubectl --\u003e|manage| flux-system\n    \n    scripts --\u003e|management_tools| pdp\n    scripts --\u003e|management_tools| combinedpep\n    scripts --\u003e|management_tools| workspacepep\n    scripts --\u003e|management_tools| adespep\n    scripts --\u003e|management_tools| dummypep\n    scripts --\u003e|redis-cli| redis\n    \n    flux --\u003e|get/logs| gitrepo\n    flux --\u003e|reconcile| kustomize\n    \n    source --\u003e|fetch| gitrepo\n    kustomize --\u003e|apply| um\n    kustomize --\u003e|apply| rm\n    kustomize --\u003e|apply| proc\n```\n\n**Operational Components Overview**\n\n| Component | Namespace | Purpose | Common Operations |\n|-----------|-----------|---------|-------------------|\n| `pdp-engine` | `um` | Policy Decision Point | Policy dumps, resource unregistration |\n| `combined-rm-pep` | `rm` | Resource Management PEP | Policy management, resource protection |\n| `workspace-api-pep` | `rm` | Workspace API PEP | Workspace access policies |\n| `ades-pep` | `proc` | ADES PEP | Processing service protection |\n| `data-access-redis-master-0` | `rm` | Registration queue | Data registration, queue inspection |\n| `source-controller` | `flux-system` | GitOps source sync | Repository synchronization |\n| `kustomize-controller` | `flux-system` | Manifest application | Reconciliation, drift detection |\n\nSources: [bin/dump-policy.sh:1-43](), [bin/unregister-resource.sh:1-55](), [system/clusters/creodias/system/flux-system/gotk-sync.yaml:1-28]()\n\n## Policy Management Operations\n\nPolicy management is a critical operational task for controlling access to protected resources. The EOEPCA system uses distributed Policy Enforcement Points (PEPs) that register resources with both local policy stores and a central Policy Decision Point (PDP).\n\n### Policy Enforcement Architecture\n\n```mermaid\ngraph LR\n    operator[\"Operator\"]\n    \n    subgraph \"Policy Components\"\n        pdpengine[\"pdp-engine\u003cbr/\u003eum namespace\"]\n        combinedpep[\"combined-rm-pep\u003cbr/\u003erm namespace\"]\n        workspacepep[\"workspace-api-pep\u003cbr/\u003erm namespace\"]\n        adespep[\"ades-pep\u003cbr/\u003eproc namespace\"]\n        dummypep[\"dummy-service-pep\u003cbr/\u003etest namespace\"]\n    end\n    \n    subgraph \"Scripts\"\n        dump[\"dump-policy.sh\"]\n        unreg[\"unregister-resource.sh\"]\n    end\n    \n    operator --\u003e|execute| dump\n    operator --\u003e|execute| unreg\n    \n    dump --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools list\"| pdpengine\n    dump --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools list\"| combinedpep\n    dump --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools list\"| workspacepep\n    dump --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools list\"| adespep\n    dump --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools list\"| dummypep\n    \n    unreg --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools remove\"| pdpengine\n    unreg --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools remove\"| combinedpep\n    unreg --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools remove\"| workspacepep\n    unreg --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools remove\"| adespep\n    unreg --\u003e|\"kubectl exec\u003cbr/\u003emanagement_tools remove\"| dummypep\n```\n\n### Dumping Policy State\n\nThe `dump-policy.sh` script extracts policy information from all PEP and PDP components. This is useful for auditing, backup, and troubleshooting access control issues.\n\n**Usage:**\n\n```bash\n# Dump all policies\n./bin/dump-policy.sh\n\n# Dump specific deployment\n./bin/dump-policy.sh \u003cnamespace\u003e \u003cdeployment\u003e\n```\n\n**Example:**\n\n```bash\n# Dump ADES PEP policies\n./bin/dump-policy.sh proc ades-pep\n\n# Dump PDP engine policies\n./bin/dump-policy.sh um pdp-engine nojson\n```\n\nThe script executes `management_tools list --all` within each PEP/PDP container and saves the output to JSON files in the current directory. The output files are named after the deployment (e.g., `ades-pep.json`, `pdp-engine.json`).\n\n**Script Implementation:**\n\nThe `dumpDeployment` function in [bin/dump-policy.sh:30-40]() constructs a `kubectl exec` command that invokes the `management_tools` utility inside the target container. For PEP components, the output is formatted as JSON using `jq`. The PDP engine dumps are output in plain text format.\n\n**Deployments Managed:**\n\n- `proc/ades-pep` - ADES service protection\n- `rm/combined-rm-pep` - Combined Resource Management services\n- `rm/workspace-api-pep` - Workspace API protection\n- `test/dummy-service-pep` - Test service protection\n- `um/pdp-engine` - Central policy decision point\n\nSources: [bin/dump-policy.sh:1-43]()\n\n### Unregistering Resources\n\nWhen resources need to be removed from the system, they must be unregistered from all PEP components and the central PDP. The `unregister-resource.sh` script automates this process.\n\n**Usage:**\n\n```bash\n./bin/unregister-resource.sh \u003cresource-id\u003e\n```\n\n**Example:**\n\n```bash\n./bin/unregister-resource.sh eric-workspace\n```\n\nThe script executes `management_tools remove -r \u003cresource-id\u003e` against each of the following components in sequence:\n\n1. `rm/combined-rm-pep` - Resource Management protection\n2. `proc/ades-pep` - Processing service protection  \n3. `rm/workspace-api-pep` - Workspace access control\n4. `test/dummy-service-pep` - Test environment cleanup\n5. `um/pdp-engine` - Central policy store\n\nThe commented-out lines in [bin/unregister-resource.sh:26-34]() show that the system previously used separate PEPs for `resource-catalogue-pep` and `data-access-pep`, but these have been consolidated into `combined-rm-pep`.\n\nSources: [bin/unregister-resource.sh:1-55]()\n\n## GitOps Operations\n\nThe EOEPCA system uses Flux CD for continuous delivery and configuration management. All system state is defined in Git and automatically reconciled to the cluster.\n\n### Flux CD Architecture\n\n```mermaid\ngraph TB\n    github[\"GitHub Repository\u003cbr/\u003eEOEPCA/eoepca\u003cbr/\u003ebranch: develop\"]\n    \n    subgraph \"flux-system namespace\"\n        gitrepo[\"GitRepository\u003cbr/\u003ename: flux-system\u003cbr/\u003einterval: 1m\"]\n        \n        source[\"source-controller\u003cbr/\u003edeployment\u003cbr/\u003ememory: 500Mi-2Gi\"]\n        kustomize[\"kustomize-controller\u003cbr/\u003edeployment\u003cbr/\u003ememory: 500Mi-2Gi\"]\n        helm[\"helm-controller\u003cbr/\u003edeployment\"]\n        notify[\"notification-controller\u003cbr/\u003edeployment\"]\n        \n        sysksync[\"Kustomization\u003cbr/\u003ename: flux-system\u003cbr/\u003epath: ./system/clusters/creodias/system\u003cbr/\u003einterval: 1m\"]\n    end\n    \n    subgraph \"Extended Kustomizations\"\n        umksync[\"Kustomization\u003cbr/\u003ename: user-management\u003cbr/\u003epath: ./system/clusters/creodias/user-management\u003cbr/\u003einterval: 1m\"]\n        rmksync[\"Kustomization\u003cbr/\u003ename: resource-management\u003cbr/\u003epath: ./system/clusters/creodias/resource-management\u003cbr/\u003einterval: 1m\"]\n        procksync[\"Kustomization\u003cbr/\u003ename: processing-and-chaining\u003cbr/\u003epath: ./system/clusters/creodias/processing-and-chaining\u003cbr/\u003einterval: 1m\"]\n    end\n    \n    github --\u003e|poll every 1m| source\n    source --\u003e|fetch/cache| gitrepo\n    \n    gitrepo --\u003e|sourceRef| sysksync\n    gitrepo --\u003e|sourceRef| umksync\n    gitrepo --\u003e|sourceRef| rmksync\n    gitrepo --\u003e|sourceRef| procksync\n    \n    kustomize --\u003e|reconcile| sysksync\n    kustomize --\u003e|reconcile| umksync\n    kustomize --\u003e|reconcile| rmksync\n    kustomize --\u003e|reconcile| procksync\n```\n\n### Reconciliation Intervals\n\nThe Flux CD system operates on defined intervals:\n\n| Resource Type | Interval | Description |\n|---------------|----------|-------------|\n| GitRepository sync | 1 minute | Fetch latest commit from `develop` branch |\n| System Kustomization | 1 minute | Apply core infrastructure changes |\n| Domain Kustomizations | 1 minute | Apply user-management, resource-management, processing changes |\n\nThe reconciliation interval is configured in [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:39]() and [system/clusters/creodias/system/flux-system/gotk-sync.yaml:9]().\n\n### Monitoring Reconciliation\n\n**Check GitRepository Status:**\n\n```bash\nflux get sources git -n flux-system\nkubectl get gitrepository -n flux-system flux-system -o yaml\n```\n\n**Check Kustomization Status:**\n\n```bash\nflux get kustomizations -A\nkubectl get kustomizations -n flux-system\n```\n\n**Check Recent Events:**\n\n```bash\nflux events\nkubectl get events -n flux-system --sort-by='.lastTimestamp'\n```\n\n**Trigger Manual Reconciliation:**\n\n```bash\n# Reconcile GitRepository\nflux reconcile source git flux-system -n flux-system\n\n# Reconcile Kustomization\nflux reconcile kustomization flux-system -n flux-system\nflux reconcile kustomization user-management -n flux-system\nflux reconcile kustomization resource-management -n flux-system\nflux reconcile kustomization processing-and-chaining -n flux-system\n```\n\n### Resource Memory Configuration\n\nThe Flux controllers have been tuned for the EOEPCA workload with resource limits specified in [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:1-40]():\n\n- `source-controller`: 500Mi request, 2Gi limit\n- `kustomize-controller`: 500Mi request, 2Gi limit\n\nThese values may need adjustment for larger deployments or when managing many HelmReleases.\n\nSources: [system/clusters/creodias/system/flux-system/gotk-sync.yaml:1-28](), [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:1-40](), [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml:1-13](), [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml:1-13](), [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml:1-13]()\n\n## Data Registration Operations\n\nData products must be registered in the Resource Catalogue to be discoverable and accessible. The EOEPCA system uses a Redis-based queue for asynchronous data registration.\n\n### Data Registration Workflow\n\n```mermaid\nsequenceDiagram\n    participant Operator\n    participant redis[\"data-access-redis-master-0\u003cbr/\u003erm namespace\"]\n    participant registrar[\"Registrar Service\u003cbr/\u003erm namespace\"]\n    participant catalogue[\"resource-catalogue\u003cbr/\u003epycsw database\"]\n    \n    Operator-\u003e\u003eredis: kubectl exec\u003cbr/\u003eredis-cli lpush register_queue\n    Note over redis: Queue: EODATA/path/to/product.SAFE\n    \n    loop Registration Worker\n        registrar-\u003e\u003eredis: Poll queue\n        redis--\u003e\u003eregistrar: Product path\n        registrar-\u003e\u003eregistrar: Parse metadata\n        registrar-\u003e\u003ecatalogue: Insert ISO 19115 record\n        catalogue--\u003e\u003eregistrar: Confirmation\n    end\n    \n    Note over catalogue: Product now discoverable\u003cbr/\u003evia CSW/OpenSearch\n```\n\n### Registering Sentinel-2 Data\n\nThe repository includes example scripts for bulk registration of Sentinel-2 products. These scripts push product paths to the Redis queue for processing.\n\n**Sentinel-2 L1C Registration:**\n\nThe script in [system/clusters/data/register-S2-L1C-data.sh:1-10]() demonstrates bulk registration:\n\n```bash\nkubectl -n rm exec --stdin --tty data-access-redis-master-0 -- \\\n  redis-cli lpush register_queue \\\n    EODATA/Sentinel-2/MSI/L1C/2020/09/30/S2A_MSIL1C_*.SAFE \\\n    EODATA/Sentinel-2/MSI/L1C/2020/09/27/S2A_MSIL1C_*.SAFE\n```\n\n**Sentinel-2 L2A Registration:**\n\nThe script in [system/clusters/data/register-S2-L2A-data.sh:1-10]() uses the same pattern:\n\n```bash\nkubectl -n rm exec --stdin --tty data-access-redis-master-0 -- \\\n  redis-cli lpush register_queue \\\n    EODATA/Sentinel-2/MSI/L2A/2020/09/02/S2B_MSIL2A_*.SAFE/ \\\n    EODATA/Sentinel-2/MSI/L2A/2020/09/03/S2A_MSIL2A_*.SAFE/\n```\n\n### Manual Data Registration\n\nTo register individual products:\n\n**Step 1: Connect to Redis pod**\n\n```bash\nkubectl -n rm exec -it data-access-redis-master-0 -- bash\n```\n\n**Step 2: Push to registration queue**\n\n```bash\nredis-cli lpush register_queue \"EODATA/path/to/product.SAFE\"\n```\n\n**Step 3: Verify queue length**\n\n```bash\nredis-cli llen register_queue\n```\n\n**Step 4: Check registration progress**\n\n```bash\n# Monitor registrar logs\nkubectl -n rm logs -f deployment/registrar-service\n\n# Verify in catalogue\nkubectl -n rm exec -it deployment/resource-catalogue -- \\\n  pycsw-admin.py -c get_repository_record -i \u003cproduct-id\u003e\n```\n\n### Queue Management\n\n**Check queue depth:**\n\n```bash\nkubectl -n rm exec data-access-redis-master-0 -- redis-cli llen register_queue\n```\n\n**Peek at queued items:**\n\n```bash\nkubectl -n rm exec data-access-redis-master-0 -- redis-cli lrange register_queue 0 10\n```\n\n**Clear the queue (caution):**\n\n```bash\nkubectl -n rm exec data-access-redis-master-0 -- redis-cli del register_queue\n```\n\nSources: [system/clusters/data/register-S2-L1C-data.sh:1-10](), [system/clusters/data/register-S2-L2A-data.sh:1-10]()\n\n## Container Image Management\n\nUnderstanding which container images are deployed is essential for troubleshooting and version management.\n\n### Listing Deployed Images\n\nThe `list-container-images.sh` script in [bin/list-container-images.sh:1-26]() provides a formatted list of all running container images:\n\n```bash\n./bin/list-container-images.sh\n```\n\nThis executes a `kubectl` command with JSONPath queries to extract container images from all pods across all namespaces. The output shows pod names with their associated images, sorted alphabetically.\n\n**Example output format:**\n\n```\nades-7d9f8c5b9-xkqzr:     eoepca/ades:latest, eoepca/pep:latest\ndata-access-5f6b7c8d9-pqrst:     eoepca/data-access:v1.2.0, eoepca/renderer:v1.1.0\n```\n\nThis is useful for:\n- Auditing deployed versions\n- Identifying images for security scanning\n- Verifying updates after Flux reconciliation\n- Troubleshooting version mismatches\n\nSources: [bin/list-container-images.sh:1-26]()\n\n## Namespace Organization\n\nThe EOEPCA system organizes components into functional namespaces:\n\n| Namespace | Purpose | Key Deployments | Configuration Path |\n|-----------|---------|-----------------|-------------------|\n| `flux-system` | GitOps controllers | source-controller, kustomize-controller, helm-controller | [system/clusters/creodias/system/flux-system]() |\n| `um` | User Management \u0026 Identity | identity-service, login-service, pdp-engine, user-profile | [system/clusters/creodias/user-management]() |\n| `rm` | Resource Management | workspace-api, resource-catalogue, data-access, bucket-operator | [system/clusters/creodias/resource-management]() |\n| `proc` | Processing \u0026 Chaining | ades, application-hub, pde-hub | [system/clusters/creodias/processing-and-chaining]() |\n| `test` | Testing \u0026 Validation | dummy-service, test fixtures | N/A |\n\nEach namespace has dedicated PEPs for access control and separate resource quotas. The namespace structure is reflected in the Kustomization resources that Flux manages.\n\nSources: [system/clusters/creodias/user-management/kustomization.yaml:1-14]()\n\n## Access Control Verification\n\nAfter modifying policies or registering new resources, verification is essential.\n\n### Testing Resource Access\n\nThe acceptance tests in [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:1-96]() demonstrate programmatic verification:\n\n**Catalogue Access Test:**\n\n```robot\nGet Csw Operations\n  ${operations}=  Get Operations\n  ${cnt}=    Get length    ${operations}\n  should be equal as numbers  ${cnt}  6\n```\n\n**Record Query Test:**\n\n```robot\nGet Csw Records Filtered\n  ${results}=  Get Results  10\n  ${returned}=  Get From Dictionary  ${results}  returned\n  should be equal as numbers  ${returned}  10\n```\n\n### Manual Access Verification\n\n**Test catalogue endpoint:**\n\n```bash\ncurl -X GET \"https://resource-catalogue.${DOMAIN}/csw?service=CSW\u0026version=3.0.0\u0026request=GetCapabilities\"\n```\n\n**Test with authentication:**\n\n```bash\n# Get token\nTOKEN=$(kubectl -n um exec deploy/identity-service -- ...)\n\n# Access protected resource\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n  \"https://workspace-api.${DOMAIN}/workspaces\"\n```\n\nSources: [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot:1-96](), [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py:1-156]()\n\n## Common Operational Tasks\n\n### Restarting Services\n\n```bash\n# Restart specific deployment\nkubectl -n rm rollout restart deployment/resource-catalogue\n\n# Restart all deployments in namespace\nkubectl -n proc rollout restart deployment\n\n# Check rollout status\nkubectl -n rm rollout status deployment/resource-catalogue\n```\n\n### Viewing Logs\n\n```bash\n# Stream logs from deployment\nkubectl -n proc logs -f deployment/ades\n\n# Logs from specific container in multi-container pod\nkubectl -n proc logs deployment/ades -c ades-pep\n\n# Previous container logs (after crash)\nkubectl -n proc logs deployment/ades --previous\n\n# All pods in namespace\nkubectl -n rm logs -l app=data-access --tail=100\n```\n\n### Scaling Services\n\n```bash\n# Scale deployment\nkubectl -n rm scale deployment/data-access-renderer --replicas=4\n\n# Scale statefulset\nkubectl -n rm scale statefulset/data-access-redis-master --replicas=1\n\n# Check autoscaling\nkubectl -n proc get hpa\n```\n\n### Port Forwarding for Debug Access\n\n```bash\n# Forward PostgreSQL port\nkubectl -n um port-forward svc/identity-postgres 5432:5432\n\n# Forward Redis port\nkubectl -n rm port-forward svc/data-access-redis-master 6379:6379\n\n# Forward application port\nkubectl -n proc port-forward svc/ades 8080:80\n```\n\n### Executing Commands in Pods\n\n```bash\n# Interactive shell\nkubectl -n rm exec -it deployment/resource-catalogue -- bash\n\n# Single command\nkubectl -n um exec deployment/identity-service -- id\n\n# With specific container\nkubectl -n proc exec deployment/ades -c ades-pep -- management_tools list\n```\n\n## SealedSecrets Operations\n\nEOEPCA uses SealedSecrets for storing encrypted credentials in Git. When secrets need to be rotated or updated, they must be sealed using the cluster's public key.\n\n**Sealing a new secret:**\n\n```bash\n# Create secret locally\nkubectl create secret generic my-secret \\\n  --from-literal=username=admin \\\n  --from-literal=password=secret \\\n  --dry-run=client -o yaml \u003e /tmp/secret.yaml\n\n# Seal it\nkubeseal -f /tmp/secret.yaml -w /tmp/sealed-secret.yaml \\\n  --controller-namespace=flux-system \\\n  --controller-name=sealed-secrets\n\n# Commit sealed version to Git\ncp /tmp/sealed-secret.yaml system/clusters/creodias/user-management/\ngit add system/clusters/creodias/user-management/sealed-secret.yaml\ngit commit -m \"Add sealed secret\"\ngit push\n```\n\nThe SealedSecret controller automatically decrypts these resources when Flux applies them to the cluster. The plaintext secrets are never stored in Git.\n\nFor OIDC client credentials and other sensitive configuration, see [OIDC Client Configuration](#10.2).\n\n## Workspace Operations\n\nUser workspaces are dynamically provisioned with isolated resources. Operators may need to inspect or manage these.\n\n### Listing Workspaces\n\n```bash\n# List workspace namespaces\nkubectl get namespaces -l eoepca.org/workspace=true\n\n# List all workspace resources\nkubectl get all -n eric-workspace\n```\n\n### Inspecting Workspace Resources\n\n```bash\n# Check workspace catalogue\nkubectl -n eric-workspace get deployment resource-catalogue\n\n# Check workspace data access\nkubectl -n eric-workspace get deployment data-access\n\n# Check workspace S3 bucket\nkubectl -n rm exec deployment/bucket-operator -- \\\n  mc ls minio/eric-workspace\n```\n\n### Cleaning Up Workspaces\n\nWorkspace deletion is handled by the Workspace API, which triggers cascading deletion of namespace resources, S3 buckets, and PEP registrations.\n\n```bash\n# Trigger workspace deletion via API\ncurl -X DELETE \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  \"https://workspace-api.${DOMAIN}/workspaces/eric-workspace\"\n```\n\n## Flux CD Maintenance\n\n### Upgrading Flux Components\n\nFlux controllers are upgraded by updating the manifest in [system/clusters/creodias/system/flux-system/gotk-components.yaml:1-4]() which is generated by the `flux install` command. The version comment at the top specifies the Flux version (currently v0.24.0).\n\nTo upgrade:\n\n```bash\n# Generate new manifests\nflux install --export \u003e /tmp/gotk-components.yaml\n\n# Review differences\ndiff system/clusters/creodias/system/flux-system/gotk-components.yaml /tmp/gotk-components.yaml\n\n# Update file and commit\ncp /tmp/gotk-components.yaml system/clusters/creodias/system/flux-system/\ngit add system/clusters/creodias/system/flux-system/gotk-components.yaml\ngit commit -m \"Upgrade Flux to vX.Y.Z\"\ngit push\n```\n\nFlux will self-reconcile and apply the updates to its own controllers.\n\n### Suspending Reconciliation\n\nTo prevent Flux from reconciling while performing maintenance:\n\n```bash\nflux suspend kustomization user-management -n flux-system\nflux suspend kustomization resource-management -n flux-system\nflux suspend kustomization processing-and-chaining -n flux-system\n```\n\nResume reconciliation:\n\n```bash\nflux resume kustomization user-management -n flux-system\nflux resume kustomization resource-management -n flux-system\nflux resume kustomization processing-and-chaining -n flux-system\n```\n\nSources: [system/clusters/creodias/system/flux-system/gotk-components.yaml:1-4]()\n\n## Best Practices\n\n### Git Workflow\n\n1. **Never apply changes directly with kubectl** - All configuration should be committed to Git\n2. **Use feature branches** - Test changes in a branch before merging to `develop`\n3. **Monitor Flux after merging** - Verify reconciliation completes successfully\n4. **Tag releases** - Use Git tags for major version deployments\n\n### Policy Management\n\n1. **Dump policies before major changes** - Use `dump-policy.sh` for backup\n2. **Test policies in isolated resources** - Use `test` namespace for validation\n3. **Document resource ownership** - Maintain a registry of protected resources\n4. **Audit policy state periodically** - Review dumped policies for inconsistencies\n\n### Data Registration\n\n1. **Batch registrations** - Use Redis queue for bulk operations\n2. **Monitor queue depth** - Large queues indicate registrar capacity issues\n3. **Validate metadata** - Check catalogue records after registration\n4. **Handle failures gracefully** - Implement retry logic for failed registrations\n\n### Monitoring\n\n1. **Set up alerts** - Use Flux notification controller for deployment failures\n2. **Track resource utilization** - Monitor memory and CPU of Flux controllers\n3. **Log aggregation** - Centralize logs from all namespaces\n4. **Regular health checks** - Automate validation of critical services\n\nFor detailed troubleshooting procedures, see [Monitoring and Troubleshooting](#11.2)."])</script><script>self.__next_f.push([1,"41:T4cd0,"])</script><script>self.__next_f.push([1,"# Policy Management Tools\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitattributes](.gitattributes)\n- [bin/dump-policy.sh](bin/dump-policy.sh)\n- [bin/list-container-images.sh](bin/list-container-images.sh)\n- [bin/unregister-resource.sh](bin/unregister-resource.sh)\n- [system/clusters/creodias/user-management/kustomization.yaml](system/clusters/creodias/user-management/kustomization.yaml)\n- [system/clusters/data/register-S2-L1C-data.sh](system/clusters/data/register-S2-L1C-data.sh)\n- [system/clusters/data/register-S2-L2A-data.sh](system/clusters/data/register-S2-L2A-data.sh)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-atom.json)\n- [test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json](test/acceptance/02__Processing/01__ADES/data/app-deploy-body-cwl.json)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml](test/acceptance/02__Processing/01__ADES/data/application-package-atom.xml)\n- [test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl](test/acceptance/02__Processing/01__ADES/data/application-package-cwl.cwl)\n- [test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py](test/acceptance/03__ResourceCatalogue/CatalogueServiceWeb.py)\n- [test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot](test/acceptance/03__ResourceCatalogue/Resource_catalogue.robot)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the operational scripts provided for managing authorization policies within the EOEPCA platform. Specifically, it covers the `dump-policy.sh` and `unregister-resource.sh` utilities that operators use to inspect and manage resource registrations across Policy Enforcement Points (PEPs) and the Policy Decision Point (PDP).\n\nFor information about how policies are enforced at runtime and the UMA authentication flow, see [Policy Enforcement (PEP/PDP)](#4.3) and [UMA Authentication Flow](#4.4). For details on resource registration during workspace provisioning, see [Multi-Tenant Workspaces](#5.5).\n\n---\n\n## Overview of Policy Management\n\nThe EOEPCA platform protects resources using a distributed authorization architecture where multiple PEP components enforce access control decisions provided by a central PDP. Resources (such as workspaces, processing jobs, or catalogue entries) must be registered with these components to be protected. The policy management tools provide operators with the ability to:\n\n- Inspect policy state across all PEP/PDP components\n- Remove stale or problematic resource registrations\n- Debug authorization issues in deployed environments\n\nAll policy management operations execute via `kubectl exec` commands that invoke the `management_tools` CLI within running containers.\n\n**Sources:** [bin/dump-policy.sh:1-43](), [bin/unregister-resource.sh:1-55]()\n\n---\n\n## Architecture Context\n\nThe following diagram illustrates where policy management tools interact with the deployed system components:\n\n```mermaid\ngraph TB\n    subgraph \"Operator Workstation\"\n        Operator[\"Operator\"]\n        DumpScript[\"dump-policy.sh\"]\n        UnregScript[\"unregister-resource.sh\"]\n    end\n    \n    subgraph \"Kubernetes Cluster\"\n        subgraph \"um namespace\"\n            PDPEngine[\"pdp-engine\u003cbr/\u003edeployment\"]\n            PDPContainer[\"pdp-engine\u003cbr/\u003econtainer\"]\n            PDPEngine --\u003e PDPContainer\n        end\n        \n        subgraph \"rm namespace\"\n            CombinedPEP[\"combined-rm-pep\u003cbr/\u003esvc\"]\n            CombinedPEPContainer[\"combined-rm-pep\u003cbr/\u003econtainer\"]\n            CombinedPEP --\u003e CombinedPEPContainer\n            \n            WSAPI[\"workspace-api-pep\u003cbr/\u003esvc\"]\n            WSAPIContainer[\"workspace-api-pep\u003cbr/\u003econtainer\"]\n            WSAPI --\u003e WSAPIContainer\n        end\n        \n        subgraph \"proc namespace\"\n            ADESPEP[\"ades-pep\u003cbr/\u003esvc\"]\n            ADESPEPContainer[\"ades-pep\u003cbr/\u003econtainer\"]\n            ADESPEP --\u003e ADESPEPContainer\n        end\n        \n        subgraph \"test namespace\"\n            DummyPEP[\"dummy-service-pep\u003cbr/\u003esvc\"]\n            DummyPEPContainer[\"dummy-service-pep\u003cbr/\u003econtainer\"]\n            DummyPEP --\u003e DummyPEPContainer\n        end\n    end\n    \n    subgraph \"Management Tools Interface\"\n        MTList[\"management_tools list --all\"]\n        MTRemove[\"management_tools remove -r\"]\n    end\n    \n    Operator --\u003e DumpScript\n    Operator --\u003e UnregScript\n    \n    DumpScript --\u003e|kubectl exec| PDPContainer\n    DumpScript --\u003e|kubectl exec| CombinedPEPContainer\n    DumpScript --\u003e|kubectl exec| WSAPIContainer\n    DumpScript --\u003e|kubectl exec| ADESPEPContainer\n    DumpScript --\u003e|kubectl exec| DummyPEPContainer\n    \n    UnregScript --\u003e|kubectl exec| PDPContainer\n    UnregScript --\u003e|kubectl exec| CombinedPEPContainer\n    UnregScript --\u003e|kubectl exec| WSAPIContainer\n    UnregScript --\u003e|kubectl exec| ADESPEPContainer\n    UnregScript --\u003e|kubectl exec| DummyPEPContainer\n    \n    PDPContainer --\u003e MTList\n    CombinedPEPContainer --\u003e MTList\n    WSAPIContainer --\u003e MTList\n    ADESPEPContainer --\u003e MTList\n    DummyPEPContainer --\u003e MTList\n    \n    PDPContainer --\u003e MTRemove\n    CombinedPEPContainer --\u003e MTRemove\n    WSAPIContainer --\u003e MTRemove\n    ADESPEPContainer --\u003e MTRemove\n    DummyPEPContainer --\u003e MTRemove\n```\n\n**Policy Component Distribution**\n\nThe policy management tools target specific deployments across namespaces:\n\n| Namespace | Service/Deployment | Container Name | Purpose |\n|-----------|-------------------|----------------|---------|\n| `um` | `pdp-engine` | `pdp-engine` | Central policy decision point |\n| `rm` | `combined-rm-pep` | `combined-rm-pep` | Combined PEP for resource management services |\n| `rm` | `workspace-api-pep` | `workspace-api-pep` | PEP protecting workspace API |\n| `proc` | `ades-pep` | `ades-pep` | PEP protecting ADES processing service |\n| `test` | `dummy-service-pep` | `dummy-service-pep` | PEP for testing and validation |\n\n**Sources:** [bin/dump-policy.sh:20-28](), [bin/unregister-resource.sh:21-54]()\n\n---\n\n## The dump-policy.sh Script\n\n### Purpose\n\nThe `dump-policy.sh` script exports the current policy state from PEP and PDP components to JSON files for inspection, debugging, or backup purposes. It can dump either all components or a specific deployment.\n\n**Sources:** [bin/dump-policy.sh:1-43]()\n\n### Usage\n\n```bash\n# Dump all policies from all components\n./bin/dump-policy.sh\n\n# Dump policy from a specific deployment\n./bin/dump-policy.sh \u003cnamespace\u003e \u003cdeployment\u003e [nojson]\n```\n\n### Script Flow\n\n```mermaid\nflowchart TD\n    Start[\"dump-policy.sh invoked\"]\n    CheckArgs{\"Arguments\u003cbr/\u003eprovided?\"}\n    DumpAll[\"dumpAll()\"]\n    DumpDeploy[\"dumpDeployment(args)\"]\n    \n    Start --\u003e CheckArgs\n    CheckArgs --\u003e|No args| DumpAll\n    CheckArgs --\u003e|Args provided| DumpDeploy\n    \n    DumpAll --\u003e ProcADES[\"Dump proc/ades-pep\"]\n    ProcADES --\u003e RMCombined[\"Dump rm/combined-rm-pep\"]\n    RMCombined --\u003e RMWorkspace[\"Dump rm/workspace-api-pep\"]\n    RMWorkspace --\u003e TestDummy[\"Dump test/dummy-service-pep\"]\n    TestDummy --\u003e UMPDP[\"Dump um/pdp-engine\"]\n    \n    DumpDeploy --\u003e ExecKubectl[\"kubectl exec into container\"]\n    ExecKubectl --\u003e RunMgmtTools[\"management_tools list --all\"]\n    RunMgmtTools --\u003e OutputJSON[\"Write to \u003cdeployment\u003e.json\"]\n    \n    UMPDP --\u003e End[\"Exit\"]\n    OutputJSON --\u003e End\n```\n\n### Implementation Details\n\nThe script defines a `dumpDeployment` function that constructs and executes `kubectl` commands:\n\n```bash\ndumpDeployment() {\n  namespace=\"${1}\"\n  deployment=\"${2}\"\n  print_json=\"$( [ \"${3}\" = \"nojson\" ] \u0026\u0026 echo \"\" || echo \"| jq\" )\"\n  dumpfile=\"${ORIG_DIR}/${deployment}.json\"\n\n  echo -n \"Dumping policy for ${namespace}/${deployment} to ${dumpfile}\"\n  cmd=\"kubectl -n \"${namespace}\" exec -it deploy/\"${deployment}\" -c \"${deployment}\" -- management_tools list --all ${print_json}\"\n  eval \"${cmd}\" \u003e \"${dumpfile}\"\n  echo \" done\"\n}\n```\n\n**Key Parameters:**\n- `namespace`: Kubernetes namespace containing the target deployment\n- `deployment`: Name of the deployment/service to query\n- `nojson`: Optional flag to skip JSON formatting (used for `pdp-engine`)\n\n**Sources:** [bin/dump-policy.sh:30-40]()\n\n### Default Targets\n\nWhen invoked without arguments, the script dumps policies from these components:\n\n```bash\ndumpAll() {\n  dumpDeployment proc ades-pep\n  dumpDeployment rm combined-rm-pep\n  dumpDeployment rm workspace-api-pep\n  dumpDeployment test dummy-service-pep\n  dumpDeployment um pdp-engine nojson\n}\n```\n\nNote that `pdp-engine` uses the `nojson` flag because its output format differs from the PEP components.\n\n**Sources:** [bin/dump-policy.sh:20-28]()\n\n### Output Files\n\nEach invocation produces JSON files named after the deployment:\n\n- `ades-pep.json`\n- `combined-rm-pep.json`\n- `workspace-api-pep.json`\n- `dummy-service-pep.json`\n- `pdp-engine.json`\n\n---\n\n## The unregister-resource.sh Script\n\n### Purpose\n\nThe `unregister-resource.sh` script removes a resource registration from all PEP and PDP components. This is useful for cleaning up stale resources, resolving authorization conflicts, or removing resources that were registered incorrectly.\n\n**Sources:** [bin/unregister-resource.sh:1-55]()\n\n### Usage\n\n```bash\n# Remove a resource by ID\n./bin/unregister-resource.sh \u003cresource-id\u003e\n```\n\nThe script requires exactly one argument: the resource ID to unregister. If invoked without arguments, it displays usage information:\n\n```bash\nif test $# -lt 1; then\n  appname=\"$(basename \"$0\")\"\n  cat - \u003c\u003cEOF\nERROR - resource id not specified\nUsage:\n  ${appname} \u003cresource-id\u003e\nEOF\n  exit 1\nfi\n```\n\n**Sources:** [bin/unregister-resource.sh:9-17]()\n\n### Unregistration Flow\n\n```mermaid\nflowchart TD\n    Start[\"unregister-resource.sh \u003cresource-id\u003e\"]\n    ValidateArgs{\"Resource ID\u003cbr/\u003eprovided?\"}\n    ShowUsage[\"Display usage and exit\"]\n    \n    Start --\u003e ValidateArgs\n    ValidateArgs --\u003e|No| ShowUsage\n    ValidateArgs --\u003e|Yes| Step1\n    \n    Step1[\"Remove from rm/combined-rm-pep\"]\n    Step2[\"Remove from proc/ades-pep\"]\n    Step3[\"Remove from rm/workspace-api-pep\"]\n    Step4[\"Remove from test/dummy-service-pep\"]\n    Step5[\"Remove from um/pdp-engine\"]\n    \n    Step1 --\u003e Step2\n    Step2 --\u003e Step3\n    Step3 --\u003e Step4\n    Step4 --\u003e Step5\n    Step5 --\u003e Complete[\"All removals complete\"]\n```\n\n### Component-by-Component Removal\n\nThe script systematically removes the resource from each component using `kubectl exec` to invoke `management_tools remove`:\n\n**1. Combined Resource Management PEP:**\n```bash\necho -n \"Delete resource ${resourceId} from combined-rm-pep...\"\nkubectl -n rm exec -it svc/combined-rm-pep -c combined-rm-pep -- management_tools remove -r ${resourceId}\necho \" done\"\n```\n\n**2. ADES PEP:**\n```bash\necho -n \"Delete resource ${resourceId} from ades-pep...\"\nkubectl -n proc exec -it svc/ades-pep -c ades-pep -- management_tools remove -r ${resourceId}\necho \" done\"\n```\n\n**3. Workspace API PEP:**\n```bash\necho -n \"Delete resource ${resourceId} from workspace-api-pep...\"\nkubectl -n rm exec -it svc/workspace-api-pep -c workspace-api-pep -- management_tools remove -r ${resourceId}\necho \" done\"\n```\n\n**4. Dummy Service PEP (Test):**\n```bash\necho -n \"Delete resource ${resourceId} from dummy-service-pep...\"\nkubectl -n test exec -it svc/dummy-service-pep -c dummy-service-pep -- management_tools remove -r ${resourceId}\necho \" done\"\n```\n\n**5. Policy Decision Point:**\n```bash\necho -n \"Delete resource ${resourceId} from pdp...\"\nkubectl -n um exec -it svc/pdp-engine -c pdp-engine -- management_tools remove -r ${resourceId}\necho \" done\"\n```\n\n**Sources:** [bin/unregister-resource.sh:21-54]()\n\n### Command Pattern\n\nEach removal command follows this pattern:\n\n```\nkubectl -n \u003cnamespace\u003e exec -it svc/\u003cservice\u003e -c \u003ccontainer\u003e -- management_tools remove -r \u003cresource-id\u003e\n```\n\nThe `-it` flags enable interactive terminal mode, and the `--` separator indicates the start of the command to execute inside the container.\n\n---\n\n## Common Operations\n\n### Inspecting Policy State\n\n**List all registered resources:**\n```bash\n./bin/dump-policy.sh\n```\n\nThis creates JSON files for each component. Examine these files to see registered resources, their owners, and associated policies.\n\n**Inspect specific component:**\n```bash\n./bin/dump-policy.sh rm combined-rm-pep\n```\n\n### Cleaning Up Resources\n\n**Remove a workspace resource:**\n```bash\n# Example: Remove eric's workspace registration\n./bin/unregister-resource.sh eric-workspace\n```\n\n**Remove a processing job resource:**\n```bash\n# Example: Remove a specific ADES job\n./bin/unregister-resource.sh ades-job-12345\n```\n\n### Debugging Authorization Issues\n\nWhen users report \"403 Forbidden\" errors, follow this workflow:\n\n```mermaid\nflowchart TD\n    Issue[\"User reports 403 Forbidden\"]\n    DumpPolicies[\"Run dump-policy.sh\"]\n    ExamineJSON[\"Examine relevant .json file\"]\n    \n    Issue --\u003e DumpPolicies\n    DumpPolicies --\u003e ExamineJSON\n    \n    CheckResource{\"Resource\u003cbr/\u003eregistered?\"}\n    ExamineJSON --\u003e CheckResource\n    \n    CheckResource --\u003e|Not registered| RegisterIssue[\"Resource not registered\u003cbr/\u003eCheck registration process\"]\n    CheckResource --\u003e|Registered| CheckOwner{\"Correct\u003cbr/\u003eowner?\"}\n    \n    CheckOwner --\u003e|Wrong owner| UnregisterReregister[\"Unregister and re-register\u003cbr/\u003ewith correct owner\"]\n    CheckOwner --\u003e|Correct owner| CheckPolicy{\"Policy\u003cbr/\u003ecorrect?\"}\n    \n    CheckPolicy --\u003e|Incorrect| UpdatePolicy[\"Update policy configuration\"]\n    CheckPolicy --\u003e|Correct| CheckToken[\"Verify user token\u003cbr/\u003eand claims\"]\n```\n\n### Batch Operations\n\nTo remove multiple resources, create a wrapper script:\n\n```bash\n#!/bin/bash\n# Example: cleanup-old-workspaces.sh\nfor resource_id in workspace-1 workspace-2 workspace-3; do\n  ./bin/unregister-resource.sh \"${resource_id}\"\ndone\n```\n\n---\n\n## Management Tools CLI\n\nBoth scripts invoke the `management_tools` command-line interface that is embedded in PEP and PDP containers. This CLI provides the following operations:\n\n### Command Reference\n\n| Command | Description | Used By |\n|---------|-------------|---------|\n| `management_tools list --all` | List all registered resources and policies | `dump-policy.sh` |\n| `management_tools remove -r \u003cid\u003e` | Remove a resource registration | `unregister-resource.sh` |\n| `management_tools register ...` | Register a new resource (not exposed in scripts) | Internal services |\n\nThe `management_tools` binary is part of the PEP/PDP container images and is not directly accessible outside the cluster without using `kubectl exec`.\n\n**Sources:** [bin/dump-policy.sh:37](), [bin/unregister-resource.sh:23-53]()\n\n---\n\n## Integration with User Management\n\nThe policy management tools interact with the broader User Management subsystem deployed in the `um` namespace:\n\n```mermaid\ngraph LR\n    subgraph \"Policy Management Tools\"\n        DumpScript[\"dump-policy.sh\"]\n        UnregScript[\"unregister-resource.sh\"]\n    end\n    \n    subgraph \"User Management Namespace (um)\"\n        PDPEngine[\"pdp-engine\u003cbr/\u003eHelmRelease\"]\n        IdentityService[\"identity-service\u003cbr/\u003eHelmRelease\"]\n        LoginService[\"login-service\u003cbr/\u003eHelmRelease\"]\n        UserProfile[\"user-profile\u003cbr/\u003eHelmRelease\"]\n    end\n    \n    DumpScript --\u003e|\"Query policies\"| PDPEngine\n    UnregScript --\u003e|\"Remove resources\"| PDPEngine\n    \n    PDPEngine -.-\u003e|\"Validate tokens\"| IdentityService\n    PDPEngine -.-\u003e|\"User lookup\"| UserProfile\n    IdentityService -.-\u003e|\"Authentication\"| LoginService\n```\n\nThe `um` namespace components are defined in the system kustomization:\n\n**Sources:** [system/clusters/creodias/user-management/kustomization.yaml:1-14]()\n\n### Namespace Resources\n\nThe User Management namespace contains:\n\n- `identity-service`: Keycloak-based identity provider\n- `login-service`: Gluu-based login interface\n- `pdp-engine`: Policy Decision Point for authorization\n- `user-profile`: User profile management service\n\nPolicy registrations in the PDP are validated against user identities stored in the Identity Service.\n\n---\n\n## Prerequisites and Access\n\n### Required Permissions\n\nOperators must have the following permissions to use these tools:\n\n1. **Kubernetes Access:** `kubectl exec` privilege on target namespaces (`um`, `rm`, `proc`, `test`)\n2. **Service Account:** Sufficient RBAC permissions to access container shells\n3. **Network Access:** Connectivity to the Kubernetes API server\n\n### Verifying Access\n\nTest access to a PEP component:\n\n```bash\nkubectl -n rm exec -it svc/combined-rm-pep -c combined-rm-pep -- management_tools list --all\n```\n\nIf successful, this command returns JSON-formatted policy data.\n\n### Troubleshooting Access Issues\n\n**Error: \"unable to upgrade connection\"**\n- Check if the pod is running: `kubectl -n rm get pods`\n- Verify service exists: `kubectl -n rm get svc`\n\n**Error: \"command not found: management_tools\"**\n- Incorrect container name specified\n- Container does not include management tools binary\n\n**Error: \"Forbidden\"**\n- Insufficient RBAC permissions\n- Check your Kubernetes context and credentials\n\n---\n\n## Best Practices\n\n### Before Removing Resources\n\n1. **Dump current state:** Always run `dump-policy.sh` before removal operations to preserve the current state\n2. **Verify resource ID:** Confirm the resource ID matches the intended resource\n3. **Check dependencies:** Ensure no active operations depend on the resource\n\n### After Removing Resources\n\n1. **Verify removal:** Re-run `dump-policy.sh` and confirm the resource is absent\n2. **Test access:** Verify that authorized users can still access their resources\n3. **Monitor logs:** Check PEP/PDP logs for authorization errors\n\n### Operational Workflow\n\n```mermaid\nsequenceDiagram\n    participant Operator\n    participant DumpScript as dump-policy.sh\n    participant UnregScript as unregister-resource.sh\n    participant PEP as PEP Components\n    participant PDP as pdp-engine\n    \n    Note over Operator: Issue reported\n    \n    Operator-\u003e\u003eDumpScript: Run dump-policy.sh\n    DumpScript-\u003e\u003ePEP: Query all PEPs\n    DumpScript-\u003e\u003ePDP: Query PDP\n    PEP--\u003e\u003eDumpScript: Policy data\n    PDP--\u003e\u003eDumpScript: Policy data\n    DumpScript--\u003e\u003eOperator: JSON files\n    \n    Note over Operator: Analyze policies\n    \n    Operator-\u003e\u003eUnregScript: Remove resource\n    UnregScript-\u003e\u003ePEP: Remove from PEPs\n    UnregScript-\u003e\u003ePDP: Remove from PDP\n    PEP--\u003e\u003eUnregScript: Confirmation\n    PDP--\u003e\u003eUnregScript: Confirmation\n    UnregScript--\u003e\u003eOperator: Complete\n    \n    Operator-\u003e\u003eDumpScript: Verify removal\n    DumpScript-\u003e\u003ePEP: Query all PEPs\n    DumpScript-\u003e\u003ePDP: Query PDP\n    PEP--\u003e\u003eDumpScript: Updated data\n    PDP--\u003e\u003eDumpScript: Updated data\n    DumpScript--\u003e\u003eOperator: Verification files\n```\n\n---\n\n## Limitations and Caveats\n\n### Script Limitations\n\n1. **No rollback capability:** The `unregister-resource.sh` script does not provide rollback functionality\n2. **No validation:** Scripts do not validate resource IDs before attempting removal\n3. **Sequential execution:** Resource removal is sequential, not atomic across components\n4. **No dry-run mode:** Scripts execute immediately without preview\n\n### Operational Considerations\n\n- **Policy regeneration:** Some resources may be automatically re-registered by services after removal\n- **Eventual consistency:** Policy changes may take time to propagate across distributed PEP instances\n- **Token caching:** Active tokens may retain access until expiration even after policy changes\n\n### Alternative Approaches\n\nFor programmatic policy management, consider:\n- Direct API calls to PEP/PDP REST interfaces\n- Integration with workspace lifecycle automation\n- Custom Kubernetes operators for policy management\n\n**Sources:** [bin/dump-policy.sh:1-43](), [bin/unregister-resource.sh:1-55]()"])</script><script>self.__next_f.push([1,"42:T576c,"])</script><script>self.__next_f.push([1,"# Monitoring and Troubleshooting\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [README.md](README.md)\n- [minikube/README.md](minikube/README.md)\n- [system/clusters/README.md](system/clusters/README.md)\n- [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml](system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml](system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml)\n- [system/clusters/creodias/system/flux-system/flux-system-patch.yaml](system/clusters/creodias/system/flux-system/flux-system-patch.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-components.yaml](system/clusters/creodias/system/flux-system/gotk-components.yaml)\n- [system/clusters/creodias/system/flux-system/gotk-sync.yaml](system/clusters/creodias/system/flux-system/gotk-sync.yaml)\n- [system/clusters/creodias/system/flux-system/kustomization.yaml](system/clusters/creodias/system/flux-system/kustomization.yaml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides operational guidance for monitoring and troubleshooting a deployed EOEPCA system. It focuses on the GitOps deployment model using Flux CD, covering reconciliation monitoring, deployment failure diagnosis, log access, and common operational issues. This page assumes familiarity with the [GitOps and Flux CD](#3.2) deployment architecture and the [Deployment Guide](#2.1).\n\nFor policy-specific troubleshooting tools, see [Policy Management Tools](#11.1). For infrastructure provisioning issues, refer to [Infrastructure Provisioning](#2.2).\n\n---\n\n## Flux CD Monitoring Architecture\n\nThe EOEPCA system uses Flux CD to continuously reconcile the cluster state with the Git repository. Flux consists of four primary controllers that monitor different aspects of the deployment:\n\n```mermaid\ngraph TB\n    subgraph \"Flux Controllers (flux-system namespace)\"\n        SourceCtrl[\"source-controller\u003cbr/\u003eMonitors GitRepository\"]\n        KustomizeCtrl[\"kustomize-controller\u003cbr/\u003eApplies Kustomization\"]\n        HelmCtrl[\"helm-controller\u003cbr/\u003eManages HelmRelease\"]\n        NotifyCtrl[\"notification-controller\u003cbr/\u003eSends Alerts\"]\n    end\n    \n    subgraph \"Custom Resources\"\n        GitRepo[\"GitRepository\u003cbr/\u003eflux-system\"]\n        Kustomization[\"Kustomization\u003cbr/\u003eflux-system\u003cbr/\u003euser-management\u003cbr/\u003eresource-management\u003cbr/\u003eprocessing-and-chaining\"]\n        HelmRelease[\"HelmRelease\u003cbr/\u003e(individual services)\"]\n    end\n    \n    subgraph \"Git Repository\"\n        GitHub[\"github.com/EOEPCA/eoepca\"]\n    end\n    \n    subgraph \"Deployed Services\"\n        UM[\"um namespace\u003cbr/\u003elogin-service\u003cbr/\u003eidentity-service\u003cbr/\u003epdp-engine\"]\n        RM[\"rm namespace\u003cbr/\u003eworkspace-api\u003cbr/\u003eresource-catalogue\u003cbr/\u003edata-access\"]\n        Proc[\"proc namespace\u003cbr/\u003eades\u003cbr/\u003eapplication-hub\"]\n    end\n    \n    SourceCtrl --\u003e|Fetches every 1m| GitHub\n    SourceCtrl --\u003e|Updates| GitRepo\n    \n    GitRepo --\u003e|Source for| KustomizeCtrl\n    KustomizeCtrl --\u003e|Reconciles| Kustomization\n    \n    Kustomization --\u003e|Creates| HelmRelease\n    HelmCtrl --\u003e|Installs/Upgrades| HelmRelease\n    \n    HelmRelease --\u003e|Deploys to| UM\n    HelmRelease --\u003e|Deploys to| RM\n    HelmRelease --\u003e|Deploys to| Proc\n    \n    NotifyCtrl -.-\u003e|Monitors| Kustomization\n    NotifyCtrl -.-\u003e|Monitors| HelmRelease\n```\n\n**Sources:** [system/clusters/creodias/system/flux-system/gotk-components.yaml:1-15](), [system/clusters/creodias/system/flux-system/gotk-sync.yaml:1-28]()\n\n---\n\n## Checking Reconciliation Status\n\n### Flux System Status\n\nThe overall Flux system status can be checked using the `flux` CLI:\n\n```bash\n# Check flux components status\nflux check\n\n# Check all Flux resources\nflux get all\n```\n\nThe `flux check` command verifies that all Flux controllers are running and healthy in the `flux-system` namespace.\n\n**Sources:** [system/clusters/README.md:26-29]()\n\n---\n\n### GitRepository Reconciliation\n\nMonitor the GitRepository resource to ensure Flux is successfully fetching from the Git repository:\n\n```bash\n# Check GitRepository status\nkubectl get gitrepository -n flux-system\n\n# Detailed status of the main GitRepository\nkubectl describe gitrepository flux-system -n flux-system\n\n# Watch for reconciliation events\nkubectl get gitrepository flux-system -n flux-system -w\n```\n\nThe GitRepository resource named `flux-system` is configured to sync from the `develop` branch every 1 minute:\n\n| Field | Value | Location |\n|-------|-------|----------|\n| `spec.url` | `ssh://git@github.com/EOEPCA/eoepca` | [gotk-sync.yaml:14]() |\n| `spec.interval` | `1m0s` | [gotk-sync.yaml:9]() |\n| `spec.ref.branch` | `develop` | [gotk-sync.yaml:11]() |\n\n**Key Status Fields:**\n\n- `status.conditions[?(@.type==\"Ready\")].status`: Should be `\"True\"` for successful sync\n- `status.artifact.revision`: Current Git commit SHA being reconciled\n- `status.artifact.lastUpdateTime`: Timestamp of last successful fetch\n\n**Sources:** [system/clusters/creodias/system/flux-system/gotk-sync.yaml:3-14](), [system/clusters/README.md:81-85]()\n\n---\n\n### Kustomization Reconciliation\n\nThe Kustomization resources orchestrate the deployment of the three main subsystems:\n\n```bash\n# List all Kustomizations\nkubectl get kustomization -n flux-system\n\n# Check specific subsystem status\nkubectl describe kustomization user-management -n flux-system\nkubectl describe kustomization resource-management -n flux-system\nkubectl describe kustomization processing-and-chaining -n flux-system\n```\n\nEach Kustomization references a path in the Git repository:\n\n| Kustomization | Path | Interval |\n|---------------|------|----------|\n| `flux-system` | `./system/clusters/creodias/system` | 10m0s |\n| `user-management` | `./system/clusters/creodias/user-management` | 1m0s |\n| `resource-management` | `./system/clusters/creodias/resource-management` | 1m0s |\n| `processing-and-chaining` | `./system/clusters/creodias/processing-and-chaining` | 1m0s |\n\n**Sources:** [system/clusters/creodias/system/flux-system/gotk-sync.yaml:16-28](), [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml:1-13](), [system/clusters/creodias/system/flux-system-extended/resource-management-sync.yaml:1-13](), [system/clusters/creodias/system/flux-system-extended/processing-and-chaining-sync.yaml:1-13]()\n\n---\n\n### HelmRelease Status\n\nHelmRelease resources represent individual service deployments:\n\n```bash\n# List all HelmReleases across all namespaces\nflux get helmreleases --all-namespaces\n\n# Check specific service\nkubectl describe helmrelease workspace-api -n rm\n\n# Get HelmRelease status for all resource management services\nkubectl get helmrelease -n rm\n```\n\n**Common HelmRelease Status Conditions:**\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Progressing: Initial deployment\n    Progressing --\u003e Ready: Installation successful\n    Progressing --\u003e Failed: Installation error\n    Ready --\u003e Progressing: Upgrade triggered\n    Failed --\u003e Progressing: Retry/Manual reconciliation\n    Ready --\u003e [*]: Service running\n```\n\n**Sources:** [README.md:82-85]()\n\n---\n\n## Flux Reconciliation Workflow\n\nUnderstanding the reconciliation workflow helps diagnose where issues occur:\n\n```mermaid\nsequenceDiagram\n    participant Git as GitHub Repository\n    participant SC as source-controller\n    participant GR as GitRepository Resource\n    participant KC as kustomize-controller\n    participant K as Kustomization Resource\n    participant HC as helm-controller\n    participant HR as HelmRelease Resource\n    participant Cluster as Kubernetes Cluster\n    \n    Note over SC,GR: Every 1 minute\n    SC-\u003e\u003eGit: Fetch repository\n    Git--\u003e\u003eSC: Return manifests\n    SC-\u003e\u003eGR: Update artifact\n    GR-\u003e\u003eGR: Update status.artifact.revision\n    \n    Note over KC,K: Triggered by artifact update\n    KC-\u003e\u003eGR: Read artifact\n    KC-\u003e\u003eK: Apply Kustomization\n    K-\u003e\u003eK: Generate HelmRelease manifests\n    K-\u003e\u003eHR: Create/Update HelmRelease\n    \n    Note over HC,HR: Triggered by HelmRelease change\n    HC-\u003e\u003eHR: Detect change\n    HC-\u003e\u003eHC: Fetch Helm chart\n    HC-\u003e\u003eCluster: Install/Upgrade release\n    Cluster--\u003e\u003eHC: Deployment status\n    HC-\u003e\u003eHR: Update status\n    \n    alt Deployment Failed\n        HR-\u003e\u003eHR: Set status.conditions[Ready]=False\n        HR-\u003e\u003eHR: Set status.failures++\n    else Deployment Successful\n        HR-\u003e\u003eHR: Set status.conditions[Ready]=True\n        HR-\u003e\u003eHR: Set status.failures=0\n    end\n```\n\n**Sources:** [system/clusters/README.md:79-85]()\n\n---\n\n## Debugging Failed Deployments\n\n### Step 1: Identify the Failing Component\n\nStart by checking the overall status to identify which component is failing:\n\n```bash\n# Quick overview of all Flux resources\nflux get all\n\n# Filter for non-ready resources\nflux get helmreleases --all-namespaces | grep -v \"True\"\nflux get kustomizations | grep -v \"True\"\n```\n\n---\n\n### Step 2: Examine Resource Events\n\nUse `kubectl describe` to see detailed events and conditions:\n\n```bash\n# For a failing HelmRelease\nkubectl describe helmrelease \u003crelease-name\u003e -n \u003cnamespace\u003e\n\n# For a failing Kustomization\nkubectl describe kustomization \u003cname\u003e -n flux-system\n```\n\n**Key fields to examine:**\n\n- `status.conditions`: Array of condition objects indicating health\n- `status.conditions[?(@.type==\"Ready\")].message`: Human-readable error description\n- `status.conditions[?(@.type==\"Ready\")].reason`: Programmatic error code\n- `status.failures`: Number of consecutive reconciliation failures\n- Events section: Recent reconciliation attempts and errors\n\n---\n\n### Step 3: Check Controller Logs\n\nEach Flux controller maintains logs that provide detailed reconciliation information:\n\n```bash\n# Source controller logs (GitRepository issues)\nkubectl logs -n flux-system deployment/source-controller -f\n\n# Kustomize controller logs (Kustomization issues)\nkubectl logs -n flux-system deployment/kustomize-controller -f\n\n# Helm controller logs (HelmRelease issues)\nkubectl logs -n flux-system deployment/helm-controller -f\n\n# Filter logs for specific resource\nkubectl logs -n flux-system deployment/helm-controller | grep \"workspace-api\"\n```\n\n**Sources:** [system/clusters/creodias/system/flux-system/gotk-components.yaml:1-15]()\n\n---\n\n### Step 4: Manual Reconciliation\n\nForce immediate reconciliation to test fixes without waiting for the interval:\n\n```bash\n# Reconcile a specific HelmRelease\nflux reconcile helmrelease \u003cname\u003e -n \u003cnamespace\u003e\n\n# Reconcile a Kustomization\nflux reconcile kustomization \u003cname\u003e\n\n# Reconcile the GitRepository (fetch latest from Git)\nflux reconcile source git flux-system\n```\n\n---\n\n## Accessing Service Logs\n\n### Application Service Logs\n\nFor deployed EOEPCA services, access logs through their pods:\n\n```bash\n# List pods in a namespace\nkubectl get pods -n um\nkubectl get pods -n rm\nkubectl get pods -n proc\n\n# View logs for a specific pod\nkubectl logs \u003cpod-name\u003e -n \u003cnamespace\u003e\n\n# Follow logs in real-time\nkubectl logs \u003cpod-name\u003e -n \u003cnamespace\u003e -f\n\n# Previous instance logs (after restart/crash)\nkubectl logs \u003cpod-name\u003e -n \u003cnamespace\u003e --previous\n\n# Logs for multi-container pods\nkubectl logs \u003cpod-name\u003e -n \u003cnamespace\u003e -c \u003ccontainer-name\u003e\n```\n\n**Common EOEPCA Service Namespaces:**\n\n| Namespace | Services |\n|-----------|----------|\n| `um` | login-service, identity-service, pdp-engine, user-profile |\n| `rm` | workspace-api, resource-catalogue, data-access, bucket-operator |\n| `proc` | ades, application-hub, pde-hub |\n\n**Sources:** Inferred from system architecture diagrams\n\n---\n\n### Workspace-Specific Service Logs\n\nUser workspaces create isolated namespaces with their own service instances:\n\n```bash\n# List workspace namespaces\nkubectl get namespaces | grep workspace\n\n# Access workspace-specific services\nkubectl logs -n eric-workspace deployment/resource-catalogue\nkubectl logs -n eric-workspace deployment/data-access\nkubectl logs -n eric-workspace deployment/resource-guard\n```\n\n**Sources:** Inferred from workspace architecture diagram\n\n---\n\n## Common Troubleshooting Scenarios\n\n### Scenario 1: GitRepository Not Syncing\n\n**Symptoms:**\n- `flux get sources git` shows `False` status\n- `status.conditions[?(@.type==\"Ready\")].message` contains \"authentication failed\" or \"connection refused\"\n\n**Diagnosis:**\n\n```bash\nkubectl describe gitrepository flux-system -n flux-system\nkubectl logs -n flux-system deployment/source-controller | tail -50\n```\n\n**Common Causes and Solutions:**\n\n| Cause | Diagnostic | Solution |\n|-------|-----------|----------|\n| SSH key expired/invalid | \"authentication failed\" in logs | Regenerate SSH key and update `flux-system` secret |\n| Repository URL changed | \"repository not found\" | Update `spec.url` in GitRepository resource |\n| Network connectivity | \"connection refused\" | Check cluster network policies and egress rules |\n| Branch doesn't exist | \"reference not found\" | Verify `spec.ref.branch` exists in repository |\n\n**Update SSH credentials:**\n\n```bash\n# The secret is managed by Flux bootstrap\n# Re-run flux bootstrap to update credentials\nflux bootstrap github \\\n  --owner=EOEPCA \\\n  --repository=eoepca \\\n  --branch=develop \\\n  --path=./system/clusters/creodias/system\n```\n\n**Sources:** [system/clusters/README.md:32-44](), [system/clusters/creodias/system/flux-system/gotk-sync.yaml:3-14]()\n\n---\n\n### Scenario 2: HelmRelease Stuck in Failed State\n\n**Symptoms:**\n- HelmRelease shows `Ready: False`\n- `status.failures` is incrementing\n- Service pods are not created or are in error state\n\n**Diagnosis:**\n\n```bash\n# Check HelmRelease details\nkubectl describe helmrelease \u003cname\u003e -n \u003cnamespace\u003e\n\n# Check Helm controller logs\nkubectl logs -n flux-system deployment/helm-controller | grep \u003cname\u003e\n\n# Check if chart exists\nflux get sources helm --all-namespaces\n\n# Check for pod errors\nkubectl get pods -n \u003cnamespace\u003e\nkubectl describe pod \u003cpod-name\u003e -n \u003cnamespace\u003e\n```\n\n**Common Causes:**\n\n1. **Chart version not found**: Update chart version in HelmRelease\n2. **Values validation failed**: Check `spec.values` against chart schema\n3. **Resource conflicts**: Existing resources with same name\n4. **Image pull errors**: Check imagePullSecrets and registry access\n5. **Insufficient resources**: Check node capacity and pod resource requests\n\n**Manual intervention:**\n\n```bash\n# Suspend reconciliation\nflux suspend helmrelease \u003cname\u003e -n \u003cnamespace\u003e\n\n# Manually delete the release\nhelm uninstall \u003cname\u003e -n \u003cnamespace\u003e\n\n# Resume reconciliation\nflux resume helmrelease \u003cname\u003e -n \u003cnamespace\u003e\n```\n\n**Sources:** [system/clusters/README.md:81-85]()\n\n---\n\n### Scenario 3: Kustomization Build Failures\n\n**Symptoms:**\n- Kustomization shows `Ready: False`\n- Message indicates \"kustomization build failed\" or \"validation failed\"\n\n**Diagnosis:**\n\n```bash\nkubectl describe kustomization \u003cname\u003e -n flux-system\nkubectl logs -n flux-system deployment/kustomize-controller | grep \u003cname\u003e\n```\n\n**Common Causes:**\n\n| Error Pattern | Meaning | Solution |\n|---------------|---------|----------|\n| \"no matches for kind\" | CRD not installed | Install required CRD or adjust dependency order |\n| \"json: cannot unmarshal\" | Invalid YAML syntax | Fix YAML formatting in Git repository |\n| \"unknown field\" | Invalid field in manifest | Check API version compatibility |\n| \"VariableReference\" | Undefined substitution variable | Define missing variables in Kustomization |\n\n**Test Kustomization locally:**\n\n```bash\n# Clone the repository\ngit clone https://github.com/EOEPCA/eoepca.git\ncd eoepca\n\n# Build Kustomization locally\nkustomize build ./system/clusters/creodias/user-management\n```\n\n**Sources:** [system/clusters/creodias/system/flux-system-extended/user-management-sync.yaml:1-13]()\n\n---\n\n### Scenario 4: Resource Controller Memory Issues\n\n**Symptoms:**\n- Source-controller or kustomize-controller pods OOMKilled\n- `kubectl get pods -n flux-system` shows CrashLoopBackOff\n\n**Diagnosis:**\n\n```bash\nkubectl get pods -n flux-system\nkubectl describe pod source-controller-\u003chash\u003e -n flux-system\nkubectl top pods -n flux-system\n```\n\nThe EOEPCA deployment configures memory limits for Flux controllers:\n\n| Controller | Memory Limit | Memory Request |\n|------------|--------------|----------------|\n| source-controller | 2Gi | 500Mi |\n| kustomize-controller | 2Gi | 500Mi |\n\n**Solution:**\n\nThese limits are configured in the flux-system-patch overlay. If encountering OOM issues, increase the limits:\n\n```bash\n# Edit the patch file\nvim system/clusters/creodias/system/flux-system/flux-system-patch.yaml\n\n# Increase memory limits, e.g., to 4Gi\n# Commit and push changes\ngit add system/clusters/creodias/system/flux-system/flux-system-patch.yaml\ngit commit -m \"Increase Flux controller memory limits\"\ngit push\n```\n\n**Sources:** [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:1-40]()\n\n---\n\n### Scenario 5: Certificate and TLS Issues\n\n**Symptoms:**\n- Services unreachable via HTTPS\n- Browser shows certificate errors\n- Ingress endpoints return 503 or connection refused\n\n**Diagnosis:**\n\n```bash\n# Check cert-manager status\nkubectl get certificates --all-namespaces\nkubectl get certificaterequests --all-namespaces\n\n# Check cert-manager logs\nkubectl logs -n cert-manager deployment/cert-manager\n\n# Check ingress configuration\nkubectl get ingress --all-namespaces\nkubectl describe ingress \u003cname\u003e -n \u003cnamespace\u003e\n```\n\n**Common Issues:**\n\n1. **LetsEncrypt rate limits**: Hostnames using `nip.io` help avoid rate limits by embedding IP addresses\n2. **Ingress class not specified**: Ensure ingress resources specify `nginx` class\n3. **DNS resolution failures**: Verify DNS records resolve to cluster IP\n\n**Sources:** [README.md:100-112]()\n\n---\n\n## Troubleshooting Decision Tree\n\n```mermaid\ngraph TD\n    Start[\"Deployment Issue Detected\"]\n    \n    Start --\u003e CheckFlux{\"flux check\u003cbr/\u003esuccessful?\"}\n    CheckFlux --\u003e|No| FluxPods[\"Check flux-system pods\u003cbr/\u003ekubectl get pods -n flux-system\"]\n    CheckFlux --\u003e|Yes| CheckGit{\"GitRepository\u003cbr/\u003eReady=True?\"}\n    \n    FluxPods --\u003e FixFlux[\"Fix controller issues\u003cbr/\u003eCheck logs, resources\"]\n    \n    CheckGit --\u003e|No| GitLogs[\"Check source-controller logs\"]\n    GitLogs --\u003e GitAuth[\"Fix SSH auth\u003cbr/\u003eor repository URL\"]\n    \n    CheckGit --\u003e|Yes| CheckKust{\"Kustomization\u003cbr/\u003eReady=True?\"}\n    \n    CheckKust --\u003e|No| KustLogs[\"Check kustomize-controller logs\"]\n    KustLogs --\u003e KustBuild[\"Fix YAML syntax\u003cbr/\u003eor CRD dependencies\"]\n    \n    CheckKust --\u003e|Yes| CheckHelm{\"HelmRelease\u003cbr/\u003eReady=True?\"}\n    \n    CheckHelm --\u003e|No| HelmLogs[\"Check helm-controller logs\"]\n    HelmLogs --\u003e HelmIssue{\"Error type?\"}\n    \n    HelmIssue --\u003e|Chart not found| ChartSource[\"Check chart repository\u003cbr/\u003eand version\"]\n    HelmIssue --\u003e|Values invalid| ValuesCheck[\"Validate spec.values\u003cbr/\u003eagainst chart schema\"]\n    HelmIssue --\u003e|Deployment failed| PodCheck[\"kubectl describe pod\"]\n    \n    CheckHelm --\u003e|Yes| ServiceIssue[\"Service-level issue\"]\n    ServiceIssue --\u003e AppLogs[\"Check application logs\u003cbr/\u003ekubectl logs \u003cpod\u003e\"]\n    \n    PodCheck --\u003e PodFix[\"Fix pod issues:\u003cbr/\u003e- Image pull\u003cbr/\u003e- Resource limits\u003cbr/\u003e- Config errors\"]\n    \n    FixFlux --\u003e Resolved[\"Issue Resolved\"]\n    GitAuth --\u003e Resolved\n    KustBuild --\u003e Resolved\n    ChartSource --\u003e Resolved\n    ValuesCheck --\u003e Resolved\n    PodFix --\u003e Resolved\n    AppLogs --\u003e Resolved\n```\n\n---\n\n## Useful Monitoring Commands Reference\n\n### Quick Status Checks\n\n```bash\n# Overall system health\nflux get all\n\n# Check all pods across EOEPCA namespaces\nkubectl get pods -n um -n rm -n proc -n flux-system\n\n# Watch for pod changes\nkubectl get pods --all-namespaces --watch\n\n# Node resource utilization\nkubectl top nodes\nkubectl top pods --all-namespaces\n```\n\n---\n\n### Reconciliation Status\n\n```bash\n# Force reconciliation of all Kustomizations\nflux reconcile kustomization flux-system\nflux reconcile kustomization user-management\nflux reconcile kustomization resource-management\nflux reconcile kustomization processing-and-chaining\n\n# Suspend/resume automatic reconciliation\nflux suspend kustomization \u003cname\u003e\nflux resume kustomization \u003cname\u003e\n```\n\n---\n\n### Event Monitoring\n\n```bash\n# Watch cluster events\nkubectl get events --all-namespaces --watch\n\n# Events for specific namespace\nkubectl get events -n rm --sort-by='.lastTimestamp'\n\n# Events for specific resource\nkubectl describe \u003cresource-type\u003e \u003cname\u003e -n \u003cnamespace\u003e\n```\n\n---\n\n### Log Aggregation\n\n```bash\n# Tail logs from multiple pods\nkubectl logs -n rm -l app=workspace-api --tail=100 -f\n\n# Get logs from all containers in a pod\nkubectl logs \u003cpod-name\u003e -n \u003cnamespace\u003e --all-containers=true\n\n# Export logs for offline analysis\nkubectl logs \u003cpod-name\u003e -n \u003cnamespace\u003e \u003e service.log\n```\n\n**Sources:** [system/clusters/README.md:1-95]()\n\n---\n\n## Flux Reconciliation Interval Configuration\n\nThe reconciliation intervals control how frequently Flux checks for changes. These are configurable per resource:\n\n| Resource Type | Default Interval | Configuration Location |\n|---------------|------------------|------------------------|\n| GitRepository | 1m0s | [gotk-sync.yaml:9]() |\n| Kustomization (flux-system) | 1m0s | [flux-system-patch.yaml:39]() |\n| Kustomization (subsystems) | 1m0s | [user-management-sync.yaml:7]() |\n\nThe interval was reduced from the default 10m to 1m in the patch configuration to enable faster deployment updates during development:\n\n```yaml\nspec:\n  interval: 1m0s\n```\n\nFor production deployments, consider increasing these intervals to reduce API server load.\n\n**Sources:** [system/clusters/creodias/system/flux-system/flux-system-patch.yaml:33-40](), [system/clusters/creodias/system/flux-system/gotk-sync.yaml:9]()\n\n---\n\n## Integration with Deployment Guide\n\nThe [Deployment Guide](https://deployment-guide.docs.eoepca.org/) provides an alternative to Flux-based deployment using direct `helm` commands. When troubleshooting issues encountered during Deployment Guide setup:\n\n1. The Deployment Guide assumes `minikube` and uses direct `helm install` commands\n2. Flux GitOps uses `HelmRelease` resources that wrap Helm installations\n3. Both approaches deploy the same underlying Helm charts\n4. To compare configurations, examine the `spec.values` section of HelmRelease resources against the Deployment Guide's helm command values\n\n**Sources:** [README.md:93-99](), [system/clusters/README.md:1-5]()"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$L15\",null,{\"repoName\":\"EOEPCA/eoepca\",\"hasConfig\":false,\"canSteer\":true,\"children\":[\"$\",\"$L16\",null,{\"wiki\":{\"metadata\":{\"repo_name\":\"EOEPCA/eoepca\",\"commit_hash\":\"7aef2f4a\",\"generated_at\":\"2026-01-13T11:25:08.372991\",\"config\":null,\"config_source\":\"none\"},\"pages\":[{\"page_plan\":{\"id\":\"1\",\"title\":\"Overview\"},\"content\":\"$17\"},{\"page_plan\":{\"id\":\"2\",\"title\":\"Getting Started\"},\"content\":\"$18\"},{\"page_plan\":{\"id\":\"2.1\",\"title\":\"Deployment Guide\"},\"content\":\"$19\"},{\"page_plan\":{\"id\":\"2.2\",\"title\":\"Infrastructure Provisioning\"},\"content\":\"$1a\"},{\"page_plan\":{\"id\":\"2.3\",\"title\":\"Testing and Validation\"},\"content\":\"$1b\"},{\"page_plan\":{\"id\":\"3\",\"title\":\"System Architecture\"},\"content\":\"$1c\"},{\"page_plan\":{\"id\":\"3.1\",\"title\":\"Building Blocks Overview\"},\"content\":\"$1d\"},{\"page_plan\":{\"id\":\"3.2\",\"title\":\"GitOps and Flux CD\"},\"content\":\"$1e\"},{\"page_plan\":{\"id\":\"3.3\",\"title\":\"Network and Ingress\"},\"content\":\"$1f\"},{\"page_plan\":{\"id\":\"4\",\"title\":\"User Management and Identity\"},\"content\":\"$20\"},{\"page_plan\":{\"id\":\"4.1\",\"title\":\"Identity Service (Keycloak)\"},\"content\":\"$21\"},{\"page_plan\":{\"id\":\"4.2\",\"title\":\"Login Service (Gluu)\"},\"content\":\"$22\"},{\"page_plan\":{\"id\":\"4.3\",\"title\":\"Policy Enforcement (PEP/PDP)\"},\"content\":\"$23\"},{\"page_plan\":{\"id\":\"4.4\",\"title\":\"UMA Authentication Flow\"},\"content\":\"$24\"},{\"page_plan\":{\"id\":\"5\",\"title\":\"Resource Management\"},\"content\":\"$25\"},{\"page_plan\":{\"id\":\"5.1\",\"title\":\"Data Access Services\"},\"content\":\"$26\"},{\"page_plan\":{\"id\":\"5.2\",\"title\":\"Resource Catalogue\"},\"content\":\"$27\"},{\"page_plan\":{\"id\":\"5.3\",\"title\":\"Workspace API\"},\"content\":\"$28\"},{\"page_plan\":{\"id\":\"5.4\",\"title\":\"Data Registration and Harvesting\"},\"content\":\"$29\"},{\"page_plan\":{\"id\":\"5.5\",\"title\":\"Multi-Tenant Workspaces\"},\"content\":\"$2a\"},{\"page_plan\":{\"id\":\"6\",\"title\":\"Processing and Chaining\"},\"content\":\"$2b\"},{\"page_plan\":{\"id\":\"6.1\",\"title\":\"ADES (Application Deployment and Execution Service)\"},\"content\":\"$2c\"},{\"page_plan\":{\"id\":\"6.2\",\"title\":\"Application Hub (JupyterHub)\"},\"content\":\"$2d\"},{\"page_plan\":{\"id\":\"6.3\",\"title\":\"Processor Development Environment (PDE)\"},\"content\":\"$2e\"},{\"page_plan\":{\"id\":\"6.4\",\"title\":\"Resource Guards and Access Control\"},\"content\":\"$2f\"},{\"page_plan\":{\"id\":\"6.5\",\"title\":\"CWL Application Packages\"},\"content\":\"$30\"},{\"page_plan\":{\"id\":\"7\",\"title\":\"Storage and Persistence\"},\"content\":\"$31\"},{\"page_plan\":{\"id\":\"7.1\",\"title\":\"S3 Storage Architecture\"},\"content\":\"$32\"},{\"page_plan\":{\"id\":\"7.2\",\"title\":\"Database Systems\"},\"content\":\"$33\"},{\"page_plan\":{\"id\":\"7.3\",\"title\":\"NFS and Persistent Volumes\"},\"content\":\"$34\"},{\"page_plan\":{\"id\":\"8\",\"title\":\"Infrastructure\"},\"content\":\"$35\"},{\"page_plan\":{\"id\":\"8.1\",\"title\":\"Kubernetes Cluster Setup\"},\"content\":\"$36\"},{\"page_plan\":{\"id\":\"8.2\",\"title\":\"Terraform Infrastructure as Code\"},\"content\":\"$37\"},{\"page_plan\":{\"id\":\"8.3\",\"title\":\"Network Architecture\"},\"content\":\"$38\"},{\"page_plan\":{\"id\":\"9\",\"title\":\"Development and Testing\"},\"content\":\"$39\"},{\"page_plan\":{\"id\":\"9.1\",\"title\":\"DemoClient Library\"},\"content\":\"$3a\"},{\"page_plan\":{\"id\":\"9.2\",\"title\":\"Acceptance Testing Framework\"},\"content\":\"$3b\"},{\"page_plan\":{\"id\":\"9.3\",\"title\":\"Local Development with Minikube\"},\"content\":\"$3c\"},{\"page_plan\":{\"id\":\"10\",\"title\":\"Configuration and Secrets Management\"},\"content\":\"$3d\"},{\"page_plan\":{\"id\":\"10.1\",\"title\":\"SealedSecrets\"},\"content\":\"$3e\"},{\"page_plan\":{\"id\":\"10.2\",\"title\":\"OIDC Client Configuration\"},\"content\":\"$3f\"},{\"page_plan\":{\"id\":\"11\",\"title\":\"Operations and Maintenance\"},\"content\":\"$40\"},{\"page_plan\":{\"id\":\"11.1\",\"title\":\"Policy Management Tools\"},\"content\":\"$41\"},{\"page_plan\":{\"id\":\"11.2\",\"title\":\"Monitoring and Troubleshooting\"},\"content\":\"$42\"}]},\"children\":\"$L43\"}]}]\n"])</script><script>self.__next_f.push([1,"43:[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]\n"])</script><script>self.__next_f.push([1,"44:I[36505,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"TechArticle\\\",\\\"headline\\\":\\\"Overview\\\",\\\"description\\\":\\\"This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes \\\",\\\"image\\\":\\\"https://deepwiki.com/EOEPCA/eoepca/og-image.png\\\",\\\"datePublished\\\":\\\"2026-01-13T11:25:08.372991\\\",\\\"dateModified\\\":\\\"2026-01-13T11:25:08.372991\\\",\\\"author\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"url\\\":\\\"https://deepwiki.com\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"logo\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://deepwiki.com/icon.png\\\"}},\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://deepwiki.com/EOEPCA/eoepca\\\"}}\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]\n"])</script><script>self.__next_f.push([1,"e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"EOEPCA/eoepca | DeepWiki\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes \"}],[\"$\",\"meta\",\"2\",{\"name\":\"keywords\",\"content\":\"EOEPCA/eoepca,EOEPCA,eoepca,documentation,wiki,codebase,AI documentation,Devin,Overview\"}],[\"$\",\"link\",\"3\",{\"rel\":\"canonical\",\"href\":\"https://deepwiki.com/EOEPCA/eoepca\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"EOEPCA/eoepca | DeepWiki\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes \"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://deepwiki.com/EOEPCA/eoepca\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"DeepWiki\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image\",\"content\":\"https://deepwiki.com/EOEPCA/eoepca/og-image.png?page=1\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:site\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:creator\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"EOEPCA/eoepca | DeepWiki\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"This document provides a high-level introduction to the EOEPCA repository, which contains the reference implementation of the Earth Observation Exploitation Platform Common Architecture. It describes \"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image\",\"content\":\"https://deepwiki.com/EOEPCA/eoepca/og-image.png?page=1\"}],[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/icon.png?1ee4c6a68a73a205\",\"type\":\"image/png\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"18\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?a4f658907db0ab87\",\"type\":\"image/png\",\"sizes\":\"180x180\"}],[\"$\",\"$L44\",\"19\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"13:\"$e:metadata\"\n"])</script></body></html>