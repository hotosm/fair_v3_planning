<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/de70bee13400563f.css?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/3e435f9a1dbb5b5b.css?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs"/><script src="/_next/static/chunks/87c73c54-dd8d81ac9604067c.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/18-46ed77b917ddd87b.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/main-app-57aa1716f0d0f500.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/5462-08221e91030fd747.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/4429-943205658cbafffe.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/9976-9250854d58eefaa3.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/1481-25d5bbc4f2d9524a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-6651f8cd8321a0db.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/25-9f305b682cea7558.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/7391-e3942dcd34ddcc66.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/6373-d56a493968555802.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" async=""></script><meta name="next-size-adjust" content=""/><title>hotosm/fAIr | DeepWiki</title><meta name="description" content="The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi"/><meta name="keywords" content="hotosm/fAIr,hotosm,fAIr,documentation,wiki,codebase,AI documentation,Devin,Overview"/><link rel="canonical" href="https://deepwiki.com/hotosm/fAIr"/><meta property="og:title" content="hotosm/fAIr | DeepWiki"/><meta property="og:description" content="The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi"/><meta property="og:url" content="https://deepwiki.com/hotosm/fAIr"/><meta property="og:site_name" content="DeepWiki"/><meta property="og:image" content="https://deepwiki.com/hotosm/fAIr/og-image.png?page=1"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@cognition"/><meta name="twitter:creator" content="@cognition"/><meta name="twitter:title" content="hotosm/fAIr | DeepWiki"/><meta name="twitter:description" content="The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi"/><meta name="twitter:image" content="https://deepwiki.com/hotosm/fAIr/og-image.png?page=1"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="icon" href="/icon.png?1ee4c6a68a73a205" type="image/png" sizes="48x48"/><link rel="apple-touch-icon" href="/apple-icon.png?a4f658907db0ab87" type="image/png" sizes="180x180"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" noModule=""></script></head><body class="__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased"><div hidden=""><!--$--><!--/$--></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","light",null,["light","dark"],null,true,true)</script><!--$?--><template id="B:0"></template><div class="flex min-h-screen w-full flex-col text-white"><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div></div><!--/$--><script>requestAnimationFrame(function(){$RT=performance.now()});</script><script src="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs" id="_R_" async=""></script><div hidden id="S:0"><div class="flex min-h-screen w-full flex-col text-white" id="codebase-wiki-repo-page"><div class="bg-background border-b-border sticky top-0 z-30 border-b border-dashed"><div class="font-geist-mono relative flex h-8 items-center justify-center text-xs font-medium sm:hidden"><div class="powered-by-devin-gradient absolute inset-0 z-[-1] h-8 w-full"></div><button class="flex items-center gap-2"><svg class="size-3 [&amp;_path]:stroke-0 [&amp;_path]:animate-[custom-pulse_1.8s_infinite_var(--delay,0s)]" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="[--delay:0.6s]" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="[--delay:1.2s]" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg>Index your code with Devin</button></div><div class="container-wrapper"><div class="container mx-auto flex w-full flex-row items-center gap-2 py-4 md:py-6"><a class="flex items-center gap-3" href="/"><span class="text-base font-medium leading-none md:text-lg hidden sm:block">DeepWiki</span></a><div class="flex-1"><div class="flex flex-row items-center gap-2"><a class="block text-xs font-medium leading-none text-white sm:hidden md:text-lg" href="/">DeepWiki</a><p class="text-sm font-normal leading-none md:text-lg"><a href="https://github.com/hotosm/fAIr" target="_blank" rel="noopener noreferrer" title="Open repository" class="text-muted-foreground hover:text-muted-foreground/80 group inline-flex items-center gap-1 transition-colors">hotosm/fAIr<!-- --> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="opacity-0 transition-opacity group-hover:opacity-100"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a></p></div></div><div class="flex items-center gap-4"><button class="group hidden items-center gap-1.5 md:flex"><div class="relative"><span class="text-foreground/70 group-hover:text-foreground text-xs font-light transition-colors">Index your code with</span><div class="bg-foreground/30 absolute bottom-0 left-0 h-[1px] w-0 transition-all duration-300 group-hover:w-full"></div></div><div class="flex items-center gap-1 transition-transform duration-300 group-hover:translate-x-0.5"><svg class="size-4 transform transition-transform duration-700 group-hover:rotate-180 [&amp;_path]:stroke-0" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg><span class="text-sm font-medium">Devin</span></div></button><button aria-label="Edit Wiki" class="flex items-center rounded-md cursor-pointer transition-all border border-border bg-surface hover:border-border-hover hover:bg-component disabled:cursor-default disabled:opacity-50 disabled:hover:border-border disabled:hover:bg-surface gap-2 px-3 py-1.5 text-sm"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 256 256"><path d="M227.32,73.37,182.63,28.69a16,16,0,0,0-22.63,0L36.69,152A15.86,15.86,0,0,0,32,163.31V208a16,16,0,0,0,16,16H216a8,8,0,0,0,0-16H115.32l112-112A16,16,0,0,0,227.32,73.37ZM92.69,208H48V163.31l88-88L180.69,120ZM192,108.69,147.32,64l24-24L216,84.69Z"></path></svg>Edit Wiki</button><button class="flex items-center rounded-md !text-white cursor-pointer transition-all border bg-blue-500 hover:bg-blue-600 border-blue-500 hover:border-blue-600 dark:bg-blue-900 dark:hover:bg-blue-800 dark:border-blue-900 dark:hover:border-blue-800 disabled:cursor-default disabled:opacity-50 disabled:hover:bg-blue-500 disabled:hover:border-blue-500 dark:disabled:hover:bg-blue-900 dark:disabled:hover:border-blue-900 gap-1.5 px-3 py-1.5 text-sm" aria-label="Share" data-state="closed" data-slot="tooltip-trigger"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg><span>Share</span></button><div class="h-8 w-8"></div></div></div></div></div><!--$?--><template id="B:1"></template><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div><!--/$--></div></div><script>$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><div hidden id="S:1"><script type="application/ld+json">{"@context":"https://schema.org","@type":"TechArticle","headline":"Overview","description":"The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi","image":"https://deepwiki.com/hotosm/fAIr/og-image.png","datePublished":"2025-04-30T15:26:23.772646","dateModified":"2025-04-30T15:26:23.772646","author":{"@type":"Organization","name":"DeepWiki","url":"https://deepwiki.com"},"publisher":{"@type":"Organization","name":"DeepWiki","logo":{"@type":"ImageObject","url":"https://deepwiki.com/icon.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://deepwiki.com/hotosm/fAIr"}}</script><div class="w-full flex-1"><div class="container-wrapper relative mx-auto h-full px-0"><div class="container relative mx-auto flex h-full w-full flex-col gap-0 max-md:!px-0 md:flex-row md:gap-6 lg:gap-10"><div class="border-r-border hidden max-h-screen border-r border-dashed py-6 pr-4 transition-[border-radius] md:sticky md:left-0 md:top-20 md:block md:h-[calc(100vh-82px)] md:w-64 md:flex-shrink-0 md:overflow-y-auto lg:py-9 xl:w-72"><div class="flex h-full w-full max-w-full flex-shrink-0 flex-col overflow-hidden" style="scrollbar-color:var(--color-border) transparent"><div class="flex-shrink-0 px-2"><div class="text-secondary pb-1 text-xs">Last indexed: <!-- -->30 April 2025<!-- --> (<a href="https://github.com/hotosm/fAIr/commits/14c64164" target="_blank" rel="noopener noreferrer" class="underline-offset-2 hover:underline">14c641</a>)</div></div><ul class="flex-1 flex-shrink-0 space-y-1 overflow-y-auto py-1" style="scrollbar-width:none"><li style="padding-left:0"><a data-selected="true" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/1-overview">Overview</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/1.1-system-architecture">System Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/1.2-key-concepts-and-terminology">Key Concepts and Terminology</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/1.3-installation-and-setup">Installation and Setup</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/2-backend-system">Backend System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/2.1-api-endpoints">API Endpoints</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/2.2-data-models">Data Models</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/2.3-asynchronous-processing">Asynchronous Processing</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/3-frontend-system">Frontend System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/3.1-component-structure">Component Structure</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/3.2-start-mapping-feature">Start Mapping Feature</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/3.3-model-creation-and-management">Model Creation and Management</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/3.4-authentication-and-user-management">Authentication and User Management</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/3.5-map-visualization-components">Map Visualization Components</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/4-development-and-deployment">Development and Deployment</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/4.1-development-environment-setup">Development Environment Setup</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/4.2-cicd-workflows">CI/CD Workflows</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/4.3-docker-containerization">Docker Containerization</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/5-configuration-and-customization">Configuration and Customization</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/5.1-environment-variables">Environment Variables</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/hotosm/fAIr/5.2-ui-content-configuration">UI Content Configuration</a></li></ul></div></div><div class="flex h-full flex-1 flex-col overflow-hidden"><div class="bg-background border-b-border sticky top-0 z-10 border-b border-dashed md:hidden"><div class="flex cursor-pointer items-center gap-2 p-3"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="transition-transform"><path d="M184.49,136.49l-80,80a12,12,0,0,1-17-17L159,128,87.51,56.49a12,12,0,1,1,17-17l80,80A12,12,0,0,1,184.49,136.49Z"></path></svg><span class="truncate text-base font-normal">Menu</span></div></div><div class="relative flex-1 overflow-y-auto px-3 pt-3 md:rounded-md md:px-0 md:pt-0 [&amp;_::selection]:bg-purple-500/40" style="scrollbar-color:var(--color-night) transparent"><div class="pb-30 mx-auto max-w-2xl md:pb-40 md:pt-6 lg:pt-8"><div class="prose prose-invert dark:prose-invert prose-headings:text-inherit prose-p:text-inherit max-w-none"><div><div class="prose-custom prose-custom-md prose-custom-gray !max-w-none text-neutral-300 [overflow-wrap:anywhere]"><h1 id="overview" class="group" data-header="true">Overview<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h1>
<details>
<summary>Relevant source files</summary>
<ul>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/Readme.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>Readme.md</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/Dockerfile" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/Dockerfile</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/Dockerfile_CPU" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/Dockerfile_CPU</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/aiproject/settings.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/aiproject/settings.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/aiproject/urls.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/aiproject/urls.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/models.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/models.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/serializers.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/serializers.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/tasks.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/tasks.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/urls.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/urls.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/utils.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/utils.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/views.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/views.py</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/requirements.txt" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/requirements.txt</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/docker-compose-cpu.yml" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>docker-compose-cpu.yml</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/docker-compose.yml" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>docker-compose.yml</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/docs/Docker-installation.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>docs/Docker-installation.md</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/.gitignore" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/.gitignore</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/Dockerfile.frontend" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/Dockerfile.frontend</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/README.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/README.md</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/eslint.config.js" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/eslint.config.js</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/index.html" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/index.html</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/src/components/ui/animated-beam/animated-beam.tsx" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/src/components/ui/animated-beam/animated-beam.tsx</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/src/components/ui/banner/banner.tsx" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/src/components/ui/banner/banner.tsx</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/src/styles/index.css" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/src/styles/index.css</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/run_migrations.sh" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>run_migrations.sh</span></a></li>
<li><a href="https://github.com/hotosm/fAIr/blob/14c64164/setup-ramp.sh" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>setup-ramp.sh</span></a></li>
</ul>
</details>
<p>The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machine learning models. This overview introduces the main components of the fAIr system, its purpose, and its core functionality.</p>
<p>fAIr&#x27;s name is derived from:</p>
<ul>
<li><strong>f</strong>: for freedom and free and open-source software</li>
<li><strong>AI</strong>: for Artificial Intelligence</li>
<li><strong>r</strong>: for resilience and responsibility for communities in humanitarian mapping</li>
</ul>
<p>The system allows OpenStreetMap (OSM) community members to create and train their own AI models for mapping in their region of interest, making AI-assisted mapping accessible across diverse geographies. For detailed information about system installation and setup, refer to <a href="/hotosm/fAIr/1.3-installation-and-setup" class="text-neutral-300 hover:text-neutral-200 hover:underline">Installation and Setup</a>.</p>
<h2 id="system-purpose" class="group" data-header="true">System Purpose<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>fAIr addresses several key challenges in humanitarian mapping:</p>
<ol>
<li><strong>Democratic Access to AI</strong>: Unlike proprietary AI services, fAIr provides free, open-source AI-assisted mapping tools that communities can control</li>
<li><strong>Local Context</strong>: Models are trained on locally relevant data to ensure accuracy in diverse geographic contexts</li>
<li><strong>Continuous Improvement</strong>: Models improve through user feedback, creating a learning cycle that enhances accuracy</li>
<li><strong>Integration with OSM Workflow</strong>: Seamlessly fits into existing OpenStreetMap contribution processes</li>
</ol>
<p>The system focuses primarily on building footprint detection but is designed to be extensible to other feature types.</p>
<h2 id="high-level-architecture" class="group" data-header="true">High-Level Architecture<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>Sources: <a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/aiproject/settings.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/aiproject/settings.py</span></a> <a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/tasks.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/tasks.py</span></a> <a href="https://github.com/hotosm/fAIr/blob/14c64164/docker-compose.yml" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>docker-compose.yml</span></a> <a href="https://github.com/hotosm/fAIr/blob/14c64164/Readme.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>Readme.md</span></a></p>
<h2 id="core-components" class="group" data-header="true">Core Components<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The fAIr system consists of these primary components:</p>
<ol>
<li><strong>Frontend</strong>: React-based web application that provides the user interface for creating datasets, training models, and viewing predictions</li>
<li><strong>Backend API</strong>: Django REST API that handles data management, authentication, and coordinates the training process</li>
<li><strong>Prediction API</strong>: Specialized API for generating real-time predictions using trained models</li>
<li><strong>Workers</strong>: Celery workers that handle asynchronous model training tasks</li>
<li><strong>Database</strong>: PostgreSQL with PostGIS extension for spatial data storage</li>
<li><strong>Storage</strong>: S3-compatible storage for model artifacts and training data</li>
</ol>
<h2 id="key-data-models-and-workflow" class="group" data-header="true">Key Data Models and Workflow<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>Sources: <a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/models.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/models.py</span></a> <a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/serializers.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/serializers.py</span></a></p>
<p>The system workflow follows this general pattern:</p>
<ol>
<li><strong>Dataset Creation</strong>: Users define datasets representing geographic areas</li>
<li><strong>AOI Definition</strong>: Within a dataset, Areas of Interest (AOIs) are defined</li>
<li><strong>Label Collection</strong>: Building footprints are fetched from OSM within AOIs</li>
<li><strong>Model Creation</strong>: Users create models associated with datasets</li>
<li><strong>Model Training</strong>: Training jobs process imagery and labels to create models</li>
<li><strong>Prediction</strong>: Trained models generate predictions on new areas</li>
<li><strong>Feedback</strong>: Users provide feedback on predictions to improve models</li>
</ol>
<p>For more detailed information about the underlying architecture, refer to <a href="/hotosm/fAIr/1.1-system-architecture" class="text-neutral-300 hover:text-neutral-200 hover:underline">System Architecture</a>.</p>
<h2 id="model-training-process" class="group" data-header="true">Model Training Process<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p>Sources: <a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/tasks.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/tasks.py</span></a> <a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/core/views.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/core/views.py</span></a></p>
<h2 id="technology-stack" class="group" data-header="true">Technology Stack<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The fAIr system uses the following key technologies:</p>

































<table><thead><tr><th>Component</th><th>Technologies</th></tr></thead><tbody><tr><td>Frontend</td><td>React, TypeScript, Vite, MapLibre GL</td></tr><tr><td>Backend</td><td>Django, Django REST Framework, GeoDjango</td></tr><tr><td>Machine Learning</td><td>TensorFlow, RAMP, YOLOv8</td></tr><tr><td>Data Storage</td><td>PostgreSQL, PostGIS, S3</td></tr><tr><td>Task Processing</td><td>Celery, Redis</td></tr><tr><td>Deployment</td><td>Docker, GitHub Actions</td></tr></tbody></table>
<p>For detailed information about key concepts and terminology used throughout the system, refer to <a href="/hotosm/fAIr/1.2-key-concepts-and-terminology" class="text-neutral-300 hover:text-neutral-200 hover:underline">Key Concepts and Terminology</a>.</p>
<p>Sources: <a href="https://github.com/hotosm/fAIr/blob/14c64164/Readme.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>Readme.md</span></a> <a href="https://github.com/hotosm/fAIr/blob/14c64164/backend/requirements.txt" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>backend/requirements.txt</span></a> <a href="https://github.com/hotosm/fAIr/blob/14c64164/frontend/README.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>frontend/README.md</span></a></p></div></div></div></div></div></div><div class="hidden overflow-hidden transition-[border-radius] xl:sticky xl:right-0 xl:top-20 xl:block xl:h-[calc(100vh-82px)] xl:w-64 xl:flex-shrink-0 2xl:w-72" style="scrollbar-width:none"><div class="flex max-h-full w-full flex-shrink-0 flex-col py-6 pt-0 text-sm lg:pb-4 lg:pt-8 xl:w-64 2xl:w-72" style="scrollbar-color:var(--color-night) transparent"><div><div class="relative mx-4 my-4 rounded-md border border-neutral-200 bg-neutral-100 p-3 text-sm text-neutral-600 dark:border-neutral-800 dark:bg-neutral-900 dark:text-neutral-400"><button class="absolute right-2 top-2 rounded-sm p-1 opacity-70 transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-neutral-400 focus:ring-offset-2"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M205.66,194.34a8,8,0,0,1-11.32,11.32L128,139.31,61.66,205.66a8,8,0,0,1-11.32-11.32L116.69,128,50.34,61.66A8,8,0,0,1,61.66,50.34L128,116.69l66.34-66.35a8,8,0,0,1,11.32,11.32L139.31,128Z"></path></svg><span class="sr-only">Dismiss</span></button><p class="text-sm font-medium">Refresh this wiki</p><button class="mt-2 flex items-center gap-1 rounded-md bg-neutral-200 px-2 py-1 text-sm font-medium text-neutral-700 transition-colors hover:bg-neutral-300 dark:bg-neutral-800 dark:text-neutral-300 dark:hover:bg-neutral-700">Enter email to refresh</button></div></div><h3 class="px-4 pb-5 text-lg font-medium leading-none">On this page</h3><ul style="scrollbar-width:none" class="min-h-0 flex-1 space-y-3 overflow-y-auto p-4 pt-0"><li class=""><a href="#overview" class="hover:text-primary pr-1 transition-all text-primary font-medium">Overview</a></li><li class="ml-3"><a href="#system-purpose" class="hover:text-primary pr-1 font-normal transition-all text-secondary">System Purpose</a></li><li class="ml-3"><a href="#high-level-architecture" class="hover:text-primary pr-1 font-normal transition-all text-secondary">High-Level Architecture</a></li><li class="ml-3"><a href="#core-components" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Core Components</a></li><li class="ml-3"><a href="#key-data-models-and-workflow" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Key Data Models and Workflow</a></li><li class="ml-3"><a href="#model-training-process" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Model Training Process</a></li><li class="ml-3"><a href="#technology-stack" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Technology Stack</a></li></ul></div></div><div class="pointer-events-none fixed bottom-2 left-2 right-2 mt-2 md:bottom-4 md:left-0 md:right-0"><div class="z-10 mx-auto max-w-3xl"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div></div></div></div><!--$--><!--/$--></div><script>$RC("B:1","S:1")</script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n"])</script><script>self.__next_f.push([1,"2:I[49138,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7177\",\"static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\"],\"RootProvider\"]\n"])</script><script>self.__next_f.push([1,"3:I[85341,[],\"\"]\n4:I[90025,[],\"\"]\n7:I[41012,[],\"ClientPageRoot\"]\n"])</script><script>self.__next_f.push([1,"8:I[57456,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"4129\",\"static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"2545\",\"static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"8461\",\"static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7198\",\"static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"5462\",\"static/chunks/5462-08221e91030fd747.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"4429\",\"static/chunks/4429-943205658cbafffe.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"9976\",\"static/chunks/9976-9250854d58eefaa3.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1481\",\"static/chunks/1481-25d5bbc4f2d9524a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"3285\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-6651f8cd8321a0db.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"b:I[15104,[],\"OutletBoundary\"]\nd:I[94777,[],\"AsyncMetadataOutlet\"]\nf:I[15104,[],\"ViewportBoundary\"]\n11:I[15104,[],\"MetadataBoundary\"]\n12:\"$Sreact.suspense\"\n14:I[34431,[],\"\"]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/de70bee13400563f.css?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"style\"]\n:HL[\"/_next/static/css/3e435f9a1dbb5b5b.css?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"yATpRlI1wOgX1cb5YQcha\",\"p\":\"\",\"c\":[\"\",\"hotosm\",\"fAIr\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"org\",\"hotosm\",\"d\"],{\"children\":[[\"repo\",\"fAIr\",\"d\"],{\"children\":[[\"wikiRoutes\",\"\",\"oc\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/de70bee13400563f.css?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/3e435f9a1dbb5b5b.css?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"className\":\"__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}],{\"children\":[[\"org\",\"hotosm\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"repo\",\"fAIr\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L5\"]}],{\"children\":[[\"wikiRoutes\",\"\",\"oc\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L6\"]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L7\",null,{\"Component\":\"$8\",\"searchParams\":{},\"params\":{\"org\":\"hotosm\",\"repo\":\"fAIr\"},\"promises\":[\"$@9\",\"$@a\"]}],null,[\"$\",\"$Lb\",null,{\"children\":[\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L11\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$12\",null,{\"fallback\":null,\"children\":\"$L13\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"9:{}\na:\"$0:f:0:1:2:children:2:children:2:children:2:children:1:props:children:0:props:params\"\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nc:null\n"])</script><script>self.__next_f.push([1,"15:I[13550,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7391\",\"static/chunks/7391-e3942dcd34ddcc66.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"6373\",\"static/chunks/6373-d56a493968555802.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"6375\",\"static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"9437\",\"static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\"],\"HeaderWrapperWithSuspense\"]\n"])</script><script>self.__next_f.push([1,"16:I[82188,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"7391\",\"static/chunks/7391-e3942dcd34ddcc66.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"6373\",\"static/chunks/6373-d56a493968555802.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"6375\",\"static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"9437\",\"static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_xmAp5tbiQAbjmsBaj3oa3eJ2LpGs\"],\"WikiContextProvider\"]\n"])</script><script>self.__next_f.push([1,"2d:I[36505,[],\"IconMark\"]\n17:T1e53,"])</script><script>self.__next_f.push([1,"# Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [Readme.md](Readme.md)\n- [backend/Dockerfile](backend/Dockerfile)\n- [backend/Dockerfile_CPU](backend/Dockerfile_CPU)\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/requirements.txt](backend/requirements.txt)\n- [docker-compose-cpu.yml](docker-compose-cpu.yml)\n- [docker-compose.yml](docker-compose.yml)\n- [docs/Docker-installation.md](docs/Docker-installation.md)\n- [frontend/.gitignore](frontend/.gitignore)\n- [frontend/Dockerfile.frontend](frontend/Dockerfile.frontend)\n- [frontend/README.md](frontend/README.md)\n- [frontend/eslint.config.js](frontend/eslint.config.js)\n- [frontend/index.html](frontend/index.html)\n- [frontend/src/components/ui/animated-beam/animated-beam.tsx](frontend/src/components/ui/animated-beam/animated-beam.tsx)\n- [frontend/src/components/ui/banner/banner.tsx](frontend/src/components/ui/banner/banner.tsx)\n- [frontend/src/styles/index.css](frontend/src/styles/index.css)\n- [run_migrations.sh](run_migrations.sh)\n- [setup-ramp.sh](setup-ramp.sh)\n\n\u003c/details\u003e\n\n\n\nThe fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machine learning models. This overview introduces the main components of the fAIr system, its purpose, and its core functionality.\n\nfAIr's name is derived from:\n- **f**: for freedom and free and open-source software\n- **AI**: for Artificial Intelligence\n- **r**: for resilience and responsibility for communities in humanitarian mapping\n\nThe system allows OpenStreetMap (OSM) community members to create and train their own AI models for mapping in their region of interest, making AI-assisted mapping accessible across diverse geographies. For detailed information about system installation and setup, refer to [Installation and Setup](#1.3).\n\n## System Purpose\n\nfAIr addresses several key challenges in humanitarian mapping:\n\n1. **Democratic Access to AI**: Unlike proprietary AI services, fAIr provides free, open-source AI-assisted mapping tools that communities can control\n2. **Local Context**: Models are trained on locally relevant data to ensure accuracy in diverse geographic contexts\n3. **Continuous Improvement**: Models improve through user feedback, creating a learning cycle that enhances accuracy\n4. **Integration with OSM Workflow**: Seamlessly fits into existing OpenStreetMap contribution processes\n\nThe system focuses primarily on building footprint detection but is designed to be extensible to other feature types.\n\n## High-Level Architecture\n\n```mermaid\nflowchart TD\n    subgraph \"User Interface Layer\"\n        Frontend[\"React Frontend\"]\n    end\n    \n    subgraph \"Application Layer\"\n        Backend[\"Django Backend API\"]\n        PredictionAPI[\"Prediction API\"]\n    end\n    \n    subgraph \"Processing Layer\"\n        Worker[\"Celery Worker\"]\n        Training[\"Model Training (RAMP/YOLO)\"]\n    end\n    \n    subgraph \"Data Layer\"\n        DB[(PostgreSQL/PostGIS)]\n        Redis[(Redis Queue)]\n        S3[(S3 Storage)]\n    end\n    \n    subgraph \"External Services\"\n        OSM[\"OpenStreetMap\"]\n        OAM[\"OpenAerialMap\"]\n        RawDataAPI[\"Raw Data API\"]\n    end\n    \n    Frontend --\u003e Backend\n    Frontend --\u003e PredictionAPI\n    Backend --\u003e Worker\n    Worker --\u003e Training\n    Backend --\u003e DB\n    Worker --\u003e DB\n    Backend --\u003e Redis\n    Worker --\u003e Redis\n    Worker --\u003e S3\n    Backend --\u003e OSM\n    Backend --\u003e RawDataAPI\n    Frontend --\u003e OAM\n```\n\nSources: [backend/aiproject/settings.py](backend/aiproject/settings.py), [backend/core/tasks.py](backend/core/tasks.py), [docker-compose.yml](docker-compose.yml), [Readme.md](Readme.md)\n\n## Core Components\n\nThe fAIr system consists of these primary components:\n\n1. **Frontend**: React-based web application that provides the user interface for creating datasets, training models, and viewing predictions\n2. **Backend API**: Django REST API that handles data management, authentication, and coordinates the training process\n3. **Prediction API**: Specialized API for generating real-time predictions using trained models\n4. **Workers**: Celery workers that handle asynchronous model training tasks\n5. **Database**: PostgreSQL with PostGIS extension for spatial data storage\n6. **Storage**: S3-compatible storage for model artifacts and training data\n\n## Key Data Models and Workflow\n\n```mermaid\nflowchart LR\n    Dataset --\u003e AOI[\"Area of Interest (AOI)\"]\n    AOI --\u003e Label\n    Dataset --\u003e Model\n    Model --\u003e Training\n    Training --\u003e Feedback\n    Training --\u003e Prediction\n\n    classDef primary fill:#f9f9f9,stroke:#333,stroke-width:1px\n    class Dataset,Model,Training,Prediction primary\n```\n\nSources: [backend/core/models.py](backend/core/models.py), [backend/core/serializers.py](backend/core/serializers.py)\n\nThe system workflow follows this general pattern:\n\n1. **Dataset Creation**: Users define datasets representing geographic areas\n2. **AOI Definition**: Within a dataset, Areas of Interest (AOIs) are defined\n3. **Label Collection**: Building footprints are fetched from OSM within AOIs\n4. **Model Creation**: Users create models associated with datasets\n5. **Model Training**: Training jobs process imagery and labels to create models\n6. **Prediction**: Trained models generate predictions on new areas\n7. **Feedback**: Users provide feedback on predictions to improve models\n\nFor more detailed information about the underlying architecture, refer to [System Architecture](#1.1).\n\n## Model Training Process\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend as \"Backend API\"\n    participant Worker as \"Celery Worker\"\n    participant Storage as \"S3 Storage\"\n\n    User-\u003e\u003eFrontend: Create dataset \u0026 define AOI\n    Frontend-\u003e\u003eBackend: POST /api/v1/dataset/ \u0026 /api/v1/aoi/\n    User-\u003e\u003eFrontend: Fetch OSM data for labels\n    Frontend-\u003e\u003eBackend: GET /api/v1/label/osm/fetch/{aoi_id}/\n    Backend-\u003e\u003eBackend: process_rawdata_task()\n    \n    User-\u003e\u003eFrontend: Create model\n    Frontend-\u003e\u003eBackend: POST /api/v1/model/\n    User-\u003e\u003eFrontend: Start training\n    Frontend-\u003e\u003eBackend: POST /api/v1/training/\n    Backend-\u003e\u003eWorker: Enqueue train_model task\n    \n    Worker-\u003e\u003eWorker: Download imagery\n    Worker-\u003e\u003eWorker: Prepare training data\n    Worker-\u003e\u003eWorker: Train model (RAMP/YOLO)\n    Worker-\u003e\u003eStorage: Upload model artifacts\n    Worker-\u003e\u003eBackend: Update training status\n    \n    Backend--\u003e\u003eFrontend: Training completed\n    User-\u003e\u003eFrontend: View model \u0026 predictions\n```\n\nSources: [backend/core/tasks.py](backend/core/tasks.py), [backend/core/views.py](backend/core/views.py)\n\n## Technology Stack\n\nThe fAIr system uses the following key technologies:\n\n| Component | Technologies |\n|-----------|--------------|\n| Frontend | React, TypeScript, Vite, MapLibre GL |\n| Backend | Django, Django REST Framework, GeoDjango |\n| Machine Learning | TensorFlow, RAMP, YOLOv8 |\n| Data Storage | PostgreSQL, PostGIS, S3 |\n| Task Processing | Celery, Redis |\n| Deployment | Docker, GitHub Actions |\n\nFor detailed information about key concepts and terminology used throughout the system, refer to [Key Concepts and Terminology](#1.2).\n\nSources: [Readme.md](Readme.md), [backend/requirements.txt](backend/requirements.txt), [frontend/README.md](frontend/README.md)"])</script><script>self.__next_f.push([1,"18:T4671,"])</script><script>self.__next_f.push([1,"# System Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/docker_build.yml](.github/workflows/docker_build.yml)\n- [.github/workflows/docker_publish_image.yml](.github/workflows/docker_publish_image.yml)\n- [.github/workflows/frontend_build.yml](.github/workflows/frontend_build.yml)\n- [.github/workflows/frontend_build_push.yml](.github/workflows/frontend_build_push.yml)\n- [Readme.md](Readme.md)\n- [backend/Dockerfile](backend/Dockerfile)\n- [backend/Dockerfile_CPU](backend/Dockerfile_CPU)\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/requirements.txt](backend/requirements.txt)\n- [docker-compose-cpu.yml](docker-compose-cpu.yml)\n- [docker-compose.yml](docker-compose.yml)\n- [docs/Docker-installation.md](docs/Docker-installation.md)\n- [frontend/.env.sample](frontend/.env.sample)\n- [frontend/Dockerfile.frontend](frontend/Dockerfile.frontend)\n- [frontend/package.json](frontend/package.json)\n- [frontend/pnpm-lock.yaml](frontend/pnpm-lock.yaml)\n- [frontend/src/components/layouts/root-layout.tsx](frontend/src/components/layouts/root-layout.tsx)\n- [frontend/src/config/env.ts](frontend/src/config/env.ts)\n- [frontend/src/config/index.ts](frontend/src/config/index.ts)\n- [frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx](frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx)\n- [frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx](frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx)\n- [frontend/src/hooks/use-map-instance.ts](frontend/src/hooks/use-map-instance.ts)\n- [frontend/src/store/model-prediction-store.ts](frontend/src/store/model-prediction-store.ts)\n- [run_migrations.sh](run_migrations.sh)\n- [setup-ramp.sh](setup-ramp.sh)\n\n\u003c/details\u003e\n\n\n\nThis document provides a comprehensive overview of the fAIr system architecture, explaining how different components interact to deliver AI-assisted mapping capabilities. It covers the overall system design, component relationships, data flow, and deployment architecture. For specific details about installation and setup, see [Installation and Setup](#1.3).\n\n## High-Level Architecture Overview\n\nfAIr employs a modern service-oriented architecture that separates concerns between frontend user interface, backend API services, asynchronous processing, and data storage. The system is designed to handle computationally intensive AI model training and inference while providing an intuitive user experience.\n\n```mermaid\nflowchart TB\n    subgraph \"User Interface Layer\"\n        Frontend[\"React Frontend\"]\n        WebBrowser[\"Web Browser\"]\n    end\n    \n    subgraph \"Application Layer\"\n        Backend[\"Django Backend API\"]\n        PredictionAPI[\"Prediction API\"]\n    end\n    \n    subgraph \"Processing Layer\"\n        Worker[\"Celery Worker\"]\n        RAMP[\"RAMP Training\"]\n        YOLO[\"YOLO Training\"]\n    end\n    \n    subgraph \"Data Layer\"\n        PostgreSQL[(\"PostgreSQL/PostGIS\")]\n        Redis[(\"Redis Queue\")]\n        S3[(\"S3 Storage\")]\n    end\n    \n    subgraph \"External Services\"\n        OSM[\"OpenStreetMap\"]\n        OAM[\"OpenAerialMap\"]\n        RawDataAPI[\"Raw Data API\"]\n    end\n    \n    WebBrowser --\u003e Frontend\n    Frontend --\u003e Backend\n    Frontend --\u003e PredictionAPI\n    Backend --\u003e Worker\n    Backend --\u003e PostgreSQL\n    Backend --\u003e S3\n    Worker --\u003e RAMP\n    Worker --\u003e YOLO\n    Worker --\u003e S3\n    Backend --\u003e Redis\n    Worker --\u003e Redis\n    Backend --\u003e OSM\n    Frontend --\u003e OAM\n    Backend --\u003e RawDataAPI\n```\n\n**Diagram: High-Level Architecture of fAIr System**\n\nSources: [backend/core/views.py:1-97](), [backend/core/tasks.py:1-43](), [Readme.md:82-85](), [backend/aiproject/settings.py:1-60]()\n\n## Core Components\n\n### 1. Frontend System\n\nThe frontend is a single-page application built with React and TypeScript. It provides the user interface for model creation, training, monitoring, and prediction visualization.\n\n```mermaid\nflowchart TD\n    subgraph \"Layout Components\"\n        RootLayout[\"RootLayout\"]\n        ModelFormsLayout[\"ModelFormsLayout\"]\n    end\n    \n    subgraph \"Map Visualization\"\n        MapComponent[\"MapInstance\"]\n        PredictionLayers[\"PredictionLayers\"]\n        AcceptedPredictionsLayer[\"AcceptedPredictionsLayer\"]\n        RejectedPredictionsLayer[\"RejectedPredictionsLayer\"]\n        TileSource[\"TMS/OAM Source\"]\n    end\n    \n    subgraph \"State Management\"\n        AuthProvider[\"AuthProvider\"]\n        ModelPredictionStore[\"ModelPredictionStore\"]\n        MapStore[\"MapStore\"]\n    end\n    \n    subgraph \"API Services\"\n        BackendAPI[\"Backend API Client\"]\n        PredictorAPI[\"Predictor API Client\"]\n    end\n    \n    RootLayout --\u003e MapComponent\n    ModelFormsLayout --\u003e MapComponent\n    MapComponent --\u003e PredictionLayers\n    PredictionLayers --\u003e AcceptedPredictionsLayer\n    PredictionLayers --\u003e RejectedPredictionsLayer\n    MapComponent --\u003e TileSource\n    BackendAPI --\u003e AuthProvider\n    BackendAPI --\u003e ModelPredictionStore\n    PredictorAPI --\u003e ModelPredictionStore\n    MapComponent --\u003e MapStore\n```\n\n**Diagram: Frontend Component Architecture**\n\nKey frontend components:\n\n- **Layout Components**: Manage the UI structure and navigation\n- **Map Visualization**: Integrates MapLibre GL for interactive maps with prediction layers\n- **State Management**: Uses Zustand for state management\n- **API Services**: Handles communication with backend and prediction APIs\n\nSources: [frontend/src/components/layouts/root-layout.tsx:1-114](), [frontend/package.json:1-91](), [frontend/src/hooks/use-map-instance.ts:1-71](), [frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx:1-70](), [frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx:1-70](), [frontend/src/store/model-prediction-store.ts:1-40]()\n\n### 2. Backend System\n\nThe backend is built with Django and Django REST Framework, providing a set of RESTful APIs for managing datasets, models, training, and predictions.\n\n```mermaid\nflowchart TD\n    subgraph \"API Layer\"\n        DatasetViewSet[\"DatasetViewSet\"]\n        AOIViewSet[\"AOIViewSet\"]\n        ModelViewSet[\"ModelViewSet\"]\n        TrainingViewSet[\"TrainingViewSet\"]\n        FeedbackViewSet[\"FeedbackViewSet\"]\n        PredictionView[\"PredictionView\"]\n        RawdataApiAOIView[\"RawdataApiAOIView\"]\n    end\n    \n    subgraph \"Data Models\"\n        Dataset[\"Dataset\"]\n        AOI[\"AOI (Area of Interest)\"]\n        Label[\"Label\"]\n        Model[\"Model\"]\n        Training[\"Training\"]\n        Feedback[\"Feedback\"]\n    end\n    \n    subgraph \"Authentication\"\n        OsmAuthentication[\"OSMAuthentication\"]\n    end\n    \n    DatasetViewSet --\u003e Dataset\n    AOIViewSet --\u003e AOI\n    ModelViewSet --\u003e Model\n    TrainingViewSet --\u003e Training\n    FeedbackViewSet --\u003e Feedback\n    RawdataApiAOIView --\u003e AOI\n    \n    AOI --\u003e Dataset\n    Label --\u003e AOI\n    Model --\u003e Dataset\n    Training --\u003e Model\n    Feedback --\u003e Training\n    \n    DatasetViewSet --\u003e OsmAuthentication\n    AOIViewSet --\u003e OsmAuthentication\n    ModelViewSet --\u003e OsmAuthentication\n    TrainingViewSet --\u003e OsmAuthentication\n    FeedbackViewSet --\u003e OsmAuthentication\n    PredictionView --\u003e OsmAuthentication\n```\n\n**Diagram: Backend Core Components**\n\nKey backend components:\n\n- **API Layer**: ViewSets and API views that handle HTTP requests\n- **Data Models**: Django models that define the database schema\n- **Authentication**: Integration with OpenStreetMap for authentication\n\nSources: [backend/core/views.py:42-282](), [backend/core/models.py:1-213](), [backend/core/serializers.py:1-100](), [backend/core/urls.py:1-90](), [backend/aiproject/urls.py:1-59]()\n\n### 3. Processing System\n\nThe processing system handles computationally intensive tasks such as model training and prediction generation using Celery for task queuing and execution.\n\n```mermaid\nflowchart TD\n    subgraph \"Task Queue\"\n        Redis[\"Redis Message Broker\"]\n        CeleryApp[\"Celery Application\"]\n    end\n    \n    subgraph \"Task Definitions\"\n        TrainModel[\"train_model Task\"]\n        ProcessRawData[\"process_rawdata_task\"]\n    end\n    \n    subgraph \"Training Engine\"\n        Trainer[\"Trainer Class\"]\n        RAMP[\"RAMP Training\"]\n        YOLO[\"YOLO Training\"]\n    end\n    \n    subgraph \"Data Processing\"\n        PrepareData[\"prepare_data\"]\n        ImageryDownload[\"download_imagery\"]\n        S3Uploader[\"S3Uploader\"]\n    end\n    \n    TrainingViewSet[\"TrainingViewSet\"] --\u003e CeleryApp\n    CeleryApp --\u003e Redis\n    Redis --\u003e TrainModel\n    \n    TrainModel --\u003e Trainer\n    Trainer --\u003e RAMP\n    Trainer --\u003e YOLO\n    \n    TrainModel --\u003e PrepareData\n    PrepareData --\u003e ImageryDownload\n    TrainModel --\u003e S3Uploader\n    \n    AOIViewSet[\"AOIViewSet\"] --\u003e ProcessRawData\n```\n\n**Diagram: Asynchronous Processing System**\n\nThe processing system consists of:\n\n- **Task Queue**: Redis serves as the message broker for Celery\n- **Task Definitions**: Celery tasks for model training and data processing\n- **Training Engine**: Components for RAMP and YOLO model training\n- **Data Processing**: Utilities for preparing data and uploading results\n\nSources: [backend/core/tasks.py:38-468](), [backend/core/views.py:203-232](), [backend/core/utils.py:192-586](), [backend/aiproject/settings.py:215-241]()\n\n### 4. Data Storage\n\nfAIr uses multiple storage systems to manage different types of data:\n\n```mermaid\nflowchart TD\n    subgraph \"Database Storage\"\n        PostgreSQL[(\"PostgreSQL/PostGIS\")]\n        subgraph \"Main Tables\"\n            Dataset[\"Dataset Table\"]\n            AOI[\"AOI Table\"]\n            Label[\"Label Table\"]\n            Model[\"Model Table\"]\n            Training[\"Training Table\"]\n            Feedback[\"Feedback Table\"]\n            User[\"OsmUser Table\"]\n        end\n    end\n    \n    subgraph \"File Storage\"\n        S3[(\"S3 Bucket\")]\n        subgraph \"S3 Structure\"\n            TrainingData[\"Training Data\"]\n            ModelCheckpoints[\"Model Checkpoints\"]\n            InputImagery[\"Input Imagery\"]\n            Preprocessed[\"Preprocessed Data\"]\n        end\n    end\n    \n    subgraph \"Cache Storage\"\n        Redis[(\"Redis\")]\n        TaskQueue[\"Task Queue\"]\n        Results[\"Task Results\"]\n    end\n    \n    PostgreSQL --\u003e Dataset\n    PostgreSQL --\u003e AOI\n    PostgreSQL --\u003e Label\n    PostgreSQL --\u003e Model\n    PostgreSQL --\u003e Training\n    PostgreSQL --\u003e Feedback\n    PostgreSQL --\u003e User\n    \n    S3 --\u003e TrainingData\n    S3 --\u003e ModelCheckpoints\n    S3 --\u003e InputImagery\n    S3 --\u003e Preprocessed\n    \n    Redis --\u003e TaskQueue\n    Redis --\u003e Results\n```\n\n**Diagram: Data Storage Architecture**\n\nThe data storage consists of:\n\n- **PostgreSQL/PostGIS**: Stores structured data, with PostGIS extension for geospatial data\n- **S3 Storage**: Stores model artifacts, training data, and imagery\n- **Redis**: Serves as a message broker and result backend for Celery tasks\n\nSources: [backend/aiproject/settings.py:163-168](), [backend/core/models.py:1-213](), [backend/core/utils.py:34-143](), [backend/aiproject/settings.py:59-66]()\n\n## Data Flow and Interactions\n\nThe following sequence diagram illustrates the end-to-end flow for model training and prediction:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Frontend\n    participant BackendAPI\n    participant CeleryWorker\n    participant S3Storage\n    \n    User-\u003e\u003eFrontend: Create model\n    Frontend-\u003e\u003eBackendAPI: POST /api/v1/model/\n    BackendAPI-\u003e\u003eBackendAPI: Create model entry\n    BackendAPI--\u003e\u003eFrontend: Return model ID\n    \n    User-\u003e\u003eFrontend: Define training area (AOI)\n    Frontend-\u003e\u003eBackendAPI: POST /api/v1/aoi/\n    BackendAPI-\u003e\u003eBackendAPI: Create AOI entry\n    BackendAPI--\u003e\u003eFrontend: Return AOI ID\n    \n    User-\u003e\u003eFrontend: Fetch OSM data for labels\n    Frontend-\u003e\u003eBackendAPI: POST /api/v1/label/osm/fetch/{aoi_id}/\n    BackendAPI-\u003e\u003eBackendAPI: Enqueue process_rawdata_task\n    BackendAPI-\u003e\u003eRawDataAPI: Request OSM features\n    RawDataAPI--\u003e\u003eBackendAPI: Return OSM data\n    BackendAPI-\u003e\u003eBackendAPI: Create label entries\n    BackendAPI--\u003e\u003eFrontend: Return success\n    \n    User-\u003e\u003eFrontend: Start training\n    Frontend-\u003e\u003eBackendAPI: POST /api/v1/training/\n    BackendAPI-\u003e\u003eCeleryWorker: Enqueue train_model task\n    BackendAPI--\u003e\u003eFrontend: Return training ID\n    \n    CeleryWorker-\u003e\u003eCeleryWorker: Download imagery\n    CeleryWorker-\u003e\u003eCeleryWorker: Prepare training data\n    CeleryWorker-\u003e\u003eCeleryWorker: Train model (RAMP/YOLO)\n    CeleryWorker-\u003e\u003eS3Storage: Upload model artifacts\n    CeleryWorker-\u003e\u003eBackendAPI: Update training status\n    \n    User-\u003e\u003eFrontend: View map area for predictions\n    Frontend-\u003e\u003eBackendAPI: POST /api/v1/prediction/\n    BackendAPI-\u003e\u003eBackendAPI: Load model \u0026 generate predictions\n    BackendAPI--\u003e\u003eFrontend: Return predictions\n    Frontend-\u003e\u003eFrontend: Display predictions on map\n    \n    User-\u003e\u003eFrontend: Submit feedback on predictions\n    Frontend-\u003e\u003eBackendAPI: POST /api/v1/feedback/\n    BackendAPI-\u003e\u003eBackendAPI: Store feedback\n    BackendAPI--\u003e\u003eFrontend: Return success\n```\n\n**Diagram: Model Training and Prediction Flow**\n\nThis sequence diagram shows the complete process of:\n1. Creating a model\n2. Defining training areas\n3. Obtaining training labels from OpenStreetMap data\n4. Training the model\n5. Generating predictions\n6. Providing feedback on predictions\n\nSources: [backend/core/views.py:92-216](), [backend/core/views.py:244-284](), [backend/core/views.py:380-611](), [backend/core/tasks.py:380-468]()\n\n## Deployment Architecture\n\nfAIr is containerized using Docker and can be deployed in various environments:\n\n```mermaid\nflowchart TD\n    subgraph \"Development Environment\"\n        DockerCompose[\"Docker Compose\"]\n        DevAPI[\"API Container\"]\n        DevWorker[\"Worker Container\"]\n        DevDB[\"PostgreSQL Container\"]\n        DevRedis[\"Redis Container\"]\n        DevFrontend[\"Frontend Container\"]\n    end\n    \n    subgraph \"Production Environment\"\n        S3Bucket[\"S3 Bucket (Frontend)\"]\n        APIContainer[\"API Container\"]\n        WorkerContainer[\"Worker Container\"]\n        ManagedDB[\"Managed PostgreSQL\"]\n        ManagedRedis[\"Managed Redis\"]\n    end\n    \n    subgraph \"CI/CD Pipeline\"\n        GithubActions[\"GitHub Actions\"]\n        DockerRegistry[\"GitHub Container Registry\"]\n        UnitTests[\"Unit Tests\"]\n        BuildSteps[\"Build Steps\"]\n    end\n    \n    DockerCompose --\u003e DevAPI\n    DockerCompose --\u003e DevWorker\n    DockerCompose --\u003e DevDB\n    DockerCompose --\u003e DevRedis\n    DockerCompose --\u003e DevFrontend\n    \n    GithubActions --\u003e UnitTests\n    GithubActions --\u003e BuildSteps\n    BuildSteps --\u003e DockerRegistry\n    BuildSteps --\u003e S3Bucket\n    \n    DockerRegistry --\u003e APIContainer\n    DockerRegistry --\u003e WorkerContainer\n    \n    APIContainer --\u003e ManagedDB\n    APIContainer --\u003e ManagedRedis\n    WorkerContainer --\u003e ManagedDB\n    WorkerContainer --\u003e ManagedRedis\n```\n\n**Diagram: Deployment Architecture**\n\nThe deployment architecture consists of:\n\n- **Development**: Docker Compose setup with all components running locally\n- **Production**: Containers deployed to production infrastructure with managed services\n- **CI/CD**: GitHub Actions for testing, building, and deploying\n\nSources: [docker-compose.yml:1-81](), [docker-compose-cpu.yml:1-54](), [.github/workflows/frontend_build_push.yml:1-71](), [.github/workflows/docker_publish_image.yml:1-102](), [.github/workflows/frontend_build.yml:1-54](), [backend/Dockerfile:1-38]()\n\n## Key Technologies\n\n| Component | Technology | Purpose |\n|-----------|------------|---------|\n| Frontend | React, TypeScript | User interface |\n| Map Visualization | MapLibre GL | Interactive mapping |\n| Backend API | Django, Django REST Framework | RESTful API endpoints |\n| Authentication | OSM OAuth | User authentication |\n| Database | PostgreSQL, PostGIS | Structured and geospatial data storage |\n| Task Queue | Celery, Redis | Asynchronous task processing |\n| Model Training | TensorFlow, YOLO | AI model frameworks |\n| File Storage | AWS S3 | Model artifacts and imagery storage |\n| Containerization | Docker | Application packaging and deployment |\n| CI/CD | GitHub Actions | Automated testing and deployment |\n\nSources: [frontend/package.json:16-50](), [backend/requirements.txt:1-7](), [backend/api-requirements.txt:1-46](), [backend/aiproject/settings.py:89-108]()\n\n## Model Types and Training Support\n\nfAIr supports multiple types of AI models for object detection in aerial imagery:\n\n```mermaid\nflowchart LR\n    subgraph \"Supported Models\"\n        RAMP[\"RAMP\"]\n        YOLOV8_V1[\"YOLOV8_V1\"]\n        YOLOV8_V2[\"YOLOV8_V2\"]\n    end\n    \n    subgraph \"Training System\"\n        Trainer[\"Trainer Class\"]\n        TrainRAMP[\"_train_ramp Method\"]\n        TrainYOLO[\"_train_yolo Method\"]\n    end\n    \n    subgraph \"Infrastructure\"\n        RAMP_HOME[\"RAMP_HOME\"]\n        YOLO_HOME[\"YOLO_HOME\"]\n        TRAINING_WORKSPACE[\"TRAINING_WORKSPACE\"]\n    end\n    \n    RAMP --\u003e TrainRAMP\n    YOLOV8_V1 --\u003e TrainYOLO\n    YOLOV8_V2 --\u003e TrainYOLO\n    \n    Trainer --\u003e TrainRAMP\n    Trainer --\u003e TrainYOLO\n    \n    TrainRAMP --\u003e RAMP_HOME\n    TrainYOLO --\u003e YOLO_HOME\n    \n    TrainRAMP --\u003e TRAINING_WORKSPACE\n    TrainYOLO --\u003e TRAINING_WORKSPACE\n```\n\n**Diagram: Model Training Architecture**\n\nKey aspects of the model training system:\n\n- **RAMP**: Rapid Assessment of Mapping of Parameters model\n- **YOLO V8**: You Only Look Once version 8 model, with two variants (V1 and V2)\n- **Training System**: A unified Trainer class that handles both model types\n- **Infrastructure**: Environment variables configuring training workspace and model homes\n\nSources: [backend/core/models.py:64-68](), [backend/core/tasks.py:99-290](), [backend/aiproject/settings.py:243-252]()"])</script><script>self.__next_f.push([1,"19:T5c93,"])</script><script>self.__next_f.push([1,"# Key Concepts and Terminology\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/requirements.txt](backend/requirements.txt)\n- [frontend/src/components/ui/form/help-text/help-text.tsx](frontend/src/components/ui/form/help-text/help-text.tsx)\n- [frontend/src/constants/ui-contents/models-content.ts](frontend/src/constants/ui-contents/models-content.ts)\n- [frontend/src/constants/ui-contents/start-mapping-content.ts](frontend/src/constants/ui-contents/start-mapping-content.ts)\n- [frontend/src/features/model-creation/components/progress-bar.tsx](frontend/src/features/model-creation/components/progress-bar.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/create-new.tsx](frontend/src/features/model-creation/components/training-dataset/create-new.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/select-existing.tsx](frontend/src/features/model-creation/components/training-dataset/select-existing.tsx)\n- [frontend/src/features/start-mapping/components/map/legend-control.tsx](frontend/src/features/start-mapping/components/map/legend-control.tsx)\n- [frontend/src/features/start-mapping/components/map/map.tsx](frontend/src/features/start-mapping/components/map/map.tsx)\n- [frontend/src/types/api.ts](frontend/src/types/api.ts)\n- [frontend/src/types/ui-contents.ts](frontend/src/types/ui-contents.ts)\n- [frontend/src/utils/__tests__/geo/geometry-utils.test.ts](frontend/src/utils/__tests__/geo/geometry-utils.test.ts)\n\n\u003c/details\u003e\n\n\n\nThis document defines and explains the fundamental concepts and technical terminology used throughout the fAIr AI-assisted mapping system. Understanding these terms is essential for both developers working with the codebase and users who want to understand how the system operates at a technical level. For information about the system architecture, see [System Architecture](#1.1).\n\n## Core System Entities\n\nThe fAIr system is built around several key entities that form the foundation of the application. These entities are represented as Django models in the backend and have corresponding TypeScript interfaces in the frontend.\n\n### Entity Relationship Diagram\n\n```mermaid\ngraph TD\n    User[\"OsmUser\n    - osm_id\n    - username\"]\n    \n    Dataset[\"Dataset\n    - name\n    - source_imagery\n    - offset\"]\n    \n    AOI[\"AOI (Area of Interest)\n    - geom\n    - label_status\"]\n    \n    Label[\"Label\n    - geom\n    - osm_id\n    - tags\"]\n    \n    Model[\"Model\n    - name\n    - base_model\n    - published_training\n    - status\"]\n    \n    Training[\"Training\n    - epochs\n    - batch_size\n    - zoom_level\n    - accuracy\n    - status\"]\n    \n    Feedback[\"Feedback\n    - geom\n    - feedback_type\n    - comments\"]\n    \n    FeedbackAOI[\"FeedbackAOI\n    - geom\n    - label_status\"]\n    \n    FeedbackLabel[\"FeedbackLabel\n    - geom\n    - osm_id\n    - tags\"]\n    \n    ApprovedPredictions[\"ApprovedPredictions\n    - geom\n    - config\"]\n    \n    User --\u003e|creates| Dataset\n    User --\u003e|creates| Model\n    User --\u003e|submits| Training\n    User --\u003e|provides| Feedback\n    User --\u003e|approves| ApprovedPredictions\n    \n    Dataset --\u003e|contains| AOI\n    AOI --\u003e|contains| Label\n    \n    Model --\u003e|trains on| Dataset\n    Model --\u003e|has| Training\n    \n    Training --\u003e|receives| Feedback\n    Training --\u003e|generates| ApprovedPredictions\n    \n    Feedback --\u003e|creates| FeedbackAOI\n    FeedbackAOI --\u003e|contains| FeedbackLabel\n```\n\nSources: [backend/core/models.py:11-212](). [frontend/src/types/api.ts:1-237]()\n\n### Dataset\n\nA Dataset represents a collection of geographical data used for training AI models. It serves as a container for Areas of Interest (AOIs) and their associated labels.\n\nKey attributes:\n- **name**: A descriptive name\n- **source_imagery**: URL to the imagery tiles (TMS URL format)\n- **status**: Current state (ACTIVE=0, ARCHIVED=1, DRAFT=-1)\n- **offset**: [x, y] coordinates to adjust for imagery misalignment\n\n```python\n# From backend/core/models.py\nclass Dataset(models.Model):\n    class DatasetStatus(models.IntegerChoices):\n        ARCHIVED = 1\n        ACTIVE = 0\n        DRAFT = -1\n\n    name = models.CharField(max_length=50)\n    user = models.ForeignKey(OsmUser, to_field=\"osm_id\", on_delete=models.CASCADE)\n    source_imagery = models.URLField(blank=True, null=True)\n    status = models.IntegerField(default=-1, choices=DatasetStatus.choices)\n    offset = ArrayField(\n        base_field=models.FloatField(),\n        size=2,\n        default=[0.0, 0.0],\n        verbose_name=\"Imagery Offset [x, y]\",\n    )\n```\n\nSources: [backend/core/models.py:11-37]()\n\n### Area of Interest (AOI)\n\nAn Area of Interest (AOI) defines a specific geographical region within a Dataset where training data is collected.\n\nKey attributes:\n- **geom**: Geographic polygon defining the area (stored as PolygonField)\n- **label_status**: Status of label collection (DOWNLOADED=1, NOT_DOWNLOADED=-1, RUNNING=0)\n- **label_fetched**: Timestamp when labels were last fetched\n\n```python\n# From backend/core/models.py\nclass AOI(models.Model):\n    class DownloadStatus(models.IntegerChoices):\n        DOWNLOADED = 1\n        NOT_DOWNLOADED = -1\n        RUNNING = 0\n\n    dataset = models.ForeignKey(Dataset, to_field=\"id\", on_delete=models.CASCADE)\n    geom = geomodels.PolygonField(srid=4326)\n    label_status = models.IntegerField(default=-1, choices=DownloadStatus.choices)\n    label_fetched = models.DateTimeField(null=True, blank=True)\n```\n\nSources: [backend/core/models.py:40-52]()\n\n### Label\n\nLabels are the actual building outlines or features within Areas of Interest, used as ground truth for training models.\n\nKey attributes:\n- **geom**: Geographic shape of the feature (stored as GeometryField)\n- **osm_id**: OpenStreetMap ID if imported from OSM\n- **tags**: Associated metadata (JSON format)\n\n```python\n# From backend/core/models.py\nclass Label(models.Model):\n    aoi = models.ForeignKey(AOI, to_field=\"id\", on_delete=models.CASCADE)\n    geom = geomodels.GeometryField(srid=4326)\n    osm_id = models.BigIntegerField(null=True, blank=True)\n    tags = models.JSONField(null=True, blank=True)\n```\n\nSources: [backend/core/models.py:55-61]()\n\n### Model\n\nA Model represents an AI model trained to detect features (primarily buildings) in satellite or aerial imagery.\n\nKey attributes:\n- **name**: Descriptive name\n- **base_model**: Foundation model type (RAMP, YOLO_V8_V1, YOLO_V8_V2)\n- **status**: Current state (PUBLISHED=0, ARCHIVED=1, DRAFT=-1)\n- **published_training**: ID of the currently published training\n\n```python\n# From backend/core/models.py\nclass Model(models.Model):\n    BASE_MODEL_CHOICES = (\n        (\"RAMP\", \"RAMP\"),\n        (\"YOLO_V8_V1\", \"YOLO_V8_V1\"),\n        (\"YOLO_V8_V2\", \"YOLO_V8_V2\"),\n    )\n\n    class ModelStatus(models.IntegerChoices):\n        ARCHIVED = 1\n        PUBLISHED = 0\n        DRAFT = -1\n\n    dataset = models.ForeignKey(Dataset, to_field=\"id\", on_delete=models.DO_NOTHING)\n    name = models.CharField(max_length=50)\n    description = models.TextField(max_length=4000, null=True, blank=True)\n    user = models.ForeignKey(OsmUser, to_field=\"osm_id\", on_delete=models.CASCADE)\n    published_training = models.PositiveIntegerField(null=True, blank=True)\n    status = models.IntegerField(default=-1, choices=ModelStatus.choices)\n    base_model = models.CharField(\n        choices=BASE_MODEL_CHOICES, default=\"RAMP\", max_length=50\n    )\n```\n\nSources: [backend/core/models.py:63-87]()\n\n### Training\n\nTraining represents a specific instance of training a Model with particular parameters.\n\nKey attributes:\n- **zoom_level**: Array of zoom levels used for imagery\n- **epochs**: Number of training iterations\n- **batch_size**: Number of samples processed in each step\n- **accuracy**: Achieved training accuracy\n- **status**: Current state (SUBMITTED, RUNNING, FINISHED, FAILED)\n- **task_id**: ID of the asynchronous task running the training\n\n```python\n# From backend/core/models.py\nclass Training(models.Model):\n    STATUS_CHOICES = (\n        (\"SUBMITTED\", \"SUBMITTED\"),\n        (\"RUNNING\", \"RUNNING\"),\n        (\"FINISHED\", \"FINISHED\"),\n        (\"FAILED\", \"FAILED\"),\n    )\n    model = models.ForeignKey(Model, to_field=\"id\", on_delete=models.CASCADE)\n    source_imagery = models.URLField(blank=True, null=True)\n    description = models.TextField(max_length=500, null=True, blank=True)\n    status = models.CharField(\n        choices=STATUS_CHOICES, default=\"SUBMITTED\", max_length=10\n    )\n    task_id = models.CharField(null=True, blank=True, max_length=100)\n    zoom_level = ArrayField(\n        models.PositiveIntegerField(),\n        size=4,\n    )\n    accuracy = models.FloatField(null=True, blank=True)\n    epochs = models.PositiveIntegerField()\n    chips_length = models.PositiveIntegerField(default=0)\n    batch_size = models.PositiveIntegerField()\n    freeze_layers = models.BooleanField(default=False)\n```\n\nSources: [backend/core/models.py:88-116](), [backend/core/serializers.py:124-241]()\n\n### Feedback\n\nFeedback represents user input on model predictions, helping to improve model accuracy.\n\nKey attributes:\n- **geom**: Geometry of the predicted feature being evaluated\n- **feedback_type**: Type of feedback (TP=True Positive, FP=False Positive, TN=True Negative, FN=False Negative)\n- **comments**: Optional user comments\n\n```python\n# From backend/core/models.py\nclass Feedback(models.Model):\n    FEEDBACK_TYPE = (\n        (\"TP\", \"True Positive\"),\n        (\"TN\", \"True Negative\"),\n        (\"FP\", \"False Positive\"),\n        (\"FN\", \"False Negative\"),\n    )\n    geom = geomodels.GeometryField(srid=4326)\n    training = models.ForeignKey(Training, to_field=\"id\", on_delete=models.CASCADE)\n    feedback_type = models.CharField(choices=FEEDBACK_TYPE, max_length=10)\n    comments = models.TextField(max_length=100, null=True, blank=True)\n    user = models.ForeignKey(OsmUser, to_field=\"osm_id\", on_delete=models.CASCADE)\n```\n\nSources: [backend/core/models.py:118-135]()\n\n### Approved Predictions\n\nApproved Predictions are model predictions that have been validated by users.\n\nKey attributes:\n- **geom**: Geometry of the approved prediction\n- **config**: Configuration used to generate the prediction\n\n```python\n# From backend/core/models.py\nclass ApprovedPredictions(models.Model):\n    training = models.ForeignKey(Training, to_field=\"id\", on_delete=models.DO_NOTHING)\n    config = models.JSONField(null=True, blank=True)\n    geom = geomodels.GeometryField(srid=4326)\n    user = models.ForeignKey(OsmUser, to_field=\"osm_id\", on_delete=models.CASCADE)\n```\n\nSources: [backend/core/models.py:164-174]()\n\n## Machine Learning Concepts\n\n### Base Models\n\nThe system supports several base models for building detection:\n\n| Base Model   | Description                                                             | Characteristics                                  |\n|--------------|-------------------------------------------------------------------------|--------------------------------------------------|\n| RAMP         | Rapid Mapping model (default)                                           | Fast training, good accuracy for basic buildings |\n| YOLO_V8_V1   | You Only Look Once version 8 (variant 1)                                | Well-balanced with good general accuracy         |\n| YOLO_V8_V2   | You Only Look Once version 8 (variant 2)                                | Advanced, designed for diverse feature detection |\n\nEach base model requires specific configurations and offers different trade-offs between speed and accuracy.\n\nSources: [backend/core/models.py:63-69](), [backend/aiproject/settings.py:70-80](), [frontend/src/constants/ui-contents/models-content.ts:37-44]()\n\n### Training Workflow\n\n```mermaid\nflowchart TD\n    U[\"User (Frontend)\"] --\u003e|\"Creates model + dataset\"| MV[\"ModelViewSet\n    (backend/core/views.py)\"]\n    MV --\u003e|\"Submits training\"| TV[\"TrainingViewSet\n    (backend/core/views.py)\"]\n    TV --\u003e|\"Enqueues task\"| TM[\"train_model\n    (backend/core/tasks.py)\"]\n    \n    subgraph \"Training Process\"\n        TM --\u003e|\"Calls\"| PD[\"prepare_data()\"]\n        PD --\u003e|\"Processes\"| T[\"Trainer Class\"]\n        \n        subgraph \"Model-Specific Training\"\n            T --\u003e|\"RAMP\"| TR[\"_train_ramp()\"]\n            T --\u003e|\"YOLO\"| TY[\"_train_yolo()\"]\n        end\n        \n        T --\u003e|\"Returns results\"| F[\"finalize()\"]\n        F --\u003e|\"Uploads model\"| S3[\"S3 Storage\"]\n    end\n    \n    TM --\u003e|\"Updates\"| Training[\"Training Model\n    (backend/core/models.py)\"]\n    \n    Training --\u003e|\"Can be published\"| PT[\"publish_training \n    (backend/core/views.py)\"]\n```\n\nSources: [backend/core/tasks.py:99-467](), [backend/core/views.py:93-264](), [backend/core/views.py:885-914]()\n\nThe training workflow involves:\n\n1. **Dataset Preparation**: User creates a dataset and defines AOIs, then fetches or uploads labels.\n2. **Training Configuration**: User creates a model and configures training parameters (epochs, batch size, etc.).\n3. **Training Submission**: The `TrainingViewSet` receives the request and enqueues an asynchronous task.\n4. **Asynchronous Processing**: The `train_model` Celery task:\n   - Downloads imagery for AOIs from the specified source\n   - Prepares labels and imagery data\n   - Invokes the appropriate training method (RAMP or YOLO)\n   - Saves the trained model artifacts\n   - Updates the training status and metadata\n5. **Model Publishing**: User can publish a successful training to make it available for predictions.\n\nSources: [backend/core/tasks.py:380-467]()\n\n### Prediction Workflow\n\n```mermaid\nflowchart TD\n    U[\"User (Frontend)\"] --\u003e|\"Selects model + area\"| PV[\"PredictionView\n    (backend/core/views.py)\"]\n    PV --\u003e|\"Loads model\"| MP[\"Model Path\"]\n    PV --\u003e|\"Calls\"| P[\"predict() \n    (predictor module)\"]\n    \n    P --\u003e|\"Downloads imagery\"| TMS[\"TMS Imagery\"]\n    P --\u003e|\"Processes\"| IMG[\"Image Tiles\"]\n    P --\u003e|\"Performs inference\"| Model[\"Loaded Model\"]\n    P --\u003e|\"Vectorizes results\"| G[\"GeoJSON Features\"]\n    \n    subgraph \"Optional Post-Processing\"\n        G --\u003e|\"If use_josm_q=true\"| O[\"orthogonalize_poly()\"]\n    end\n    \n    PV --\u003e|\"Returns\"| GJ[\"GeoJSON Features\"]\n    U --\u003e|\"Reviews predictions\"| FB[\"Feedback\n    (Accept/Reject)\"]\n    FB --\u003e|\"Submits\"| FV[\"FeedbackView/\n    ApprovedPredictionsViewSet\"]\n```\n\nSources: [backend/core/views.py:786-883](), [backend/core/views.py:508-543](), [backend/core/views.py:717-777]()\n\nThe prediction workflow involves:\n\n1. **Model Selection**: User selects a published model to use for predictions.\n2. **Area Definition**: User zooms to an area of interest on the map.\n3. **Parameter Configuration**: User configures prediction parameters:\n   - **confidence**: Minimum confidence threshold (0-100%)\n   - **use_josm_q**: Option to orthogonalize building shapes\n   - **max_angle_change**: Maximum angle change for orthogonalization\n   - **skew_tolerance**: Tolerance for skewed angles\n4. **Prediction Generation**: The system:\n   - Loads the trained model\n   - Downloads imagery for the selected area\n   - Runs inference on the imagery\n   - Post-processes results (optional orthogonalization)\n   - Returns predictions as GeoJSON features\n5. **User Review**: User reviews and provides feedback on predictions:\n   - **Accept**: Approves predictions for potential OSM import\n   - **Reject**: Flags incorrect predictions\n   - **Comment**: Provides feedback with comments\n\nSources: [frontend/src/types/api.ts:210-227](), [frontend/src/features/start-mapping/components/map/map.tsx:32-174]()\n\n## Model Enhancement and Feedback\n\n### Feedback Mechanism\n\n```mermaid\nflowchart TD\n    U[\"User\"] --\u003e|\"Provides feedback\"| F[\"Feedback\n    (TP/TN/FP/FN)\"]\n    U --\u003e|\"Creates\"| FA[\"FeedbackAOI\"]\n    FA --\u003e|\"Contains\"| FL[\"FeedbackLabel\"]\n    \n    F --\u003e|\"Used in\"| FM[\"FeedbackView\n    (backend/core/views.py)\"]\n    FM --\u003e|\"Creates\"| NT[\"New Training\"]\n    NT --\u003e|\"Uses\"| BT[\"Base Training\"]\n    NT --\u003e|\"Runs\"| TM[\"train_model with \n    freeze_layers=True\"]\n    \n    TM --\u003e|\"Produces\"| EM[\"Enhanced Model\"]\n    \n    U --\u003e|\"Can publish\"| EM\n```\n\nSources: [backend/core/views.py:717-777](), [backend/core/tasks.py:380-467]()\n\nThe feedback mechanism allows users to:\n\n1. **Provide direct feedback** on predictions (True Positive, False Positive, etc.)\n2. **Create FeedbackAOIs** with additional training examples\n3. **Fetch OSM data** for feedback areas using `RawdataApiFeedbackView`\n4. **Submit a feedback-based training** that enhances the existing model\n\nThis feedback loop is crucial for improving model accuracy over time through continuous learning.\n\nSources: [backend/core/views.py:278-322](), [backend/core/views.py:546-575]()\n\n## Technical Configuration Parameters\n\n### Training Parameters\n\n| Parameter | Description | Default/Range | Configuration Location |\n|-----------|-------------|--------------|------------------------|\n| epochs | Number of complete training iterations | RAMP: 40 max, YOLO: 200 max | aiproject/settings.py |\n| batch_size | Number of samples processed per step | Max: 8 | aiproject/settings.py |\n| zoom_level | Zoom levels of imagery used | Typically 19-21 | Model-dependent |\n| freeze_layers | Whether to freeze base model layers | true/false | Training-specific |\n| multimasks | Use multiple masking techniques | true/false | Training-specific |\n\n### Prediction Parameters\n\n| Parameter | Description | Default/Range | Configuration Location |\n|-----------|-------------|--------------|------------------------|\n| confidence | Minimum confidence threshold | 0-100% | UI configurable |\n| use_josm_q | Orthogonalize building shapes | true/false | UI configurable |\n| max_angle_change | Maximum angle change in orthogonalization | 0-45 degrees | UI configurable |\n| skew_tolerance | Tolerance for skewed angles | 0-45 degrees | UI configurable |\n| tile_overlap_distance | Overlap between adjacent tiles | 0.0-1.0 | UI configurable |\n\nSources: [backend/aiproject/settings.py:70-84](), [backend/core/serializers.py:305-449](), [frontend/src/constants/ui-contents/start-mapping-content.ts:41-72]()\n\n## Frontend Components\n\n### Map Visualization\n\nThe map visualization system is built around several key components:\n\n```mermaid\ngraph TD\n    SMM[\"StartMappingMapComponent\"] --\u003e|contains| MC[\"MapComponent\"]\n    MC --\u003e|displays| OAM[\"OpenAerialMap\"]\n    MC --\u003e|contains| PIL[\"PredictionImageryLayer\"]\n    \n    SMM --\u003e|contains| PL[\"PredictionLayers\"]\n    PL --\u003e|includes| APL[\"AcceptedPredictionsLayer\"]\n    PL --\u003e|includes| RPL[\"RejectedPredictionsLayer\"]\n    PL --\u003e|includes| ALLPL[\"AllPredictionsLayer\"]\n    \n    SMM --\u003e|contains| PFAP[\"PredictedFeatureActionPopup\"]\n    SMM --\u003e|contains| L[\"Legend\"]\n    \n    MC --\u003e|contains| MCTL[\"MapCursorToolTip\"]\n```\n\nSources: [frontend/src/features/start-mapping/components/map/map.tsx:32-174](), [frontend/src/features/start-mapping/components/map/legend-control.tsx:1-110]()\n\nKey mapping components include:\n- **MapComponent**: Core map rendering container\n- **OpenAerialMap**: Imagery layer from TMS sources\n- **PredictionLayers**: Container for prediction visualization\n- **AcceptedPredictionsLayer**: Shows accepted predictions\n- **RejectedPredictionsLayer**: Shows rejected predictions\n- **AllPredictionsLayer**: Shows all predictions\n- **PredictedFeatureActionPopup**: UI for interacting with predictions\n- **Legend**: Displays a legend for the map\n\nSources: [frontend/src/features/start-mapping/components/map/map.tsx:159-175]()\n\n### Model Creation Workflow\n\nThe model creation process is a multi-step workflow:\n\n```mermaid\ngraph TD\n    MCF[\"ModelCreationForm\"] --\u003e|step 1| MD[\"ModelDetails\n    - name\n    - base_model\n    - description\"]\n    \n    MD --\u003e|step 2| TD[\"TrainingDataset\n    - create new\n    - select existing\"]\n    \n    TD --\u003e|step 3| TA[\"TrainingArea\n    - draw areas\n    - fetch labels\"]\n    \n    TA --\u003e|step 4| TS[\"TrainingSettings\n    - epochs\n    - batch_size\n    - zoom_level\"]\n    \n    TS --\u003e|step 5| MS[\"ModelSummary\n    - review all settings\"]\n    \n    MS --\u003e|step 6| C[\"Confirmation\n    - training started\"]\n    \n    PB[\"ProgressBar\"] --\u003e|tracks| MCF\n```\n\nSources: [frontend/src/features/model-creation/components/progress-bar.tsx:14-91](), [frontend/src/constants/ui-contents/models-content.ts:21-205]()\n\nThe workflow guides users through:\n1. **Model Details**: Setting basic model information\n2. **Training Dataset**: Creating or selecting a dataset\n3. **Training Area**: Defining geographic areas and fetching labels\n4. **Training Settings**: Configuring training parameters\n5. **Model Summary**: Reviewing all settings\n6. **Confirmation**: Submitting and starting the training process\n\nSources: [frontend/src/features/model-creation/components/training-dataset/create-new.tsx:20-170](), [frontend/src/features/model-creation/components/training-dataset/select-existing.tsx:12-86]()\n\n## API Structure\n\nThe fAIr backend provides a comprehensive RESTful API for managing all aspects of the system:\n\n| API Endpoint | Purpose | View/ViewSet | HTTP Methods |\n|--------------|---------|--------------|--------------|\n| /api/v1/dataset/ | Manage datasets | DatasetViewSet | GET, POST, PUT, DELETE |\n| /api/v1/aoi/ | Manage areas of interest | AOIViewSet | GET, POST, PUT, DELETE |\n| /api/v1/label/ | Manage labels | LabelViewSet | GET, POST, PUT, DELETE |\n| /api/v1/model/ | Manage models | ModelViewSet | GET, POST, PUT, DELETE |\n| /api/v1/training/ | Manage trainings | TrainingViewSet | GET, POST, DELETE |\n| /api/v1/feedback/ | Manage feedback | FeedbackViewSet | GET, POST, PATCH, DELETE |\n| /api/v1/prediction/ | Generate predictions | PredictionView | POST |\n| /api/v1/training/publish/{id}/ | Publish a training | publish_training | POST |\n\nSources: [backend/core/urls.py:38-90](), [backend/aiproject/urls.py:38-58]()\n\n## Environment Configuration\n\nThe system's behavior can be customized through environment variables in `aiproject/settings.py`:\n\n| Environment Variable | Purpose | Default |\n|----------------------|---------|---------|\n| RAMP_EPOCHS_LIMIT | Maximum epochs for RAMP models | 40 |\n| RAMP_BATCH_SIZE_LIMIT | Maximum batch size for RAMP | 8 |\n| YOLO_EPOCHS_LIMIT | Maximum epochs for YOLO models | 200 |\n| YOLO_BATCH_SIZE_LIMIT | Maximum batch size for YOLO | 8 |\n| BUCKET_NAME | S3 bucket for model storage | fair-dev |\n| EXPORT_TOOL_API_URL | URL for raw data API | https://api-prod.raw-data.hotosm.org/v1 |\n| ENABLE_PREDICTION_API | Enable/disable prediction API | False |\n\nSources: [backend/aiproject/settings.py:70-84](), [backend/aiproject/settings.py:254-254]()\n\n## Asynchronous Processing\n\nThe fAIr system uses Celery and Redis for asynchronous processing of computationally intensive tasks:\n\n| Component | Purpose | Configuration |\n|-----------|---------|---------------|\n| Celery Worker | Executes background tasks | CELERY_BROKER_URL |\n| Redis | Message broker and result backend | CELERY_RESULT_BACKEND |\n| train_model | Main training task | Queued based on model type |\n| process_rawdata_task | Processes OSM data | Async task |\n\nSources: [backend/core/tasks.py:380-467](), [backend/aiproject/settings.py:215-232]()\n\nThe asynchronous architecture ensures that resource-intensive operations like model training don't block the main application thread, allowing the UI to remain responsive while these tasks run in the background.\n\nSources: [backend/core/views.py:586-610](), [backend/core/views.py:674-714]()"])</script><script>self.__next_f.push([1,"1a:T30fb,"])</script><script>self.__next_f.push([1,"# Installation and Setup\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [.gitignore](.gitignore)\n- [Readme.md](Readme.md)\n- [backend/Dockerfile](backend/Dockerfile)\n- [backend/Dockerfile_CPU](backend/Dockerfile_CPU)\n- [backend/README.md](backend/README.md)\n- [backend/aiproject/__init__.py](backend/aiproject/__init__.py)\n- [backend/aiproject/asgi.py](backend/aiproject/asgi.py)\n- [backend/aiproject/celery.py](backend/aiproject/celery.py)\n- [backend/aiproject/utils.py](backend/aiproject/utils.py)\n- [backend/aiproject/wsgi.py](backend/aiproject/wsgi.py)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n- [docker-compose-cpu.yml](docker-compose-cpu.yml)\n- [docker-compose.yml](docker-compose.yml)\n- [docs/Docker-installation.md](docs/Docker-installation.md)\n- [frontend/.env.sample](frontend/.env.sample)\n- [frontend/Dockerfile.frontend](frontend/Dockerfile.frontend)\n- [frontend/src/config/env.ts](frontend/src/config/env.ts)\n- [frontend/src/config/index.ts](frontend/src/config/index.ts)\n- [run_migrations.sh](run_migrations.sh)\n- [setup-ramp.sh](setup-ramp.sh)\n\n\u003c/details\u003e\n\n\n\nThis document provides comprehensive instructions for setting up the fAIr AI-assisted mapping system in a development environment. It covers both Docker-based and native installation methods, along with the necessary configuration steps. For information about the overall architecture, refer to [System Architecture](#1.1).\n\n## System Components\n\nBefore diving into installation steps, it's important to understand the main components of the fAIr system:\n\n```mermaid\ngraph TD\n    subgraph \"User Interface\"\n        Frontend[\"React Frontend\\n(Port 3000)\"]\n    end\n    \n    subgraph \"Application Servers\"\n        BackendAPI[\"Django Backend API\\n(Port 8000)\"]\n        WorkerProcess[\"Celery Worker\\n(AI Training)\"]\n        FlowerDashboard[\"Flower Dashboard\\n(Port 5500)\"]\n    end\n    \n    subgraph \"Data Services\"\n        PostgreSQL[(PostgreSQL/PostGIS\\n(Port 5432/5434))]\n        Redis[(Redis\\n(Port 6379))]\n    end\n    \n    subgraph \"External Dependencies\"\n        RAMP[\"RAMP Base Model\"]\n        OSM[\"OpenStreetMap API\"]\n    end\n    \n    Frontend --\u003e BackendAPI\n    BackendAPI --\u003e PostgreSQL\n    BackendAPI --\u003e Redis\n    BackendAPI --\u003e OSM\n    Redis --\u003e WorkerProcess\n    WorkerProcess --\u003e RAMP\n    FlowerDashboard --\u003e Redis\n```\n\nSources: [Readme.md:80-93](), [docker-compose.yml]()\n\n## Prerequisites\n\n### Hardware Requirements\n\n- **Recommended**: NVIDIA GPU with appropriate drivers for model training\n- **CPU-only**: Possible but not recommended for production use\n\n### Software Requirements\n\n- Git\n- Docker and Docker Compose (for Docker installation)\n- Python 3.8+ (for native installation)\n- PostgreSQL with PostGIS extension\n- Redis server\n- Node.js and npm/pnpm (for frontend development)\n\n### External Accounts\n\n- OpenStreetMap account (for authentication)\n\nSources: [Readme.md:22-24](), [docs/Docker-installation.md:20-27](), [backend/README.md:7-13]()\n\n## Docker Installation (Recommended)\n\nDocker provides the easiest way to set up the development environment with all necessary components.\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/hotosm/fAIr.git\ncd fAIr\n```\n\n### 2. Set Up RAMP Base Model\n\nThe RAMP (Rapide Assessment Mapping Protocol) base model is required for training AI models.\n\n```mermaid\ngraph LR\n    subgraph \"RAMP Setup Process\"\n        A[\"Create RAMP Directory\"] --\u003e B[\"Download Base Model\"]\n        B --\u003e C[\"Clone RAMP Code\"]\n        C --\u003e D[\"Extract Checkpoint\"]\n        D --\u003e E[\"Set Environment Variables\"]\n    end\n```\n\nExecute the following commands:\n\n```bash\nmkdir ramp\ngdown --fuzzy https://drive.google.com/file/d/1YQsY61S_rGfJ_f6kLQq4ouYE2l3iRe1k/view\ngit clone https://github.com/kshitijrajsharma/ramp-code-fAIr.git ramp-code\nunzip checkpoint.tf.zip -d ramp-code/ramp\nexport RAMP_HOME=/path/to/ramp\nexport TRAINING_WORKSPACE=/path/to/fAIr/trainings\n```\n\nAlternatively, you can use the provided setup script:\n\n```bash\nbash setup-ramp.sh\nsource .env\n```\n\nSources: [docs/Docker-installation.md:31-64](), [setup-ramp.sh]()\n\n### 3. Register OAuth Application with OpenStreetMap\n\n1. Log in to your OpenStreetMap account at https://www.openstreetmap.org/\n2. Go to \"My Settings\" \u003e \"OAuth2 Applications\"\n3. Register a new application:\n   - Name: \"fAIr Dev Local\"\n   - Redirect URI: http://127.0.0.1:3000/authenticate/\n   - Permissions: \"Read user preferences\"\n4. Save the generated `OSM_CLIENT_ID` and `OSM_CLIENT_SECRET` for later use\n\nSources: [docs/Docker-installation.md:65-74]()\n\n### 4. Configure Environment Variables\n\nCreate backend environment file:\n\n```bash\ncd backend\ncp docker_sample_env .env\n```\n\nEdit the `.env` file to add your OSM credentials and set a random string for `OSM_SECRET_KEY`.\n\nCreate frontend environment file:\n\n```bash\ncd ../frontend\ncp .env.sample .env\n```\n\nSources: [docs/Docker-installation.md:75-92](), [backend/docker_sample_env](), [frontend/.env.sample]()\n\n### 5. Build and Start Docker Containers\n\n```bash\ndocker compose build\ndocker compose up\n```\n\nFor CPU-only systems:\n\n```bash\ndocker compose -f docker-compose-cpu.yml build\ndocker compose -f docker-compose-cpu.yml up\n```\n\nSources: [docs/Docker-installation.md:96-102](), [docker-compose.yml](), [docker-compose-cpu.yml]()\n\n### 6. Run Database Migrations\n\nIn a new terminal window:\n\n```bash\nbash run_migrations.sh\n```\n\nOr manually:\n\n```bash\ndocker exec -it api bash -c \"python manage.py makemigrations\"\ndocker exec -it api bash -c \"python manage.py makemigrations login\"\ndocker exec -it api bash -c \"python manage.py makemigrations core\"\ndocker exec -it api bash -c \"python manage.py migrate\"\n```\n\nSources: [docs/Docker-installation.md:104-125](), [run_migrations.sh]()\n\n### 7. Access the Application\n\n- Frontend: http://127.0.0.1:3000\n- Backend API: http://localhost:8000/api/v1/\n- Flower Dashboard: http://localhost:5500\n\n**Note**: Use `127.0.0.1:3000` instead of `localhost:3000` to ensure login functionality works correctly.\n\nSources: [docs/Docker-installation.md:134-136]()\n\n## Native Installation\n\nIf you prefer not to use Docker, you can install and configure each component separately.\n\n### 1. Backend Setup\n\n#### Install Python and Virtual Environment\n\n```bash\nsudo apt-get install python3 python3-pip\nsudo apt install python3-virtualenv\nvirtualenv env\nsource ./env/bin/activate\n```\n\n#### Install GDAL and Dependencies\n\n```bash\nsudo apt-get update\nsudo add-apt-repository ppa:ubuntugis/ppa\nsudo apt-get update\nsudo apt-get install gdal-bin libgdal-dev python3-gdal\nsudo apt-get install python3-psycopg2\nsudo apt install redis\npip install numpy==1.23.5\npip install GDAL==$(gdal-config --version) --global-option=build_ext --global-option=\"-I/usr/include/gdal\"\n```\n\n#### Set Up RAMP Base Model\n\nFollow the same RAMP setup steps as in the Docker installation section.\n\n#### Install Backend Dependencies\n\n```bash\ncd backend\npip install -r requirements.txt\npip install -r api-requirements.txt\n```\n\n#### Configure Environment Variables\n\n```bash\ncp sample_env .env\n```\n\nEdit the `.env` file to set the following variables:\n\n```\nDATABASE_URL=postgis://admin:password@localhost:5432/ai\nRAMP_HOME=/path/to/ramp\nTRAINING_WORKSPACE=/path/to/training/workspace\nOSM_CLIENT_ID=your_client_id\nOSM_CLIENT_SECRET=your_client_secret\nOSM_SECRET_KEY=your_secret_key\n```\n\n#### Set Up PostgreSQL with PostGIS\n\n```bash\nsudo apt-get install postgresql postgresql-contrib postgis\nsudo -u postgres createuser -P admin\nsudo -u postgres createdb -O admin ai\nsudo -u postgres psql -d ai -c \"CREATE EXTENSION postgis;\"\n```\n\n#### Run Migrations\n\n```bash\npython manage.py makemigrations\npython manage.py makemigrations login\npython manage.py makemigrations core\npython manage.py migrate\n```\n\n#### Start Celery Workers\n\n```bash\ncelery -A aiproject worker --loglevel=debug -Q ramp_training,yolo_training\n```\n\n#### Start Flower Dashboard (Optional)\n\n```bash\ncelery -A aiproject --broker=redis://localhost:6379/ flower\n```\n\n#### Start Django Server\n\n```bash\npython manage.py runserver\n```\n\nSources: [backend/README.md](), [backend/sample_env](), [backend/api-requirements.txt]()\n\n### 2. Frontend Setup\n\n#### Install Node.js and Dependencies\n\n```bash\ncd frontend\nnpm install\n```\n\n#### Configure Environment Variables\n\n```bash\ncp .env.sample .env\n```\n\nEdit the `.env` file to point to your backend API:\n\n```\nVITE_BASE_API_URL=http://localhost:8000/api/v1/\n```\n\n#### Start Development Server\n\n```bash\nnpm run dev\n```\n\nSources: [frontend/.env.sample](), [frontend/src/config/env.ts](), [frontend/src/config/index.ts]()\n\n## Environment Variables Reference\n\n### Backend Environment Variables\n\n| Variable | Description | Example Value |\n|----------|-------------|---------------|\n| `DEBUG` | Enables debug mode | `True` |\n| `SECRET_KEY` | Django secret key | `random_string` |\n| `DATABASE_URL` | PostgreSQL connection URL | `postgis://admin:password@localhost:5432/ai` |\n| `CORS_ALLOWED_ORIGINS` | Allowed CORS origins | `http://localhost:3000` |\n| `OSM_CLIENT_ID` | OSM OAuth client ID | `your_client_id` |\n| `OSM_CLIENT_SECRET` | OSM OAuth client secret | `your_client_secret` |\n| `OSM_SECRET_KEY` | Secret key for OSM integration | `random_string` |\n| `RAMP_HOME` | Path to RAMP directory | `/path/to/ramp` |\n| `TRAINING_WORKSPACE` | Path to training workspace | `/path/to/trainings` |\n| `CELERY_BROKER_URL` | Redis URL for Celery | `redis://localhost:6379/0` |\n\nSources: [backend/sample_env](), [backend/docker_sample_env]()\n\n### Frontend Environment Variables\n\n| Variable | Description | Default Value |\n|----------|-------------|---------------|\n| `VITE_BASE_API_URL` | Backend API URL | `http://localhost:8000/api/v1/` |\n| `VITE_FAIR_PREDICTOR_API_URL` | Model prediction API URL | `https://predictor-dev.fair.hotosm.org/predict/` |\n| `VITE_MAX_TRAINING_AREA_SIZE` | Maximum training area size (sq m) | `5000000` |\n| `VITE_MIN_TRAINING_AREA_SIZE` | Minimum training area size (sq m) | `5797` |\n| `VITE_MAX_ZOOM_LEVEL` | Maximum map zoom level | `22` |\n| `VITE_MIN_ZOOM_LEVEL_FOR_START_MAPPING_PREDICTION` | Min zoom for predictions | `19` |\n\nSources: [frontend/.env.sample](), [frontend/src/config/env.ts](), [frontend/src/config/index.ts:46-51]()\n\n## Verification and Testing\n\n### Backend Testing\n\nTo verify that the backend is working properly, run the included tests:\n\n```bash\ncd backend\npython manage.py test tests\n```\n\nSources: [backend/README.md:180-182](), [backend/tests/test_endpoints.py](), [backend/tests/test_views.py]()\n\n### API Documentation\n\nThe API documentation is available at the Swagger UI endpoint:\n\n```\nhttp://localhost:8000/api/v1/docs/\n```\n\n### Testing Authentication\n\nTo test the authentication flow:\n\n1. Visit `http://localhost:8000/api/v1/auth/login/` to get the login URL\n2. Navigate to the login URL and authenticate with OSM\n3. After successful authentication, check if you can access `http://localhost:8000/api/v1/auth/me/`\n\nSources: [backend/README.md:146-155]()\n\n## Troubleshooting\n\n### GPU Issues\n\nIf you encounter GPU-related errors, ensure:\n\n1. NVIDIA drivers are properly installed\n2. NVIDIA Container Toolkit is installed (for Docker)\n3. GPU is correctly detected by running `nvidia-smi`\n\n### Database Connection Issues\n\nIf you encounter database connection errors:\n\n1. Check that PostgreSQL is running\n2. Verify the `DATABASE_URL` is correct\n3. Ensure the PostGIS extension is installed in the database\n\n### Authorization Issues\n\nIf OpenStreetMap authentication is not working:\n\n1. Verify that your OAuth credentials are correct\n2. Ensure the redirect URI exactly matches what you registered with OSM\n3. Use `127.0.0.1` instead of `localhost` in the URLs\n\nSources: [docs/Docker-installation.md:20-27](), [backend/README.md:146-155]()\n\n## System Testing\n\nAfter completing the installation, you should be able to:\n\n1. Log in with your OSM account\n2. Create a model\n3. Define a training area\n4. Fetch OSM data\n5. Start model training\n6. View predictions on the map\n\nIf any of these steps fail, check the relevant logs:\n\n- Backend logs: `docker logs api`\n- Worker logs: `docker logs worker`\n- Frontend logs: Browser console\n\nSources: [Readme.md:72-79]()"])</script><script>self.__next_f.push([1,"1b:T4224,"])</script><script>self.__next_f.push([1,"# Backend System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [backend/.gitignore](backend/.gitignore)\n- [backend/.pre-commit-config.yaml](backend/.pre-commit-config.yaml)\n- [backend/README.md](backend/README.md)\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/core/admin.py](backend/core/admin.py)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/login/admin.py](backend/login/admin.py)\n- [backend/login/authentication.py](backend/login/authentication.py)\n- [backend/login/permissions.py](backend/login/permissions.py)\n- [backend/login/views.py](backend/login/views.py)\n- [backend/pdm.lock](backend/pdm.lock)\n- [backend/pyproject.toml](backend/pyproject.toml)\n- [backend/requirements.txt](backend/requirements.txt)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/factories.py](backend/tests/factories.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n\n\u003c/details\u003e\n\n\n\nThe Backend System of fAIr provides a comprehensive infrastructure for managing AI-assisted mapping tasks through a robust RESTful API, data persistence, and asynchronous processing capabilities. It serves as the foundation for model training, prediction generation, and data management within the fAIr platform.\n\nFor information about frontend interactions with this backend, see [Frontend System](#3). For details on setting up the backend for development, see [Installation and Setup](#1.3).\n\n## Architecture Overview\n\nThe backend follows a layered architecture with clear separation of concerns:\n\n```mermaid\ngraph TD\n    subgraph \"API Layer\"\n        DatasetViewSet[\"DatasetViewSet\"]\n        ModelViewSet[\"ModelViewSet\"]\n        TrainingViewSet[\"TrainingViewSet\"]\n        AOIViewSet[\"AOIViewSet\"]\n        LabelViewSet[\"LabelViewSet\"]\n        FeedbackViewset[\"FeedbackViewset\"]\n        PredictionView[\"PredictionView\"]\n    end\n    \n    subgraph \"Data Layer\"\n        Dataset[(\"Dataset\")]\n        Model[(\"Model\")]\n        Training[(\"Training\")]\n        AOI[(\"AOI\")]\n        Label[(\"Label\")]\n        Feedback[(\"Feedback\")]\n    end\n    \n    subgraph \"Processing Layer\"\n        TrainModelTask[\"train_model Task\"]\n        Trainer[\"Trainer Class\"]\n        ProcessRawData[\"process_rawdata\"]\n    end\n    \n    subgraph \"Storage Layer\"\n        S3[\"S3 Storage\"]\n        Redis[\"Redis Queue\"]\n        PostgreSQL[\"PostgreSQL/PostGIS\"]\n    end\n    \n    subgraph \"External Services\"\n        OSM[\"OpenStreetMap Auth\"]\n        RawDataAPI[\"Raw Data API\"]\n    end\n    \n    DatasetViewSet --\u003e Dataset\n    ModelViewSet --\u003e Model\n    TrainingViewSet --\u003e Training\n    TrainingViewSet --\u003e TrainModelTask\n    AOIViewSet --\u003e AOI\n    LabelViewSet --\u003e Label\n    FeedbackViewset --\u003e Feedback\n    \n    Dataset --\u003e PostgreSQL\n    Model --\u003e PostgreSQL\n    Training --\u003e PostgreSQL\n    AOI --\u003e PostgreSQL\n    Label --\u003e PostgreSQL\n    Feedback --\u003e PostgreSQL\n    \n    PredictionView --\u003e Model\n    PredictionView --\u003e Training\n    \n    TrainModelTask --\u003e Trainer\n    Trainer --\u003e RAMP[\"RAMP Training\"]\n    Trainer --\u003e YOLO[\"YOLO Training\"]\n    \n    AOIViewSet --\u003e ProcessRawData\n    ProcessRawData --\u003e RawDataAPI\n    \n    TrainModelTask --\u003e S3\n    TrainModelTask --\u003e Redis\n    \n    OSM --\u003e DatasetViewSet\n    OSM --\u003e ModelViewSet\n    OSM --\u003e TrainingViewSet\n    OSM --\u003e AOIViewSet\n    OSM --\u003e LabelViewSet\n    OSM --\u003e FeedbackViewset\n    OSM --\u003e PredictionView\n```\n\nSources: [backend/core/views.py:1-986](), [backend/core/tasks.py:1-468](), [backend/core/models.py:1-213](), [backend/aiproject/settings.py:1-303]()\n\n## Core Components\n\n### API Endpoints\n\nThe backend provides RESTful API endpoints implemented as Django REST Framework ViewSets in `core/views.py`. These endpoints handle client requests for managing datasets, models, training jobs, and predictions.\n\n| Endpoint | ViewSet/View | Description |\n|----------|--------------|-------------|\n| `/dataset/` | `DatasetViewSet` | Manage training datasets |\n| `/model/` | `ModelViewSet` | Manage AI models |\n| `/training/` | `TrainingViewSet` | Manage model training jobs |\n| `/aoi/` | `AOIViewSet` | Manage areas of interest for training |\n| `/label/` | `LabelViewSet` | Manage training labels |\n| `/feedback/` | `FeedbackViewset` | Manage user feedback on predictions |\n| `/prediction/` | `PredictionView` | Generate predictions using trained models |\n| `/auth/` | Auth views in `login/views.py` | Handle OSM authentication |\n\nEach ViewSet provides standard CRUD operations for its resource, with additional specialized endpoints for specific actions like fetching OSM data or publishing trained models.\n\n```mermaid\nclassDiagram\n    class DatasetViewSet {\n        +queryset: Dataset.objects.all()\n        +serializer_class: DatasetSerializer\n        +list()\n        +retrieve()\n        +create()\n        +update()\n        +destroy()\n    }\n    \n    class ModelViewSet {\n        +queryset: Model.objects.all()\n        +serializer_class: ModelSerializer\n        +list()\n        +retrieve()\n        +create()\n        +update()\n        +destroy()\n    }\n    \n    class TrainingViewSet {\n        +queryset: Training.objects.all()\n        +serializer_class: TrainingSerializer\n        +list()\n        +retrieve()\n        +create()\n        +destroy()\n    }\n    \n    class PredictionView {\n        +post()\n    }\n    \n    class AOIViewSet {\n        +queryset: AOI.objects.all()\n        +serializer_class: AOISerializer\n        +list()\n        +retrieve()\n        +create()\n        +update()\n        +destroy()\n    }\n    \n    class LabelViewSet {\n        +queryset: Label.objects.all()\n        +serializer_class: LabelSerializer\n        +list()\n        +retrieve()\n        +create()\n        +update()\n        +destroy()\n    }\n    \n    class RawdataApiAOIView {\n        +post()\n    }\n    \n    AOIViewSet --\u003e RawdataApiAOIView\n```\n\nSources: [backend/core/views.py:93-105](), [backend/core/views.py:324-347](), [backend/core/views.py:244-276](), [backend/core/views.py:786-886](), [backend/core/views.py:389-398](), [backend/core/views.py:400-433](), [backend/core/views.py:578-595]()\n\n### Data Models\n\nThe core data models are defined in `core/models.py` using Django's ORM with GeoDjango extensions for spatial data.\n\n```mermaid\nerDiagram\n    Dataset {\n        int id PK\n        string name\n        int user FK\n        string source_imagery\n        int status\n        float[] offset\n    }\n    \n    AOI {\n        int id PK\n        int dataset FK\n        geometry geom\n        int label_status\n        datetime label_fetched\n        int user FK\n    }\n    \n    Label {\n        int id PK\n        int aoi FK\n        geometry geom\n        bigint osm_id\n        json tags\n    }\n    \n    Model {\n        int id PK\n        int dataset FK\n        string name\n        string base_model\n        int status\n        int published_training\n        int user FK\n    }\n    \n    Training {\n        int id PK\n        int model FK\n        string status\n        int epochs\n        int batch_size\n        int[] zoom_level\n        float accuracy\n        string task_id\n        int user FK\n    }\n    \n    Feedback {\n        int id PK\n        int training FK\n        geometry geom\n        string feedback_type\n        int user FK\n    }\n    \n    Dataset ||--o{ AOI : \"contains\"\n    AOI ||--o{ Label : \"contains\"\n    Dataset ||--o{ Model : \"has\"\n    Model ||--o{ Training : \"undergoes\"\n    Training ||--o{ Feedback : \"receives\"\n```\n\nSources: [backend/core/models.py:11-36](), [backend/core/models.py:40-52](), [backend/core/models.py:55-61](), [backend/core/models.py:64-86](), [backend/core/models.py:89-116](), [backend/core/models.py:119-135]()\n\n### Asynchronous Processing\n\nCelery is used for asynchronous processing of computationally intensive tasks like model training. The key components are:\n\n- `train_model` task in `core/tasks.py`: Main task for model training\n- `Trainer` class in `core/tasks.py`: Handles model-specific training logic\n- Redis: Message broker for Celery tasks\n\nThe `train_model` task is triggered when a user creates a new Training instance through the API:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant TrainingViewSet\n    participant TrainingSerializer\n    participant Redis\n    participant CeleryWorker\n    participant Trainer\n    participant S3\n    \n    Client-\u003e\u003eTrainingViewSet: POST /api/v1/training/\n    TrainingViewSet-\u003e\u003eTrainingSerializer: create()\n    TrainingSerializer-\u003e\u003eTrainingSerializer: validate()\n    TrainingSerializer-\u003e\u003eRedis: train_model.apply_async()\n    TrainingSerializer-\u003e\u003eClient: Return Training ID\n    \n    Redis-\u003e\u003eCeleryWorker: task\n    CeleryWorker-\u003e\u003eCeleryWorker: prepare_data()\n    CeleryWorker-\u003e\u003eTrainer: run()\n    \n    alt RAMP Model\n        Trainer-\u003e\u003eTrainer: _train_ramp()\n    else YOLO Model\n        Trainer-\u003e\u003eTrainer: _train_yolo()\n    end\n    \n    Trainer-\u003e\u003eCeleryWorker: Return results\n    CeleryWorker-\u003e\u003eS3: upload_to_s3()\n    CeleryWorker-\u003e\u003eCeleryWorker: finalize()\n    CeleryWorker-\u003e\u003eCeleryWorker: Update Training status\n```\n\nSources: [backend/core/tasks.py:380-467](), [backend/core/tasks.py:99-290](), [backend/core/views.py:124-232]()\n\n### Authentication\n\nThe backend uses OpenStreetMap OAuth 2.0 for authentication, implemented in the `login` app:\n\n- `OsmAuthentication` class in `login/authentication.py`: Custom authentication backend\n- `IsOsmAuthenticated` permission class in `login/permissions.py`: Custom permission class\n- Auth views in `login/views.py`: Handle login, callback, and user data endpoints\n\nThe authentication process works as follows:\n\n1. User is redirected to OSM for login\n2. After successful authentication, OSM redirects back with an authorization code\n3. The backend exchanges the code for an access token\n4. API requests include the access token in the `access-token` header\n5. The `OsmAuthentication` class validates the token and identifies the user\n\nSources: [backend/login/authentication.py:10-54](), [backend/login/permissions.py:6-60](), [backend/login/views.py:23-159]()\n\n## Key Processes\n\n### Model Training Flow\n\nThe model training process is a key feature of the backend. It handles the following steps:\n\n```mermaid\nflowchart TD\n    Start([Start]) --\u003e CreateTraining[\"TrainingSerializer.create()\n    - Validate parameters\n    - Create Training record\n    (serializers.py)\"]\n    \n    CreateTraining --\u003e EnqueueTask[\"train_model.apply_async()\n    - Submit task to Celery\n    (serializers.py)\"]\n    \n    EnqueueTask --\u003e PrepareData[\"prepare_data()\n    - Get AOIs and Labels\n    - Download imagery\n    (tasks.py)\"]\n    \n    PrepareData --\u003e TrainModel[\"Trainer.run()\n    - Select model type (RAMP/YOLO)\n    - Train model\n    - Evaluate performance\n    (tasks.py)\"]\n    \n    TrainModel --\u003e UploadModel[\"upload_to_s3()\n    - Store model artifacts in S3\n    (tasks.py)\"]\n    \n    UploadModel --\u003e Finalize[\"finalize()\n    - Update Training record\n    - Set status to FINISHED\n    - Record accuracy\n    (tasks.py)\"]\n    \n    Finalize --\u003e End([End])\n```\n\nSources: [backend/core/tasks.py:380-467](), [backend/core/tasks.py:302-351](), [backend/core/tasks.py:99-290](), [backend/core/serializers.py:124-242]()\n\n### Prediction Flow\n\nWhen enabled, the prediction API generates predictions for new areas:\n\n```mermaid\nflowchart TD\n    Start([Start]) --\u003e ReceiveRequest[\"PredictionView.post()\n    - Receive bbox, model_id, params\n    (views.py)\"]\n    \n    ReceiveRequest --\u003e ValidateParams[\"PredictionParamSerializer\n    - Validate input parameters\n    (serializers.py)\"]\n    \n    ValidateParams --\u003e LoadModel[\"Load published model\n    - Get Training by model.published_training\n    (views.py)\"]\n    \n    LoadModel --\u003e GetImagery[\"Download imagery for bbox\n    (predictor module)\"]\n    \n    GetImagery --\u003e RunModel[\"predict()\n    - Run model inference\n    (predictor module)\"]\n    \n    RunModel --\u003e FormatResponse[\"Format predictions as GeoJSON\n    (views.py)\"]\n    \n    FormatResponse --\u003e End([End])\n```\n\nSources: [backend/core/views.py:786-886](), [backend/core/serializers.py:360-449]()\n\n### OSM Data Collection Flow\n\nThe backend provides functionality to fetch OSM data for training:\n\n```mermaid\nflowchart TD\n    Start([Start]) --\u003e ReceiveRequest[\"RawdataApiAOIView.post()\n    - Receive aoi_id\n    (views.py)\"]\n    \n    ReceiveRequest --\u003e EnqueueTask[\"process_rawdata_task()\n    - Asynchronous task\n    (views.py)\"]\n    \n    EnqueueTask --\u003e UpdateStatus1[\"Update AOI label_status to RUNNING\n    (views.py)\"]\n    \n    UpdateStatus1 --\u003e FetchData[\"request_rawdata()\n    - Call Raw Data API with geometry\n    (utils.py)\"]\n    \n    FetchData --\u003e ProcessData[\"process_rawdata()\n    - Convert GeoJSON to Labels\n    (utils.py)\"]\n    \n    ProcessData --\u003e UpdateStatus2[\"Update AOI label_status to DOWNLOADED\n    (views.py)\"]\n    \n    UpdateStatus2 --\u003e End([End])\n```\n\nSources: [backend/core/views.py:578-610](), [backend/core/utils.py:222-247](), [backend/core/utils.py:250-281]()\n\n## Configuration\n\nThe backend configuration is managed through environment variables, which can be set in a `.env` file or in the deployment environment. The configuration is defined in `aiproject/settings.py`.\n\nKey configuration options include:\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `DATABASE_URL` | PostgreSQL connection string | `postgis://admin:password@localhost:5432/ai` |\n| `CELERY_BROKER_URL` | Redis connection for Celery | `redis://127.0.0.1:6379/0` |\n| `RAMP_HOME` | Path to RAMP model code | - |\n| `TRAINING_WORKSPACE` | Path for training data | - |\n| `BUCKET_NAME` | S3 bucket for model storage | `fair-dev` |\n| `OSM_CLIENT_ID` | OSM OAuth client ID | - |\n| `OSM_CLIENT_SECRET` | OSM OAuth client secret | - |\n| `RAMP_EPOCHS_LIMIT` | Maximum epochs for RAMP training | 40 |\n| `YOLO_EPOCHS_LIMIT` | Maximum epochs for YOLO training | 200 |\n| `ENABLE_PREDICTION_API` | Enable prediction API | False |\n\nSources: [backend/aiproject/settings.py:30-86](), [backend/aiproject/settings.py:217-222](), [backend/aiproject/settings.py:243-254]()\n\n## Technologies and Dependencies\n\nThe backend relies on several key technologies and libraries:\n\n| Technology | Purpose |\n|------------|---------|\n| Django 4.1.4 | Web framework |\n| Django REST Framework | API development |\n| PostgreSQL/PostGIS | Spatial database |\n| Celery | Asynchronous task processing |\n| Redis | Message broker for Celery |\n| GeoDjango | Spatial data handling |\n| S3/boto3 | Model artifact storage |\n| RAMP/YOLO | AI model architectures |\n\nThe complete list of dependencies can be found in the project's `requirements.txt` and `api-requirements.txt` files.\n\nSources: [backend/requirements.txt:1-7](), [backend/api-requirements.txt:1-28]()\n\n## Development and Deployment\n\n### Local Development\n\nFor local development, you'll need:\n\n1. PostgreSQL with PostGIS extension\n2. Redis server\n3. Python 3.8+ environment\n4. RAMP/YOLO model code repositories\n\nOnce set up, you can run:\n\n```bash\n# Create virtual environment and install dependencies\npip install -r requirements.txt\n\n# Apply database migrations\npython manage.py migrate\n\n# Start development server\npython manage.py runserver\n\n# Start Celery worker\ncelery -A aiproject worker --loglevel=debug -Q ramp_training,yolo_training\n```\n\n### Containerized Deployment\n\nThe project includes Docker configuration for containerized deployment:\n\n```bash\n# Build and start containers\ndocker-compose up -d --build\n```\n\nSources: [backend/README.md:1-196](), [backend/.github/workflows/backend_build.yml:1-139]()\n\n## Conclusion\n\nThe fAIr Backend System provides a robust foundation for AI-assisted mapping with a well-structured architecture. The combination of Django REST Framework for the API, Celery for asynchronous processing, and PostgreSQL/PostGIS for spatial data storage creates a powerful platform for managing model training, prediction, and data management.\n\nThe system is designed to be scalable and maintainable, with clear separation of concerns and modular components. The integration with OpenStreetMap provides authentication and data access, while S3 storage ensures that trained models are persisted and accessible.\n\nFor detailed information about the API endpoints, see [API Endpoints](#2.1). For information on the data models, see [Data Models](#2.2). For details on the asynchronous processing system, see [Asynchronous Processing](#2.3)."])</script><script>self.__next_f.push([1,"1c:T4c80,"])</script><script>self.__next_f.push([1,"# API Endpoints\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [backend/.gitignore](backend/.gitignore)\n- [backend/.pre-commit-config.yaml](backend/.pre-commit-config.yaml)\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/login/views.py](backend/login/views.py)\n- [backend/pdm.lock](backend/pdm.lock)\n- [backend/pyproject.toml](backend/pyproject.toml)\n- [backend/requirements.txt](backend/requirements.txt)\n- [frontend/src/features/start-mapping/components/map/map.tsx](frontend/src/features/start-mapping/components/map/map.tsx)\n- [frontend/src/types/api.ts](frontend/src/types/api.ts)\n- [frontend/src/utils/__tests__/geo/geometry-utils.test.ts](frontend/src/utils/__tests__/geo/geometry-utils.test.ts)\n\n\u003c/details\u003e\n\n\n\nThis document provides detailed information about the RESTful API endpoints available in the fAIr system. These endpoints form the backend interface that allows the frontend and other clients to interact with the system's functionality for AI-assisted mapping. For information about the backend system as a whole, see [Backend System](#2).\n\n## API Overview\n\nThe fAIr API is organized around REST principles. It accepts JSON-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes to indicate the success or failure of requests. The API is versioned with all endpoints prefixed with `/api/v1/`.\n\n```mermaid\ngraph TD\n    subgraph \"Authentication\"\n        Auth[\"/api/v1/auth/login\"]\n        Callback[\"/api/v1/auth/callback\"]\n        User[\"/api/v1/auth/me\"]\n    end\n    \n    subgraph \"Core Resources\"\n        Dataset[\"/api/v1/dataset/\"]\n        Model[\"/api/v1/model/\"]\n        Training[\"/api/v1/training/\"]\n        AOI[\"/api/v1/aoi/\"]\n        Label[\"/api/v1/label/\"]\n        Feedback[\"/api/v1/feedback/\"]\n        Prediction[\"/api/v1/prediction/\"]\n    end\n    \n    subgraph \"Specialized Endpoints\"\n        OSMFetch[\"/api/v1/label/osm/fetch/{aoi_id}\"]\n        Publish[\"/api/v1/training/publish/{training_id}\"]\n        Status[\"/api/v1/training/status/{run_id}\"]\n        GPX[\"/api/v1/aoi/gpx/{aoi_id}\"]\n        KPI[\"/api/v1/kpi/stats/\"]\n    end\n    \n    Client --\u003e Auth\n    Auth --\u003e Callback\n    Callback --\u003e User\n    \n    User --\u003e Dataset\n    User --\u003e Model\n    User --\u003e Training\n    User --\u003e AOI\n    User --\u003e Label\n    User --\u003e Feedback\n    User --\u003e Prediction\n    \n    AOI --\u003e OSMFetch\n    Training --\u003e Publish\n    Training --\u003e Status\n    AOI --\u003e GPX\n    Client --\u003e KPI\n```\n\nSources: [backend/aiproject/urls.py:37-58](), [backend/core/urls.py:43-91]()\n\n## Authentication\n\nThe fAIr system uses OpenStreetMap (OSM) OAuth for authentication. All API requests that require authentication must include an `access-token` header with a valid OSM access token.\n\n### Authentication Endpoints\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/auth/login` | GET | Generates an OSM login URL to initiate the authentication flow |\n| `/api/v1/auth/callback` | GET | Callback endpoint for OSM OAuth, returns an access token |\n| `/api/v1/auth/me` | GET | Returns the authenticated user's profile information |\n| `/api/v1/auth/me` | PATCH | Updates the authenticated user's profile information |\n\n#### Login Flow\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant fAIrAPI as \"fAIr API\"\n    participant OSM as \"OpenStreetMap OAuth\"\n    \n    Client-\u003e\u003efAIrAPI: GET /api/v1/auth/login\n    fAIrAPI-\u003e\u003eClient: { \"login_url\": \"https://openstreetmap.org/oauth/...\" }\n    Client-\u003e\u003eOSM: Redirect to login_url\n    OSM-\u003e\u003eClient: Redirect to callback URL with code \u0026 state\n    Client-\u003e\u003efAIrAPI: GET /api/v1/auth/callback?code=...\u0026state=...\n    fAIrAPI-\u003e\u003eOSM: Exchange code for access token\n    OSM-\u003e\u003efAIrAPI: Access token\n    fAIrAPI-\u003e\u003eClient: { \"access_token\": \"...\" }\n    \n    Client-\u003e\u003efAIrAPI: GET /api/v1/auth/me with access-token header\n    fAIrAPI-\u003e\u003eClient: User profile data\n```\n\nSources: [backend/login/views.py:34-46](), [backend/login/views.py:48-62](), [backend/login/views.py:64-99]()\n\n## Resource Endpoints\n\n### Dataset Endpoints\n\nThe Dataset resource represents a collection of geospatial data that can be used for training AI models.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/dataset/` | GET | List all datasets (pagination supported) |\n| `/api/v1/dataset/{id}/` | GET | Retrieve a specific dataset |\n| `/api/v1/dataset/` | POST | Create a new dataset |\n| `/api/v1/dataset/{id}/` | PUT | Update a dataset (full update) |\n| `/api/v1/dataset/{id}/` | PATCH | Update a dataset (partial update) |\n| `/api/v1/dataset/{id}/` | DELETE | Delete a dataset |\n| `/api/v1/datasets/centroid/` | GET | Get centroid information for all datasets (for map visualization) |\n\n#### Dataset Object Structure\n\n```\n{\n  \"id\": 1,\n  \"name\": \"Building Dataset for Accra\",\n  \"source_imagery\": \"https://server.com/imagery/{z}/{x}/{y}.png\",\n  \"last_modified\": \"2023-06-15T10:30:45Z\",\n  \"created_at\": \"2023-06-10T08:15:20Z\",\n  \"status\": 0,\n  \"models_count\": 3,\n  \"offset\": [0.0, 0.0],\n  \"user\": {\n    \"osm_id\": 12345,\n    \"username\": \"johndoe\"\n  }\n}\n```\n\nSources: [backend/core/views.py:93-116](), [backend/core/serializers.py:32-70](), [backend/core/models.py:11-38](), [backend/core/views.py:363-373]()\n\n### Area of Interest (AOI) Endpoints\n\nAOIs represent specific geographic areas within a dataset that contain training data.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/aoi/` | GET | List all AOIs (filterable by dataset) |\n| `/api/v1/aoi/{id}/` | GET | Retrieve a specific AOI |\n| `/api/v1/aoi/` | POST | Create a new AOI |\n| `/api/v1/aoi/{id}/` | DELETE | Delete an AOI |\n| `/api/v1/aoi/gpx/{aoi_id}/` | GET | Generate GPX format of an AOI's boundary |\n| `/api/v1/label/osm/fetch/{aoi_id}/` | POST | Fetch OpenStreetMap data for an AOI |\n\n#### AOI Object Structure\n\n```\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[[lon1, lat1], [lon2, lat2], ...]]\n  },\n  \"properties\": {\n    \"dataset\": 1,\n    \"label_status\": -1,\n    \"label_fetched\": null,\n    \"created_at\": \"2023-06-12T14:20:30Z\",\n    \"last_modified\": \"2023-06-12T14:20:30Z\"\n  }\n}\n```\n\nSources: [backend/core/views.py:389-398](), [backend/core/serializers.py:171-197](), [backend/core/models.py:40-52](), [backend/core/views.py:578-594](), [backend/core/views.py:917-923]()\n\n### Label Endpoints\n\nLabels represent the ground truth features extracted from OpenStreetMap within an AOI.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/label/` | GET | List all labels (filterable by AOI or dataset) |\n| `/api/v1/label/{id}/` | GET | Retrieve a specific label |\n| `/api/v1/label/` | POST | Create a new label |\n| `/api/v1/label/{id}/` | PUT | Update a label |\n| `/api/v1/label/{id}/` | DELETE | Delete a label |\n| `/api/v1/label/upload/{aoi_id}/` | POST | Upload GeoJSON labels for an AOI |\n\n#### Label Object Structure\n\n```\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[[lon1, lat1], [lon2, lat2], ...]]\n  },\n  \"properties\": {\n    \"aoi\": 1,\n    \"osm_id\": 987654321,\n    \"tags\": {\n      \"building\": \"yes\",\n      \"building:levels\": \"2\",\n      \"amenity\": \"school\"\n    },\n    \"created_at\": \"2023-06-12T15:45:22Z\"\n  }\n}\n```\n\nSources: [backend/core/views.py:400-432](), [backend/core/serializers.py:240-248](), [backend/core/models.py:55-61](), [backend/core/views.py:435-486]()\n\n### Model Endpoints\n\nModels represent AI models that can be trained to identify features in imagery.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/model/` | GET | List all models (pagination supported) |\n| `/api/v1/model/{id}/` | GET | Retrieve a specific model |\n| `/api/v1/model/` | POST | Create a new model |\n| `/api/v1/model/{id}/` | PUT | Update a model (full update) |\n| `/api/v1/model/{id}/` | PATCH | Update a model (partial update) |\n| `/api/v1/model/{id}/` | DELETE | Delete a model |\n| `/api/v1/models/centroid/` | GET | Get centroid information for all models (for map visualization) |\n\n#### Model Object Structure\n\n```\n{\n  \"id\": 1,\n  \"name\": \"Building Detection Model v1\",\n  \"created_at\": \"2023-06-15T11:20:45Z\",\n  \"last_modified\": \"2023-06-20T09:30:15Z\",\n  \"description\": \"Model for detecting buildings in urban areas\",\n  \"user\": {\n    \"osm_id\": 12345,\n    \"username\": \"johndoe\"\n  },\n  \"published_training\": 3,\n  \"status\": 0,\n  \"dataset\": 1,\n  \"accuracy\": 85.7,\n  \"base_model\": \"RAMP\",\n  \"thumbnail_url\": \"https://server.com/imagery/18/12345/67890.png\"\n}\n```\n\nSources: [backend/core/views.py:324-347](), [backend/core/serializers.py:72-122](), [backend/core/models.py:63-86](), [backend/core/views.py:350-360]()\n\n### Training Endpoints\n\nTrainings represent individual training runs for models.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/training/` | GET | List all trainings (filterable by model, status, user) |\n| `/api/v1/training/{id}/` | GET | Retrieve a specific training |\n| `/api/v1/training/` | POST | Create and start a new training |\n| `/api/v1/training/{id}/` | DELETE | Delete a training |\n| `/api/v1/training/status/{run_id}/` | GET | Check the status of a running training task |\n| `/api/v1/training/publish/{training_id}/` | POST | Publish a successfully completed training |\n| `/api/v1/training/terminate/{training_id}/` | POST | Terminate a running training |\n\n#### Training Object Structure\n\n```\n{\n  \"id\": 3,\n  \"source_imagery\": \"https://server.com/imagery/{z}/{x}/{y}.png\",\n  \"description\": \"RAMP training with optimized parameters\",\n  \"created_at\": \"2023-06-18T14:30:22Z\",\n  \"status\": \"FINISHED\",\n  \"task_id\": \"12345-abcde-67890\",\n  \"zoom_level\": [19, 20, 21],\n  \"started_at\": \"2023-06-18T14:30:25Z\",\n  \"finished_at\": \"2023-06-18T16:15:40Z\",\n  \"accuracy\": 85.7,\n  \"epochs\": 20,\n  \"chips_length\": 1056,\n  \"batch_size\": 8,\n  \"freeze_layers\": false,\n  \"model\": {\n    \"id\": 1,\n    \"name\": \"Building Detection Model v1\",\n    \"dataset\": 1,\n    \"base_model\": \"RAMP\",\n    \"status\": 0\n  },\n  \"user\": {\n    \"osm_id\": 12345,\n    \"username\": \"johndoe\"\n  },\n  \"feedback_count\": 0,\n  \"approved_predictions_count\": 0\n}\n```\n\nSources: [backend/core/views.py:124-147](), [backend/core/views.py:244-275](), [backend/core/serializers.py:124-241](), [backend/core/models.py:88-116](), [backend/core/views.py:674-714](), [backend/core/views.py:885-913](), [backend/core/tasks.py:380-467]()\n\n### Feedback Endpoints\n\nFeedback represents user corrections to model predictions that can be used to improve the model.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/feedback/` | GET | List all feedback items (filterable by training, user, type) |\n| `/api/v1/feedback/{id}/` | GET | Retrieve a specific feedback item |\n| `/api/v1/feedback/` | POST | Create a new feedback item |\n| `/api/v1/feedback/{id}/` | PATCH | Update a feedback item |\n| `/api/v1/feedback/{id}/` | DELETE | Delete a feedback item |\n| `/api/v1/feedback-aoi/` | GET/POST/PATCH/DELETE | CRUD for feedback AOIs |\n| `/api/v1/feedback-label/` | GET/POST/PATCH/DELETE | CRUD for feedback labels |\n| `/api/v1/feedback/training/submit/` | POST | Submit feedback for retraining a model |\n\n#### Feedback Object Structure\n\n```\n{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[[lon1, lat1], [lon2, lat2], ...]]\n  },\n  \"properties\": {\n    \"id\": 1,\n    \"training\": 3,\n    \"created_at\": \"2023-06-25T10:15:30Z\",\n    \"zoom_level\": 20,\n    \"feedback_type\": \"FP\",\n    \"comments\": \"This is not a building, but a shadow\",\n    \"user\": 12345,\n    \"source_imagery\": \"https://server.com/imagery/{z}/{x}/{y}.png\"\n  }\n}\n```\n\nSources: [backend/core/views.py:278-293](), [backend/core/serializers.py:221-237](), [backend/core/models.py:118-135](), [backend/core/views.py:295-322](), [backend/core/views.py:717-777]()\n\n### Prediction Endpoint\n\nThe prediction endpoint allows making predictions using trained models.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/prediction/` | POST | Generate predictions for a given area using a published model |\n| `/api/v1/approved-prediction/` | GET | List approved predictions (filterable by training) |\n| `/api/v1/approved-prediction/` | POST | Approve a prediction |\n| `/api/v1/approved-prediction/{id}/` | DELETE | Delete an approved prediction |\n\n#### Prediction Request Structure\n\n```\n{\n  \"bbox\": [lon1, lat1, lon2, lat2],\n  \"model_id\": 1,\n  \"zoom_level\": 20,\n  \"use_josm_q\": true,\n  \"source\": \"https://server.com/imagery/{z}/{x}/{y}.png\",\n  \"confidence\": 80,\n  \"max_angle_change\": 15,\n  \"skew_tolerance\": 15,\n  \"tolerance\": 0.5,\n  \"tile_overlap_distance\": 0.15\n}\n```\n\nSources: [backend/core/views.py:786-883](), [backend/core/serializers.py:360-449](), [backend/core/views.py:508-543](), [backend/core/serializers.py:250-255](), [backend/core/models.py:164-174]()\n\n## Utility Endpoints\n\n### Workspace Endpoints\n\nThese endpoints provide access to training workspace files.\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/workspace/{lookup_dir}/` | GET | List files and directories in a training workspace |\n| `/api/v1/workspace/download/{lookup_dir}/` | GET | Download a file from the training workspace |\n\nSources: [backend/core/views.py:935-965](), [backend/core/utils.py:105-123](), [backend/core/utils.py:60-70]()\n\n### Data Conversion Endpoints\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/geojson2osm/` | POST | Convert GeoJSON to OSM XML format |\n| `/api/v1/conflate/` | POST | Conflate GeoJSON with existing OSM data |\n\nSources: [backend/core/views.py:648-671]()\n\n### Notification Endpoints\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/notifications/me` | GET | List user notifications |\n| `/api/v1/notifications/mark-as-read/{notification_id}/` | POST | Mark a notification as read |\n| `/api/v1/notifications/mark-all-as-read/` | POST | Mark all notifications as read |\n\nSources: [backend/core/serializers.py:531-555](), [backend/core/models.py:191-212]()\n\n### Statistics Endpoint\n\n| Endpoint | Method | Description |\n|----------|--------|-------------|\n| `/api/v1/kpi/stats/` | GET | Get key performance indicators of the system |\n\nSources: [backend/core/views.py:983-997]()\n\n## Data Flow Between Endpoints\n\nThe following diagram illustrates the typical data flow between endpoints in a common workflow:\n\n```mermaid\nflowchart TD\n    subgraph \"Dataset Management\"\n        CreateDataset[\"POST /api/v1/dataset/\"]\n        CreateAOI[\"POST /api/v1/aoi/\"]\n        FetchOSM[\"POST /api/v1/label/osm/fetch/{aoi_id}/\"]\n    end\n    \n    subgraph \"Model Training\"\n        CreateModel[\"POST /api/v1/model/\"]\n        StartTraining[\"POST /api/v1/training/\"]\n        CheckStatus[\"GET /api/v1/training/status/{run_id}/\"]\n        PublishTraining[\"POST /api/v1/training/publish/{training_id}/\"]\n    end\n    \n    subgraph \"Prediction \u0026 Feedback\"\n        GetPrediction[\"POST /api/v1/prediction/\"]\n        ApprovePrediction[\"POST /api/v1/approved-prediction/\"]\n        CreateFeedback[\"POST /api/v1/feedback/\"]\n        SubmitFeedback[\"POST /api/v1/feedback/training/submit/\"]\n    end\n    \n    CreateDataset --\u003e CreateAOI\n    CreateAOI --\u003e FetchOSM\n    FetchOSM --\u003e CreateModel\n    CreateModel --\u003e StartTraining\n    StartTraining --\u003e CheckStatus\n    CheckStatus --\u003e |\"status: FINISHED\"| PublishTraining\n    PublishTraining --\u003e GetPrediction\n    GetPrediction --\u003e ApprovePrediction\n    GetPrediction --\u003e CreateFeedback\n    CreateFeedback --\u003e SubmitFeedback\n    SubmitFeedback --\u003e StartTraining\n```\n\nSources: [backend/core/views.py](), [backend/core/tasks.py]()\n\n## API Response Formats\n\n### Successful Responses\n\nSuccessful responses include the appropriate HTTP status code (typically 200 OK, 201 Created, or 204 No Content) and a JSON response body for GET, POST, and PUT/PATCH requests.\n\n#### List Response Format (with pagination)\n\n```\n{\n  \"count\": 42,\n  \"next\": \"https://api.example.org/api/v1/dataset/?limit=10\u0026offset=10\",\n  \"previous\": null,\n  \"results\": [\n    {\n      // resource object 1\n    },\n    {\n      // resource object 2\n    },\n    // ...\n  ]\n}\n```\n\n#### Detail Response Format\n\n```\n{\n  // Single resource object\n}\n```\n\n### Error Responses\n\nError responses include the appropriate HTTP status code (4xx for client errors, 5xx for server errors) and a JSON response body with an error message.\n\n```\n{\n  \"error\": \"Detailed error message\"\n}\n```\n\nor\n\n```\n{\n  \"field_name\": [\n    \"Field-specific error message\"\n  ],\n  \"another_field\": [\n    \"Another field-specific error message\"\n  ]\n}\n```\n\n## Authentication and Permissions\n\nMost API endpoints require authentication using an OpenStreetMap access token, which should be included in the `access-token` HTTP header.\n\n```\nGET /api/v1/dataset/\nHeaders:\n  access-token: your_osm_access_token\n```\n\nThe API uses different permission levels:\n\n1. **Public methods**: Endpoints marked with `public_methods = [\"GET\"]` allow GET requests without authentication\n2. **Authenticated user methods**: All authenticated users can access these endpoints\n3. **Admin/Staff methods**: Only admin or staff users can access these endpoints, such as `/api/v1/users/` and `/api/v1/banner/`\n\nSources: [backend/core/views.py:96-98](), [backend/login/authentication.py](), [backend/login/permissions.py]()\n\n## API Relationships Model\n\nThe following diagram shows the relationships between the main resources in the API:\n\n```mermaid\nerDiagram\n    User ||--o{ Dataset : creates\n    Dataset ||--o{ AOI : contains\n    AOI ||--o{ Label : has\n    Dataset ||--o{ Model : trains\n    Model ||--o{ Training : has\n    Training ||--o{ Feedback : receives\n    Training ||--o{ ApprovedPrediction : generates\n    User ||--o{ Feedback : submits\n    User ||--o{ ApprovedPrediction : approves\n    Training ||--o{ FeedbackAOI : has\n    FeedbackAOI ||--o{ FeedbackLabel : contains\n    User ||--o{ UserNotification : receives\n    \n    User {\n        int osm_id\n        string username\n        string email\n        bool email_verified\n    }\n    \n    Dataset {\n        int id\n        string name\n        string source_imagery\n        array offset\n        int status\n    }\n    \n    AOI {\n        int id\n        int dataset\n        geometry geom\n        int label_status\n        datetime label_fetched\n    }\n    \n    Label {\n        int id\n        int aoi\n        geometry geom\n        bigint osm_id\n        json tags\n    }\n    \n    Model {\n        int id\n        int dataset\n        string name\n        string description\n        string base_model\n        int published_training\n        int status\n    }\n    \n    Training {\n        int id\n        int model\n        string source_imagery\n        string description\n        string status\n        array zoom_level\n        int epochs\n        int batch_size\n        float accuracy\n    }\n    \n    Feedback {\n        int id\n        int training\n        geometry geom\n        string feedback_type\n        string comments\n    }\n    \n    ApprovedPrediction {\n        int id\n        int training\n        geometry geom\n        json config\n    }\n}\n```\n\nSources: [backend/core/models.py](), [backend/login/models.py]()"])</script><script>self.__next_f.push([1,"1d:T4ff5,"])</script><script>self.__next_f.push([1,"# Data Models\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [backend/README.md](backend/README.md)\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/core/admin.py](backend/core/admin.py)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/login/admin.py](backend/login/admin.py)\n- [backend/login/authentication.py](backend/login/authentication.py)\n- [backend/login/permissions.py](backend/login/permissions.py)\n- [backend/requirements.txt](backend/requirements.txt)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/factories.py](backend/tests/factories.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n\n\u003c/details\u003e\n\n\n\nThis document explains the database models and their relationships in the fAIr backend system. It provides a comprehensive view of the data structures that support AI-assisted mapping functionality. For information about asynchronous processing that uses these models, see [Asynchronous Processing](#2.3).\n\n## Overview\n\nThe fAIr system uses Django's Object-Relational Mapping (ORM) with PostgreSQL/PostGIS to store and manage data. The data models represent various components including datasets, areas of interest (AOIs), AI models, trainings, labels, and user feedback.\n\nThe data models are designed to support the following key functions:\n- Managing geospatial datasets and labels\n- Creating and training AI models for feature detection\n- Managing the training lifecycle\n- Capturing user feedback and approved predictions\n- User notifications and system announcements\n\nSources: [backend/core/models.py:1-213](backend/core/models.py:1-213)\n\n## Core Data Model Relationships\n\nThe following diagram illustrates the relationships between the primary data models in the system:\n\n```mermaid\nerDiagram\n    \"OsmUser\" ||--o{ \"Dataset\" : creates\n    \"Dataset\" ||--o{ \"AOI\" : contains\n    \"Dataset\" ||--o{ \"Model\" : \"trained_on\"\n    \"AOI\" ||--o{ \"Label\" : contains\n    \"Model\" ||--o{ \"Training\" : has\n    \"Training\" ||--o{ \"Feedback\" : receives\n    \"Training\" ||--o{ \"FeedbackAOI\" : has\n    \"Training\" ||--o{ \"ApprovedPredictions\" : collects\n    \"FeedbackAOI\" ||--o{ \"FeedbackLabel\" : contains\n    \"OsmUser\" ||--o{ \"Training\" : creates\n    \"OsmUser\" ||--o{ \"Feedback\" : provides\n    \"OsmUser\" ||--o{ \"ApprovedPredictions\" : approves\n    \"OsmUser\" ||--o{ \"UserNotification\" : receives\n    \"Training\" ||--o{ \"UserNotification\" : generates\n```\n\nSources: [backend/core/models.py:1-213](backend/core/models.py:1-213), [backend/core/serializers.py:1-556](backend/core/serializers.py:1-556)\n\n## Detailed Data Models\n\n### User Model\n\nThe user model extends Django's authentication system with OpenStreetMap (OSM) user information:\n\n```mermaid\nclassDiagram\n    class \"OsmUser\" {\n        +BigInteger osm_id\n        +CharField username\n        +URLField img_url\n        +EmailField email\n        +BooleanField email_verified\n        +DateTimeField date_joined\n        +DateTimeField last_login\n        +JSONField notifications_delivery_methods\n        +BooleanField newsletter_subscription\n        +BooleanField account_deletion_requested\n    }\n```\n\nThe `OsmUser` model is used for authentication and is referenced by several other models to track ownership and interactions. The system uses OSM OAuth for authentication, as implemented in `OsmAuthentication` class.\n\nSources: [backend/login/authentication.py:1-55](backend/login/authentication.py:1-55), [backend/login/admin.py:1-94](backend/login/admin.py:1-94)\n\n### Dataset Model\n\n```mermaid\nclassDiagram\n    class \"Dataset\" {\n        +CharField name\n        +ForeignKey user\n        +DateTimeField last_modified\n        +DateTimeField created_at\n        +URLField source_imagery\n        +IntegerField status\n        +ArrayField offset\n    }\n\n    class \"DatasetStatus\" {\n        \u003c\u003cenumeration\u003e\u003e\n        ARCHIVED = 1\n        ACTIVE = 0\n        DRAFT = -1\n    }\n```\n\nThe `Dataset` model is the top-level container for mapping data. It has the following key fields:\n- `name`: Name of the dataset\n- `user`: Reference to the creator\n- `source_imagery`: URL template for the imagery source\n- `status`: Current status of the dataset (ACTIVE, ARCHIVED, DRAFT)\n- `offset`: An array of two float values representing [x, y] offset in meters for imagery alignment\n\nSources: [backend/core/models.py:11-39](backend/core/models.py:11-39), [backend/core/serializers.py:32-69](backend/core/serializers.py:32-69)\n\n### Area of Interest (AOI) Model\n\n```mermaid\nclassDiagram\n    class \"AOI\" {\n        +ForeignKey dataset\n        +PolygonField geom\n        +IntegerField label_status\n        +DateTimeField label_fetched\n        +DateTimeField created_at\n        +DateTimeField last_modified\n        +ForeignKey user\n    }\n\n    class \"DownloadStatus\" {\n        \u003c\u003cenumeration\u003e\u003e\n        DOWNLOADED = 1\n        NOT_DOWNLOADED = -1\n        RUNNING = 0\n    }\n```\n\nThe `AOI` model represents geographic areas within a dataset that will be used for model training. Key fields include:\n- `dataset`: Reference to the parent dataset\n- `geom`: Polygon geometry defining the area boundary\n- `label_status`: Status of label download/processing\n- `label_fetched`: When labels were last fetched from OSM\n\nSources: [backend/core/models.py:40-53](backend/core/models.py:40-53), [backend/core/serializers.py:171-198](backend/core/serializers.py:171-198)\n\n### Label Model\n\n```mermaid\nclassDiagram\n    class \"Label\" {\n        +ForeignKey aoi\n        +GeometryField geom\n        +BigIntegerField osm_id\n        +JSONField tags\n        +DateTimeField created_at\n    }\n```\n\nThe `Label` model represents features (like buildings) extracted from OpenStreetMap within an Area of Interest. These labels are used as training data:\n- `aoi`: Reference to the parent AOI\n- `geom`: Geometry of the feature (typically a polygon for buildings)\n- `osm_id`: OpenStreetMap ID of the feature\n- `tags`: JSON object containing OSM tags for the feature\n\nSources: [backend/core/models.py:55-61](backend/core/models.py:55-61), [backend/core/serializers.py:240-249](backend/core/serializers.py:240-249)\n\n### Model Model\n\n```mermaid\nclassDiagram\n    class \"Model\" {\n        +ForeignKey dataset\n        +CharField name\n        +DateTimeField created_at\n        +DateTimeField last_modified\n        +TextField description\n        +ForeignKey user\n        +PositiveIntegerField published_training\n        +IntegerField status\n        +CharField base_model\n    }\n\n    class \"ModelStatus\" {\n        \u003c\u003cenumeration\u003e\u003e\n        ARCHIVED = 1\n        PUBLISHED = 0\n        DRAFT = -1\n    }\n\n    class \"BASE_MODEL_CHOICES\" {\n        \u003c\u003cenumeration\u003e\u003e\n        RAMP\n        YOLO_V8_V1\n        YOLO_V8_V2\n    }\n```\n\nThe `Model` represents an AI model configuration that can be trained to detect features. Key fields include:\n- `dataset`: Reference to the dataset it's trained on\n- `name`: Name of the model\n- `user`: Reference to the creator\n- `published_training`: ID of the published training run\n- `status`: Current status of the model (PUBLISHED, ARCHIVED, DRAFT)\n- `base_model`: The underlying AI model architecture (RAMP, YOLO_V8_V1, YOLO_V8_V2)\n\nSources: [backend/core/models.py:63-86](backend/core/models.py:63-86), [backend/core/serializers.py:72-122](backend/core/serializers.py:72-122)\n\n### Training Model\n\n```mermaid\nclassDiagram\n    class \"Training\" {\n        +ForeignKey model\n        +URLField source_imagery\n        +TextField description\n        +DateTimeField created_at\n        +CharField status\n        +CharField task_id\n        +ArrayField zoom_level\n        +ForeignKey user\n        +DateTimeField started_at\n        +DateTimeField finished_at\n        +FloatField accuracy\n        +PositiveIntegerField epochs\n        +PositiveIntegerField chips_length\n        +PositiveIntegerField batch_size\n        +BooleanField freeze_layers\n        +PointField centroid\n    }\n\n    class \"STATUS_CHOICES\" {\n        \u003c\u003cenumeration\u003e\u003e\n        SUBMITTED\n        RUNNING\n        FINISHED\n        FAILED\n    }\n```\n\nThe `Training` model represents a specific training run for a model. Key fields include:\n- `model`: Reference to the model being trained\n- `status`: Current status of the training (SUBMITTED, RUNNING, FINISHED, FAILED)\n- `task_id`: ID of the Celery task running the training\n- `zoom_level`: Array of zoom levels used for imagery\n- `user`: User who initiated the training\n- `accuracy`: Accuracy achieved by the model\n- `epochs`: Number of training epochs\n- `batch_size`: Batch size used for training\n- `freeze_layers`: Whether to freeze base model layers\n\nSources: [backend/core/models.py:88-116](backend/core/models.py:88-116), [backend/core/serializers.py:124-241](backend/core/serializers.py:124-241), [backend/core/tasks.py:380-468](backend/core/tasks.py:380-468)\n\n## Feedback and Prediction Models\n\n### Feedback Model\n\n```mermaid\nclassDiagram\n    class \"Feedback\" {\n        +GeometryField geom\n        +ForeignKey training\n        +DateTimeField created_at\n        +PositiveIntegerField zoom_level\n        +CharField feedback_type\n        +TextField comments\n        +ForeignKey user\n        +URLField source_imagery\n    }\n\n    class \"FEEDBACK_TYPE\" {\n        \u003c\u003cenumeration\u003e\u003e\n        TP: \"True Positive\"\n        TN: \"True Negative\"\n        FP: \"False Positive\"\n        FN: \"False Negative\"\n    }\n```\n\nThe `Feedback` model captures user feedback on model predictions:\n- `geom`: Geometry of the feedback area\n- `training`: Reference to the training run\n- `feedback_type`: Type of feedback (TP, TN, FP, FN)\n- `user`: User who provided the feedback\n\nSources: [backend/core/models.py:118-136](backend/core/models.py:118-136), [backend/core/serializers.py:221-238](backend/core/serializers.py:221-238)\n\n### FeedbackAOI Model\n\n```mermaid\nclassDiagram\n    class \"FeedbackAOI\" {\n        +ForeignKey training\n        +PolygonField geom\n        +IntegerField label_status\n        +DateTimeField label_fetched\n        +DateTimeField created_at\n        +DateTimeField last_modified\n        +URLField source_imagery\n        +ForeignKey user\n    }\n\n    class \"DownloadStatus\" {\n        \u003c\u003cenumeration\u003e\u003e\n        DOWNLOADED = 1\n        NOT_DOWNLOADED = -1\n        RUNNING = 0\n    }\n```\n\nThe `FeedbackAOI` model defines areas for feedback collection:\n- `training`: Reference to the training run\n- `geom`: Polygon geometry defining the feedback area\n- `label_status`: Status of OSM data download\n- `source_imagery`: URL template for the imagery source\n\nSources: [backend/core/models.py:137-151](backend/core/models.py:137-151), [backend/core/serializers.py:200-219](backend/core/serializers.py:200-219)\n\n### FeedbackLabel Model\n\n```mermaid\nclassDiagram\n    class \"FeedbackLabel\" {\n        +BigIntegerField osm_id\n        +ForeignKey feedback_aoi\n        +JSONField tags\n        +PolygonField geom\n        +DateTimeField created_at\n    }\n```\n\nThe `FeedbackLabel` model represents features within feedback areas:\n- `feedback_aoi`: Reference to the parent feedback AOI\n- `geom`: Polygon geometry of the feature\n- `osm_id`: OpenStreetMap ID of the feature\n- `tags`: JSON object containing OSM tags\n\nSources: [backend/core/models.py:153-162](backend/core/models.py:153-162), [backend/core/serializers.py:257-262](backend/core/serializers.py:257-262)\n\n### ApprovedPredictions Model\n\n```mermaid\nclassDiagram\n    class \"ApprovedPredictions\" {\n        +ForeignKey training\n        +JSONField config\n        +GeometryField geom\n        +DateTimeField approved_at\n        +ForeignKey user\n    }\n```\n\nThe `ApprovedPredictions` model stores predictions that users have approved:\n- `training`: Reference to the training run that generated the prediction\n- `config`: Configuration used for vectorization/prediction\n- `geom`: Geometry of the approved prediction\n- `user`: User who approved the prediction\n\nSources: [backend/core/models.py:164-174](backend/core/models.py:164-174), [backend/core/serializers.py:250-255](backend/core/serializers.py:250-255)\n\n## System Models\n\n### Banner Model\n\n```mermaid\nclassDiagram\n    class \"Banner\" {\n        +TextField message\n        +DateTimeField start_date\n        +DateTimeField end_date\n        +is_displayable()\n    }\n```\n\nThe `Banner` model is used for system-wide announcements:\n- `message`: Content of the banner\n- `start_date`: When to start displaying the banner\n- `end_date`: When to stop displaying the banner (optional)\n- `is_displayable()`: Method to check if banner should be displayed\n\nSources: [backend/core/models.py:176-188](backend/core/models.py:176-188), [backend/core/serializers.py:452-460](backend/core/serializers.py:452-460)\n\n### UserNotification Model\n\n```mermaid\nclassDiagram\n    class \"UserNotification\" {\n        +ForeignKey user\n        +BooleanField is_read\n        +DateTimeField created_at\n        +DateTimeField read_at\n        +TextField message\n        +ForeignKey training\n        +mark_as_read()\n    }\n```\n\nThe `UserNotification` model stores notifications for users:\n- `user`: User who receives the notification\n- `is_read`: Whether the notification has been read\n- `message`: Content of the notification\n- `training`: Associated training run\n- `mark_as_read()`: Method to mark notification as read\n\nSources: [backend/core/models.py:191-212](backend/core/models.py:191-212), [backend/core/serializers.py:531-556](backend/core/serializers.py:531-556)\n\n## Data Flow During Training Process\n\nThe following sequence diagram illustrates how data flows through the models during the training process:\n\n```mermaid\nsequenceDiagram\n    participant User as \"User\"\n    participant Dataset as \"Dataset\"\n    participant AOI as \"AOI\"\n    participant Label as \"Label\"\n    participant Model as \"Model\"\n    participant Training as \"Training\"\n    participant Worker as \"Celery Worker\"\n    \n    User-\u003e\u003eDataset: Create dataset\n    User-\u003e\u003eAOI: Define training area\n    User-\u003e\u003eLabel: Fetch OSM features\n    User-\u003e\u003eModel: Create model\n    User-\u003e\u003eTraining: Start training\n    Training-\u003e\u003eWorker: Enqueue train_model task\n    Worker-\u003e\u003eLabel: Retrieve labels\n    Worker-\u003e\u003eAOI: Get imagery bounds\n    Worker-\u003e\u003eTraining: Update status to RUNNING\n    Worker-\u003e\u003eWorker: Process imagery and labels\n    Worker-\u003e\u003eWorker: Train model\n    Worker-\u003e\u003eTraining: Update accuracy and status\n    Training--\u003e\u003eModel: Can be published if finished\n    Training--\u003e\u003eUserNotification: Create notification\n```\n\nSources: [backend/core/tasks.py:380-468](backend/core/tasks.py:380-468), [backend/core/views.py:124-232](backend/core/views.py:124-232)\n\n## Database Schema Diagram with Fields\n\nThis diagram shows the full database schema with key fields:\n\n```mermaid\nerDiagram\n    \"OsmUser\" {\n        BigInteger osm_id PK\n        string username\n        string email\n        datetime date_joined\n        boolean is_staff\n        boolean is_superuser\n        json notifications_delivery_methods\n    }\n    \n    \"Dataset\" {\n        int id PK\n        string name\n        BigInteger user FK\n        datetime created_at\n        datetime last_modified\n        string source_imagery\n        int status\n        float[] offset\n    }\n    \n    \"AOI\" {\n        int id PK\n        int dataset FK\n        polygon geom\n        int label_status\n        datetime label_fetched\n        BigInteger user FK\n    }\n    \n    \"Label\" {\n        int id PK\n        int aoi FK\n        geometry geom\n        BigInteger osm_id\n        json tags\n    }\n    \n    \"Model\" {\n        int id PK\n        int dataset FK\n        string name\n        string description\n        BigInteger user FK\n        int published_training\n        int status\n        string base_model\n    }\n    \n    \"Training\" {\n        int id PK\n        int model FK\n        string source_imagery\n        string status\n        string task_id\n        int[] zoom_level\n        BigInteger user FK\n        datetime started_at\n        datetime finished_at\n        float accuracy\n        int epochs\n        int batch_size\n    }\n    \n    \"Feedback\" {\n        int id PK\n        geometry geom\n        int training FK\n        int zoom_level\n        string feedback_type\n        string comments\n        BigInteger user FK\n    }\n    \n    \"FeedbackAOI\" {\n        int id PK\n        int training FK\n        polygon geom\n        int label_status\n        string source_imagery\n        BigInteger user FK\n    }\n    \n    \"FeedbackLabel\" {\n        int id PK\n        int feedback_aoi FK\n        polygon geom\n        BigInteger osm_id\n        json tags\n    }\n    \n    \"ApprovedPredictions\" {\n        int id PK\n        int training FK\n        json config\n        geometry geom\n        datetime approved_at\n        BigInteger user FK\n    }\n    \n    \"UserNotification\" {\n        int id PK\n        BigInteger user FK\n        boolean is_read\n        datetime created_at\n        datetime read_at\n        string message\n        int training FK\n    }\n    \n    \"Banner\" {\n        int id PK\n        string message\n        datetime start_date\n        datetime end_date\n    }\n    \n    \"OsmUser\" ||--o{ \"Dataset\" : creates\n    \"OsmUser\" ||--o{ \"AOI\" : creates\n    \"OsmUser\" ||--o{ \"Model\" : creates\n    \"OsmUser\" ||--o{ \"Training\" : runs\n    \"OsmUser\" ||--o{ \"Feedback\" : provides\n    \"OsmUser\" ||--o{ \"FeedbackAOI\" : creates\n    \"OsmUser\" ||--o{ \"ApprovedPredictions\" : approves\n    \"OsmUser\" ||--o{ \"UserNotification\" : receives\n    \n    \"Dataset\" ||--o{ \"AOI\" : contains\n    \"Dataset\" ||--o{ \"Model\" : trains\n    \n    \"AOI\" ||--o{ \"Label\" : contains\n    \n    \"Model\" ||--o{ \"Training\" : runs\n    \n    \"Training\" ||--o{ \"Feedback\" : receives\n    \"Training\" ||--o{ \"FeedbackAOI\" : contains\n    \"Training\" ||--o{ \"ApprovedPredictions\" : collects\n    \"Training\" ||--o{ \"UserNotification\" : triggers\n    \n    \"FeedbackAOI\" ||--o{ \"FeedbackLabel\" : contains\n```\n\nSources: [backend/core/models.py:1-213](backend/core/models.py:1-213), [backend/aiproject/settings.py:162-167](backend/aiproject/settings.py:162-167)\n\n## Serialization and API Representation\n\nThe models are exposed through Django REST Framework API endpoints, using serializers to convert model instances to JSON and handle validation. Key serializers include:\n\n| Model | Serializer | API Endpoint | Key Functions |\n|-------|------------|--------------|---------------|\n| Dataset | DatasetSerializer | `/api/v1/dataset/` | CRUD operations for datasets |\n| AOI | AOISerializer | `/api/v1/aoi/` | Define areas of interest |\n| Label | LabelSerializer | `/api/v1/label/` | Manage feature labels |\n| Model | ModelSerializer | `/api/v1/model/` | Create and manage AI models |\n| Training | TrainingSerializer | `/api/v1/training/` | Run and monitor training |\n| Feedback | FeedbackSerializer | `/api/v1/feedback/` | Submit feedback on predictions |\n| FeedbackAOI | FeedbackAOISerializer | `/api/v1/feedback-aoi/` | Define feedback areas |\n| FeedbackLabel | FeedbackLabelSerializer | `/api/v1/feedback-label/` | Manage feedback labels |\n| ApprovedPredictions | ApprovedPredictionsSerializer | `/api/v1/approved-prediction/` | Store approved predictions |\n\nSources: [backend/core/serializers.py:1-556](backend/core/serializers.py:1-556), [backend/core/urls.py:43-56](backend/core/urls.py:43-56), [backend/core/views.py:93-543](backend/core/views.py:93-543)\n\n## Storing Geospatial Data\n\nThe system uses PostGIS, a spatial database extension for PostgreSQL, to store and query geospatial data:\n\n- `PolygonField`, `GeometryField`, and `PointField` store spatial geometries\n- Django's `GeoFeatureModelSerializer` converts these to GeoJSON for API responses\n- Spatial filters like `InBBoxFilter` enable querying by geographic bounds\n\nThis allows the system to efficiently store, query, and process mapping features for AI model training and prediction.\n\nSources: [backend/aiproject/settings.py:162-167](backend/aiproject/settings.py:162-167), [backend/core/serializers.py:5-7](backend/core/serializers.py:5-7), [backend/core/views.py:407-416](backend/core/views.py:407-416)"])</script><script>self.__next_f.push([1,"1e:T3540,"])</script><script>self.__next_f.push([1,"# Asynchronous Processing\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [.gitignore](.gitignore)\n- [backend/README.md](backend/README.md)\n- [backend/aiproject/__init__.py](backend/aiproject/__init__.py)\n- [backend/aiproject/asgi.py](backend/aiproject/asgi.py)\n- [backend/aiproject/celery.py](backend/aiproject/celery.py)\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/aiproject/utils.py](backend/aiproject/utils.py)\n- [backend/aiproject/wsgi.py](backend/aiproject/wsgi.py)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/requirements.txt](backend/requirements.txt)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains the asynchronous processing system implemented in the fAIr platform, which enables computationally intensive tasks like model training to be handled in the background while keeping the web server responsive. The document covers the Celery configuration, task definitions, execution workflow, and status monitoring mechanisms.\n\nSources: [backend/aiproject/celery.py](), [backend/aiproject/settings.py:215-232]()\n\n## Overview\n\nThe fAIr platform uses Celery, a distributed task queue system, to handle computationally intensive operations asynchronously. This is particularly important for AI model training processes, which can take significant time and resources. By moving these operations to background workers, the main application remains responsive to user requests.\n\nThe primary asynchronous operations in the system include:\n\n1. Model training (RAMP and YOLO)\n2. Raw data processing\n3. Feedback incorporation\n\n```mermaid\nflowchart TD\n    subgraph \"User Interface\"\n        WebRequest[\"Web Request\"]\n    end\n\n    subgraph \"API Layer\"\n        DjangoView[\"Django Views/APIs\"]\n    end\n\n    subgraph \"Task Queue\"\n        Redis[\"Redis Message Broker\"]\n    end\n\n    subgraph \"Worker Processing\"\n        CeleryWorker[\"Celery Worker\"]\n        RampQueue[\"RAMP Training Queue\"]\n        YoloQueue[\"YOLO Training Queue\"]\n    end\n\n    subgraph \"Storage\"\n        Database[\"PostgreSQL Database\"]\n        S3[\"S3 Storage\"]\n        LogFiles[\"Log Files\"]\n    end\n\n    WebRequest --\u003e DjangoView\n    DjangoView --\u003e Redis\n    Redis --\u003e CeleryWorker\n    CeleryWorker --\u003e RampQueue\n    CeleryWorker --\u003e YoloQueue\n    CeleryWorker --\u003e Database\n    CeleryWorker --\u003e S3\n    CeleryWorker --\u003e LogFiles\n    DjangoView --\u003e Database\n```\n\nSources: [backend/core/tasks.py](), [backend/core/views.py:203-223](), [backend/aiproject/settings.py:215-232]()\n\n## Celery Configuration\n\nThe Celery task queue is configured in the Django application to handle asynchronous processing.\n\n### Key Settings\n\nThe primary Celery configuration is defined in the project settings:\n\n```python\nCELERY_BROKER_URL = env(\"CELERY_BROKER_URL\", default=\"redis://127.0.0.1:6379/0\")\nCELERY_RESULT_BACKEND = env(\"CELERY_RESULT_BACKEND\", default=\"redis://127.0.0.1:6379/0\")\n```\n\nBy default, Redis is used both as the message broker and result backend. The Celery application is initialized in the `aiproject/celery.py` file and configured to discover tasks automatically from all registered Django apps.\n\n### Queues and Routing\n\nThe system uses specialized queues for different types of model training:\n\n1. `ramp_training` - Queue for RAMP model training tasks\n2. `yolo_training` - Queue for YOLO model training tasks\n\nTasks are routed to the appropriate queue based on the model type when they are submitted for execution.\n\nSources: [backend/aiproject/celery.py](), [backend/aiproject/settings.py:215-232](), [backend/core/views.py:217-220]()\n\n## Task Definition and Implementation\n\n### The `train_model` Task\n\nThe primary asynchronous task in the system is `train_model`, which handles the training of machine learning models. This task is defined in `core/tasks.py` as a shared task that can be executed by Celery workers.\n\n```python\n@shared_task\ndef train_model(\n    dataset_id,\n    training_id,\n    epochs,\n    batch_size,\n    zoom_level,\n    source_imagery,\n    feedback=None,\n    freeze_layers=False,\n    multimasks=False,\n    input_contact_spacing=8,\n    input_boundary_width=3,\n):\n    # Task implementation...\n```\n\nThe task performs several operations:\n\n1. Retrieves the relevant Training and Model instances\n2. Updates the training status to \"RUNNING\"\n3. Sets up logging for the task\n4. Prepares training data, including downloading imagery\n5. Executes the appropriate training algorithm (RAMP or YOLO)\n6. Uploads the trained model and results to S3 storage\n7. Updates the training status and accuracy in the database\n\n### The `Trainer` Class\n\nModel training is handled by the `Trainer` class, which provides a unified interface for different model types:\n\n```mermaid\nclassDiagram\n    class Trainer {\n        +String model_type\n        +Tuple args\n        +__init__(model_type, *args)\n        +run(output_path) String\n        -_train_yolo(output_path) Dict\n        -_train_ramp(output_path) Dict\n    }\n    \n    Trainer --\u003e YOLOTraining: Uses for YOLO models\n    Trainer --\u003e RAMPTraining: Uses for RAMP models\n    \n    class YOLOTraining {\n        +train()\n    }\n    \n    class RAMPTraining {\n        +train()\n    }\n```\n\nThe `Trainer` class determines which training method to use based on the model type and executes the appropriate training pipeline.\n\nSources: [backend/core/tasks.py:99-290](), [backend/core/tasks.py:380-467]()\n\n## Task Execution Workflow\n\n### Initiating Training Tasks\n\nThe asynchronous training process is typically initiated through the REST API when a user creates a new training instance. This happens in the `create` method of the `TrainingSerializer` class in `core/views.py`:\n\n```mermaid\nsequenceDiagram\n    participant Client as \"Client\"\n    participant API as \"Training API\"\n    participant DB as \"Database\"\n    participant Redis as \"Redis Broker\"\n    participant Worker as \"Celery Worker\"\n    participant S3 as \"S3 Storage\"\n    \n    Client-\u003e\u003eAPI: POST /api/v1/training/ (training parameters)\n    API-\u003e\u003eDB: Validate and create Training record\n    API-\u003e\u003eRedis: Submit train_model task\n    API-\u003e\u003eClient: Return Training ID and task_id\n    \n    Redis-\u003e\u003eWorker: Dispatch task to worker\n    Worker-\u003e\u003eDB: Update training status to RUNNING\n    Worker-\u003e\u003eWorker: Download imagery \u0026 prepare data\n    Worker-\u003e\u003eWorker: Train model (RAMP/YOLO)\n    Worker-\u003e\u003eS3: Upload model artifacts\n    Worker-\u003e\u003eDB: Update training status to FINISHED\n    Worker-\u003e\u003eDB: Update accuracy and finished_at time\n```\n\nThe task is submitted to Celery using `apply_async` with appropriate parameters and queue selection:\n\n```python\ntask = train_model.apply_async(\n    kwargs={\n        \"dataset_id\": instance.model.dataset.id,\n        \"training_id\": instance.id,\n        \"epochs\": instance.epochs,\n        \"batch_size\": instance.batch_size,\n        \"zoom_level\": instance.zoom_level,\n        \"source_imagery\": instance.source_imagery\n        or instance.model.dataset.source_imagery,\n        \"freeze_layers\": instance.freeze_layers,\n        \"multimasks\": multimasks,\n        \"input_contact_spacing\": input_contact_spacing,\n        \"input_boundary_width\": input_boundary_width,\n    },\n    queue=(\n        \"ramp_training\"\n        if instance.model.base_model == \"RAMP\"\n        else \"yolo_training\"\n    ),\n)\n```\n\nThe task ID is stored in the Training model for later reference and status tracking.\n\n### Task Validation\n\nBefore submitting a task, the system performs several validations:\n\n1. Checks if another training is already running for the same model\n2. Verifies that labels exist for the dataset\n3. Validates that epochs and batch size are within configured limits\n\nThese validations ensure that system resources are used efficiently and prevent invalid training attempts.\n\nSources: [backend/core/views.py:148-232](), [backend/core/tasks.py:380-467]()\n\n## Task Status Monitoring\n\n### Status Tracking Endpoint\n\nThe system provides an API endpoint for monitoring the status of asynchronous tasks. The `run_task_status` view in `core/views.py` allows clients to check the current state of a training task:\n\n```python\n@api_view([\"GET\"])\ndef run_task_status(request, run_id: str):\n    # Implementation...\n```\n\nThis endpoint uses Celery's `AsyncResult` to retrieve the current status of the task and returns information including:\n\n- Task ID\n- Current status (PENDING, STARTED, RUNNING, FINISHED, FAILED)\n- Result or error information\n- Log output for running tasks\n\n### Status and Notification Updates\n\nThe asynchronous task updates the `Training` model's status field at key points in the execution:\n\n- \"SUBMITTED\" - Initial state when the task is created\n- \"RUNNING\" - When the worker starts processing the task\n- \"FINISHED\" - Upon successful completion\n- \"FAILED\" - If an error occurs during execution\n\nAdditionally, the system sends notifications to users when training status changes:\n\n```python\ndef send_notification(training_instance, status):\n    if \"web\" in training_instance.user.notifications_delivery_methods:\n        UserNotification.objects.create(\n            user=training_instance.user,\n            message=f\"Training {training_instance.id} has {status}.\",\n            training=training_instance,\n        )\n    if \"email\" in training_instance.user.notifications_delivery_methods:\n        # Email notification logic...\n```\n\nThis allows users to be informed of training progress without having to continuously check the status.\n\nSources: [backend/core/views.py:673-714](), [backend/core/utils.py:489-510](), [backend/core/tasks.py:402-404](), [backend/core/tasks.py:456-463]()\n\n## Secondary Asynchronous System: Django Q\n\nIn addition to Celery, the fAIr platform also uses Django Q for lighter background tasks. This is particularly used for processing raw data from OpenStreetMap:\n\n```python\nasync_task(\"core.views.process_rawdata_task\", obj.geom.geojson, aoi_id)\n```\n\nDjango Q is configured in the settings with parameters for workers, retries, and timeouts:\n\n```python\nQ_CLUSTER = {\n    \"name\": \"DjangORM\",\n    \"workers\": 4,\n    \"retry\": 60 * 6,\n    \"max_retires\": 1,\n    \"recycle\": 50,\n    \"queue_limit\": 50,\n    \"timeout\": 60 * 5,  # number of seconds\n    \"label\": \"Django Q\",\n    \"orm\": \"default\",\n}\n```\n\nThis provides a lightweight alternative to Celery for tasks that don't require the full power of a distributed task queue.\n\nSources: [backend/core/views.py:593](), [backend/aiproject/settings.py:222-232]()\n\n## Deployment Considerations\n\nWhen deploying the fAIr platform, the Celery workers must be started alongside the Django application. This is typically done with a command like:\n\n```bash\ncelery -A aiproject worker --loglevel=debug -Q ramp_training,yolo_training\n```\n\nThe workers subscribe to the specified queues and process tasks as they are submitted to the message broker. Additionally, the Flower dashboard can be started to monitor the Celery workers and tasks:\n\n```bash\ncelery -A aiproject --broker=redis://localhost:6379/ flower\n```\n\nFor Django Q, the worker process is started with:\n\n```bash\npython manage.py qcluster\n```\n\nThese processes should be managed by a process supervisor in production environments to ensure they remain running.\n\nSources: [.github/workflows/backend_build.yml:97-102](), [backend/README.md:160-176]()\n\n## Error Handling and Recovery\n\nThe asynchronous processing system includes robust error handling to manage failures in background tasks:\n\n1. Exceptions in the `train_model` task are caught and logged\n2. The training status is updated to \"FAILED\" in case of errors\n3. Error details are stored and made available through the status API\n4. Users are notified of training failures\n\nThis ensures that issues in background processing don't affect the overall system stability and users are informed of any problems.\n\n```mermaid\nflowchart TD\n    Start[\"Task Starts\"] --\u003e Running[\"Update Status: RUNNING\"]\n    Running --\u003e TryCatch{\"Try-Catch Block\"}\n    \n    TryCatch --\u003e|Success| Finalize[\"Finalize Training:\n    - Upload to S3\n    - Update accuracy\n    - Set status to FINISHED\"]\n    \n    TryCatch --\u003e|Exception| HandleError[\"Handle Error:\n    - Log exception\n    - Set status to FAILED\n    - Send notification\"]\n    \n    Finalize --\u003e Notify[\"Send completion notification\"]\n    HandleError --\u003e End[\"Task Ends\"]\n    Notify --\u003e End\n```\n\nSources: [backend/core/tasks.py:459-467]()\n\n## Conclusion\n\nThe asynchronous processing system in fAIr provides a scalable and efficient mechanism for handling resource-intensive tasks like model training. By leveraging Celery and Redis, the platform is able to distribute workloads across workers while maintaining a responsive user interface. The status tracking and notification mechanisms ensure that users stay informed about the progress of their tasks without requiring continuous polling.\n\nFor more details on specific models and data structures referenced in this document, see [Data Models](#2.2)."])</script><script>self.__next_f.push([1,"1f:T450d,"])</script><script>self.__next_f.push([1,"# Frontend System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [frontend/.gitignore](frontend/.gitignore)\n- [frontend/README.md](frontend/README.md)\n- [frontend/eslint.config.js](frontend/eslint.config.js)\n- [frontend/index.html](frontend/index.html)\n- [frontend/package.json](frontend/package.json)\n- [frontend/pnpm-lock.yaml](frontend/pnpm-lock.yaml)\n- [frontend/src/app/index.tsx](frontend/src/app/index.tsx)\n- [frontend/src/app/providers/auth-provider.tsx](frontend/src/app/providers/auth-provider.tsx)\n- [frontend/src/app/providers/models-provider.tsx](frontend/src/app/providers/models-provider.tsx)\n- [frontend/src/app/router.tsx](frontend/src/app/router.tsx)\n- [frontend/src/components/layouts/model-forms-layout.tsx](frontend/src/components/layouts/model-forms-layout.tsx)\n- [frontend/src/components/layouts/root-layout.tsx](frontend/src/components/layouts/root-layout.tsx)\n- [frontend/src/components/ui/animated-beam/animated-beam.tsx](frontend/src/components/ui/animated-beam/animated-beam.tsx)\n- [frontend/src/components/ui/banner/banner.tsx](frontend/src/components/ui/banner/banner.tsx)\n- [frontend/src/features/model-creation/components/progress-buttons.tsx](frontend/src/features/model-creation/components/progress-buttons.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/training-dataset.tsx](frontend/src/features/model-creation/components/training-dataset/training-dataset.tsx)\n- [frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx](frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx)\n- [frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx](frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx)\n- [frontend/src/hooks/use-login.ts](frontend/src/hooks/use-login.ts)\n- [frontend/src/hooks/use-map-instance.ts](frontend/src/hooks/use-map-instance.ts)\n- [frontend/src/hooks/use-storage.ts](frontend/src/hooks/use-storage.ts)\n- [frontend/src/services/api-client.ts](frontend/src/services/api-client.ts)\n- [frontend/src/services/auth.ts](frontend/src/services/auth.ts)\n- [frontend/src/store/model-prediction-store.ts](frontend/src/store/model-prediction-store.ts)\n- [frontend/src/styles/index.css](frontend/src/styles/index.css)\n\n\u003c/details\u003e\n\n\n\nThe Frontend System in the fAIr project provides the web-based user interface for AI-assisted mapping. It is a React-based application that enables users to create AI models, train them on OSM data, visualize predictions on aerial imagery, and provide feedback on these predictions to improve model performance.\n\nThis document covers the architecture, structure, and key components of the frontend system. For information about specific frontend features, see [Component Structure](#3.1), [Start Mapping Feature](#3.2), and [Model Creation and Management](#3.3).\n\n## Architecture Overview\n\nThe frontend is built with React 18, TypeScript, and Vite as a modern single-page application (SPA). It follows component-based architecture principles with well-defined responsibilities for each part of the system.\n\n```mermaid\ngraph TD\n    subgraph \"Application Entry\"\n        entry[\"main.tsx\"]\n        app[\"App Component\"]\n        router[\"AppRouter\"]\n    end\n\n    subgraph \"Global Providers\"\n        queryProvider[\"QueryClientProvider\"]\n        authProvider[\"AuthProvider\"]\n        modelsProvider[\"ModelsProvider\"]\n    end\n\n    subgraph \"Routing Layer\"\n        routes[\"Route Definitions\"]\n        layouts[\"Layout Components\"]\n        pages[\"Page Components\"]\n        protectedRoute[\"ProtectedRoute\"]\n    end\n\n    subgraph \"Core Features\"\n        modelCreation[\"Model Creation\"]\n        startMapping[\"Start Mapping\"]\n        auth[\"Authentication\"]\n        profile[\"User Profile\"]\n    end\n\n    subgraph \"UI Layer\"\n        mapComponents[\"Map Components\"]\n        uiComponents[\"UI Components\"]\n        forms[\"Form Components\"]\n    end\n\n    subgraph \"State Management\"\n        zustandStores[\"Zustand Stores\"]\n        reactContext[\"React Context\"]\n        reactQuery[\"TanStack Query\"]\n    end\n\n    subgraph \"Services\"\n        apiClient[\"API Client\"]\n        authService[\"Auth Service\"]\n    end\n\n    entry --\u003e app\n    app --\u003e queryProvider\n    app --\u003e router\n    queryProvider --\u003e authProvider\n    authProvider --\u003e modelsProvider\n    \n    router --\u003e routes\n    routes --\u003e layouts\n    routes --\u003e protectedRoute\n    layouts --\u003e pages\n    protectedRoute --\u003e pages\n    \n    pages --\u003e modelCreation\n    pages --\u003e startMapping\n    pages --\u003e auth\n    pages --\u003e profile\n    \n    modelCreation --\u003e uiComponents\n    modelCreation --\u003e mapComponents\n    modelCreation --\u003e forms\n    startMapping --\u003e mapComponents\n    \n    modelCreation --\u003e modelsProvider\n    modelCreation --\u003e zustandStores\n    modelCreation --\u003e reactQuery\n    startMapping --\u003e zustandStores\n    auth --\u003e authProvider\n    \n    modelsProvider --\u003e apiClient\n    authProvider --\u003e authService\n    authService --\u003e apiClient\n    reactQuery --\u003e apiClient\n```\n\nSources: [frontend/src/app/index.tsx:1-52](), [frontend/src/app/router.tsx:1-404](), [frontend/src/main.tsx]()\n\n## Key Technologies\n\nThe frontend system uses the following key technologies:\n\n| Technology | Purpose |\n|------------|---------|\n| React 18 | UI library |\n| TypeScript | Type safety and developer experience |\n| Vite | Build tool and development server |\n| React Router | Client-side routing |\n| TanStack Query | Data fetching, caching, and state management |\n| Zustand | Client-side state management |\n| MapLibre GL | Map visualization |\n| Tailwind CSS | Utility-first styling |\n| Shoelace | UI component library |\n\nSources: [frontend/package.json:1-91]()\n\n## Core Architecture\n\n### Application Bootstrap\n\nThe frontend application bootstrapping process begins at `main.tsx`, which renders the `App` component. The `App` component initializes global providers and mounts the router:\n\n```mermaid\nsequenceDiagram\n    participant Main as main.tsx\n    participant App as App Component\n    participant Providers as Global Providers\n    participant Router as AppRouter\n    participant RootLayout as RootLayout\n    participant Page as Current Page\n\n    Main-\u003e\u003eApp: Initialize app\n    App-\u003e\u003eProviders: Set up QueryClientProvider\n    App-\u003e\u003eRouter: Mount router\n    Router-\u003e\u003eRootLayout: Render root layout\n    RootLayout-\u003e\u003ePage: Render current page via Outlet\n```\n\nSources: [frontend/src/app/index.tsx:1-52]()\n\n### Routing System\n\nThe application uses React Router for navigation with a declarative route configuration. Routes are organized hierarchically and use code splitting with lazy loading for better performance:\n\n```typescript\n// Route structure (simplified)\nconst router = createBrowserRouter([\n  {\n    element: \u003cRootLayout /\u003e,\n    children: [\n      {\n        path: APPLICATION_ROUTES.HOMEPAGE,\n        lazy: async () =\u003e {\n          const { LandingPage } = await import(\"@/app/routes/landing\");\n          return { Component: LandingPage };\n        },\n      },\n      // More routes...\n    ],\n  },\n]);\n```\n\nProtected routes ensure authenticated access where needed:\n\n```mermaid\ngraph TD\n    subgraph \"Route Protection\"\n        routes[\"Routes\"]\n        auth[\"AuthProvider\"]\n        protected[\"ProtectedRoute\"]\n        redirect[\"Redirect to login\"]\n        component[\"Protected Component\"]\n    end\n\n    routes --\u003e protected\n    protected --\u003e auth\n    auth -- \"isAuthenticated=false\" --\u003e redirect\n    auth -- \"isAuthenticated=true\" --\u003e component\n```\n\nSources: [frontend/src/app/router.tsx:1-404]()\n\n### Layout Components\n\nThe application has two primary layout components:\n\n1. **RootLayout**: The main application wrapper that includes the navbar, footer, and common UI elements.\n2. **ModelFormsLayout**: A specialized layout for the model creation workflow with progress tracking.\n\n```typescript\n// RootLayout structure (simplified)\nexport const RootLayout = () =\u003e {\n  return (\n    \u003cmain className=\"min-h-screen relative flex flex-col\"\u003e\n      \u003cNavBar /\u003e\n      \u003cdiv className=\"flex-1 mx-auto w-full\"\u003e\n        \u003cOutlet /\u003e {/* Page content rendered here */}\n      \u003c/div\u003e\n      \u003cFooter /\u003e\n    \u003c/main\u003e\n  );\n};\n```\n\nSources: [frontend/src/components/layouts/root-layout.tsx:1-114](), [frontend/src/components/layouts/model-forms-layout.tsx:1-181]()\n\n## State Management\n\nThe frontend uses a hybrid approach to state management, combining several strategies based on the specific needs of each feature:\n\n```mermaid\ngraph TD\n    subgraph \"State Management\"\n        global[\"Global State\"]\n        server[\"Server State\"]\n        local[\"Local UI State\"]\n        persistent[\"Persistent State\"]\n    end\n\n    subgraph \"Implementation\"\n        context[\"React Context\"]\n        query[\"TanStack Query\"]\n        zustand[\"Zustand Stores\"]\n        storage[\"Browser Storage\"]\n    end\n\n    global --\u003e context\n    server --\u003e query\n    local --\u003e zustand\n    persistent --\u003e storage\n    storage --\u003e context\n```\n\n### Context Providers\n\nTwo main context providers manage global application state:\n\n1. **AuthProvider**: Manages authentication state, user information, and authentication flows.\n\n```typescript\n// AuthProvider structure (simplified)\nexport const AuthProvider = ({ children }) =\u003e {\n  const [token, setToken] = useState(localStorage.getItem(TOKEN_KEY));\n  const [user, setUser] = useState(null);\n  const isAuthenticated = user !== null \u0026\u0026 token !== undefined;\n  \n  // Authentication methods and effects\n  \n  return (\n    \u003cAuthContext.Provider value={{ \n      token, user, isAuthenticated, authenticateUser, logout \n    }}\u003e\n      {children}\n    \u003c/AuthContext.Provider\u003e\n  );\n};\n```\n\n2. **ModelsProvider**: Manages state for model creation and training workflows.\n\n```typescript\n// ModelsProvider structure (simplified)\nexport const ModelsProvider = ({ children }) =\u003e {\n  const [formData, setFormData] = useState(initialFormState);\n  \n  // Form management methods and API interactions\n  \n  return (\n    \u003cModelsContext.Provider value={{ \n      formData, setFormData, handleChange, handleModelCreationAndUpdate \n    }}\u003e\n      {children}\n    \u003c/ModelsContext.Provider\u003e\n  );\n};\n```\n\nSources: [frontend/src/app/providers/auth-provider.tsx:1-213](), [frontend/src/app/providers/models-provider.tsx:1-582]()\n\n### Zustand Stores\n\nZustand is used for UI-specific state that doesn't need to be globally available through context:\n\n```typescript\n// ModelPredictionStore example (simplified)\nexport const useModelPredictionStore = create\u003cModelPredictionState\u003e((set, get) =\u003e ({\n  modelPredictions: { accepted: [], rejected: [], all: [] },\n  \n  setModelPredictions: (newPredictions) =\u003e {\n    set({ modelPredictions: newPredictions });\n  },\n  \n  moveFeatureBetweenBuckets: (from, to, id, updatedProps = {}) =\u003e {\n    // Implementation to move predictions between categories\n  },\n}));\n```\n\nSources: [frontend/src/store/model-prediction-store.ts:1-58]()\n\n### Browser Storage\n\nThe application uses browser storage (localStorage and sessionStorage) to persist state across page reloads:\n\n```typescript\n// useLocalStorage hook (simplified)\nexport const useLocalStorage = () =\u003e {\n  const getValue = (key: string): string | undefined =\u003e {\n    try {\n      return localStorage.getItem(key) || undefined;\n    } catch (error) {\n      showErrorToast(error);\n    }\n  };\n  \n  const setValue = (key: string, value: string): void =\u003e {\n    try {\n      localStorage.setItem(key, value);\n    } catch (error) {\n      showErrorToast(error);\n    }\n  };\n  \n  return { getValue, setValue, removeValue };\n};\n```\n\nSources: [frontend/src/hooks/use-storage.ts:1-83]()\n\n## Key Features\n\n### Authentication System\n\nThe authentication system integrates with OpenStreetMap (OSM) OAuth for user authentication:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant App as fAIr App\n    participant Auth as AuthProvider\n    participant OSM as OpenStreetMap OAuth\n    participant Backend as fAIr Backend\n\n    User-\u003e\u003eApp: Click Login\n    App-\u003e\u003eBackend: Request login URL\n    Backend--\u003e\u003eApp: Return login URL\n    App-\u003e\u003eOSM: Redirect to OSM OAuth\n    User-\u003e\u003eOSM: Authorize application\n    OSM--\u003e\u003eApp: Redirect with code and state\n    App-\u003e\u003eAuth: Process code and state\n    Auth-\u003e\u003eBackend: Exchange for access token\n    Backend--\u003e\u003eAuth: Return access token\n    Auth-\u003e\u003eBackend: Get user profile\n    Backend--\u003e\u003eAuth: Return user info\n    Auth--\u003e\u003eApp: Set authenticated state\n    App--\u003e\u003eUser: Show authenticated UI\n```\n\nThe `useLogin` hook encapsulates the login flow:\n\n```typescript\n// useLogin hook (simplified)\nexport const useLogin = () =\u003e {\n  const [loading, setLoading] = useState(false);\n  \n  const handleLogin = async (): Promise\u003cvoid\u003e =\u003e {\n    setLoading(true);\n    setSessionValue(REDIRECT_KEY, currentPathWithQuery);\n    try {\n      await authService.initializeOAuthFlow();\n    } catch (error) {\n      showErrorToast(error);\n    } finally {\n      setLoading(false);\n    }\n  };\n  \n  return { loading, handleLogin };\n};\n```\n\nSources: [frontend/src/app/providers/auth-provider.tsx:1-213](), [frontend/src/hooks/use-login.ts:1-42](), [frontend/src/services/auth.ts:1-114]()\n\n### Map Visualization\n\nThe map visualization system uses MapLibre GL and several custom components to display geographic data:\n\n```mermaid\ngraph TD\n    subgraph \"Map System\"\n        hook[\"useMapInstance Hook\"]\n        map[\"Map Component\"]\n        terraDraw[\"TerraDraw Integration\"]\n    end\n    \n    subgraph \"Prediction Layers\"\n        accepted[\"AcceptedPredictionsLayer\"]\n        rejected[\"RejectedPredictionsLayer\"]\n        all[\"AllPredictionsLayer\"]\n    end\n    \n    subgraph \"Data Sources\"\n        imagery[\"Aerial Imagery\"]\n        predictions[\"Model Predictions\"]\n        drawings[\"User Drawings\"]\n    end\n    \n    hook --\u003e map\n    hook --\u003e terraDraw\n    map --\u003e accepted\n    map --\u003e rejected\n    map --\u003e all\n    imagery --\u003e map\n    predictions --\u003e accepted\n    predictions --\u003e rejected\n    predictions --\u003e all\n    drawings --\u003e terraDraw\n```\n\nThe `useMapInstance` hook initializes and manages the map:\n\n```typescript\n// useMapInstance hook (simplified)\nexport const useMapInstance = (pmtiles: boolean = false) =\u003e {\n  const mapContainerRef = useRef\u003cHTMLDivElement\u003e(null);\n  const [map, setMap] = useState\u003cMap | null\u003e(null);\n  const [drawingMode, setDrawingMode] = useState\u003cDrawingModes\u003e(DrawingModes.STATIC);\n  \n  // Map initialization and terra-draw setup\n  \n  return {\n    mapContainerRef,\n    map,\n    drawingMode,\n    setDrawingMode,\n    terraDraw,\n  };\n};\n```\n\nPrediction layers show model outputs on the map:\n\n```typescript\n// AcceptedPredictionsLayer component (simplified)\nexport const AcceptedPredictionsLayer = ({ map, features }) =\u003e {\n  useEffect(() =\u003e {\n    if (!map || !map.getStyle()) return;\n    \n    // Add source and layers for accepted predictions\n    \n    return () =\u003e {\n      // Cleanup layers and sources\n    };\n  }, [map]);\n  \n  // Update feature data when it changes\n  \n  return null;\n};\n```\n\nSources: [frontend/src/hooks/use-map-instance.ts:1-70](), [frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx:1-84](), [frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx:1-85]()\n\n## Development and Tooling\n\n### Build Process\n\nThe frontend uses Vite for development and building:\n\n| Command | Description |\n|---------|-------------|\n| `pnpm dev` | Start development server |\n| `pnpm build` | Build production bundle |\n| `pnpm lint` | Run ESLint |\n| `pnpm format` | Format code with Prettier |\n| `pnpm test` | Run tests with Vitest |\n\nSources: [frontend/package.json:1-91]()\n\n### Code Quality Tools\n\nThe codebase employs several tools for maintaining code quality:\n\n1. **TypeScript** for static type checking\n2. **ESLint** for linting with custom rules\n3. **Prettier** for code formatting\n4. **Vitest** for testing\n\n```typescript\n// ESLint configuration (simplified)\nexport default [\n  {\n    files: ['**/*.{ts,tsx}'],\n    languageOptions: {\n      parser: '@typescript-eslint/parser',\n    },\n    plugins: {\n      'react-hooks': reactHooks,\n      'react-refresh': reactRefresh,\n      '@tanstack/query': '@tanstack/query',\n      'prettier': prettierPlugin,\n    },\n    // Rules configuration\n  },\n  // Additional configurations\n];\n```\n\nSources: [frontend/eslint.config.js:1-42]()\n\n## Styling System\n\nThe frontend uses Tailwind CSS for styling with custom design tokens defined in CSS variables:\n\n```css\n:root {\n  /* Color tokens */\n  --hot-fair-color-primary: #d63f40;\n  --hot-fair-color-secondary: #ffeded;\n  --hot-fair-color-dark: #2c3038;\n  \n  /* Font sizes */\n  --hot-fair-font-size-extra-large: 4.25rem;\n  --hot-fair-font-size-large-title: 3rem;\n  \n  /* Font weights */\n  --hot-fair-font-weight-regular: 400;\n  --hot-fair-font-weight-medium: 500;\n  \n  /* Spacing */\n  --hot-fair-spacing-extra-large: 5rem;\n  --hot-fair-spacing-large: 2rem;\n}\n```\n\nCustom utility classes extend Tailwind's functionality:\n\n```css\n@layer components {\n  .icon {\n    @apply inline-block h-4 w-4;\n  }\n  \n  .app-padding {\n    @apply px-small md:px-large 2xl:px-extra-large;\n  }\n  \n  .map-elements-z-index {\n    @apply z-[1];\n  }\n}\n```\n\nSources: [frontend/src/styles/index.css:1-241]()\n\n## Conclusion\n\nThe Frontend System of the fAIr project is a modern React application that provides a user-friendly interface for AI-assisted mapping. It integrates with the backend API to enable model creation, training, and prediction review workflows. The system is built with maintainability and extensibility in mind, using contemporary web development practices and tools.\n\nFor more detailed information about specific features, please refer to the linked wiki pages at the top of this document."])</script><script>self.__next_f.push([1,"20:T36db,"])</script><script>self.__next_f.push([1,"# Component Structure\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [frontend/.gitignore](frontend/.gitignore)\n- [frontend/README.md](frontend/README.md)\n- [frontend/eslint.config.js](frontend/eslint.config.js)\n- [frontend/index.html](frontend/index.html)\n- [frontend/package.json](frontend/package.json)\n- [frontend/pnpm-lock.yaml](frontend/pnpm-lock.yaml)\n- [frontend/src/components/layouts/root-layout.tsx](frontend/src/components/layouts/root-layout.tsx)\n- [frontend/src/components/ui/animated-beam/animated-beam.tsx](frontend/src/components/ui/animated-beam/animated-beam.tsx)\n- [frontend/src/components/ui/banner/banner.tsx](frontend/src/components/ui/banner/banner.tsx)\n- [frontend/src/components/ui/dropdown/dropdown.tsx](frontend/src/components/ui/dropdown/dropdown.tsx)\n- [frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx](frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx)\n- [frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx](frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx)\n- [frontend/src/features/user-profile/api/notifications.ts](frontend/src/features/user-profile/api/notifications.ts)\n- [frontend/src/features/user-profile/hooks/use-notifications.ts](frontend/src/features/user-profile/hooks/use-notifications.ts)\n- [frontend/src/hooks/__tests__/use-click-outside.test.ts](frontend/src/hooks/__tests__/use-click-outside.test.ts)\n- [frontend/src/hooks/use-click-outside.ts](frontend/src/hooks/use-click-outside.ts)\n- [frontend/src/hooks/use-map-instance.ts](frontend/src/hooks/use-map-instance.ts)\n- [frontend/src/store/model-prediction-store.ts](frontend/src/store/model-prediction-store.ts)\n- [frontend/src/styles/index.css](frontend/src/styles/index.css)\n- [frontend/src/utils/regex-utils.ts](frontend/src/utils/regex-utils.ts)\n\n\u003c/details\u003e\n\n\n\nThis page documents the frontend component architecture of the fAIr system, explaining how components are organized, their hierarchical relationships, and the patterns used throughout the application. For information about the backend API endpoints that these components interact with, see [API Endpoints](#2.1).\n\n## Overview of Frontend Architecture\n\nThe fAIr frontend is built using React 18 with TypeScript, adopting a feature-based organization pattern. The application follows modern React best practices including functional components, hooks, and a combination of context and store-based state management.\n\n```mermaid\ngraph TD\n    subgraph \"Entry Point\"\n        Main[\"main.tsx\"]\n    end\n\n    subgraph \"App Structure\"\n        Routes[\"Application Routes\"]\n        Providers[\"Context Providers\"]\n        Layouts[\"Layout Components\"]\n    end\n\n    subgraph \"Components\"\n        UI[\"UI Components\"]\n        Shared[\"Shared Components\"]\n        FeatureComponents[\"Feature Components\"]\n    end\n\n    subgraph \"State Management\"\n        Stores[\"Zustand Stores\"]\n        ReactQuery[\"React Query\"]\n    end\n\n    subgraph \"Services\"\n        API[\"API Services\"]\n        Auth[\"Authentication\"]\n    end\n\n    Main --\u003e Routes\n    Main --\u003e Providers\n    Routes --\u003e Layouts\n    Layouts --\u003e FeatureComponents\n    FeatureComponents --\u003e UI\n    FeatureComponents --\u003e Shared\n    FeatureComponents --\u003e API\n    FeatureComponents --\u003e Stores\n    FeatureComponents --\u003e ReactQuery\n    Providers --\u003e Auth\n```\n\nSources: [frontend/src/main.tsx](), [frontend/README.md:55-76](), [frontend/package.json:16-50]()\n\n## Directory Structure\n\nThe frontend codebase follows a well-organized directory structure that separates concerns and promotes reusability. The primary source code is located in the `src` directory with the following organization:\n\n```mermaid\ngraph TD\n    src[\"src/\"]\n    app[\"app/ - Routes \u0026 Providers\"]\n    assets[\"assets/ - Static assets\"]\n    components[\"components/ - Reusable components\"]\n    config[\"config/ - Environment variables\"]\n    constants[\"constants/ - UI constants\"]\n    enums[\"enums/ - TypeScript enums\"]\n    features[\"features/ - Feature modules\"]\n    hooks[\"hooks/ - Custom hooks\"]\n    layouts[\"layouts/ - Core layouts\"]\n    services[\"services/ - API clients\"]\n    styles[\"styles/ - Global styles\"]\n    types[\"types/ - TypeScript types\"]\n    utils[\"utils/ - Utility functions\"]\n    main[\"main.tsx - Entry point\"]\n    \n    src --\u003e app\n    src --\u003e assets\n    src --\u003e components\n    src --\u003e config\n    src --\u003e constants\n    src --\u003e enums\n    src --\u003e features\n    src --\u003e hooks\n    src --\u003e layouts\n    src --\u003e services\n    src --\u003e styles\n    src --\u003e types\n    src --\u003e utils\n    src --\u003e main\n```\n\nSources: [frontend/README.md:55-76]()\n\n## Core Layout Components\n\nThe application uses a hierarchical layout system where the `RootLayout` component serves as the main container for the application, handling the rendering of common UI elements across all pages.\n\n```mermaid\ngraph TD\n    subgraph \"Layout Structure\"\n        RootLayout[\"RootLayout - Main container\"]\n        NavBar[\"NavBar\"]\n        Footer[\"Footer\"]\n        Banner[\"Banner\"]\n        Content[\"Page Content (Outlet)\"]\n    end\n\n    RootLayout --\u003e NavBar\n    RootLayout --\u003e Banner\n    RootLayout --\u003e Content\n    RootLayout --\u003e Footer\n    \n    subgraph \"Conditional Rendering\"\n        Auth[\"Authentication Modal\"]\n        HotTracking[\"Tracking Component\"]\n    end\n    \n    RootLayout --\u003e Auth\n    RootLayout --\u003e HotTracking\n```\n\nThe `RootLayout` handles conditional rendering based on the current route:\n\n1. It shows/hides the Banner based on pathname and timeout\n2. It conditionally renders the NavBar except on certain pages\n3. It applies different padding and background styles based on the route\n4. The Footer is hidden on specific routes (mapping, model creation)\n5. Authentication modals appear when needed\n\nSources: [frontend/src/components/layouts/root-layout.tsx:16-114]()\n\n## Component Types\n\nThe fAIr frontend employs several types of components, each with distinct responsibilities:\n\n### 1. UI Components\n\nThese are the basic building blocks - reusable, presentational components that compose the user interface.\n\n```mermaid\nflowchart TD\n    subgraph \"UI Components\"\n        direction TB\n        Button[\"Button Components\"]\n        Dropdown[\"Dropdown Menus\"]\n        Banner[\"Banner Component\"]\n        AnimatedBeam[\"Animated UI Elements\"]\n        Icons[\"Icon Components\"]\n    end\n    \n    Dropdown --\u003e ShoelaceIntegration[\"Shoelace Integration\\n(@shoelace-style/shoelace)\"]\n```\n\nUI components like `Dropdown` integrate with the Shoelace component library while adding custom functionality. The `Dropdown` component, for example, enhances Shoelace's dropdown with features like multi-select, checkbox integration, and custom styling.\n\nSources: [frontend/src/components/ui/dropdown/dropdown.tsx:40-166](), [frontend/src/components/ui/banner/banner.tsx:19-49](), [frontend/src/components/ui/animated-beam/animated-beam.tsx:27-190]()\n\n### 2. Feature Components\n\nFeature components implement specific application features and are organized in feature modules.\n\n```mermaid\nflowchart TD\n    subgraph \"Feature Components\"\n        StartMapping[\"Start Mapping Feature\"]\n        ModelCreation[\"Model Creation \u0026 Management\"]\n        UserProfile[\"User Profile \u0026 Notifications\"]\n    end\n    \n    subgraph \"Start Mapping Layers\"\n        AcceptedLayer[\"AcceptedPredictionsLayer\"]\n        RejectedLayer[\"RejectedPredictionsLayer\"]\n    end\n    \n    StartMapping --\u003e AcceptedLayer\n    StartMapping --\u003e RejectedLayer\n    \n    UserProfile --\u003e Notifications[\"Notification Management\"]\n```\n\nFeature components typically combine UI components with business logic and state management. The mapping feature, for example, includes specialized layers for visualizing model predictions.\n\nSources: [frontend/src/features/start-mapping/components/map/layers/accepted-prediction-layer.tsx:10-84](), [frontend/src/features/start-mapping/components/map/layers/rejected-prediction-layer.tsx:10-85](), [frontend/src/features/user-profile/hooks/use-notifications.ts:18-87]()\n\n### 3. Layout Components\n\nLayout components define the overall structure of the application and individual pages.\n\n### 4. Shared Components\n\nShared components are reused across different features but aren't simple UI elements.\n\n## State Management\n\nThe application uses a multi-faceted approach to state management:\n\n```mermaid\nflowchart TD\n    subgraph \"State Management\"\n        direction TB\n        LocalState[\"Component Local State\\n(useState)\"]\n        ContextAPI[\"React Context API\"]\n        ZustandStores[\"Zustand Stores\"]\n        ReactQuery[\"React Query\\n(Server State)\"]\n    end\n    \n    ZustandStores --\u003e ModelPrediction[\"ModelPredictionStore\"]\n    ZustandStores --\u003e MapStore[\"MapStore\"]\n    \n    ContextAPI --\u003e AuthContext[\"Authentication Context\"]\n    ContextAPI --\u003e ModelsContext[\"Models Context\"]\n    \n    ReactQuery --\u003e Queries[\"Data Fetching\"]\n    ReactQuery --\u003e Mutations[\"Data Updates\"]\n```\n\n1. **Zustand Stores**: Used for UI-related global state\n   - `ModelPredictionStore`: Manages model predictions for mapping\n   - `MapStore`: Manages map state (zoom, view, etc.)\n\n2. **React Context**: Used for authentication and configuration state\n   - `AuthProvider`: Manages user authentication state\n   - `ModelsProvider`: Provides model-related data and operations\n\n3. **React Query**: Manages server state, including data fetching and caching\n\nSources: [frontend/src/store/model-prediction-store.ts:4-58](), [frontend/src/hooks/use-map-instance.ts:14-70]()\n\n## Custom Hooks\n\nThe application uses custom hooks extensively to encapsulate and reuse logic.\n\n```mermaid\nflowchart LR\n    subgraph \"Custom Hooks\"\n        direction TB\n        useMapInstance[\"useMapInstance\\nManages map initialization\"]\n        useClickOutside[\"useClickOutside\\nDetects clicks outside an element\"]\n        useNotifications[\"useNotifications\\nFetches notification data\"]\n    end\n    \n    useMapInstance --\u003e maplibre[\"maplibre-gl Library\"]\n    useMapInstance --\u003e terraDraw[\"terra-draw Library\"]\n    \n    useNotifications --\u003e ReactQuery[\"React Query\"]\n```\n\nCustom hooks abstract complex logic and provide reusable functionality:\n\n1. **Map-related hooks**: Initialize and manage map instances, layers, etc.\n2. **UI hooks**: Handle UI interactions like detecting outside clicks\n3. **Data hooks**: Wrap API calls and data management with React Query\n\nSources: [frontend/src/hooks/use-map-instance.ts:14-70](), [frontend/src/hooks/use-click-outside.ts:9-31](), [frontend/src/features/user-profile/hooks/use-notifications.ts:18-87]()\n\n## Integration with External Libraries\n\nThe frontend integrates several key libraries to provide specialized functionality:\n\n```mermaid\nflowchart TD\n    subgraph \"External Library Integration\"\n        direction TB\n        Shoelace[\"@shoelace-style/shoelace\\nUI Component Library\"]\n        MapLibre[\"maplibre-gl\\nMap Rendering Library\"]\n        TerraDraw[\"terra-draw\\nMap Drawing Tools\"]\n        TanStack[\"TanStack Libraries\\n(React Query, React Table)\"]\n        FramerMotion[\"framer-motion\\nAnimations\"]\n    end\n    \n    Shoelace --\u003e CustomComponents[\"Custom Component Wrappers\"]\n    MapLibre --\u003e MapInstance[\"Map Instance\"]\n    TerraDraw --\u003e DrawingTools[\"Drawing Tools Integration\"]\n    TanStack --\u003e DataFetching[\"Data Fetching \u0026 State Management\"]\n    FramerMotion --\u003e Animations[\"UI Animations\"]\n```\n\nEach external library is carefully integrated and often wrapped in custom components or hooks to match the application's specific requirements.\n\nSources: [frontend/package.json:16-50](), [frontend/src/components/ui/dropdown/dropdown.tsx:4-8]()\n\n## Styling Approach\n\nThe application uses a combination of Tailwind CSS and CSS custom properties (variables) for styling.\n\n```mermaid\nflowchart TD\n    subgraph \"Styling System\"\n        direction TB\n        TailwindCSS[\"Tailwind CSS\"]\n        CSSVariables[\"CSS Custom Properties\"]\n        UtilityFunctions[\"Utility Functions (cn)\"]\n    end\n    \n    CSSVariables --\u003e Colors[\"Color Tokens\\n--hot-fair-color-*\"]\n    CSSVariables --\u003e Typography[\"Typography Tokens\\n--hot-fair-font-*\"]\n    CSSVariables --\u003e Spacing[\"Spacing Tokens\\n--hot-fair-spacing-*\"]\n    \n    TailwindCSS --\u003e CustomClasses[\"Custom Utility Classes\"]\n    TailwindCSS --\u003e Components[\"Component Styling\"]\n    \n    UtilityFunctions --\u003e ClassMerging[\"Class Name Merging\"]\n```\n\nThe styling system uses:\n\n1. **CSS Variables**: Design tokens for colors, typography, spacing, etc.\n2. **Tailwind CSS**: For rapid UI development with utility classes\n3. **CSS Utilities**: Custom utility classes for common styling patterns\n4. **Class Merging**: Utility function for combining class names conditionally\n\nSources: [frontend/src/styles/index.css:10-70](), [frontend/src/utils/regex-utils.ts:1-10]()\n\n## Component Communication Patterns\n\nThe application employs several patterns for component communication:\n\n1. **Props**: Standard React props for parent-child communication\n2. **Context API**: For sharing state across component trees\n3. **Zustand Stores**: For global state accessible anywhere\n4. **Events**: DOM events and custom event handling\n5. **React Query**: For server state synchronization\n\nThese patterns are used together depending on the specific requirements of each feature and component.\n\n## Routing Structure\n\nThe application uses React Router for routing with a nested route structure. The `RootLayout` component serves as the main layout for most routes, with conditional rendering based on the current route.\n\nRoutes are defined in constants and organized hierarchically to reflect the application's feature structure.\n\nSources: [frontend/src/components/layouts/root-layout.tsx:16-114]()\n\n## Conclusion\n\nThe fAIr frontend employs a well-structured component architecture that separates concerns, promotes reusability, and follows modern React best practices. The combination of feature-based organization, custom hooks, and thoughtful state management creates a maintainable and scalable codebase."])</script><script>self.__next_f.push([1,"21:T3fb1,"])</script><script>self.__next_f.push([1,"# Start Mapping Feature\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [frontend/src/app/routes/start-mapping.tsx](frontend/src/app/routes/start-mapping.tsx)\n- [frontend/src/assets/images/index.ts](frontend/src/assets/images/index.ts)\n- [frontend/src/components/map/layers/open-aerial-map.tsx](frontend/src/components/map/layers/open-aerial-map.tsx)\n- [frontend/src/components/map/layers/tile-boundaries.tsx](frontend/src/components/map/layers/tile-boundaries.tsx)\n- [frontend/src/components/map/setups/setup-terra-draw.ts](frontend/src/components/map/setups/setup-terra-draw.ts)\n- [frontend/src/components/ui/form/radio-group/radio-group.css](frontend/src/components/ui/form/radio-group/radio-group.css)\n- [frontend/src/components/ui/form/radio-group/radio-group.tsx](frontend/src/components/ui/form/radio-group/radio-group.tsx)\n- [frontend/src/enums/start-mapping.ts](frontend/src/enums/start-mapping.ts)\n- [frontend/src/features/models/components/model-detail-user.tsx](frontend/src/features/models/components/model-detail-user.tsx)\n- [frontend/src/features/start-mapping/components/index.ts](frontend/src/features/start-mapping/components/index.ts)\n- [frontend/src/features/start-mapping/components/map/layers/all-prediction-layer.tsx](frontend/src/features/start-mapping/components/map/layers/all-prediction-layer.tsx)\n- [frontend/src/features/start-mapping/components/map/map.tsx](frontend/src/features/start-mapping/components/map/map.tsx)\n- [frontend/src/features/start-mapping/components/mobile-drawer.tsx](frontend/src/features/start-mapping/components/mobile-drawer.tsx)\n- [frontend/src/features/start-mapping/components/model-settings.tsx](frontend/src/features/start-mapping/components/model-settings.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector-trigger-button.tsx](frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector-trigger-button.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector.tsx](frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/model-selector-trigger-button.tsx](frontend/src/features/start-mapping/components/replicable-models/model-selector-trigger-button.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/model-selector.tsx](frontend/src/features/start-mapping/components/replicable-models/model-selector.tsx)\n- [frontend/src/hooks/use-dropdown-menu.ts](frontend/src/hooks/use-dropdown-menu.ts)\n- [frontend/src/hooks/use-map-layer.ts](frontend/src/hooks/use-map-layer.ts)\n- [frontend/src/types/api.ts](frontend/src/types/api.ts)\n- [frontend/src/utils/__tests__/geo/geometry-utils.test.ts](frontend/src/utils/__tests__/geo/geometry-utils.test.ts)\n- [frontend/src/utils/general-utils.ts](frontend/src/utils/general-utils.ts)\n\n\u003c/details\u003e\n\n\n\nThe Start Mapping feature allows users to view and interact with AI model predictions on a map interface. This feature enables users to select prediction models, choose imagery sources, configure prediction parameters, and manage prediction results by accepting or rejecting them. Users can also download predictions in various formats for use in external applications like JOSM (Java OpenStreetMap Editor).\n\nFor information about creating and training models, see [Model Creation and Management](#3.3).\n\n## Overview\n\nThe Start Mapping feature is a comprehensive interface for working with trained AI models to predict features on a map. It provides tools for:\n\n1. Selecting models and checkpoints for prediction\n2. Choosing imagery sources\n3. Configuring prediction parameters (confidence level, tolerance, area)\n4. Viewing predictions on the map\n5. Accepting or rejecting predictions\n6. Downloading predictions in various formats\n\n```mermaid\nflowchart TD\n    subgraph \"Start Mapping Process\"\n        A[\"Model Selection\"] --\u003e B[\"Imagery Selection\"]\n        B --\u003e C[\"Configure Parameters\"]\n        C --\u003e D[\"Generate Predictions\"]\n        D --\u003e E[\"View on Map\"]\n        E --\u003e F[\"Accept/Reject\"]\n        F --\u003e G[\"Download Results\"]\n    end\n```\n\nSources: [frontend/src/app/routes/start-mapping.tsx:64-517]()\n\n## User Interface Components\n\nThe Start Mapping page is composed of several key components that work together to provide the mapping experience:\n\n```mermaid\nflowchart TD\n    StartMapping[\"StartMappingPage\"] --\u003e Header[\"StartMappingHeader\"]\n    StartMapping --\u003e MapComponent[\"StartMappingMapComponent\"]\n    StartMapping --\u003e MobileDrawer[\"StartMappingMobileDrawer\"]\n    \n    Header --\u003e ModelSelector[\"ModelSelector\"]\n    Header --\u003e ImagerySelector[\"ImagerySourceSelector\"]\n    Header --\u003e ModelSettings[\"ModelSettings\"]\n    \n    MapComponent --\u003e PredictionLayers[\"PredictionLayers\"]\n    PredictionLayers --\u003e AcceptedLayer[\"AcceptedPredictionsLayer\"]\n    PredictionLayers --\u003e RejectedLayer[\"RejectedPredictionsLayer\"]\n    PredictionLayers --\u003e AllLayer[\"AllPredictionsLayer\"]\n    MapComponent --\u003e FeaturePopup[\"PredictedFeatureActionPopup\"]\n    \n    MobileDrawer --\u003e MobileModelSelector[\"ModelSelector (Mobile)\"]\n    MobileDrawer --\u003e MobileImagerySelector[\"ImagerySourceSelector (Mobile)\"]\n    MobileDrawer --\u003e MobileModelSettings[\"ModelSettings (Mobile)\"]\n```\n\nSources: [frontend/src/app/routes/start-mapping.tsx:368-516](), [frontend/src/features/start-mapping/components/index.ts:1-9]()\n\n### Main Page Layout\n\nThe Start Mapping page is structured with:\n\n- A header section (on desktop) containing model information and control options\n- A map component that displays the imagery and prediction layers\n- A mobile drawer that provides controls on smaller screens\n\nThe layout adapts based on screen size, with different UI components shown for desktop and mobile views.\n\nSources: [frontend/src/app/routes/start-mapping.tsx:368-516]()\n\n### Desktop Header\n\nThe desktop header includes:\n- Model name and information\n- Model selector\n- Imagery source selector\n- Settings configuration\n- Download options\n\nSources: [frontend/src/app/routes/start-mapping.tsx:442-470]()\n\n### Map Component\n\nThe map component is the central element of the interface, displaying:\n- Background imagery from the selected source\n- Prediction layers (accepted, rejected, and all predictions)\n- Interactive controls for navigating the map\n- Popups for interacting with individual predictions\n\nSources: [frontend/src/features/start-mapping/components/map/map.tsx:29-175]()\n\n### Mobile Interface\n\nOn mobile devices, a drawer interface provides access to the same controls as the desktop header. The drawer can be expanded and collapsed, and includes:\n- Model selection\n- Imagery source selection\n- Prediction settings\n- Download options\n\nSources: [frontend/src/features/start-mapping/components/mobile-drawer.tsx:22-201]()\n\n## Model Selection and Configuration\n\n### Model Selection\n\nThe Start Mapping page allows users to select from different models for prediction:\n\n1. **Default Model**: The model associated with the current page\n2. **RAMP**: Rapid Assessment Multi-Purpose framework model\n3. **YOLO v8 v1**: You Only Look Once version 8 (first variant)\n4. **YOLO v8 v2**: You Only Look Once version 8 (second variant)\n5. **Custom Model**: User-provided model checkpoint\n\nModel selection is handled by the `ModelSelector` component, which provides a radio button interface for selecting models.\n\n```mermaid\nclassDiagram\n    class ModelSelector {\n        +predictionModel: string\n        +setPredictionModel: function\n        +predictionModelCheckpoint: string\n        +setPredictionModelCheckpoint: function\n        +modelInfo: TModelDetails\n        +render()\n    }\n    \n    class PredictionModel {\n        \u003c\u003cenumeration\u003e\u003e\n        DEFAULT\n        RAMP\n        YOLOV8_V1\n        YOLOV8_V2\n        CUSTOM\n    }\n    \n    ModelSelector -- PredictionModel : uses\n```\n\nSources: [frontend/src/features/start-mapping/components/replicable-models/model-selector.tsx:16-191](), [frontend/src/enums/start-mapping.ts:10-14]()\n\n### Imagery Source Selection\n\nUsers can select from different imagery sources for the background map:\n\n1. **Model Default**: The imagery associated with the model's training dataset\n2. **Custom Imagery**: User-provided XYZ/TMS tile server URL\n3. **OpenAerialMap Mosaic**: All OpenAerialMap images in one mosaic layer\n\nImagery selection is handled by the `ImagerySourceSelector` component.\n\n```mermaid\nclassDiagram\n    class ImagerySourceSelector {\n        +setPredictionImageryURL: function\n        +predictionImagerySource: PredictionImagerySource\n        +setPredictionImagerySource: function\n        +modelDefaultImageryURL: string\n        +render()\n    }\n    \n    class PredictionImagerySource {\n        \u003c\u003cenumeration\u003e\u003e\n        ModelDefault\n        CustomImagery\n        Kontour\n    }\n    \n    ImagerySourceSelector -- PredictionImagerySource : uses\n```\n\nSources: [frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector.tsx:36-151](), [frontend/src/enums/start-mapping.ts:3-8]()\n\n### Prediction Settings\n\nUsers can configure various settings for predictions:\n\n1. **Use JOSM Q**: A setting for JOSM integration\n2. **Confidence Level**: Threshold for confidence in predictions (25%, 50%, 75%, 90%)\n3. **Tolerance**: Tolerance for prediction matching\n4. **Area**: Area threshold for predictions\n\nSettings are managed by the `ModelSettings` component.\n\n| Setting | Description | Default Value | Range |\n|---------|-------------|---------------|-------|\n| Use JOSM Q | Enable JOSM Q for editing | true | boolean |\n| Confidence Level | Threshold for prediction confidence | 50% | 25%, 50%, 75%, 90% |\n| Tolerance | Tolerance for prediction matching | 0.3 | 0-Max |\n| Area | Area threshold for predictions | 3 | 0-Max |\n\nSources: [frontend/src/features/start-mapping/components/model-settings.tsx:41-197](), [frontend/src/app/routes/start-mapping.tsx:172-182]()\n\n## Prediction Interaction\n\n### Viewing Predictions\n\nPredictions are displayed on the map in three categories:\n\n1. **All Predictions**: All predictions generated by the model\n2. **Accepted Predictions**: Predictions that have been accepted by the user\n3. **Rejected Predictions**: Predictions that have been rejected by the user\n\nEach category has its own layer on the map with distinct styling to differentiate between them.\n\n```mermaid\ngraph TD\n    subgraph \"Prediction Layers\"\n        A[\"AllPredictionsLayer\"]\n        B[\"AcceptedPredictionsLayer\"]\n        C[\"RejectedPredictionsLayer\"]\n    end\n    \n    Map[\"MapComponent\"] --\u003e A\n    Map --\u003e B\n    Map --\u003e C\n    \n    ModelPredictionStore[\"useModelPredictionStore\"] --\u003e A\n    ModelPredictionStore --\u003e B\n    ModelPredictionStore --\u003e C\n```\n\nSources: [frontend/src/features/start-mapping/components/map/map.tsx:159-175](), [frontend/src/features/start-mapping/components/map/layers/all-prediction-layer.tsx:15-87]()\n\n### Accepting and Rejecting Predictions\n\nUsers can interact with predictions on the map to accept or reject them. This is handled by the `PredictedFeatureActionPopup` component, which displays when users click on a prediction.\n\nThe accepted and rejected predictions are stored in the `useModelPredictionStore` and displayed in separate layers on the map.\n\nSources: [frontend/src/app/routes/start-mapping.tsx:70]()\n\n### Downloading Predictions\n\nUsers can download predictions in various formats:\n\n1. **Download All Features**: Download all predictions as GeoJSON\n2. **Download Accepted Features**: Download only accepted predictions as GeoJSON\n3. **Open All Features in JOSM**: Open all predictions in JOSM for editing\n4. **Open Accepted Features in JOSM**: Open only accepted predictions in JOSM for editing\n\nDownload options are available in the header for desktop and in the mobile drawer for mobile devices.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant UI as \"UI Components\"\n    participant Handler as \"Download Handlers\"\n    participant Store as \"ModelPredictionStore\"\n    participant JOSM as \"JOSM Editor\"\n    \n    User-\u003e\u003eUI: Click download option\n    UI-\u003e\u003eHandler: Call handler function\n    Handler-\u003e\u003eStore: Retrieve predictions\n    \n    alt Download GeoJSON\n        Handler-\u003e\u003eUser: Download GeoJSON file\n    else Open in JOSM\n        Handler-\u003e\u003eJOSM: Send features to JOSM\n    end\n```\n\nSources: [frontend/src/app/routes/start-mapping.tsx:256-340]()\n\n## Technical Implementation\n\n### Component Hierarchy\n\nThe Start Mapping feature follows a hierarchical component structure:\n\n```mermaid\ngraph TD\n    App[\"App\"] --\u003e Routes[\"Routes\"]\n    Routes --\u003e StartMappingPage[\"StartMappingPage\"]\n    \n    StartMappingPage --\u003e StartMappingHeader[\"StartMappingHeader\"]\n    StartMappingPage --\u003e StartMappingMapComponent[\"StartMappingMapComponent\"]\n    StartMappingPage --\u003e StartMappingMobileDrawer[\"StartMappingMobileDrawer\"]\n    \n    StartMappingHeader --\u003e ModelSelector[\"ModelSelector\"]\n    StartMappingHeader --\u003e ImagerySourceSelector[\"ImagerySourceSelector\"]\n    StartMappingHeader --\u003e ModelSettings[\"ModelSettings\"]\n    \n    StartMappingMapComponent --\u003e PredictionLayers[\"PredictionLayers\"]\n    StartMappingMapComponent --\u003e PredictedFeatureActionPopup[\"PredictedFeatureActionPopup\"]\n    \n    PredictionLayers --\u003e AcceptedPredictionsLayer[\"AcceptedPredictionsLayer\"]\n    PredictionLayers --\u003e RejectedPredictionsLayer[\"RejectedPredictionsLayer\"]\n    PredictionLayers --\u003e AllPredictionsLayer[\"AllPredictionsLayer\"]\n    PredictionLayers --\u003e PredictionImageryLayer[\"PredictionImageryLayer\"]\n```\n\nSources: [frontend/src/app/routes/start-mapping.tsx:64-517](), [frontend/src/features/start-mapping/components/index.ts:1-9]()\n\n### State Management\n\nThe Start Mapping page uses various state management approaches:\n\n1. **Component State**: For UI-specific state (e.g., dropdown visibility)\n2. **URL Query Parameters**: For preserving settings between sessions\n3. **Store State**: For managing model predictions\n\nKey state elements include:\n\n- `predictionImageryURL`: The URL of the imagery source\n- `predictionImagerySource`: The type of imagery source (Model Default, Custom, etc.)\n- `predictionModel`: The selected model for prediction\n- `predictionModelCheckpoint`: The path to the model checkpoint\n- `query`: Settings for prediction (confidence level, tolerance, area)\n- `modelPredictions`: Predictions from the model (all, accepted, rejected)\n\nSources: [frontend/src/app/routes/start-mapping.tsx:70-120](), [frontend/src/app/routes/start-mapping.tsx:172-182]()\n\n### Map Integration\n\nThe map is powered by MapLibre GL, with several custom layers for displaying predictions:\n\n1. **Base Map**: The background imagery layer\n2. **Tile Boundaries**: Visual indicators of map tile boundaries\n3. **Prediction Layers**: Layers for displaying predictions\n4. **Interactive Elements**: Popups and controls for interacting with the map\n\nMap layers are dynamically added and updated using the `useDynamicMapLayer` hook, which manages the lifecycle of map layers.\n\nSources: [frontend/src/features/start-mapping/components/map/map.tsx:29-175](), [frontend/src/hooks/use-map-layer.ts:42-92](), [frontend/src/components/map/layers/open-aerial-map.tsx:1-36]()\n\n### Model Prediction Process\n\nThe process of generating and displaying predictions follows these steps:\n\n1. User selects a model and imagery source\n2. User configures prediction settings\n3. System generates predictions using the selected model and settings\n4. Predictions are displayed on the map\n5. User interacts with predictions (accept/reject)\n6. User can download the results\n\n```mermaid\nflowchart LR\n    A[\"Model Selection\"] --\u003e B[\"Settings Configuration\"]\n    B --\u003e C[\"Prediction Generation\"]\n    C --\u003e D[\"Prediction Display\"]\n    D --\u003e E[\"User Interaction\"]\n    E --\u003e F[\"Download/Export\"]\n```\n\nSources: [frontend/src/app/routes/start-mapping.tsx:64-517]()\n\n## Conclusion\n\nThe Start Mapping feature provides a comprehensive interface for working with AI model predictions in a mapping context. It combines model selection, imagery configuration, and interactive mapping tools to offer a powerful environment for generating and managing predictions. The feature is designed with both desktop and mobile users in mind, with responsive UI components that adapt to different screen sizes."])</script><script>self.__next_f.push([1,"22:T6147,"])</script><script>self.__next_f.push([1,"# Model Creation and Management\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [frontend/src/app/index.tsx](frontend/src/app/index.tsx)\n- [frontend/src/app/providers/models-provider.tsx](frontend/src/app/providers/models-provider.tsx)\n- [frontend/src/app/routes/models/model-details-card.tsx](frontend/src/app/routes/models/model-details-card.tsx)\n- [frontend/src/app/routes/models/models-list.tsx](frontend/src/app/routes/models/models-list.tsx)\n- [frontend/src/components/layouts/model-forms-layout.tsx](frontend/src/components/layouts/model-forms-layout.tsx)\n- [frontend/src/components/map/controls/zoom-control.tsx](frontend/src/components/map/controls/zoom-control.tsx)\n- [frontend/src/components/map/map.tsx](frontend/src/components/map/map.tsx)\n- [frontend/src/components/ui/button/button.css](frontend/src/components/ui/button/button.css)\n- [frontend/src/components/ui/button/button.tsx](frontend/src/components/ui/button/button.tsx)\n- [frontend/src/components/ui/button/icon-button.tsx](frontend/src/components/ui/button/icon-button.tsx)\n- [frontend/src/components/ui/form/input/input.tsx](frontend/src/components/ui/form/input/input.tsx)\n- [frontend/src/enums/common.ts](frontend/src/enums/common.ts)\n- [frontend/src/features/model-creation/components/dialogs/file-upload-dialog.tsx](frontend/src/features/model-creation/components/dialogs/file-upload-dialog.tsx)\n- [frontend/src/features/model-creation/components/model-summary.tsx](frontend/src/features/model-creation/components/model-summary.tsx)\n- [frontend/src/features/model-creation/components/progress-buttons.tsx](frontend/src/features/model-creation/components/progress-buttons.tsx)\n- [frontend/src/features/model-creation/components/training-area/open-area-map.tsx](frontend/src/features/model-creation/components/training-area/open-area-map.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area-item.tsx](frontend/src/features/model-creation/components/training-area/training-area-item.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area-list.tsx](frontend/src/features/model-creation/components/training-area/training-area-list.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area-map.tsx](frontend/src/features/model-creation/components/training-area/training-area-map.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area.tsx](frontend/src/features/model-creation/components/training-area/training-area.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/training-dataset.tsx](frontend/src/features/model-creation/components/training-dataset/training-dataset.tsx)\n- [frontend/src/features/model-creation/components/training-settings/training-settings-form.tsx](frontend/src/features/model-creation/components/training-settings/training-settings-form.tsx)\n- [frontend/src/features/model-creation/hooks/use-tms-tilejson.ts](frontend/src/features/model-creation/hooks/use-tms-tilejson.ts)\n- [frontend/src/features/models/api/factory.ts](frontend/src/features/models/api/factory.ts)\n- [frontend/src/features/models/api/get-trainings.ts](frontend/src/features/models/api/get-trainings.ts)\n- [frontend/src/features/models/api/update-trainings.ts](frontend/src/features/models/api/update-trainings.ts)\n- [frontend/src/features/models/components/dialogs/model-files-dialog.tsx](frontend/src/features/models/components/dialogs/model-files-dialog.tsx)\n- [frontend/src/features/models/components/dialogs/training-details-dialog.tsx](frontend/src/features/models/components/dialogs/training-details-dialog.tsx)\n- [frontend/src/features/models/components/directory-tree.tsx](frontend/src/features/models/components/directory-tree.tsx)\n- [frontend/src/features/models/components/header.tsx](frontend/src/features/models/components/header.tsx)\n- [frontend/src/features/models/components/maps/models-map.tsx](frontend/src/features/models/components/maps/models-map.tsx)\n- [frontend/src/features/models/components/maps/training-area-map.tsx](frontend/src/features/models/components/maps/training-area-map.tsx)\n- [frontend/src/features/models/components/model-details-info.tsx](frontend/src/features/models/components/model-details-info.tsx)\n- [frontend/src/features/models/components/model-details-properties.tsx](frontend/src/features/models/components/model-details-properties.tsx)\n- [frontend/src/features/models/components/model-files-button.tsx](frontend/src/features/models/components/model-files-button.tsx)\n- [frontend/src/features/models/components/training-history-table.tsx](frontend/src/features/models/components/training-history-table.tsx)\n- [frontend/src/features/models/hooks/use-models.ts](frontend/src/features/models/hooks/use-models.ts)\n- [frontend/src/features/models/hooks/use-training.ts](frontend/src/features/models/hooks/use-training.ts)\n- [frontend/src/features/models/layouts/grid.tsx](frontend/src/features/models/layouts/grid.tsx)\n- [frontend/src/features/models/layouts/table.tsx](frontend/src/features/models/layouts/table.tsx)\n- [frontend/src/hooks/use-storage.ts](frontend/src/hooks/use-storage.ts)\n- [frontend/src/services/api-routes.ts](frontend/src/services/api-routes.ts)\n- [frontend/src/types/common.ts](frontend/src/types/common.ts)\n- [frontend/src/utils/string-utils.ts](frontend/src/utils/string-utils.ts)\n\n\u003c/details\u003e\n\n\n\nThis document explains the model creation, training, and management features within the fAIr system. It covers the complete lifecycle of AI models, from initial creation through training configuration to deployment and management of existing models. For information about using trained models for mapping, see [Start Mapping Feature](#3.2).\n\n## Overview\n\nThe Model Creation and Management functionality enables users to create custom AI models for detecting features in aerial imagery. The system provides a guided, step-by-step wizard that walks users through the entire model creation process. Once created, models can be trained with different parameters, monitored, and managed through a detailed interface.\n\n### Key Concepts\n\n- **Models**: AI models that can detect features in imagery\n- **Training Datasets**: Collections of aerial imagery used to train models \n- **Training Areas**: Specific regions within a dataset where features have been labeled\n- **Training Settings**: Configuration parameters that control how a model is trained\n\nSources: [frontend/src/app/providers/models-provider.tsx:1-582]()\n\n## Model Creation Workflow\n\nThe model creation process follows a structured workflow with distinct steps, managed through a wizard-like interface. This helps users systematically provide all necessary information to create and train an effective model.\n\n```mermaid\nflowchart TD\n    subgraph \"Model Creation Workflow\"\n        A[\"1. Model Details\"] --\u003e B[\"2. Training Dataset\"]\n        B --\u003e C[\"3. Training Area\"]\n        C --\u003e D[\"4. Training Settings\"]\n        D --\u003e E[\"5. Model Summary\"]\n        E --\u003e F[\"6. Confirmation\"]\n    end\n\n    A_details[\"Name, Description, Base Model\"]\n    B_details[\"Select Existing or Create New Dataset\"]\n    C_details[\"Draw or Upload Areas, Fetch OSM Data\"]\n    D_details[\"Zoom Levels, Training Type, Advanced Parameters\"]\n    E_details[\"Review Settings\"]\n    F_details[\"Training Initiated\"]\n\n    A --- A_details\n    B --- B_details\n    C --- C_details\n    D --- D_details\n    E --- E_details\n    F --- F_details\n```\n\nEach step must be completed before proceeding to the next, with validation to ensure all required information is properly provided.\n\nSources: \n- [frontend/src/components/layouts/model-forms-layout.tsx:1-182]()\n- [frontend/src/features/model-creation/components/progress-buttons.tsx:1-150]()\n\n## Model Details\n\nThe first step in model creation is defining basic model information:\n\n1. **Model Name**: A descriptive name for the model (10-40 characters)\n2. **Model Description**: Extended description of the model's purpose and features (10-500 characters)\n3. **Base Model**: Selection of the underlying model architecture:\n   - **RAMP**: Raster And Machine learning Pipeline\n   - **YOLOV8_V1**: You Only Look Once version 8 (v1)\n   - **YOLOV8_V2**: You Only Look Once version 8 (v2)\n\nThe choice of base model affects available training parameters in later steps.\n\nSources: \n- [frontend/src/app/providers/models-provider.tsx:52-146]()\n- [frontend/src/enums/common.ts:1-83]()\n\n## Training Dataset Management\n\nUsers can either select an existing training dataset or create a new one:\n\n### Creating a New Training Dataset\n1. Enter a dataset name (10-40 characters)\n2. Provide a TMS (Tile Map Service) URL, which is validated for proper format\n3. System extracts TileJSON information from the URL to determine bounds, zoom levels, etc.\n\n### Selecting an Existing Dataset\nBrowse available datasets with search and filtering capabilities\n\nThe dataset provides the aerial imagery that will be used for model training.\n\n```mermaid\nflowchart TD\n    subgraph \"Training Dataset Selection\"\n        decision[\"Select Option\"]\n        existing[\"Use Existing Dataset\"]\n        new[\"Create New Dataset\"]\n        \n        decision --\u003e |\"Select Existing\"| existing\n        decision --\u003e |\"Create New\"| new\n        \n        existing --\u003e |\"Search \u0026 Select\"| dataset_selected[\"Dataset Selected\"]\n        new --\u003e |\"Enter Details\"| validate[\"Validate TMS URL\"]\n        validate --\u003e |\"Valid\"| create[\"Create Dataset\"]\n        create --\u003e dataset_selected\n    end\n\n    dataset_selected --\u003e |\"Proceed to Training Area\"| next[\"Training Area Configuration\"]\n```\n\nSources:\n- [frontend/src/features/model-creation/components/training-dataset/training-dataset.tsx:1-60]()\n- [frontend/src/app/providers/models-provider.tsx:408-428]()\n\n## Training Area Configuration\n\nTraining areas define the regions within the aerial imagery that contain features for training. This step involves:\n\n1. **Viewing Dataset**: Display the aerial imagery from the selected dataset\n2. **Creating Training Areas**: \n   - Draw rectangular areas directly on the map\n   - Upload GeoJSON files containing training area polygons\n3. **Fetching Labels**: For each training area, fetch OpenStreetMap data as labels\n4. **Managing Training Areas**: Delete, download, or modify training areas as needed\n\nThe system validates that training areas are appropriately sized and contain valid geometries.\n\n```mermaid\nflowchart TD\n    subgraph \"Training Area Component Structure\"\n        TF[\"TrainingAreaForm\"] --\u003e |\"Contains\"| TAM[\"TrainingAreaMap\"]\n        TF --\u003e |\"Contains\"| TAL[\"TrainingAreaList\"]\n        TAL --\u003e |\"Contains Multiple\"| TAI[\"TrainingAreaItem\"]\n        TAM --\u003e |\"Visualizes\"| TA[\"Training Areas\"]\n        TAM --\u003e |\"Visualizes\"| OAM[\"OpenAerialMap\"]\n        TAM --\u003e |\"Enables\"| Drawing[\"Drawing Controls\"]\n        \n        TAI --\u003e |\"Enables\"| OSM[\"OSM Data Fetching\"]\n        TAI --\u003e |\"Enables\"| Upload[\"Label Upload\"]\n        TAI --\u003e |\"Enables\"| Download[\"Area Download\"]\n    end\n```\n\nThe system enforces minimum and maximum size constraints for training areas to ensure effective model training:\n- Minimum area: Defined in `MIN_TRAINING_AREA_SIZE`\n- Maximum area: Defined in `MAX_TRAINING_AREA_SIZE`\n\nWhen drawing training areas, users receive immediate feedback if the area is too small or too large.\n\nSources:\n- [frontend/src/features/model-creation/components/training-area/training-area.tsx:1-245]()\n- [frontend/src/features/model-creation/components/training-area/training-area-map.tsx:1-298]()\n- [frontend/src/features/model-creation/components/training-area/training-area-item.tsx:1-534]()\n- [frontend/src/features/model-creation/components/training-area/training-area-list.tsx:1-117]()\n- [frontend/src/features/model-creation/components/dialogs/file-upload-dialog.tsx:1-346]()\n\n## Training Settings Configuration\n\nAfter defining training areas, users configure the training process with various parameters:\n\n1. **Zoom Levels**: Select which zoom levels (19, 20, 21) to use in training\n2. **Training Type**: Choose a complexity level with preset parameter configurations:\n   - **Basic**: Minimal parameters, fastest training\n   - **Intermediate**: Balanced parameters\n   - **Advanced**: Comprehensive parameters, longer training but potentially higher accuracy\n\n3. **Advanced Settings** (optional):\n   - **Epochs**: Number of training cycles (varies by base model)\n   - **Batch Size**: Number of samples processed before updating model weights\n   - **Contact Spacing**: Parameter for RAMP model only\n   - **Boundary Width**: Parameter for RAMP model only\n\nEach parameter has model-specific validation constraints to ensure optimal training.\n\n```mermaid\nflowchart TD\n    subgraph \"Training Settings Flow\"\n        ZL[\"Select Zoom Levels\"] --\u003e TT[\"Choose Training Type\"]\n        TT --\u003e |\"Basic\"| Basic[\"Preset Basic Parameters\"]\n        TT --\u003e |\"Intermediate\"| Inter[\"Preset Intermediate Parameters\"]\n        TT --\u003e |\"Advanced\"| Adv[\"Preset Advanced Parameters\"]\n        Basic --\u003e AS[\"Advanced Settings (Optional)\"]\n        Inter --\u003e AS\n        Adv --\u003e AS\n        AS --\u003e |\"Epochs\"| Ep[\"Set Epochs\"]\n        AS --\u003e |\"Batch Size\"| BS[\"Set Batch Size\"]\n        AS --\u003e |\"RAMP-specific\"| RAMP[\"Contact Spacing \u0026 Boundary Width\"]\n    end\n```\n\nEach training type has different default parameter values based on the selected base model, optimized for different use cases.\n\nSources:\n- [frontend/src/features/model-creation/components/training-settings/training-settings-form.tsx:1-265]()\n- [frontend/src/app/providers/models-provider.tsx:60-146]()\n\n## Model Summary and Submission\n\nBefore final submission, users review all configuration settings:\n\n1. Model details (name, description, base model)\n2. Dataset information\n3. Training area summary\n4. Training settings\n\nUpon submission:\n1. A model creation request is sent to the API\n2. A training request is submitted with the specified parameters\n3. The user is redirected to a confirmation page\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Frontend\n    participant ModelsProvider\n    participant BackendAPI\n    \n    User-\u003e\u003eFrontend: Review and submit model\n    Frontend-\u003e\u003eModelsProvider: handleModelCreationAndUpdate()\n    \n    alt New Model\n        ModelsProvider-\u003e\u003eBackendAPI: POST /api/v1/model/\n        BackendAPI--\u003e\u003eModelsProvider: Return model ID\n    else Update Existing Model\n        ModelsProvider-\u003e\u003eBackendAPI: PATCH /api/v1/model/{modelId}/\n        BackendAPI--\u003e\u003eModelsProvider: Return updated model\n    end\n    \n    ModelsProvider-\u003e\u003eBackendAPI: POST /api/v1/training/\n    Note over BackendAPI: Submit training request with parameters\n    BackendAPI--\u003e\u003eModelsProvider: Return training ID\n    \n    ModelsProvider-\u003e\u003eFrontend: Navigate to confirmation\n    Frontend-\u003e\u003eUser: Display confirmation\n```\n\nSources:\n- [frontend/src/features/model-creation/components/model-summary.tsx:1-109]()\n- [frontend/src/app/providers/models-provider.tsx:427-528]()\n\n## Model Management Interface\n\nOnce models are created, they can be managed through a dedicated interface that provides:\n\n1. **Model Listing**: Browse, search, and filter models\n2. **Model Details**: View comprehensive information about each model\n3. **Training History**: Monitor past and current training sessions\n4. **Model Properties**: View technical parameters and performance metrics\n\n### Model Listing\n\nThe model listing interface provides a comprehensive view of all available models with flexible viewing options:\n\n- **Grid View**: Card-based display with thumbnails and key information\n- **List View**: Tabular display with sortable columns\n- **Map View**: Geographical display showing models by location\n\nFiltering options include:\n- Search by name or description\n- Filter by date range\n- Sort by various attributes (creation date, accuracy, etc.)\n\nSources:\n- [frontend/src/app/routes/models/models-list.tsx:1-258]()\n- [frontend/src/features/models/layouts/grid.tsx:1-52]()\n- [frontend/src/features/models/layouts/table.tsx:1-80]()\n- [frontend/src/features/models/components/maps/models-map.tsx:1-161]()\n\n### Model Details Page\n\nThe model details page provides comprehensive information about a specific model:\n\n1. **Basic Information**:\n   - Model name and description\n   - Creator and creation date\n   - Base model information\n   - Dataset details\n\n2. **Properties**:\n   - Training parameters (epochs, batch size, etc.)\n   - Accuracy and performance metrics\n   - Zoom levels\n   - Source imagery information\n\n3. **Training History**:\n   - List of all training sessions\n   - Status indicators (submitted, running, finished, failed)\n   - Performance metrics for each training\n   - Option to set active training dataset\n\n```mermaid\nflowchart TD\n    subgraph \"Model Details Page Structure\"\n        MDP[\"Model Details Page\"] --\u003e MDI[\"Model Details Info\"]\n        MDP --\u003e MDP[\"Model Details Properties\"]\n        MDP --\u003e THT[\"Training History Table\"]\n        \n        MDI --\u003e ModelInfo[\"Basic Model Info\"]\n        MDI --\u003e TrainingArea[\"Training Area Viewer\"]\n        MDI --\u003e FileDownload[\"Model Files Download\"]\n        \n        MDP --\u003e TParams[\"Training Parameters\"]\n        MDP --\u003e Metrics[\"Accuracy Metrics\"]\n        MDP --\u003e Graph[\"Training Results Graph\"]\n        \n        THT --\u003e History[\"Training Session History\"]\n        THT --\u003e Actions[\"Training Management Actions\"]\n    end\n```\n\nSources:\n- [frontend/src/app/routes/models/model-details-card.tsx:1-130]() \n- [frontend/src/features/models/components/model-details-info.tsx:1-148]()\n- [frontend/src/features/models/components/model-details-properties.tsx:1-357]()\n- [frontend/src/features/models/components/training-history-table.tsx:1-420]()\n\n### Training Management\n\nThe training history interface allows users to:\n\n1. **View Training Details**:\n   - Parameters used for each training session\n   - Performance metrics (accuracy)\n   - Start time and duration\n   - Status (submitted, running, finished, failed)\n\n2. **Manage Training Sessions**:\n   - Set a specific training as the active one for the model\n   - Cancel ongoing training sessions\n   - View detailed training logs and results\n\n3. **Access Training Files**:\n   - Browse directory structure of training artifacts\n   - Download model weights, configuration files, and training logs\n   - Access visualization files like graphs and metrics\n\n```mermaid\nflowchart TD\n    subgraph \"Training Management Components\"\n        THT[\"TrainingHistoryTable\"] --\u003e TD[\"Training Details\"]\n        THT --\u003e TA[\"Training Actions\"]\n        \n        TD --\u003e TDP[\"Training Details Dialog\"]\n        TA --\u003e Publish[\"Set as Active\"]\n        TA --\u003e Cancel[\"Cancel Training\"]\n        TA --\u003e View[\"View Details\"]\n        \n        TDP --\u003e Properties[\"Training Properties\"]\n        TDP --\u003e Logs[\"Training Logs\"]\n        TDP --\u003e Files[\"Training Files\"]\n        \n        Files --\u003e MFD[\"Model Files Dialog\"]\n        MFD --\u003e DT[\"Directory Tree Browser\"]\n        DT --\u003e Download[\"File Download\"]\n    end\n```\n\nSources:\n- [frontend/src/features/models/components/training-history-table.tsx:1-420]()\n- [frontend/src/features/models/components/directory-tree.tsx:1-315]()\n- [frontend/src/features/models/components/dialogs/training-details-dialog.tsx:1-46]()\n- [frontend/src/features/models/components/model-files-button.tsx:1-27]()\n- [frontend/src/features/models/components/dialogs/model-files-dialog.tsx:1-43]()\n\n## Data Flow and State Management\n\nThe model creation and management functionality is implemented using a context-based state management approach with React's Context API.\n\n### ModelsProvider Context\n\nThe `ModelsProvider` context serves as the central state management system for the model creation process:\n\n```mermaid\nflowchart TD\n    subgraph \"Models State Management\"\n        MP[\"ModelsProvider Context\"] --\u003e |\"Manages\"| FormData[\"Form Data State\"]\n        MP --\u003e |\"Provides\"| API[\"API Mutations\"]\n        MP --\u003e |\"Controls\"| Navigation[\"Navigation Flow\"]\n        \n        FormData --\u003e |\"Stores\"| ModelDetails[\"Model Details\"]\n        FormData --\u003e |\"Stores\"| DatasetInfo[\"Dataset Information\"]\n        FormData --\u003e |\"Stores\"| TrainingAreas[\"Training Areas\"]\n        FormData --\u003e |\"Stores\"| Settings[\"Training Settings\"]\n        \n        API --\u003e CreateModel[\"Create Model\"]\n        API --\u003e UpdateModel[\"Update Model\"]\n        API --\u003e CreateTraining[\"Create Training Request\"]\n        API --\u003e CreateDataset[\"Create Dataset\"]\n        \n        MP --\u003e |\"Validates\"| Validation[\"Form Validation\"]\n        MP --\u003e |\"Persists\"| LocalStorage[\"Local Storage\"]\n    end\n```\n\nKey features of the ModelsProvider:\n\n1. **Form State Management**: Manages all form data across multiple steps\n2. **Form Validation**: Validates input according to defined rules\n3. **API Integration**: Handles all API calls for model creation and training\n4. **Navigation Control**: Controls the flow between steps based on validation\n5. **Persistence**: Saves progress to local storage to allow returning to the form\n\nSources:\n- [frontend/src/app/providers/models-provider.tsx:1-582]()\n- [frontend/src/hooks/use-storage.ts:1-84]()\n\n### API Integration\n\nThe model creation and management functionality interacts with the backend API through several endpoints:\n\n| Endpoint | Purpose | HTTP Method |\n|----------|---------|-------------|\n| `/api/v1/model/` | Create a new model | POST |\n| `/api/v1/model/{modelId}/` | Update an existing model | PATCH |\n| `/api/v1/dataset/` | Create a new training dataset | POST |\n| `/api/v1/aoi/` | Create a training area | POST |\n| `/api/v1/label/osm/fetch/{aoi_id}/` | Fetch OSM labels for a training area | GET |\n| `/api/v1/label/upload/{aoi_id}/` | Upload custom labels for a training area | POST |\n| `/api/v1/training/` | Create a training request | POST |\n| `/api/v1/training/publish/{id}/` | Set a training as active | POST |\n| `/api/v1/training/terminate/{id}/` | Cancel a training process | POST |\n| `/api/v1/workspace/training_{id}/{directory_name}` | Browse training files | GET |\n\nAPI requests are managed through React Query, providing caching, refetching, and error handling capabilities.\n\nSources:\n- [frontend/src/services/api-routes.ts:1-97]()\n- [frontend/src/features/models/api/factory.ts:1-139]()\n- [frontend/src/features/models/api/update-trainings.ts:1-74]()\n- [frontend/src/features/models/api/get-trainings.ts:1-63]()\n- [frontend/src/features/models/hooks/use-training.ts:1-60]()\n- [frontend/src/features/models/hooks/use-models.ts:1-117]()\n\n## User Interface Components\n\nThe model creation and management interface consists of multiple specialized components that provide a cohesive user experience.\n\n### Model Forms Layout\n\nThe `ModelFormsLayout` component provides the structure for the multi-step model creation wizard:\n\n- Progress stepper to visualize the current step\n- Navigation buttons for moving between steps\n- Validation to ensure all required data is provided\n- Form fields appropriate to each step\n\nSources:\n- [frontend/src/components/layouts/model-forms-layout.tsx:1-182]()\n- [frontend/src/features/model-creation/components/progress-buttons.tsx:1-150]()\n\n### Map Components\n\nSeveral specialized map components are used for visualizing and interacting with geospatial data:\n\n1. **Training Area Map**: For drawing and viewing training areas\n2. **Model Map**: For geographical visualization of models\n3. **Training Area Drawer**: For viewing training areas in existing models\n\nThese components leverage MapLibre GL JS for rendering maps and Terra Draw for interactive drawing capabilities.\n\nSources:\n- [frontend/src/components/map/map.tsx:1-119]()\n- [frontend/src/features/model-creation/components/training-area/training-area-map.tsx:1-298]()\n- [frontend/src/features/models/components/maps/training-area-map.tsx:1-245]()\n- [frontend/src/components/map/controls/zoom-control.tsx:1-60]()\n\n### UI Elements\n\nThe system uses a combination of custom UI components and Shoelace Web Components to create a consistent and responsive interface:\n\n- Buttons and form controls\n- Dialogs and modals\n- Tables and cards\n- Interactive visualizations\n\nSources:\n- [frontend/src/components/ui/button/button.tsx:1-71]()\n- [frontend/src/components/ui/button/button.css:1-58]()\n- [frontend/src/components/ui/button/icon-button.tsx:1-53]()\n- [frontend/src/components/ui/form/input/input.tsx:1-156]()\n\n## Conclusion\n\nThe Model Creation and Management functionality provides a comprehensive set of tools for creating, training, and managing AI models for mapping. The step-by-step wizard approach guides users through the complex process of model creation, while the management interface provides detailed insights into model performance and training history.\n\nThis functionality forms a critical part of the fAIr system, enabling users to create customized models tailored to their specific mapping needs. Once models are created and trained, they can be used in the Start Mapping feature to assist in the mapping process."])</script><script>self.__next_f.push([1,"23:T2cb3,"])</script><script>self.__next_f.push([1,"# Authentication and User Management\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [backend/.gitignore](backend/.gitignore)\n- [backend/.pre-commit-config.yaml](backend/.pre-commit-config.yaml)\n- [backend/core/admin.py](backend/core/admin.py)\n- [backend/login/admin.py](backend/login/admin.py)\n- [backend/login/authentication.py](backend/login/authentication.py)\n- [backend/login/permissions.py](backend/login/permissions.py)\n- [backend/login/views.py](backend/login/views.py)\n- [backend/pdm.lock](backend/pdm.lock)\n- [backend/pyproject.toml](backend/pyproject.toml)\n- [backend/tests/factories.py](backend/tests/factories.py)\n- [frontend/src/app/providers/auth-provider.tsx](frontend/src/app/providers/auth-provider.tsx)\n- [frontend/src/app/router.tsx](frontend/src/app/router.tsx)\n- [frontend/src/hooks/use-login.ts](frontend/src/hooks/use-login.ts)\n- [frontend/src/services/api-client.ts](frontend/src/services/api-client.ts)\n- [frontend/src/services/auth.ts](frontend/src/services/auth.ts)\n\n\u003c/details\u003e\n\n\n\nThis document explains the authentication and user management system in the fAIr application. It covers how users authenticate using OpenStreetMap (OSM) OAuth, how user data is stored and managed, how authentication state is maintained in the frontend, and how protected routes are implemented.\n\n## 1. Overview\n\nThe fAIr application uses OpenStreetMap (OSM) as its authentication provider. This enables users to log in with their existing OSM accounts without creating a new account specifically for fAIr. The system implements a complete OAuth flow for secure authentication and manages user sessions through token-based authentication.\n\n### Authentication Flow Diagram\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant OSM as \"OpenStreetMap OAuth\"\n    \n    User-\u003e\u003eFrontend: Click \"Login\"\n    Frontend-\u003e\u003eBackend: GET /api/v1/login/\n    Backend-\u003e\u003eFrontend: Return OSM login URL\n    Frontend-\u003e\u003eOSM: Redirect to OSM login page\n    User-\u003e\u003eOSM: Enter credentials\n    OSM-\u003e\u003eFrontend: Redirect with code and state\n    Frontend-\u003e\u003eBackend: GET /api/v1/callback/?code=xyz\u0026state=abc\n    Backend-\u003e\u003eOSM: Verify code and state\n    OSM-\u003e\u003eBackend: Return access token\n    Backend-\u003e\u003eFrontend: Return access token\n    Frontend-\u003e\u003elocalStorage: Store access token\n    Frontend-\u003e\u003eBackend: GET /api/v1/user/ with token\n    Backend-\u003e\u003eFrontend: Return user profile\n    Frontend-\u003e\u003eFrontend: Update auth context state\n```\n\nSources: [frontend/src/app/providers/auth-provider.tsx:140-153](), [frontend/src/services/auth.ts:32-59](), [backend/login/views.py:34-61]()\n\n## 2. User Model\n\nThe system uses a custom user model that extends Django's base user model to store OSM-specific user information.\n\n### User Data Model\n\n```mermaid\nclassDiagram\n    class OsmUser {\n        +osm_id: Integer\n        +username: String\n        +email: String\n        +email_verified: Boolean\n        +img_url: String\n        +is_staff: Boolean\n        +is_superuser: Boolean\n        +is_active: Boolean\n        +last_login: DateTime\n        +date_joined: DateTime\n    }\n```\n\nSources: [backend/login/admin.py:23-36](), [backend/tests/factories.py:16-21]()\n\n## 3. Frontend Authentication Implementation\n\n### 3.1. Auth Provider\n\nThe frontend uses a React context provider (`AuthProvider`) to manage authentication state and provide authentication-related functionality throughout the application. This provider handles:\n\n- Storing and retrieving the access token\n- Fetching user profile data\n- Handling login and logout\n- Monitoring authentication state\n- Email verification\n\n```mermaid\nflowchart TD\n    subgraph \"Authentication Components\"\n        AuthProvider[\"AuthProvider Component\"]\n        AuthContext[\"AuthContext\"]\n        useAuth[\"useAuth Hook\"]\n        useLogin[\"useLogin Hook\"]\n    end\n    \n    subgraph \"Auth State\"\n        token[\"Access Token\"]\n        user[\"User Object\"]\n        isAuthenticated[\"isAuthenticated Flag\"]\n    end\n    \n    subgraph \"Auth Services\"\n        authService[\"authService\"]\n        apiClient[\"API Client\"]\n    end\n    \n    subgraph \"Storage\"\n        localStorage[\"Local Storage\"]\n        sessionStorage[\"Session Storage\"]\n    end\n    \n    AuthProvider --\u003e|\"Creates\"| AuthContext\n    useAuth --\u003e|\"Consumes\"| AuthContext\n    AuthProvider --\u003e|\"Manages\"| token\n    AuthProvider --\u003e|\"Manages\"| user\n    AuthProvider --\u003e|\"Computes\"| isAuthenticated\n    AuthProvider --\u003e|\"Uses\"| authService\n    authService --\u003e|\"Uses\"| apiClient\n    useLogin --\u003e|\"Uses\"| authService\n    AuthProvider --\u003e|\"Interacts with\"| localStorage\n    AuthProvider --\u003e|\"Interacts with\"| sessionStorage\n```\n\nSources: [frontend/src/app/providers/auth-provider.tsx:38-213](), [frontend/src/hooks/use-login.ts:9-42]()\n\n### 3.2. Key Authentication Methods\n\nThe auth provider exposes several key methods:\n\n1. **authenticateUser**: Exchanges OAuth code and state for an access token\n   ```typescript\n   authenticateUser(state: string, code: string): Promise\u003cvoid\u003e\n   ```\n\n2. **logout**: Clears authentication state and token\n   ```typescript\n   logout(): void\n   ```\n\n3. **verifyUserEmail**: Verifies a user's email with provided token\n   ```typescript\n   verifyUserEmail(uid: string, token: string): Promise\u003cvoid\u003e\n   ```\n\nSources: [frontend/src/app/providers/auth-provider.tsx:140-181]()\n\n### 3.3. Login Hook\n\nThe application provides a `useLogin` hook for initiating the login process:\n\n```typescript\nconst { loading, handleLogin } = useLogin();\n```\n\nThe hook stores the current page path in session storage to enable redirection back to the same page after successful authentication.\n\nSources: [frontend/src/hooks/use-login.ts:22-42]()\n\n## 4. Backend Authentication Implementation\n\n### 4.1. Authentication Mechanism\n\nThe backend implements a custom authentication scheme based on OSM OAuth:\n\n```mermaid\nflowchart TD\n    subgraph \"Backend Authentication\"\n        OsmAuthentication[\"OsmAuthentication Class\"]\n        IsOsmAuthenticated[\"IsOsmAuthenticated Permission\"]\n        AccessToken[\"Access Token Header\"]\n        OsmAuth[\"OSM Auth Library\"]\n    end\n    \n    subgraph \"API Views\"\n        LoginView[\"login View\"]\n        CallbackView[\"callback View\"]\n        GetMyDataView[\"GetMyData View\"]\n        VerifyEmailView[\"VerifyEmail View\"]\n    end\n    \n    subgraph \"Database\"\n        OsmUserModel[\"OsmUser Model\"]\n    end\n    \n    AccessToken --\u003e|\"Passed to\"| OsmAuthentication\n    OsmAuthentication --\u003e|\"Uses\"| OsmAuth\n    OsmAuth --\u003e|\"Deserializes\"| AccessToken\n    OsmAuthentication --\u003e|\"Creates/Updates\"| OsmUserModel\n    IsOsmAuthenticated --\u003e|\"Checks\"| OsmUserModel\n    \n    LoginView --\u003e|\"Generates\"| OsmAuth\n    CallbackView --\u003e|\"Processes\"| OsmAuth\n    GetMyDataView --\u003e|\"Protected by\"| OsmAuthentication\n    GetMyDataView --\u003e|\"Protected by\"| IsOsmAuthenticated\n    VerifyEmailView --\u003e|\"Verifies\"| OsmUserModel\n```\n\nSources: [backend/login/authentication.py:10-54](), [backend/login/views.py:34-159]()\n\n### 4.2. Authentication Flow API Endpoints\n\nThe backend provides several API endpoints to handle the authentication flow:\n\n1. `/api/v1/login/`: Generates a login URL for OSM OAuth\n2. `/api/v1/callback/`: Callback endpoint that exchanged code and state for an access token\n3. `/api/v1/user/`: Returns the authenticated user's information\n4. `/api/v1/verify-email/`: Verifies a user's email address\n\nSources: [backend/login/views.py:34-159]()\n\n### 4.3. Permissions System\n\nThe backend implements several custom permission classes to control access to resources:\n\n1. **IsOsmAuthenticated**: Checks if a user is authenticated with OSM\n2. **IsAdminUser**: Checks if a user is an admin\n3. **IsStaffUser**: Checks if a user is a staff member\n\nThese permissions are used to protect API endpoints that require authentication.\n\nSources: [backend/login/permissions.py:6-60]()\n\n## 5. Email Verification\n\nThe system includes email verification functionality to verify user email addresses.\n\n### 5.1. Email Verification Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant Email as \"Email Service\"\n    \n    User-\u003e\u003eFrontend: Update email in profile\n    Frontend-\u003e\u003eBackend: PATCH /api/v1/user/ with new email\n    Backend-\u003e\u003eBackend: Update user email and set email_verified=false\n    Backend-\u003e\u003eFrontend: Return updated user data\n    User-\u003e\u003eFrontend: Request email verification\n    Frontend-\u003e\u003eBackend: POST /api/v1/request-email-verification/\n    Backend-\u003e\u003eBackend: Generate verification token\n    Backend-\u003e\u003eEmail: Send verification email with token\n    Email-\u003e\u003eUser: Deliver verification email\n    User-\u003e\u003eFrontend: Click verification link\n    Frontend-\u003e\u003eBackend: GET /api/v1/verify-email/?uid=xxx\u0026token=yyy\n    Backend-\u003e\u003eBackend: Verify token and update email_verified=true\n    Backend-\u003e\u003eFrontend: Return success message\n```\n\nSources: [backend/login/views.py:102-159](), [frontend/src/app/providers/auth-provider.tsx:155-181]()\n\n## 6. Protected Routes\n\nThe frontend implements protected routes that require authentication. If a user attempts to access a protected route without being authenticated, they are redirected to the login page.\n\n### 6.1. Protected Route Implementation\n\nProtected routes are wrapped with the `ProtectedRoute` component, which checks if the user is authenticated before rendering the route component.\n\n```mermaid\nflowchart TD\n    subgraph \"Route Configuration\"\n        Router[\"App Router\"]\n        ProtectedRoute[\"ProtectedRoute Component\"]\n        ModelFormsLayout[\"ModelFormsLayout\"]\n        UserProfileLayout[\"UserProfileLayout\"]\n        StartMappingPage[\"StartMappingPage\"]\n    end\n    \n    subgraph \"Authentication State\"\n        AuthContext[\"Auth Context\"]\n        IsAuthenticated[\"isAuthenticated Flag\"]\n    end\n    \n    Router --\u003e|\"Uses\"| ProtectedRoute\n    ProtectedRoute --\u003e|\"Wraps\"| ModelFormsLayout\n    ProtectedRoute --\u003e|\"Wraps\"| UserProfileLayout\n    ProtectedRoute --\u003e|\"Wraps\"| StartMappingPage\n    ProtectedRoute --\u003e|\"Checks\"| IsAuthenticated\n    IsAuthenticated --\u003e|\"From\"| AuthContext\n```\n\nSources: [frontend/src/app/router.tsx:89-291]()\n\n### 6.2. Protected Routes in the Application\n\nThe following routes are protected and require authentication:\n\n| Route | Purpose |\n|-------|---------|\n| `/models/create/*` | Model creation |\n| `/models/edit/*` | Model editing |\n| `/start-mapping` | Start mapping feature |\n| `/profile/*` | User profile pages |\n\nSources: [frontend/src/app/router.tsx:89-337]()\n\n## 7. Admin Interface\n\nThe system includes a Django admin interface for managing users. Administrators can view and manage user accounts, including:\n\n- View user profiles\n- Change permissions (staff, superuser)\n- Manage user activation status\n- View important dates (last login, date joined)\n\nSources: [backend/login/admin.py:37-93](), [backend/core/admin.py:6-63]()\n\n## 8. Integration with Other Systems\n\nThe authentication system is integrated with other components of the application:\n\n### 8.1. API Client Integration\n\nThe frontend API client includes interceptors to handle authentication-related errors, such as automatically logging out when receiving a 401 Unauthorized response.\n\nSources: [frontend/src/services/api-client.ts:25-41]()\n\n### 8.2. Model and Dataset Ownership\n\nUser authentication is used to associate models and datasets with specific users, enabling ownership-based permissions.\n\nSources: [backend/core/admin.py:9-22]()"])</script><script>self.__next_f.push([1,"24:T427b,"])</script><script>self.__next_f.push([1,"# Map Visualization Components\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [frontend/src/app/routes/start-mapping.tsx](frontend/src/app/routes/start-mapping.tsx)\n- [frontend/src/assets/images/index.ts](frontend/src/assets/images/index.ts)\n- [frontend/src/components/map/map.tsx](frontend/src/components/map/map.tsx)\n- [frontend/src/components/map/setups/setup-terra-draw.ts](frontend/src/components/map/setups/setup-terra-draw.ts)\n- [frontend/src/components/ui/form/radio-group/radio-group.css](frontend/src/components/ui/form/radio-group/radio-group.css)\n- [frontend/src/components/ui/form/radio-group/radio-group.tsx](frontend/src/components/ui/form/radio-group/radio-group.tsx)\n- [frontend/src/enums/start-mapping.ts](frontend/src/enums/start-mapping.ts)\n- [frontend/src/features/model-creation/components/dialogs/file-upload-dialog.tsx](frontend/src/features/model-creation/components/dialogs/file-upload-dialog.tsx)\n- [frontend/src/features/model-creation/components/training-area/open-area-map.tsx](frontend/src/features/model-creation/components/training-area/open-area-map.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area-item.tsx](frontend/src/features/model-creation/components/training-area/training-area-item.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area-list.tsx](frontend/src/features/model-creation/components/training-area/training-area-list.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area-map.tsx](frontend/src/features/model-creation/components/training-area/training-area-map.tsx)\n- [frontend/src/features/model-creation/components/training-area/training-area.tsx](frontend/src/features/model-creation/components/training-area/training-area.tsx)\n- [frontend/src/features/model-creation/hooks/use-tms-tilejson.ts](frontend/src/features/model-creation/hooks/use-tms-tilejson.ts)\n- [frontend/src/features/models/components/model-detail-user.tsx](frontend/src/features/models/components/model-detail-user.tsx)\n- [frontend/src/features/start-mapping/components/mobile-drawer.tsx](frontend/src/features/start-mapping/components/mobile-drawer.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector-trigger-button.tsx](frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector-trigger-button.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector.tsx](frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/model-selector-trigger-button.tsx](frontend/src/features/start-mapping/components/replicable-models/model-selector-trigger-button.tsx)\n- [frontend/src/features/start-mapping/components/replicable-models/model-selector.tsx](frontend/src/features/start-mapping/components/replicable-models/model-selector.tsx)\n- [frontend/src/utils/general-utils.ts](frontend/src/utils/general-utils.ts)\n- [frontend/src/utils/string-utils.ts](frontend/src/utils/string-utils.ts)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the map visualization components in the fAIr system that provide the foundation for displaying and interacting with geospatial data. These components enable users to view aerial imagery, create training areas, visualize model predictions, and interact with prediction results. For information about the overall frontend architecture, see [Frontend System](#3).\n\n## Core Map Component Architecture\n\nThe map visualization system is built around MapLibre GL JS, an open-source JavaScript library for interactive maps. It provides a modular architecture with components that can be combined for different mapping needs.\n\n```mermaid\ngraph TD\n    subgraph \"Map Component System\"\n        MapComponent[\"MapComponent\"]\n        MapControls[\"Map Controls\"]\n        MapLayers[\"Map Layers\"]\n        TerraDraw[\"TerraDraw Integration\"]\n    end\n\n    MapComponent --\u003e MapControls\n    MapComponent --\u003e MapLayers\n    MapComponent --\u003e TerraDraw\n\n    subgraph \"Map Controls\"\n        GeoLocationControl[\"GeolocationControl\"]\n        ZoomControls[\"ZoomControls\"]\n        ZoomLevel[\"ZoomLevel\"]\n        LayerControl[\"LayerControl\"]\n        FitToBounds[\"FitToBounds\"]\n        DrawControl[\"DrawControl\"]\n    end\n\n    MapControls --\u003e GeoLocationControl\n    MapControls --\u003e ZoomControls\n    MapControls --\u003e ZoomLevel\n    MapControls --\u003e LayerControl\n    MapControls --\u003e FitToBounds\n    MapControls --\u003e DrawControl\n\n    subgraph \"Map Layers\"\n        Basemaps[\"Basemaps\"]\n        OpenAerialMap[\"OpenAerialMap\"]\n        TileBoundaries[\"TileBoundaries\"]\n        PredictionLayers[\"PredictionLayers\"]\n        TrainingLayers[\"TrainingAreaLayers\"]\n    end\n\n    MapLayers --\u003e Basemaps\n    MapLayers --\u003e OpenAerialMap\n    MapLayers --\u003e TileBoundaries\n    MapLayers --\u003e PredictionLayers\n    MapLayers --\u003e TrainingLayers\n```\n\nSources: [frontend/src/components/map/map.tsx:47-118]()\n\n### MapComponent\n\nThe `MapComponent` is the foundational building block for all map visualizations in the system. It renders a MapLibre GL map and provides various configurable controls and layers. Key properties include:\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `map` | `Map \\| null` | MapLibre GL map instance |\n| `mapContainerRef` | `RefObject\u003cHTMLDivElement\u003e` | Reference to container element |\n| `geolocationControl` | `boolean` | Enable geolocation control |\n| `drawControl` | `boolean` | Enable drawing functionality |\n| `layerControl` | `boolean` | Enable layer control panel |\n| `showTileBoundaries` | `boolean` | Show tile boundaries for debugging |\n| `openAerialMap` | `boolean` | Enable OpenAerialMap imagery |\n| `basemaps` | `boolean` | Enable basemap selector |\n| `terraDraw` | `TerraDraw \\| undefined` | TerraDraw instance for drawing |\n| `drawingMode` | `DrawingModes` | Current drawing mode |\n\nSources: [frontend/src/components/map/map.tsx:19-45]()\n\n## Map Controls\n\nThe system provides multiple map controls that enhance user interaction with the map:\n\n### LayerControl\n\nThe `LayerControl` component allows toggling visibility of different map layers, grouped by category. It can manage both base layers (basemaps) and overlay layers (features, predictions).\n\n### Navigation Controls\n\nNavigation controls include:\n- `ZoomControls`: Buttons for zooming in and out\n- `ZoomLevel`: Display showing current zoom level\n- `FitToBounds`: Button to center map on specific bounds\n- `GeolocationControl`: Button to focus on user location\n\n### DrawControl\n\nEnables drawing functionality with TerraDraw, supporting rectangle mode for creating training areas.\n\nSources: [frontend/src/components/map/map.tsx:68-92]()\n\n## Feature-Specific Map Implementations\n\n### Training Area Map\n\nThe `TrainingAreaMap` component is used for defining training areas for model training. It provides:\n\n- Drawing tools for creating rectangular training areas\n- Display of existing training areas\n- Visualization of OSM labels within training areas\n- Real-time validation of area size constraints\n\n```mermaid\nflowchart TD\n    subgraph \"TrainingAreaMap Workflow\"\n        Start[\"User Accesses Training Area Interface\"]\n        Draw[\"User Draws Rectangle on Map\"]\n        Validate[\"System Validates Area Size\"]\n        Check{{\"Valid Size?\"}}\n        Save[\"Save Training Area\"]\n        Fetch[\"Fetch OSM Labels\"]\n        Display[\"Display Training Area \u0026 Labels\"]\n        Reject[\"Show Error Message\"]\n    end\n\n    Start --\u003e Draw\n    Draw --\u003e Validate\n    Validate --\u003e Check\n    Check --\u003e|\"Yes\"| Save\n    Check --\u003e|\"No\"| Reject\n    Reject --\u003e Draw\n    Save --\u003e Fetch\n    Fetch --\u003e Display\n```\n\nKey features:\n- Area size validation (min/max restrictions)\n- Snapping to tile boundaries\n- OSM label fetching for training areas\n- Visual feedback during drawing\n\nSources: [frontend/src/features/model-creation/components/training-area/training-area-map.tsx:39-297](), [frontend/src/features/model-creation/components/training-area/training-area.tsx:34-244]()\n\n### Start Mapping Map\n\nThe `StartMappingMapComponent` is used for viewing and interacting with model predictions. It displays:\n\n- Model prediction layers (accepted, rejected, all)\n- Selected imagery source\n- Layer controls for toggling visibility\n\n```mermaid\ngraph TD\n    subgraph \"StartMappingPage Components\"\n        StartMappingPage[\"StartMappingPage\"]\n        StartMappingMapComponent[\"StartMappingMapComponent\"]\n        StartMappingHeader[\"StartMappingHeader\"]\n        StartMappingMobileDrawer[\"StartMappingMobileDrawer\"]\n    end\n\n    StartMappingPage --\u003e StartMappingMapComponent\n    StartMappingPage --\u003e StartMappingHeader\n    StartMappingPage --\u003e StartMappingMobileDrawer\n\n    subgraph \"Prediction Visualization\"\n        PredictionLayers[\"PredictionLayers\"]\n        ACCEPTED_MODEL_PREDICTIONS[\"ACCEPTED_MODEL_PREDICTIONS_FILL_LAYER\"]\n        REJECTED_MODEL_PREDICTIONS[\"REJECTED_MODEL_PREDICTIONS_FILL_LAYER\"]\n        ALL_MODEL_PREDICTIONS[\"ALL_MODEL_PREDICTIONS_FILL_LAYER\"]\n    end\n\n    StartMappingMapComponent --\u003e PredictionLayers\n    PredictionLayers --\u003e ACCEPTED_MODEL_PREDICTIONS\n    PredictionLayers --\u003e REJECTED_MODEL_PREDICTIONS\n    PredictionLayers --\u003e ALL_MODEL_PREDICTIONS\n\n    subgraph \"Configuration Options\"\n        ModelSelector[\"ModelSelector\"]\n        ImagerySourceSelector[\"ImagerySourceSelector\"]\n        DownloadOptions[\"Download Options\"]\n    end\n\n    StartMappingHeader --\u003e ModelSelector\n    StartMappingHeader --\u003e ImagerySourceSelector\n    StartMappingHeader --\u003e DownloadOptions\n    StartMappingMobileDrawer --\u003e ModelSelector\n    StartMappingMobileDrawer --\u003e ImagerySourceSelector\n    StartMappingMobileDrawer --\u003e DownloadOptions\n```\n\nThe Start Mapping interface provides:\n- Model prediction visualization on the map\n- Imagery source selection\n- Model selection for prediction\n- Download options for GeoJSON exports\n- JOSM integration for external editing\n\nSources: [frontend/src/app/routes/start-mapping.tsx:64-517](), [frontend/src/features/start-mapping/components/mobile-drawer.tsx:22-201]()\n\n## Imagery Source Management\n\nThe system supports different imagery sources for mapping through the `ImagerySourceSelector` component:\n\n```mermaid\ngraph TD\n    subgraph \"Imagery Source Selection\"\n        ImagerySourceSelector[\"ImagerySourceSelector\"]\n        ImagerySourceOptions[\"PredictionImagerySource Enum\"]\n        CustomImageryInput[\"CustomImageryInput\"]\n    end\n\n    ImagerySourceSelector --\u003e ImagerySourceOptions\n    ImagerySourceSelector --\u003e CustomImageryInput\n\n    subgraph \"Imagery Sources\"\n        ModelDefault[\"ModelDefault - Training imagery\"]\n        CustomImagery[\"CustomImagery - User-provided XYZ/TMS\"]\n        Kontour[\"Kontour - OpenAerialMap Mosaic\"]\n    end\n\n    ImagerySourceOptions --\u003e ModelDefault\n    ImagerySourceOptions --\u003e CustomImagery\n    ImagerySourceOptions --\u003e Kontour\n    \n    CustomImageryInput --\u003e CustomImagery\n```\n\nAvailable imagery sources include:\n\n| Source | Description | Example URL |\n|--------|-------------|-------------|\n| Model Default | Original imagery used for training the model | Varies by model |\n| Custom Imagery | User-specified XYZ/TMS tile server URL | `https://server.com/tiles/{z}/{x}/{y}.png` |\n| OpenAerialMap Mosaic | All OpenAerialMap images in a mosaic | `https://apps.kontur.io/raster-tiler/oam/mosaic/{z}/{x}/{y}.png` |\n\nThe imagery source selection affects:\n- Visual context for model predictions\n- Prediction accuracy (best when using the same imagery as training)\n- Consistent visual reference for multiple users\n\nSources: [frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector.tsx:36-151](), [frontend/src/features/start-mapping/components/replicable-models/imagery-source-selector-trigger-button.tsx:11-81](), [frontend/src/enums/start-mapping.ts:3-8]()\n\n## Model Selection\n\nThe system allows selecting different models for predictions through the `ModelSelector` component:\n\n```mermaid\ngraph TD\n    subgraph \"Model Selection\"\n        ModelSelector[\"ModelSelector\"]\n        PredictionModel[\"PredictionModel Enum\"]\n        CustomModelInput[\"CustomModelInput\"]\n    end\n\n    ModelSelector --\u003e PredictionModel\n    ModelSelector --\u003e CustomModelInput\n\n    subgraph \"Model Types\"\n        DEFAULT[\"DEFAULT - Current model\"]\n        RAMP[\"RAMP - Base RAMP model\"]\n        YOLOV8_V1[\"YOLOV8_V1 - YOLOv8 version 1\"]\n        YOLOV8_V2[\"YOLOV8_V2 - YOLOv8 version 2\"]\n        CUSTOM[\"CUSTOM - User-defined checkpoint\"]\n    end\n\n    PredictionModel --\u003e DEFAULT\n    PredictionModel --\u003e RAMP\n    PredictionModel --\u003e YOLOV8_V1\n    PredictionModel --\u003e YOLOV8_V2\n    PredictionModel --\u003e CUSTOM\n    \n    CustomModelInput --\u003e CUSTOM\n```\n\nWhen a model is selected, the system retrieves the appropriate model checkpoint path using `constructModelCheckpointPath` for default models or uses the user-provided path for custom models.\n\nSources: [frontend/src/features/start-mapping/components/replicable-models/model-selector.tsx:16-191](), [frontend/src/features/start-mapping/components/replicable-models/model-selector-trigger-button.tsx:12-92](), [frontend/src/enums/start-mapping.ts:10-14](), [frontend/src/utils/general-utils.ts:90-105]()\n\n## TerraDraw Integration\n\nThe system integrates TerraDraw for drawing functionality:\n\n```mermaid\ngraph TD\n    subgraph \"TerraDraw Setup\"\n        setupTerraDraw[\"setupTerraDraw Function\"]\n        TerraDrawMapLibreGLAdapter[\"TerraDrawMapLibreGLAdapter\"]\n        TerraDrawRectangleMode[\"TerraDrawRectangleMode\"]\n        ValidationFunctions[\"Validation Functions\"]\n    end\n\n    setupTerraDraw --\u003e TerraDrawMapLibreGLAdapter\n    setupTerraDraw --\u003e TerraDrawRectangleMode\n    TerraDrawRectangleMode --\u003e ValidationFunctions\n\n    subgraph \"Drawing Events\"\n        onChange[\"onChange Event\"]\n        onFinish[\"onFinish Event\"]\n        handleFeatureChange[\"handleFeatureChange Function\"]\n        handleFinish[\"handleFinish Function\"]\n    end\n\n    TerraDrawRectangleMode --\u003e onChange\n    TerraDrawRectangleMode --\u003e onFinish\n    onChange --\u003e handleFeatureChange\n    onFinish --\u003e handleFinish\n    \n    handleFeatureChange --\u003e updateArea[\"Update Area Size\"]\n    handleFinish --\u003e validateArea[\"Validate Area Size\"]\n    validateArea --\u003e saveArea[\"Save Training Area\"]\n```\n\nThe TerraDraw integration provides:\n- Rectangle drawing mode for training areas\n- Validation of drawn shapes (size, self-intersection)\n- Real-time feedback during drawing\n- Style configuration for visual feedback\n\nSources: [frontend/src/components/map/setups/setup-terra-draw.ts:16-75](), [frontend/src/features/model-creation/components/training-area/training-area-map.tsx:114-171]()\n\n## File Upload and Download\n\nThe system supports GeoJSON file operations for training areas and predictions:\n\n### File Upload\n\nThe `FileUploadDialog` component allows uploading GeoJSON files containing:\n- Training areas (polygons) for model training\n- Labels for specific areas of interest\n\nKey features:\n- GeoJSON format validation\n- Size and feature count limitations\n- Area size validation for training areas\n\nSources: [frontend/src/features/model-creation/components/dialogs/file-upload-dialog.tsx:49-345]()\n\n### File Download\n\nThe system provides multiple options for downloading prediction results:\n- Download all predictions as GeoJSON\n- Download only accepted predictions as GeoJSON\n- Open predictions in JOSM for editing in OSM\n\nSources: [frontend/src/app/routes/start-mapping.tsx:256-298]()\n\n## Mobile Support\n\nThe map components support responsive design for mobile devices:\n\n- `StartMappingMobileDrawer` provides a mobile-friendly interface for settings and controls\n- Adaptive layout changes based on viewport size\n- Touch-friendly controls for mobile interaction\n- Specialized mobile dialogs for model and imagery selection\n\nSources: [frontend/src/features/start-mapping/components/mobile-drawer.tsx:22-201](), [frontend/src/app/routes/start-mapping.tsx:412-441]()\n\n## Usage Patterns\n\n### Creating a Map Interface\n\nTo implement a custom map interface:\n\n1. Use the `useMapInstance` hook to get a map instance and container reference\n2. Configure the `MapComponent` with appropriate controls and layers\n3. Add custom layer components as children\n4. Implement event handlers for map interactions\n\n### Adding Custom Layers\n\nTo add custom layers to a map:\n\n1. Define the layer IDs and source IDs\n2. Create a component that adds the layers to the map\n3. Handle layer visibility through the `LayerControl` component\n4. Implement data fetching and updates for the layers\n\n### Integrating Drawing Functionality\n\nTo enable drawing on a map:\n\n1. Set up TerraDraw with appropriate modes\n2. Implement event handlers for drawing events\n3. Add validation and feedback during drawing\n4. Handle the drawn features (save, display, etc.)\n\nThe map visualization components provide a flexible and extensible foundation for building mapping interfaces in the fAIr system."])</script><script>self.__next_f.push([1,"25:T37e2,"])</script><script>self.__next_f.push([1,"# Development and Deployment\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [.github/workflows/docker_build.yml](.github/workflows/docker_build.yml)\n- [.github/workflows/docker_publish_image.yml](.github/workflows/docker_publish_image.yml)\n- [.github/workflows/frontend_build.yml](.github/workflows/frontend_build.yml)\n- [.github/workflows/frontend_build_push.yml](.github/workflows/frontend_build_push.yml)\n- [Readme.md](Readme.md)\n- [backend/Dockerfile](backend/Dockerfile)\n- [backend/Dockerfile_CPU](backend/Dockerfile_CPU)\n- [backend/README.md](backend/README.md)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n- [docker-compose-cpu.yml](docker-compose-cpu.yml)\n- [docker-compose.yml](docker-compose.yml)\n- [docs/Docker-installation.md](docs/Docker-installation.md)\n- [frontend/Dockerfile.frontend](frontend/Dockerfile.frontend)\n- [run_migrations.sh](run_migrations.sh)\n- [setup-ramp.sh](setup-ramp.sh)\n\n\u003c/details\u003e\n\n\n\nThis document provides an overview of the development workflow and deployment processes for the fAIr AI-assisted mapping system. It covers local development setup, CI/CD pipelines, Docker containerization, and production deployment methods. For installation instructions, see [Installation and Setup](#1.3).\n\n## Development Environment Setup\n\nThe fAIr system provides multiple development environment options, with Docker being the recommended approach for consistency across development environments.\n\n### Docker-Based Development\n\nDocker Compose configuration provides a complete development environment with all required services:\n\n```mermaid\ngraph TD\n    subgraph \"Docker Environment\"\n        Frontend[\"Frontend Container\u003cbr\u003e(React)\"]\n        API[\"API Container\u003cbr\u003e(Django)\"]\n        Worker[\"Worker Container\u003cbr\u003e(Celery)\"]\n        Flower[\"Flower Dashboard\u003cbr\u003e(Worker Monitoring)\"]\n        Redis[\"Redis Container\u003cbr\u003e(Queue \u0026 Cache)\"]\n        PostgreSQL[\"PostgreSQL/PostGIS\u003cbr\u003e(Database)\"]\n    end\n\n    Frontend --\u003e API\n    API --\u003e PostgreSQL\n    API --\u003e Redis\n    Worker --\u003e Redis\n    Worker --\u003e PostgreSQL\n    Flower --\u003e Redis\n    \n    subgraph \"Host System\"\n        RAMP_HOME[\"RAMP_HOME\u003cbr\u003e(AI Model Base)\"]\n        TRAINING_WORKSPACE[\"TRAINING_WORKSPACE\u003cbr\u003e(Training Data)\"]\n    end\n    \n    API --\u003e RAMP_HOME\n    Worker --\u003e RAMP_HOME\n    API --\u003e TRAINING_WORKSPACE\n    Worker --\u003e TRAINING_WORKSPACE\n```\n\nTo set up the Docker-based development environment:\n\n1. Clone the repository and navigate to the project directory\n2. Install prerequisites:\n   - Docker and Docker Compose\n   - NVIDIA Container Toolkit (for GPU support)\n3. Set up RAMP base model:\n   ```bash\n   bash setup-ramp.sh\n   ```\n4. Configure environment variables:\n   - Create `.env` file in the backend directory using `docker_sample_env` as a template\n   - Create `.env` file in the frontend directory using `.env_sample` as a template\n5. Build and start the containers:\n   ```bash\n   docker compose build\n   docker compose up\n   ```\n6. Run database migrations:\n   ```bash\n   bash run_migrations.sh\n   ```\n\nThe development environment will be available at:\n- Frontend: http://127.0.0.1:3000\n- Backend API: http://localhost:8000\n- Flower dashboard: http://localhost:5500\n\nSources: [docs/Docker-installation.md](), [docker-compose.yml](), [backend/docker_sample_env](), [run_migrations.sh](), [setup-ramp.sh]()\n\n### GPU vs CPU Configuration\n\nThe system supports both GPU and CPU configurations:\n\n- For GPU support (recommended for model training):\n  ```bash\n  docker compose up\n  ```\n\n- For CPU-only environments:\n  ```bash\n  docker compose -f docker-compose-cpu.yml up\n  ```\n\nNote that model training performance will be significantly degraded without GPU acceleration.\n\nSources: [docker-compose.yml](), [docker-compose-cpu.yml](), [backend/Dockerfile](), [backend/Dockerfile_CPU]()\n\n### Local Development Environment\n\nFor direct local development without Docker:\n\n1. Install Python 3.8+, pip, and virtualenv\n2. Set up a virtual environment:\n   ```bash\n   virtualenv env\n   source ./env/bin/activate\n   ```\n3. Set up RAMP dependencies and basemodel:\n   - Clone RAMP code repository\n   - Download basemodel checkpoint\n   - Install GDAL and other dependencies\n4. Install required packages:\n   ```bash\n   pip install -r backend/requirements.txt\n   pip install -r backend/api-requirements.txt\n   ```\n5. Configure environment variables in `.env` file\n6. Start backend services:\n   ```bash\n   cd backend\n   python manage.py runserver\n   celery -A aiproject worker --loglevel=debug -Q ramp_training,yolo_training\n   python manage.py qcluster\n   ```\n7. Start frontend development server:\n   ```bash\n   cd frontend\n   npm install\n   npm run dev\n   ```\n\nSources: [backend/README.md](), [backend/api-requirements.txt](), [backend/sample_env]()\n\n## CI/CD Workflows\n\nThe project uses GitHub Actions for continuous integration and deployment.\n\n### CI/CD Pipeline Architecture\n\n```mermaid\nflowchart TD\n    subgraph \"Source Control\"\n        GitRepo[\"GitHub Repository\"]\n    end\n    \n    subgraph \"Continuous Integration\"\n        BackendBuild[\"Backend Build Workflow\u003cbr\u003ebackend_build.yml\"]\n        FrontendBuild[\"Frontend Build Workflow\u003cbr\u003efrontend_build.yml\"]\n        DockerBuild[\"Docker Build Workflow\u003cbr\u003edocker_build.yml\"]\n    end\n    \n    subgraph \"Continuous Deployment\"\n        DockerPublish[\"Docker Publish Workflow\u003cbr\u003edocker_publish_image.yml\"]\n        FrontendDeploy[\"Frontend Deploy Workflow\u003cbr\u003efrontend_build_push.yml\"]\n    end\n    \n    subgraph \"Deployment Targets\"\n        GHCR[\"GitHub Container Registry\"]\n        S3Bucket[\"AWS S3 Bucket\"]\n    end\n    \n    GitRepo -- \"Push/PR to master/develop\u003cbr\u003e(backend/** changes)\" --\u003e BackendBuild\n    GitRepo -- \"Push/PR to master/develop\u003cbr\u003e(frontend/** changes)\" --\u003e FrontendBuild\n    GitRepo -- \"Push/PR to master/develop\u003cbr\u003e(backend/** changes)\" --\u003e DockerBuild\n    \n    GitRepo -- \"Push to master/develop\" --\u003e DockerPublish\n    GitRepo -- \"Release created\" --\u003e FrontendDeploy\n    \n    DockerPublish --\u003e GHCR\n    FrontendDeploy --\u003e S3Bucket\n```\n\nSources: [.github/workflows/backend_build.yml](), [.github/workflows/frontend_build.yml](), [.github/workflows/docker_build.yml](), [.github/workflows/docker_publish_image.yml](), [.github/workflows/frontend_build_push.yml]()\n\n### Backend Build Workflow\n\nThe backend build workflow:\n1. Sets up a test environment with PostgreSQL and Redis\n2. Installs dependencies including RAMP and GDAL\n3. Runs database migrations\n4. Executes backend tests\n\nThis workflow runs on pushes and pull requests to master/develop branches when backend files are changed.\n\nSources: [.github/workflows/backend_build.yml]()\n\n### Frontend Build Workflow\n\nThe frontend build workflow:\n1. Sets up Node.js environment (testing against multiple Node versions)\n2. Installs dependencies with pnpm\n3. Builds the frontend application\n\nThis workflow runs on pushes and pull requests to master/develop branches when frontend files are changed.\n\nSources: [.github/workflows/frontend_build.yml]()\n\n### Docker Build and Publish\n\nThe Docker build workflow tests building the Docker images without publishing them.\n\nThe Docker publish workflow builds and publishes two container images to GitHub Container Registry:\n1. API container (`Dockerfile.API`) - Backend API service\n2. Worker container (`Dockerfile`) - Celery worker for asynchronous processing\n\nThis workflow runs on pushes to master/develop branches, and when releases are created.\n\nSources: [.github/workflows/docker_build.yml](), [.github/workflows/docker_publish_image.yml]()\n\n### Frontend Deployment\n\nFrontend deployment workflow:\n1. Builds the frontend with production environment variables\n2. Authenticates to AWS using OIDC\n3. Syncs the built files to an S3 bucket for static hosting\n\nThis workflow runs when a release is created or manually triggered.\n\nSources: [.github/workflows/frontend_build_push.yml]()\n\n## Docker Containerization\n\nThe system is containerized using Docker, with different containers for each component.\n\n### Container Architecture\n\n```mermaid\ngraph TD\n    subgraph \"Frontend Layer\"\n        Frontend[\"Frontend Container\u003cbr\u003eReact Application\"]\n    end\n    \n    subgraph \"API Layer\"\n        API[\"API Container\u003cbr\u003eDjango REST API\"]\n    end\n    \n    subgraph \"Worker Layer\"\n        Worker[\"Worker Container\u003cbr\u003eCelery Worker\"]\n        Flower[\"Flower Dashboard\u003cbr\u003eWorker Monitoring\"]\n    end\n    \n    subgraph \"Data Layer\"\n        Redis[\"Redis\u003cbr\u003eMessage Broker\"]\n        PostgreSQL[\"PostgreSQL/PostGIS\u003cbr\u003eDatabase\"]\n    end\n    \n    Frontend --\u003e API\n    API --\u003e Redis\n    API --\u003e PostgreSQL\n    Worker --\u003e Redis\n    Worker --\u003e PostgreSQL\n    Flower --\u003e Redis\n```\n\nSources: [docker-compose.yml](), [backend/Dockerfile](), [backend/Dockerfile.API](), [frontend/Dockerfile.frontend]()\n\n### Container Configurations\n\n#### API Container\n- Base image: `tensorflow/tensorflow:2.9.2` (GPU or CPU variant)\n- Includes Django, REST framework, and dependencies\n- Handles HTTP requests and database operations\n- Dockerfile: `backend/Dockerfile.API` (production) or `backend/Dockerfile`/`backend/Dockerfile_CPU` (development)\n\n#### Worker Container\n- Base image: `tensorflow/tensorflow:2.9.2-gpu` (for GPU support)\n- Runs Celery worker for background tasks\n- Handles model training and prediction\n- Requires GPU capabilities for optimal performance\n- Dockerfile: `backend/Dockerfile`\n\n#### Frontend Container\n- Base image: `node:22` (build) / `alpine:latest` (serving)\n- Builds and serves React application\n- Development mode runs with hot reloading\n- Production mode builds static files\n- Dockerfile: `frontend/Dockerfile.frontend`\n\nSources: [backend/Dockerfile](), [backend/Dockerfile_CPU](), [frontend/Dockerfile.frontend]()\n\n### Development vs Production\n\nDevelopment containers:\n- Include development dependencies\n- Mount code volumes for live editing\n- Run development servers with debugging\n- Include Flower dashboard for Celery monitoring\n\nProduction containers:\n- Optimized for performance and security\n- API container runs with WSGI server\n- Worker container optimized for background processing\n- Frontend served as static files from S3\n\nSources: [docker-compose.yml](), [.github/workflows/docker_publish_image.yml]()\n\n## Deployment Process\n\n### Production Deployment Architecture\n\n```mermaid\ngraph TD\n    subgraph \"User\"\n        Browser[\"Web Browser\"]\n    end\n    \n    subgraph \"Frontend\"\n        S3[\"AWS S3 Bucket\u003cbr\u003eStatic Files\"]\n        CloudFront[\"CDN\u003cbr\u003e(Optional)\"]\n    end\n    \n    subgraph \"Backend Services\"\n        API[\"API Container\u003cbr\u003eghcr.io/hotosm/fair_api\"]\n        Worker[\"Worker Container\u003cbr\u003eghcr.io/hotosm/fair_worker\"]\n    end\n    \n    subgraph \"Data Storage\"\n        Redis[\"Redis\u003cbr\u003eMessage Broker\"]\n        PostgreSQL[\"PostgreSQL/PostGIS\u003cbr\u003eDatabase\"]\n        Storage[\"Object Storage\u003cbr\u003eModel Files \u0026 Training Data\"]\n    end\n    \n    Browser --\u003e S3\n    Browser --\u003e CloudFront\n    CloudFront --\u003e S3\n    Browser --\u003e API\n    API --\u003e Redis\n    API --\u003e PostgreSQL\n    API --\u003e Storage\n    Worker --\u003e Redis\n    Worker --\u003e PostgreSQL\n    Worker --\u003e Storage\n```\n\nSources: [.github/workflows/frontend_build_push.yml](), [.github/workflows/docker_publish_image.yml]()\n\n### Frontend Deployment\n\nThe frontend is deployed as static files to AWS S3:\n\n1. GitHub Actions workflow builds the frontend with production environment variables\n2. Built files are uploaded to an S3 bucket\n3. S3 bucket is configured for static website hosting\n4. Optionally, a CDN like CloudFront can be configured for improved performance\n\nRequired environment variables for frontend build:\n- `VITE_BASE_API_URL`: Backend API URL\n- `VITE_MATOMO_ID`: Analytics ID (optional)\n- `VITE_MATOMO_APP_DOMAIN`: Analytics domain (optional)\n- `VITE_OSM_HASHTAGS`: OSM hashtags for changesets\n- `VITE_FAIR_PREDICTOR_API_URL`: Prediction API URL\n\nSources: [.github/workflows/frontend_build_push.yml]()\n\n### Backend Deployment\n\nBackend deployment uses Docker containers:\n\n1. GitHub Actions workflow builds and publishes container images to GitHub Container Registry\n2. Images are tagged based on Git branch/tag\n3. API and Worker containers are deployed to target infrastructure\n4. Containers connect to production database and Redis instances\n\nRequired environment variables for backend deployment:\n- Database connection details\n- Redis connection details\n- OSM authentication credentials\n- Storage configuration\n- RAMP and training workspace paths\n\nSources: [.github/workflows/docker_publish_image.yml](), [backend/docker_sample_env](), [backend/sample_env]()\n\n### Environment Configuration\n\nProduction environment should be configured with:\n\n| Variable | Description | Example |\n|----------|-------------|---------|\n| `DATABASE_URL` | PostgreSQL database connection string | `postgis://user:password@host:port/dbname` |\n| `CELERY_BROKER_URL` | Redis connection for Celery broker | `redis://host:port/0` |\n| `CELERY_RESULT_BACKEND` | Redis connection for Celery results | `redis://host:port/0` |\n| `OSM_CLIENT_ID` | OpenStreetMap OAuth client ID | `your-osm-client-id` |\n| `OSM_CLIENT_SECRET` | OpenStreetMap OAuth client secret | `your-osm-client-secret` |\n| `OSM_LOGIN_REDIRECT_URI` | OpenStreetMap OAuth redirect URI | `https://your-domain/authenticate/` |\n| `RAMP_HOME` | Path to RAMP base model | `/path/to/ramp` |\n| `TRAINING_WORKSPACE` | Path to training workspace | `/path/to/training` |\n| `CORS_ALLOWED_ORIGINS` | Allowed origins for CORS | `https://your-frontend-domain` |\n\nSources: [backend/sample_env](), [backend/docker_sample_env]()\n\n## Testing\n\n### Running Backend Tests\n\nBackend tests can be run locally:\n\n```bash\ncd backend\npython manage.py test tests\n```\n\nThe test suite includes:\n- API endpoint tests\n- Model tests\n- Authentication tests\n- Task processing tests\n\nTests are automatically run as part of the CI pipeline.\n\nSources: [backend/tests/test_endpoints.py](), [backend/tests/test_views.py](), [.github/workflows/backend_build.yml]()"])</script><script>self.__next_f.push([1,"26:T2ce8,"])</script><script>self.__next_f.push([1,"# Development Environment Setup\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [Readme.md](Readme.md)\n- [backend/Dockerfile](backend/Dockerfile)\n- [backend/Dockerfile_CPU](backend/Dockerfile_CPU)\n- [backend/aiproject/__init__.py](backend/aiproject/__init__.py)\n- [backend/aiproject/asgi.py](backend/aiproject/asgi.py)\n- [backend/aiproject/celery.py](backend/aiproject/celery.py)\n- [backend/aiproject/utils.py](backend/aiproject/utils.py)\n- [backend/aiproject/wsgi.py](backend/aiproject/wsgi.py)\n- [docker-compose-cpu.yml](docker-compose-cpu.yml)\n- [docker-compose.yml](docker-compose.yml)\n- [docs/Docker-installation.md](docs/Docker-installation.md)\n- [frontend/.gitignore](frontend/.gitignore)\n- [frontend/Dockerfile.frontend](frontend/Dockerfile.frontend)\n- [frontend/README.md](frontend/README.md)\n- [frontend/eslint.config.js](frontend/eslint.config.js)\n- [frontend/index.html](frontend/index.html)\n- [frontend/src/components/ui/animated-beam/animated-beam.tsx](frontend/src/components/ui/animated-beam/animated-beam.tsx)\n- [frontend/src/components/ui/banner/banner.tsx](frontend/src/components/ui/banner/banner.tsx)\n- [frontend/src/styles/index.css](frontend/src/styles/index.css)\n- [run_migrations.sh](run_migrations.sh)\n- [setup-ramp.sh](setup-ramp.sh)\n\n\u003c/details\u003e\n\n\n\nThis page provides comprehensive instructions for setting up a local development environment for the fAIr AI-assisted mapping system. This guide focuses specifically on configuring your local machine for developing and testing the fAIr application. For deployment instructions, see [Docker Containerization](#4.3) and for continuous integration workflows, see [CI/CD Workflows](#4.2).\n\n## Prerequisites\n\nBefore setting up the fAIr development environment, ensure you have the following:\n\n- 64-bit operating system (Linux, macOS, or Windows)\n- At least 8GB RAM (16GB recommended)\n- At least 20GB free disk space\n- Git installed\n- Docker and Docker Compose installed\n- For optimal performance: NVIDIA GPU with appropriate drivers\n\n## Development Environment Architecture\n\nThe fAIr development environment consists of several interconnected components running in Docker containers:\n\n```mermaid\ngraph TD\n    subgraph \"Docker Environment\"\n        Frontend[\"Frontend Container\u003cbr\u003eReact/TypeScript\"]\n        BackendAPI[\"Backend API Container\u003cbr\u003eDjango\"]\n        Worker[\"Worker Container\u003cbr\u003eCelery\"]\n        PostgreSQL[\"PostgreSQL/PostGIS\u003cbr\u003eDatabase\"]\n        Redis[\"Redis\u003cbr\u003eMessage Broker\"]\n        Flower[\"Flower\u003cbr\u003eTask Monitor\"]\n    end\n    \n    subgraph \"External Components\"\n        RAMP[\"RAMP Home\u003cbr\u003eAI Model Framework\"]\n        TrainingWS[\"Training Workspace\u003cbr\u003eModel Training Data\"]\n    end\n    \n    subgraph \"External Services\"\n        OSM[\"OpenStreetMap API\u003cbr\u003eAuth \u0026 Data\"]\n        OAM[\"OpenAerialMap\u003cbr\u003eImagery\"]\n    end\n    \n    Frontend --\u003e|\"HTTP Requests\"| BackendAPI\n    BackendAPI --\u003e|\"Enqueue Tasks\"| Redis\n    Worker --\u003e|\"Process Tasks\"| Redis\n    BackendAPI --\u003e|\"Store Data\"| PostgreSQL\n    Worker --\u003e|\"Read/Write Data\"| PostgreSQL\n    Flower --\u003e|\"Monitor\"| Redis\n    \n    BackendAPI --\u003e|\"Mount Volume\"| RAMP\n    Worker --\u003e|\"Mount Volume\"| RAMP\n    BackendAPI --\u003e|\"Mount Volume\"| TrainingWS\n    Worker --\u003e|\"Mount Volume\"| TrainingWS\n    \n    BackendAPI --\u003e|\"Authentication\"| OSM\n    BackendAPI --\u003e|\"Fetch Imagery\"| OAM\n```\n\nSources: [docker-compose.yml:1-82](), [docker-compose-cpu.yml:1-76](), [Readme.md:72-84]()\n\n## Setup Process\n\n### 1. Clone the Repository\n\n```bash\ngit clone https://github.com/hotosm/fAIr.git\ncd fAIr\n```\n\n### 2. RAMP Setup\n\nfAIr requires the RAMP (Remote sensing Analytics for Mapping in Python) framework, which includes baseline models. You can set it up automatically using the provided script:\n\n```bash\nbash setup-ramp.sh\n```\n\nAlternatively, follow these manual steps:\n\n1. Create a directory for RAMP:\n   ```bash\n   mkdir ramp\n   ```\n\n2. Download the baseline model checkpoint:\n   ```bash\n   pip install gdown\n   gdown --fuzzy https://drive.google.com/file/d/1YQsY61S_rGfJ_f6kLQq4ouYE2l3iRe1k/view\n   ```\n\n3. Clone the RAMP code:\n   ```bash\n   git clone https://github.com/kshitijrajsharma/ramp-code-fAIr.git ramp-code\n   ```\n\n4. Unzip the baseline model:\n   ```bash\n   unzip checkpoint.tf.zip -d ramp-code/ramp\n   ```\n\n5. Set environment variables:\n   ```bash\n   export RAMP_HOME=/absolute/path/to/ramp\n   export TRAINING_WORKSPACE=/absolute/path/to/fAIr/trainings\n   ```\n\nSources: [setup-ramp.sh:1-29](), [docs/Docker-installation.md:29-64]()\n\n### 3. OpenStreetMap OAuth Setup\n\nfAIr uses OpenStreetMap for authentication:\n\n1. Go to [OpenStreetMap](https://www.openstreetmap.org/) and log in\n2. Navigate to your profile  My Settings  OAuth2 Applications\n3. Register a new application:\n   - Name: \"fAIr Dev Local\"\n   - Permissions: \"Read user preferences\"\n   - Redirect URI: `http://127.0.0.1:3000/authenticate/`\n4. Save the provided `OSM_CLIENT_ID` and `OSM_CLIENT_SECRET` for the next step\n\nSources: [docs/Docker-installation.md:66-73]()\n\n### 4. Environment Configuration\n\n#### Backend Environment\n\n```bash\ncd backend\ncp docker_sample_env .env\n```\n\nEdit the `.env` file to include your OSM credentials and a secret key:\n\n```\nOSM_CLIENT_ID=your_client_id_here\nOSM_CLIENT_SECRET=your_client_secret_here\nOSM_SECRET_KEY=generate_a_random_key_here\n```\n\n#### Frontend Environment\n\n```bash\ncd frontend\ncp .env.sample .env\n```\n\nFor local development, the default values should work without modification.\n\nSources: [docs/Docker-installation.md:75-92](), [.gitignore:12-36]()\n\n### 5. Docker Containers Setup\n\nfAIr offers two Docker Compose configurations:\n\n#### GPU Setup (Recommended)\n\nFor systems with an NVIDIA GPU:\n\n```bash\n# Verify GPU is recognized\nnvidia-smi\n\n# Build and start containers\ndocker compose build\ndocker compose up\n```\n\nSources: [docker-compose.yml:1-82](), [backend/Dockerfile:1-39]()\n\n#### CPU-only Setup\n\nFor systems without a compatible GPU:\n\n```bash\ndocker compose -f docker-compose-cpu.yml build\ndocker compose -f docker-compose-cpu.yml up\n```\n\nSources: [docker-compose-cpu.yml:1-76](), [backend/Dockerfile_CPU:1-41]()\n\n### 6. Database Initialization\n\nAfter starting the containers, run migrations to initialize the database:\n\n```bash\nbash run_migrations.sh\n```\n\nOr manually:\n\n```bash\ndocker exec -it api bash -c \"python manage.py makemigrations\"\ndocker exec -it api bash -c \"python manage.py makemigrations login\"\ndocker exec -it api bash -c \"python manage.py makemigrations core\"\ndocker exec -it api bash -c \"python manage.py migrate\"\n```\n\nSources: [run_migrations.sh:1-7](), [docs/Docker-installation.md:104-125]()\n\n## Container Structure and Service Mapping\n\n```mermaid\ngraph TD\n    subgraph \"Docker Service Architecture\"\n        Frontend[\"frontend\u003cbr\u003ePort: 3000\"]\n        BackendAPI[\"backend-api\u003cbr\u003ePort: 8000\"]\n        BackendWorker[\"backend-worker\"]\n        Postgres[\"postgres\u003cbr\u003ePort: 5434\"]\n        Redis[\"redis\u003cbr\u003ePort: 6379\"]\n        Flower[\"worker-dashboard\u003cbr\u003ePort: 5500\"]\n    end\n    \n    subgraph \"Volume Mounts\"\n        FrontendSrc[\"./frontend  /app\"]\n        BackendSrc[\"./backend  /app\"]\n        RampHome[\"RAMP_HOME  /RAMP_HOME\"]\n        TrainingWS[\"TRAINING_WORKSPACE  /TRAINING_WORKSPACE\"]\n        PostgresData[\"./postgres-data  /var/lib/postgresql/data\"]\n    end\n    \n    FrontendSrc --\u003e|\"mounted to\"| Frontend\n    BackendSrc --\u003e|\"mounted to\"| BackendAPI\n    BackendSrc --\u003e|\"mounted to\"| BackendWorker\n    RampHome --\u003e|\"mounted to\"| BackendAPI\n    RampHome --\u003e|\"mounted to\"| BackendWorker\n    TrainingWS --\u003e|\"mounted to\"| BackendAPI\n    TrainingWS --\u003e|\"mounted to\"| BackendWorker\n    PostgresData --\u003e|\"mounted to\"| Postgres\n    \n    Frontend --\u003e|\"HTTP requests\"| BackendAPI\n    BackendAPI --\u003e|\"enqueue tasks\"| Redis\n    BackendWorker --\u003e|\"process tasks\"| Redis\n    BackendAPI --\u003e|\"DB access\"| Postgres\n    BackendWorker --\u003e|\"DB access\"| Postgres\n    Flower --\u003e|\"monitor tasks\"| Redis\n```\n\nSources: [docker-compose.yml:1-82](), [docker-compose-cpu.yml:1-76]()\n\n## Frontend Development\n\nThe frontend is a React application built with TypeScript, Vite, and various UI libraries:\n\n### Key Frontend Facts\n\n- Port: 3000\n- Development server: Runs with hot-reloading enabled\n- Source location: `/frontend` directory\n\nIf you prefer to run the frontend outside Docker:\n\n```bash\ncd frontend\npnpm install\npnpm dev\n```\n\nThe frontend will be available at `http://127.0.0.1:5173`.\n\nSources: [frontend/README.md:1-146](), [frontend/index.html:1-26]()\n\n## Backend Development\n\nThe backend is a Django application with several key components:\n\n### Key Backend Facts\n\n- Port: 8000\n- API: RESTful endpoints for frontend communication\n- Database: PostgreSQL with PostGIS extension\n- Asynchronous task queue: Celery with Redis broker\n\n### Working with Celery Tasks\n\nThe backend uses Celery for asynchronous tasks like model training:\n\n- Tasks are defined in the backend code\n- Redis serves as the message broker\n- Flower dashboard (port 5500) monitors task execution\n\nIf you make changes to task code, restart the worker:\n\n```bash\ndocker compose restart backend-worker\n```\n\nSources: [backend/aiproject/celery.py:1-30](), [backend/aiproject/__init__.py:1-6]()\n\n## Accessing the Applications\n\nAfter setup, access the applications at:\n\n| Component | URL | Description |\n|-----------|-----|-------------|\n| Frontend | http://127.0.0.1:3000 | React application UI |\n| Backend API | http://localhost:8000 | Django REST API |\n| Flower Dashboard | http://localhost:5500 | Celery task monitor |\n\n**Note**: Use `127.0.0.1:3000` rather than `localhost:3000` to ensure login functionality works correctly.\n\nSources: [docs/Docker-installation.md:135-136]()\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Docker containers fail to start**\n   - Check if the required ports are already in use\n   - Ensure Docker has sufficient resources allocated\n\n2. **GPU not detected**\n   - Run `nvidia-smi` to verify your GPU is recognized\n   - Install NVIDIA Container Toolkit if missing\n\n3. **Database migration errors**\n   - Try removing the postgres-data directory and restarting containers\n   - Run migrations manually step by step\n\n4. **Frontend authentication issues**\n   - Ensure you're using `127.0.0.1:3000` instead of `localhost:3000`\n   - Check that OSM credentials are correctly configured\n\n5. **Model training failures**\n   - Verify RAMP environment is correctly set up\n   - Check worker logs for specific errors:\n     ```bash\n     docker compose logs backend-worker\n     ```\n\nSources: [docs/Docker-installation.md:20-28](), [.gitignore:1-55]()\n\n## Custom TileServer Integration\n\nTo use local tiles for development:\n\n1. Set up a local tile server using [titiler](https://github.com/developmentseed/titiler), [gdal2tiles](https://gdal.org/programs/gdal2tiles.html), or nginx\n2. Modify `docker-compose.yml` to add `network_mode: \"host\"` to API and Worker services\n3. Update backend `.env` to use localhost URLs:\n   ```\n   DATABASE_URL=postgis://postgres:admin@localhost:5434/ai\n   CELERY_BROKER_URL=\"redis://localhost:6379/0\"\n   CELERY_RESULT_BACKEND=\"redis://localhost:6379/0\"\n   ```\n\nSources: [docs/Docker-installation.md:139-190]()\n\n## Next Steps\n\nAfter setting up your development environment, you can:\n\n1. Create your first model using the web interface\n2. Explore the codebase to understand the system architecture\n3. Run tests to ensure everything is working properly\n\nFor more information on the CI/CD workflows, see [CI/CD Workflows](#4.2)."])</script><script>self.__next_f.push([1,"27:T2af5,"])</script><script>self.__next_f.push([1,"# CI/CD Workflows\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [.github/workflows/docker_build.yml](.github/workflows/docker_build.yml)\n- [.github/workflows/docker_publish_image.yml](.github/workflows/docker_publish_image.yml)\n- [.github/workflows/frontend_build.yml](.github/workflows/frontend_build.yml)\n- [.github/workflows/frontend_build_push.yml](.github/workflows/frontend_build_push.yml)\n- [backend/README.md](backend/README.md)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n\n\u003c/details\u003e\n\n\n\nThis page documents the continuous integration and continuous deployment (CI/CD) workflows implemented in the fAIr project. It covers the automated processes for testing, building, and deploying both the frontend and backend components of the system. For information on the development environment setup, see [Development Environment Setup](#4.1), and for details on Docker containerization, see [Docker Containerization](#4.3).\n\n## Overview of CI/CD Pipelines\n\nThe fAIr project uses GitHub Actions as its CI/CD platform to automate the software development lifecycle. These workflows ensure code quality through automated testing, build Docker images for containerized deployment, and publish frontend builds to cloud storage.\n\n```mermaid\nflowchart TD\n    subgraph \"Code Repository\"\n        GitRepo[\"GitHub Repository (fAIr)\"]\n    end\n    \n    subgraph \"CI Workflows\"\n        BackendBuild[\"Backend Build Workflow\n        Validates backend code\"]\n        FrontendBuild[\"Frontend Build Workflow\n        Validates frontend code\"]\n        DockerBuild[\"Docker Build Workflow\n        Tests Docker image builds\"]\n    end\n    \n    subgraph \"CD Workflows\"\n        DockerPublish[\"Docker Publish Workflow\n        Pushes images to GitHub Container Registry\"]\n        FrontendDeploy[\"Frontend Deploy Workflow\n        Deploys to S3\"]\n    end\n    \n    subgraph \"Deployment Targets\"\n        GHCR[\"GitHub Container Registry\n        (API and Worker images)\"]\n        S3[\"AWS S3 Bucket\n        (Frontend static files)\"]\n    end\n    \n    GitRepo --\u003e|\"PR/Push to master/develop\n    (backend files)\"| BackendBuild\n    GitRepo --\u003e|\"PR/Push to master/develop\n    (frontend files)\"| FrontendBuild\n    GitRepo --\u003e|\"PR/Push to master/develop\n    (backend files)\"| DockerBuild\n    \n    GitRepo --\u003e|\"Push to master/develop\"| DockerPublish\n    GitRepo --\u003e|\"Release\"| FrontendDeploy\n    \n    DockerPublish --\u003e GHCR\n    FrontendDeploy --\u003e S3\n```\n\nSources: [.github/workflows/backend_build.yml:1-17](), [.github/workflows/frontend_build.yml:1-14](), [.github/workflows/docker_build.yml:1-20](), [.github/workflows/docker_publish_image.yml:1-15](), [.github/workflows/frontend_build_push.yml:1-7]()\n\n## Workflow Triggers\n\nDifferent workflows are triggered based on specific events in the repository:\n\n| Workflow | Trigger | File Path Filter | Purpose |\n|----------|---------|------------------|---------|\n| Backend Build | Push to master/develop, Pull Requests | `backend/**`, `.github/workflows/backend_build.yml` | Run backend tests to ensure code quality |\n| Frontend Build | Push to master/develop, Pull Requests | `frontend/**`, `.github/workflows/frontend_build.yml` | Validate frontend build process |\n| Docker Build | Push to master/develop, Pull Requests | `backend/**`, `.github/workflows/docker_build.yml` | Test Docker image builds without publishing |\n| Docker Publish | Push to master/develop, Release | Excludes CI workflow files | Build and publish Docker images to GitHub Container Registry |\n| Frontend Deploy | Release, Manual dispatch | None | Build and deploy frontend to AWS S3 |\n\nSources: [.github/workflows/backend_build.yml:2-16](), [.github/workflows/frontend_build.yml:2-13](), [.github/workflows/docker_build.yml:3-19](), [.github/workflows/docker_publish_image.yml:3-14](), [.github/workflows/frontend_build_push.yml:3-6]()\n\n## Backend CI/CD Pipeline\n\nThe backend CI/CD pipeline ensures that the Django backend application is tested and deployed correctly.\n\n### Backend Testing Workflow\n\nThe backend build workflow performs the following steps:\n\n1. Set up a PostgreSQL database with PostGIS extension for testing\n2. Install GDAL and other system dependencies\n3. Clone and set up RAMP code for AI model integration\n4. Install Python dependencies\n5. Set up required environment variables\n6. Run database migrations\n7. Execute tests with coverage reporting\n\n```mermaid\nflowchart TD\n    checkout[\"Check out code\"] --\u003e setupPython[\"Set up Python 3.8\"]\n    setupPython --\u003e cloneRamp[\"Clone RAMP code\"]\n    cloneRamp --\u003e downloadBasemodel[\"Download Basemodel\"]\n    downloadBasemodel --\u003e installDeps[\"Install dependencies\n    (GDAL, Redis, Python packages)\"]\n    installDeps --\u003e setupEnv[\"Set up environment variables\"]\n    setupEnv --\u003e runCelery[\"Run Celery worker\"]\n    runCelery --\u003e runMigrations[\"Run database migrations\"]\n    runMigrations --\u003e runTests[\"Run tests with coverage\"]\n```\n\nSources: [.github/workflows/backend_build.yml:18-137]()\n\n### Docker Image Building and Publishing\n\nTwo Docker images are built for the backend:\n\n1. **API Container**: Runs the Django API server\n2. **Worker Container**: Runs the Celery worker for asynchronous tasks\n\nThe Docker publish workflow builds and pushes these images to GitHub Container Registry when code is pushed to main branches or when a release is created.\n\n```mermaid\nflowchart TD\n    checkout[\"Check out code\"] --\u003e login[\"Log in to GitHub Container Registry\"]\n    login --\u003e extractMetadata[\"Extract metadata (tags, labels)\"]\n    extractMetadata --\u003e setupBuildx[\"Set up Docker Buildx\"]\n    setupBuildx --\u003e buildPushAPI[\"Build and push API image\"]\n    buildPushAPI --\u003e buildPushWorker[\"Build and push Worker image\"]\n```\n\nSources: [.github/workflows/docker_publish_image.yml:20-102]()\n\n## Frontend CI/CD Pipeline\n\nThe frontend CI/CD pipeline builds and deploys the React frontend application.\n\n### Frontend Build Process\n\nThe frontend build workflow verifies that the frontend can be successfully built across multiple Node.js versions (18, 20, 22):\n\n1. Check out the repository\n2. Set up Node.js\n3. Install pnpm package manager\n4. Install dependencies with pnpm\n5. Build the frontend\n\n```mermaid\nflowchart TD\n    checkout[\"Check out repository\"] --\u003e setupNode[\"Set up Node.js\"]\n    setupNode --\u003e installPnpm[\"Install pnpm\"]\n    installPnpm --\u003e cachePnpm[\"Cache pnpm store\"]\n    cachePnpm --\u003e installDeps[\"Install Node.js dependencies\"]\n    installDeps --\u003e build[\"Build frontend\"]\n```\n\nSources: [.github/workflows/frontend_build.yml:15-54]()\n\n### Frontend Deployment Process\n\nWhen a release is created or the workflow is manually triggered, the frontend deploy workflow:\n\n1. Builds the frontend with production environment variables\n2. Authenticates to AWS using OIDC (OpenID Connect)\n3. Syncs the built files to an S3 bucket\n\n```mermaid\nflowchart TD\n    checkout[\"Check out repository\"] --\u003e setupNode[\"Set up Node.js\"]\n    setupNode --\u003e installPnpm[\"Install pnpm\"]\n    installPnpm --\u003e cachePnpm[\"Cache pnpm store\"]\n    cachePnpm --\u003e installDeps[\"Install Node.js dependencies\"]\n    installDeps --\u003e buildFrontend[\"Build frontend with environment variables\"]\n    buildFrontend --\u003e awsAuth[\"Authenticate to AWS\"]\n    awsAuth --\u003e uploadS3[\"Upload to S3 bucket\"]\n```\n\nSources: [.github/workflows/frontend_build_push.yml:12-70]()\n\n## Environment Variables and Secrets\n\nThe workflows rely on various environment variables and secrets stored in GitHub:\n\n### Backend Environment Variables\n\nThe backend build workflow requires these secrets:\n\n- `TESTING_TOKEN`: Token for testing authentication\n- `OSM_CLIENT_ID`: OpenStreetMap OAuth client ID\n- `OSM_CLIENT_SECRET`: OpenStreetMap OAuth client secret\n- `OSM_SECRET_KEY`: Secret key for OpenStreetMap integration\n\nSources: [.github/workflows/backend_build.yml:116-135](), [backend/sample_env:1-22]()\n\n### Frontend Environment Variables\n\nThe frontend deployment workflow uses these environment variables:\n\n- `VITE_BASE_API_URL`: Base URL for API requests\n- `VITE_MATOMO_ID`: Matomo analytics ID\n- `VITE_MATOMO_APP_DOMAIN`: Matomo app domain\n- `VITE_OSM_HASHTAGS`: OpenStreetMap hashtags\n- `VITE_FAIR_PREDICTOR_API_URL`: URL for the fair predictor API\n\nAnd these secrets:\n- `AWS_OIDC_ROLE`: AWS IAM role ARN for OIDC authentication\n- `FRONTEND_BUCKET`: S3 bucket name for frontend deployment\n\nSources: [.github/workflows/frontend_build_push.yml:53-70]()\n\n## CI/CD Infrastructure Management\n\nThe workflows are executed on GitHub-hosted runners with specific configurations:\n\n| Workflow | Runner | Services | Cache Strategy |\n|----------|--------|----------|---------------|\n| Backend Build | ubuntu-latest | PostgreSQL with PostGIS | pip cache |\n| Frontend Build | ubuntu-latest | None | pnpm store cache |\n| Docker Build | ubuntu-latest | None | Docker layer cache |\n| Docker Publish | ubuntu-24.04 | None | Docker layer cache |\n| Frontend Deploy | ubuntu-latest | None | pnpm store cache |\n\n### Caching Strategies\n\nThe workflows implement caching to improve build performance:\n\n- Frontend workflows cache the pnpm store to speed up dependency installation\n- Docker workflows use layer caching to avoid rebuilding unchanged layers\n\nSources: [.github/workflows/backend_build.yml:20-32](), [.github/workflows/frontend_build.yml:37-45](), [.github/workflows/docker_publish_image.yml:58-60](), [.github/workflows/frontend_build_push.yml:36-43]()\n\n## Workflow Status and Monitoring\n\nThe CI/CD status badge is included in the backend README to show the current status of the backend build workflow:\n\n```\n![example workflow](https://github.com/omranlm/TDB/actions/workflows/backend_build.yml/badge.svg)\n```\n\nSources: [backend/README.md:1-1]()\n\n## Integration with Development Process\n\nThe CI/CD workflows are integrated with the development process to ensure quality control:\n\n```mermaid\nsequenceDiagram\n    participant Developer\n    participant GitHub\n    participant CI as GitHub Actions CI\n    participant CD as GitHub Actions CD\n    participant Registry as GitHub Container Registry\n    participant S3 as AWS S3\n\n    Developer-\u003e\u003eGitHub: Create feature branch\n    Developer-\u003e\u003eGitHub: Push changes\n    Developer-\u003e\u003eGitHub: Create Pull Request\n    GitHub-\u003e\u003eCI: Trigger CI workflows\n    CI-\u003e\u003eGitHub: Report build/test status\n    \n    alt PR Approved and Merged\n        Developer-\u003e\u003eGitHub: Merge to master/develop\n        GitHub-\u003e\u003eCD: Trigger CD workflows\n        CD-\u003e\u003eRegistry: Push Docker images\n    end\n    \n    alt Release Created\n        Developer-\u003e\u003eGitHub: Create release\n        GitHub-\u003e\u003eCD: Trigger frontend deploy\n        CD-\u003e\u003eS3: Deploy frontend\n    end\n```"])</script><script>self.__next_f.push([1,"28:T2940,"])</script><script>self.__next_f.push([1,"# Docker Containerization\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [.github/workflows/docker_build.yml](.github/workflows/docker_build.yml)\n- [.github/workflows/docker_publish_image.yml](.github/workflows/docker_publish_image.yml)\n- [.github/workflows/frontend_build.yml](.github/workflows/frontend_build.yml)\n- [.github/workflows/frontend_build_push.yml](.github/workflows/frontend_build_push.yml)\n- [Readme.md](Readme.md)\n- [backend/Dockerfile](backend/Dockerfile)\n- [backend/Dockerfile_CPU](backend/Dockerfile_CPU)\n- [backend/README.md](backend/README.md)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n- [docker-compose-cpu.yml](docker-compose-cpu.yml)\n- [docker-compose.yml](docker-compose.yml)\n- [docs/Docker-installation.md](docs/Docker-installation.md)\n- [frontend/Dockerfile.frontend](frontend/Dockerfile.frontend)\n- [run_migrations.sh](run_migrations.sh)\n- [setup-ramp.sh](setup-ramp.sh)\n\n\u003c/details\u003e\n\n\n\nThis page documents the Docker containerization approach used in the fAIr project. It covers the container architecture, development environment setup, image building, and CI/CD pipelines for Docker images. For information about general development environment setup, see [Development Environment Setup](#4.1), and for CI/CD workflows beyond Docker, see [CI/CD Workflows](#4.2).\n\n## Container Architecture\n\nThe fAIr project uses Docker to containerize all major components of the system, ensuring consistency across development and deployment environments. The system is composed of several services that work together:\n\n```mermaid\ngraph TD\n    subgraph \"Database Layer\"\n        PG[\"PostgreSQL/PostGIS (postgres)\"]\n        RD[\"Redis (redis)\"]\n    end\n    \n    subgraph \"Application Layer\"\n        API[\"Backend API (backend-api)\"]\n        WRK[\"Backend Worker (backend-worker)\"]\n        FLW[\"Worker Dashboard (flower)\"]\n    end\n    \n    subgraph \"Presentation Layer\"\n        FE[\"Frontend (frontend)\"]\n    end\n    \n    subgraph \"External Resources\"\n        EXT_RAMP[\"RAMP_HOME Volume\"]\n        EXT_TRAIN[\"TRAINING_WORKSPACE Volume\"]\n    end\n    \n    FE --\u003e API\n    API --\u003e PG\n    API --\u003e RD\n    API --\u003e EXT_RAMP\n    API --\u003e EXT_TRAIN\n    WRK --\u003e RD\n    WRK --\u003e PG\n    WRK --\u003e EXT_RAMP\n    WRK --\u003e EXT_TRAIN\n    FLW --\u003e RD\n```\n\n### Services Description\n\n| Service | Purpose | Container Name | Base Image |\n|---------|---------|---------------|------------|\n| postgres | PostgreSQL database with PostGIS extension | pgsql | postgis/postgis |\n| redis | Message broker for Celery and caching | redis | redis |\n| backend-api | Django REST API server | api | tensorflow/tensorflow |\n| backend-worker | Celery worker for async processing | worker | tensorflow/tensorflow |\n| worker-dashboard | Flower dashboard for monitoring Celery | flower | mher/flower |\n| frontend | React frontend application | frontend | node |\n\nSources: [docker-compose.yml](), [docker-compose-cpu.yml]()\n\n## Docker Images\n\n### Backend Images\n\nThe fAIr backend uses two main Dockerfiles:\n\n1. **GPU-enabled Dockerfile** (`Dockerfile`): \n   - Based on `tensorflow/tensorflow:2.9.2-gpu`\n   - Includes GDAL, OpenCV, and other geospatial libraries\n   - Used for the backend worker that performs model training\n\n2. **CPU-only Dockerfile** (`Dockerfile_CPU`):\n   - Based on `tensorflow/tensorflow:2.9.2` (without GPU support)\n   - Contains the same dependencies but works on systems without NVIDIA GPUs\n   - Can be used for development or API-only deployments\n\nBoth Dockerfiles install:\n- GDAL and geospatial libraries for processing geographic data\n- Python dependencies from `requirements.txt` and `api-requirements.txt`\n- RAMP dependencies and Solaris library for machine learning model training\n\nExample backend Dockerfile structure (GPU version):\n\n```mermaid\ngraph TD\n    subgraph \"Dockerfile Building Steps\"\n        baseImg[\"tensorflow/tensorflow:2.9.2-gpu\"]\n        sysLib[\"Install system libraries\u003cbr\u003e(GDAL, OpenCV)\"]\n        gdal[\"Install GDAL Python bindings\"]\n        rampDeps[\"Install RAMP dependencies\"]\n        apiReqs[\"Install API requirements\"]\n        appCode[\"Copy application code\"]\n    end\n    \n    baseImg --\u003e sysLib\n    sysLib --\u003e gdal\n    gdal --\u003e rampDeps\n    rampDeps --\u003e apiReqs\n    apiReqs --\u003e appCode\n```\n\nSources: [backend/Dockerfile](), [backend/Dockerfile_CPU]()\n\n### Frontend Image\n\nThe frontend uses a multi-stage Dockerfile:\n\n1. **Build stage**:\n   - Based on `node:22`\n   - Installs dependencies via pnpm\n   - Builds the static assets\n\n2. **Export stage**:\n   - Based on lightweight `alpine:latest`\n   - Only contains the built application dist folder\n   - Used for extracting the built assets for deployment\n\nSources: [frontend/Dockerfile.frontend]()\n\n## Development Environment\n\n### Setting Up Local Development with Docker\n\nThe project provides a Docker Compose configuration for local development that includes all necessary services:\n\n```mermaid\nflowchart LR\n    subgraph \"Setup Process\"\n        setupRamp[\"Setup RAMP model\"]\n        createEnv[\"Create environment files\"]\n        buildImages[\"Build Docker images\"]\n        runContainers[\"Run Docker Compose\"]\n        runMigrations[\"Run DB migrations\"]\n    end\n    \n    setupRamp --\u003e createEnv\n    createEnv --\u003e buildImages\n    buildImages --\u003e runContainers\n    runContainers --\u003e runMigrations\n```\n\nKey steps for setting up the development environment:\n\n1. Clone the repository\n2. Set up RAMP model and create the RAMP_HOME directory\n3. Create `.env` files for backend and frontend using the sample templates\n4. Configure OSM authentication\n5. Build and run containers with Docker Compose\n6. Run database migrations\n\nSources: [docs/Docker-installation.md](), [run_migrations.sh]()\n\n### Docker Compose Configurations\n\nThe project offers two Docker Compose configurations:\n\n1. **Standard configuration** (`docker-compose.yml`):\n   - Includes GPU support for the worker container\n   - Recommended for development machines with NVIDIA GPUs\n\n2. **CPU-only configuration** (`docker-compose-cpu.yml`):\n   - Uses CPU-only images for all services\n   - Suitable for development on machines without dedicated GPUs\n\nKey environment variables required in `.env`:\n- `RAMP_HOME`: Path to the RAMP model directory\n- `TRAINING_WORKSPACE`: Path for storing training data\n- OSM authentication details (`OSM_CLIENT_ID`, `OSM_CLIENT_SECRET`, etc.)\n\nSources: [docker-compose.yml](), [docker-compose-cpu.yml](), [backend/docker_sample_env]()\n\n## CI/CD Pipeline for Docker Images\n\nThe project uses GitHub Actions to automate the building and publishing of Docker images:\n\n```mermaid\nflowchart TD\n    subgraph \"GitHub Events\"\n        push[\"Push to master/develop\"]\n        release[\"Release published\"]\n        pr[\"Pull request\"]\n    end\n    \n    subgraph \"Docker Workflows\"\n        buildTest[\"Build \u0026 test images\"]\n        publish[\"Publish to GitHub Container Registry\"]\n    end\n    \n    push --\u003e buildTest\n    push --\u003e publish\n    release --\u003e publish\n    pr --\u003e buildTest\n    \n    subgraph \"Image Outputs\"\n        apiImage[\"API image\u003cbr\u003e(ghcr.io/hotosm/fair_api:tag)\"]\n        workerImage[\"Worker image\u003cbr\u003e(ghcr.io/hotosm/fair_worker:tag)\"]\n    end\n    \n    publish --\u003e apiImage\n    publish --\u003e workerImage\n```\n\n### Build Workflow\n\nThe build workflow (`docker_build.yml`):\n- Triggers on pushes and pull requests to master/develop branches\n- Builds both API and worker images\n- Does not push images to the registry (used for validation)\n- Uses Docker Buildx for optimized builds\n- Leverages GitHub Actions cache to speed up builds\n\n### Publish Workflow\n\nThe publish workflow (`docker_publish_image.yml`):\n- Triggers on pushes to master/develop or releases\n- Builds and publishes both API and worker images to GitHub Container Registry\n- Uses metadata to tag images (based on branch/tag)\n- Uses Docker Buildx with caching for faster builds\n\nSources: [.github/workflows/docker_build.yml](), [.github/workflows/docker_publish_image.yml]()\n\n## Frontend Deployment\n\nAlthough not strictly part of the Docker containerization, the frontend is built using Docker and then deployed to an S3 bucket:\n\n```mermaid\nflowchart LR\n    subgraph \"GitHub Actions\"\n        release[\"Release published\"]\n        buildFlow[\"Frontend build workflow\"]\n    end\n    \n    subgraph \"Build Process\"\n        buildDocker[\"Build with Docker\"]\n        buildAssets[\"Generate static assets\"]\n    end\n    \n    subgraph \"Deployment\"\n        s3Upload[\"Upload to S3 bucket\"]\n    end\n    \n    release --\u003e buildFlow\n    buildFlow --\u003e buildDocker\n    buildDocker --\u003e buildAssets\n    buildAssets --\u003e s3Upload\n```\n\nThe workflow:\n1. Triggered on release or manually\n2. Sets up Node.js environment with pnpm\n3. Builds the frontend with configured environment variables\n4. Authenticates to AWS using OIDC\n5. Uploads the built assets to the configured S3 bucket\n\nSources: [.github/workflows/frontend_build_push.yml]()\n\n## GPU Support Considerations\n\nfAIr's backend worker requires GPU acceleration for optimal performance of AI model training. The Docker configuration includes:\n\n1. **GPU passthrough** in Docker Compose:\n```yaml\ndeploy:\n  resources:\n    reservations:\n      devices:\n        - driver: nvidia\n          capabilities: [gpu]\n```\n\n2. **NVIDIA Container Toolkit requirement** for the host machine\n3. **GPU-specific TensorFlow base image** (`tensorflow/tensorflow:2.9.2-gpu`)\n\nFor development environments without a compatible GPU, the CPU-only configuration can be used, but model training will be significantly slower.\n\nSources: [docker-compose.yml](), [docs/Docker-installation.md]()\n\n## Environment Configuration\n\nContainer environment variables are configured through `.env` files:\n\n1. **Backend environment**: Controls database connection, Redis connection, API URLs, and credentials\n2. **Frontend environment**: Controls API endpoints and application settings\n\nFor local development, sample environment files are provided:\n- `backend/docker_sample_env` for the backend\n- `frontend/.env_sample` for the frontend\n\nThese must be copied to `.env` files and populated with appropriate values before running the containers.\n\nSources: [backend/docker_sample_env](), [docs/Docker-installation.md]()"])</script><script>self.__next_f.push([1,"29:T48fe,"])</script><script>self.__next_f.push([1,"# Configuration and Customization\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/requirements.txt](backend/requirements.txt)\n- [frontend/.env.sample](frontend/.env.sample)\n- [frontend/src/components/ui/form/help-text/help-text.tsx](frontend/src/components/ui/form/help-text/help-text.tsx)\n- [frontend/src/config/env.ts](frontend/src/config/env.ts)\n- [frontend/src/config/index.ts](frontend/src/config/index.ts)\n- [frontend/src/constants/ui-contents/models-content.ts](frontend/src/constants/ui-contents/models-content.ts)\n- [frontend/src/constants/ui-contents/start-mapping-content.ts](frontend/src/constants/ui-contents/start-mapping-content.ts)\n- [frontend/src/features/model-creation/components/progress-bar.tsx](frontend/src/features/model-creation/components/progress-bar.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/create-new.tsx](frontend/src/features/model-creation/components/training-dataset/create-new.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/select-existing.tsx](frontend/src/features/model-creation/components/training-dataset/select-existing.tsx)\n- [frontend/src/features/start-mapping/components/map/legend-control.tsx](frontend/src/features/start-mapping/components/map/legend-control.tsx)\n- [frontend/src/types/ui-contents.ts](frontend/src/types/ui-contents.ts)\n\n\u003c/details\u003e\n\n\n\nThis document explains how to configure and customize the fAIr AI-Assisted Mapping system. It covers environment variables, backend configuration options, frontend customization, and UI text modifications. For information about system architecture, see [System Architecture](#1.1), and for deployment instructions, see [Development and Deployment](#4).\n\n## Configuration Overview\n\nThe fAIr system is highly configurable through a combination of environment variables and customizable UI content. This flexibility allows deployers to adapt the system to different environments, use cases, and branding requirements without modifying source code.\n\n```mermaid\nflowchart TD\n    subgraph \"Configuration Points\"\n        ENV[\"Environment Variables\"]\n        UI[\"UI Content Constants\"]\n        DB[\"Database Settings\"]\n        S3[\"S3 Storage Settings\"]\n        OSM[\"OSM Integration\"]\n        MLP[\"Model Training Parameters\"]\n        MAP[\"Map Visualization\"]\n        END[\"API Endpoints\"]\n        EMAIL[\"Email Notifications\"]\n    end\n\n    subgraph \"Configuration Files\"\n        BACKEND_ENV[\".env (Backend)\"]\n        FRONTEND_ENV[\".env (Frontend)\"]\n        SETTINGS[\"settings.py\"]\n        UI_CONTENT[\"UI Content Files\"]\n    end\n\n    ENV --\u003e BACKEND_ENV\n    ENV --\u003e FRONTEND_ENV\n    BACKEND_ENV --\u003e SETTINGS\n    FRONTEND_ENV --\u003e REACT[\"Frontend App\"]\n    \n    UI --\u003e UI_CONTENT\n    UI_CONTENT --\u003e REACT\n    \n    SETTINGS --\u003e DB\n    SETTINGS --\u003e S3\n    SETTINGS --\u003e OSM\n    SETTINGS --\u003e MLP\n    SETTINGS --\u003e EMAIL\n    \n    REACT --\u003e MAP\n    REACT --\u003e END\n```\n\nSources: [backend/aiproject/settings.py:16-38](), [frontend/src/config/env.ts:1-92](), [frontend/src/config/index.ts:1-482]()\n\n## Environment Variables Configuration\n\n### Backend Environment Variables\n\nThe backend uses Django's settings module to load environment variables with default values through the `environ.Env()` utility.\n\nKey backend environment variables include:\n\n| Variable | Purpose | Default | \n|----------|---------|---------|\n| `DEBUG` | Enable debug mode | `False` |\n| `SECRET_KEY` | Django secret key | `\"default_secret_key\"` |\n| `BUCKET_NAME` | S3 bucket for storing model data | `\"fair-dev\"` |\n| `PARENT_BUCKET_FOLDER` | Path prefix in S3 bucket | `\"dev\"` |\n| `AWS_REGION` | AWS region for S3 | `\"us-east-1\"` |\n| `OSM_CLIENT_ID` | OpenStreetMap OAuth client ID | None |\n| `OSM_CLIENT_SECRET` | OpenStreetMap OAuth client secret | None |\n| `RAMP_EPOCHS_LIMIT` | Maximum epochs for RAMP model training | `40` |\n| `YOLO_EPOCHS_LIMIT` | Maximum epochs for YOLO model training | `200` |\n| `ENABLE_PREDICTION_API` | Enable the prediction API | `False` |\n| `TRAINING_WORKSPACE` | Local directory for training data | Current directory + `/training` |\n| `CELERY_BROKER_URL` | Redis URL for Celery | `\"redis://127.0.0.1:6379/0\"` |\n\nThese variables can be set in a `.env` file in the backend root directory or as system environment variables.\n\nSources: [backend/aiproject/settings.py:22-303]()\n\n### Frontend Environment Variables\n\nThe frontend application uses environment variables via Vite's `import.meta.env` mechanism. These are loaded from `.env` files and exposed through a structured configuration system.\n\nKey frontend environment variables include:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `VITE_BASE_API_URL` | Backend API endpoint | `\"http://localhost:8000/api/v1/\"` |\n| `VITE_MAX_TRAINING_AREA_SIZE` | Maximum training area size (sq meters) | `5000000` |\n| `VITE_MIN_TRAINING_AREA_SIZE` | Minimum training area size (sq meters) | `5797` |\n| `VITE_MAX_ZOOM_LEVEL` | Maximum map zoom level | `21` |\n| `VITE_MIN_ZOOM_LEVEL_FOR_START_MAPPING_PREDICTION` | Minimum zoom for predictions | `19` |\n| `VITE_FAIR_PREDICTOR_API_URL` | Model prediction endpoint | `\"https://predictor-dev.fair.hotosm.org/predict/\"` |\n| `VITE_TRAINING_AREAS_AOI_FILL_COLOR` | Training area fill color | `\"#247DCACC\"` |\n| `VITE_OSM_HASHTAGS` | Hashtags for OSM edits | `\"#HOT-fAIr\"` |\n\nThe frontend provides a comprehensive `.env.sample` file that documents all available configuration options with detailed descriptions and default values.\n\nSources: [frontend/src/config/env.ts:1-92](), [frontend/.env.sample:1-212]()\n\n## Backend Configuration\n\n### Database Configuration\n\nfAIr uses PostgreSQL with PostGIS extension for geospatial data storage. The database connection is configured through the `DATABASE_URL` environment variable or defaults to `postgis://admin:password@localhost:5432/ai`:\n\n```python\nDATABASES = {}\nDATABASES[\"default\"] = dj_database_url.config(\n    default=\"postgis://admin:password@localhost:5432/ai\", conn_max_age=500\n)\n```\n\nSources: [backend/aiproject/settings.py:163-167]()\n\n### Model Training Configuration\n\nThe system allows customization of AI model training parameters through environment variables to control resource usage and model quality:\n\n```mermaid\nflowchart TD\n    subgraph \"Training Configuration Parameters\"\n        EPOCHS[\"Epoch Limits\"]\n        BATCH[\"Batch Size Limits\"]\n        WORKSPACE[\"Training Workspace\"]\n        BASE_MODEL[\"Base Model Selection\"]\n    end\n\n    subgraph \"Environment Variables\"\n        RAMP_EPOCHS[\"RAMP_EPOCHS_LIMIT\"]\n        YOLO_EPOCHS[\"YOLO_EPOCHS_LIMIT\"]\n        RAMP_BATCH[\"RAMP_BATCH_SIZE_LIMIT\"]\n        YOLO_BATCH[\"YOLO_BATCH_SIZE_LIMIT\"]\n        TRAINING_WORKSPACE_VAR[\"TRAINING_WORKSPACE\"]\n        RAMP_HOME[\"RAMP_HOME\"]\n        YOLO_HOME[\"YOLO_HOME\"]\n    end\n\n    subgraph \"Training Process\"\n        VALIDATOR[\"Training Validator\"]\n        TASK[\"train_model Task\"]\n        TRAINER[\"Trainer Class\"]\n    end\n\n    RAMP_EPOCHS --\u003e EPOCHS\n    YOLO_EPOCHS --\u003e EPOCHS\n    RAMP_BATCH --\u003e BATCH\n    YOLO_BATCH --\u003e BATCH\n    TRAINING_WORKSPACE_VAR --\u003e WORKSPACE\n    RAMP_HOME --\u003e BASE_MODEL\n    YOLO_HOME --\u003e BASE_MODEL\n\n    EPOCHS --\u003e VALIDATOR\n    BATCH --\u003e VALIDATOR\n    WORKSPACE --\u003e TASK\n    BASE_MODEL --\u003e TRAINER\n\n    VALIDATOR --\u003e TASK\n    TASK --\u003e TRAINER\n```\n\nThe system enforces limits on training parameters to prevent resource exhaustion:\n\n```python\n# In TrainingSerializer.create method\nif model.base_model == \"RAMP\":\n    if epochs \u003e settings.RAMP_EPOCHS_LIMIT:\n        raise ValidationError(\n            f\"Epochs can't be greater than {settings.RAMP_EPOCHS_LIMIT} on this server\"\n        )\n    if batch_size \u003e settings.RAMP_BATCH_SIZE_LIMIT:\n        raise ValidationError(\n            f\"Batch size can't be greater than {settings.RAMP_BATCH_SIZE_LIMIT} on this server\"\n        )\n```\n\nSources: [backend/aiproject/settings.py:70-85](), [backend/core/views.py:167-186](), [backend/core/tasks.py:380-467]()\n\n### S3 Storage Configuration\n\nThe system uses AWS S3 for storing model artifacts. This is configured through these environment variables:\n\n```python\n# S3\nBUCKET_NAME = env(\"BUCKET_NAME\", default=\"fair-dev\")\nPARENT_BUCKET_FOLDER = env(\n    \"PARENT_BUCKET_FOLDER\", default=\"dev\"\n)  # use prod for production\nAWS_REGION = env(\"AWS_REGION\", default=\"us-east-1\")\nAWS_ACCESS_KEY_ID = env(\"AWS_ACCESS_KEY_ID\", default=None)\nAWS_SECRET_ACCESS_KEY = env(\"AWS_SECRET_ACCESS_KEY\", default=None)\nPRESIGNED_URL_EXPIRY = env(\"PRESIGNED_URL_EXPIRY\", default=3600)\n```\n\nThe S3Uploader utility in `core/utils.py` handles uploading model artifacts to S3 with the configured credentials.\n\nSources: [backend/aiproject/settings.py:59-67](), [backend/core/utils.py:397-460]()\n\n### OpenStreetMap Integration\n\nfAIr integrates with OpenStreetMap for authentication and data import. Configuration options include:\n\n```python\nOSM_CLIENT_ID = env(\"OSM_CLIENT_ID\")\nOSM_CLIENT_SECRET = env(\"OSM_CLIENT_SECRET\")\nOSM_URL = env(\"OSM_URL\", default=\"https://www.openstreetmap.org\")\nOSM_SCOPE = env(\"OSM_SCOPE\", default=\"read_prefs\")\nOSM_LOGIN_REDIRECT_URI = env(\n    \"OSM_LOGIN_REDIRECT_URI\", default=\"http://127.0.0.1:8000/api/v1/auth/callback/\"\n)\nOSM_SECRET_KEY = env(\"OSM_SECRET_KEY\")\n```\n\nSources: [backend/aiproject/settings.py:49-56]()\n\n### Email Notification Configuration\n\nThe system can send email notifications for training events through SMTP:\n\n```python\nEMAIL_BACKEND = \"django.core.mail.backends.smtp.EmailBackend\"\nEMAIL_HOST = os.getenv(\"EMAIL_HOST\", \"smtp.gmail.com\")\nEMAIL_PORT = int(os.getenv(\"EMAIL_PORT\", 587))\nEMAIL_USE_TLS = os.getenv(\"EMAIL_USE_TLS\", \"True\") == \"True\"\nEMAIL_USE_SSL = os.getenv(\"EMAIL_USE_SSL\", \"False\") == \"True\"\nEMAIL_HOST_USER = os.getenv(\"EMAIL_HOST_USER\", \"example-email@example.com\")\nEMAIL_HOST_PASSWORD = os.getenv(\"EMAIL_HOST_PASSWORD\", \"example-email-password\")\nDEFAULT_FROM_EMAIL = os.getenv(\"DEFAULT_FROM_EMAIL\", \"no-reply@example.com\")\n```\n\nSources: [backend/aiproject/settings.py:263-271](), [backend/core/utils.py:463-487]()\n\n## Frontend Configuration\n\n### API Endpoints Configuration\n\nThe frontend configures API endpoints through environment variables, which are then processed through helper functions:\n\n```typescript\n/**\n * Helper function to safely parse environment variables as strings.\n */\nexport const parseStringEnv = (\n  value: string | undefined,\n  defaultValue: string,\n): string =\u003e (value \u0026\u0026 value.trim() !== \"\" ? value.trim() : defaultValue);\n\n/**\n * The backend api endpoint url.\n * Note: Ensure CORs is enabled in the backend and access is given to your port.\n */\nexport const BASE_API_URL: string = parseStringEnv(\n  ENVS.BASE_API_URL,\n  \"http://localhost:8000/api/v1/\",\n);\n```\n\nThis pattern is used for all API endpoints including OpenAerialMap, JOSM, and the fAIr predictor API.\n\nSources: [frontend/src/config/index.ts:33-90]()\n\n### Map Visualization Configuration\n\nThe map visualization can be customized through various environment variables that control appearance and behavior:\n\n```typescript\n/**\n * The maximum zoom level for the map.\n * Model predictions require a max zoom of 22.\n * 21 is used here because 1 is already added to the 'currentZoom' in the useMapInstance() hook.\n */\nexport const MAX_ZOOM_LEVEL: number = parseIntEnv(ENVS.MAX_ZOOM_LEVEL, 21);\n\n/**\n * The minimum zoom level for the map before the prediction components can be activated.\n */\nexport const MIN_ZOOM_LEVEL_FOR_START_MAPPING_PREDICTION: number = parseIntEnv(\n  ENVS.MIN_ZOOM_LEVEL_FOR_START_MAPPING_PREDICTION,\n  19,\n);\n```\n\nMap colors and styling are also configurable:\n\n```typescript\nexport const TRAINING_AREAS_AOI_FILL_COLOR: string = parseStringEnv(\n  ENVS.TRAINING_AREAS_AOI_FILL_COLOR,\n  \"#247DCACC\",\n);\n```\n\nSources: [frontend/src/config/index.ts:196-323]()\n\n### Model Prediction Configuration\n\nThe frontend allows configuration of model prediction parameters:\n\n```typescript\n/**\n * The maximum tolerance for a prediction to be considered valid in the start mapping page.\n */\nexport const MAXIMUM_PREDICTION_TOLERANCE: number = parseIntEnv(\n  ENVS.MAXIMUM_PREDICTION_TOLERANCE,\n  10,\n);\n\n/**\n * The maximum area for a prediction to be considered valid in the start mapping page.\n */\nexport const MAXIMUM_PREDICTION_AREA: number = parseIntEnv(\n  ENVS.MAXIMUM_PREDICTION_AREA,\n  20,\n);\n```\n\nThese settings impact the quality and filtering of model predictions when users are mapping.\n\nSources: [frontend/src/config/index.ts:448-458]()\n\n## UI Content Customization\n\nThe fAIr system has a sophisticated UI content system that allows customization of text, labels, and instructions throughout the application without modifying code. This is implemented through TypeScript constants that are strongly typed for reliability.\n\n```mermaid\nflowchart TD\n    subgraph \"UI Content System\"\n        subgraph \"Content Definition Files\"\n            MODELS_CONTENT[\"models-content.ts\"]\n            START_MAPPING_CONTENT[\"start-mapping-content.ts\"]\n            SHARED_CONTENT[\"shared-content.ts\"]\n        end\n\n        subgraph \"Types\"\n            CONTENT_TYPES[\"ui-contents.ts Types\"]\n        end\n\n        subgraph \"Components\"\n            MODEL_CREATION[\"Model Creation Components\"]\n            START_MAPPING[\"Start Mapping Components\"]\n            SHARED_COMPONENTS[\"Shared Components\"]\n        end\n    end\n\n    CONTENT_TYPES --\u003e MODELS_CONTENT\n    CONTENT_TYPES --\u003e START_MAPPING_CONTENT\n    CONTENT_TYPES --\u003e SHARED_CONTENT\n\n    MODELS_CONTENT --\u003e MODEL_CREATION\n    START_MAPPING_CONTENT --\u003e START_MAPPING\n    SHARED_CONTENT --\u003e SHARED_COMPONENTS\n```\n\nSources: [frontend/src/constants/ui-contents/models-content.ts:1-334](), [frontend/src/constants/ui-contents/start-mapping-content.ts:1-105](), [frontend/src/types/ui-contents.ts:1-456]()\n\n### Customizing UI Text\n\nTo customize text in the application:\n\n1. Locate the relevant content file in `frontend/src/constants/ui-contents/`\n2. Modify the text values while preserving the structure\n3. Rebuild the frontend application\n\nFor example, to change the text for the model creation process, modify `models-content.ts`:\n\n```typescript\nexport const MODELS_CONTENT: TModelsContent = {\n  trainingArea: {\n    // The retry button when the training area map fails to load\n    retryButton: \"retry\", // Customizable text\n    modalTitle: \"Training Area\", // Customizable text\n    map: {\n      loadingText: \"Loading map...\", // Customizable text\n    },\n  },\n  // ...more content\n}\n```\n\nThe types in `ui-contents.ts` ensure that all required text fields are present.\n\nSources: [frontend/src/constants/ui-contents/models-content.ts:1-334](), [frontend/src/types/ui-contents.ts:1-305]()\n\n### UI Structure and Configuration Points\n\nThe frontend UI has several key configuration points that affect the user experience:\n\n```mermaid\nclassDiagram\n    class UIConfigurationPoints {\n        +ModelCreation\n        +StartMapping\n        +FormValidation\n        +MapVisuals\n    }\n    \n    class ModelCreation {\n        +pageTitle: string\n        +formLabels: object\n        +helpText: string\n        +validationMessages: string\n    }\n    \n    class StartMapping {\n        +pageTitle: function\n        +buttonLabels: object\n        +tooltips: object\n        +messages: object\n    }\n    \n    class FormValidation {\n        +minLength: number\n        +maxLength: number\n        +pattern: regex\n    }\n    \n    class MapVisuals {\n        +colors: object\n        +opacities: number\n        +lineWidths: number\n    }\n    \n    UIConfigurationPoints --\u003e ModelCreation\n    UIConfigurationPoints --\u003e StartMapping\n    UIConfigurationPoints --\u003e FormValidation\n    UIConfigurationPoints --\u003e MapVisuals\n```\n\nEach component in the UI typically loads text from the content constants:\n\n```typescript\n// Example from training dataset component\n\u003cInput\n  handleInput={(e) =\u003e\n    handleChange(MODEL_CREATION_FORM_NAME.DATASET_NAME, e.target.value)\n  }\n  value={formData.datasetName}\n  toolTipContent={\n    MODELS_CONTENT.modelCreation.trainingDataset.form.datasetName\n      .toolTip\n  }\n  label={\n    MODELS_CONTENT.modelCreation.trainingDataset.form.datasetName.label\n  }\n  labelWithTooltip\n  placeholder={\n    MODELS_CONTENT.modelCreation.trainingDataset.form.datasetName\n      .placeholder\n  }\n/\u003e\n```\n\nThis approach allows complete customization of the application text while maintaining a consistent user experience.\n\nSources: [frontend/src/features/model-creation/components/training-dataset/create-new.tsx:91-122](), [frontend/src/components/ui/form/help-text/help-text.tsx:1-25]()\n\n## Practical Examples\n\n### Changing Model Training Limits\n\nTo adjust the maximum epochs and batch size for model training:\n\n1. Set environment variables in backend `.env` file:\n   ```\n   RAMP_EPOCHS_LIMIT=60\n   RAMP_BATCH_SIZE_LIMIT=16\n   YOLO_EPOCHS_LIMIT=300\n   YOLO_BATCH_SIZE_LIMIT=12\n   ```\n\n2. Restart the backend application to apply changes\n\n### Customizing Frontend Map Colors\n\nTo change the appearance of training areas on the map:\n\n1. Set environment variables in frontend `.env` file:\n   ```\n   VITE_TRAINING_AREAS_AOI_FILL_COLOR=\"#336699CC\"\n   VITE_TRAINING_AREAS_AOI_OUTLINE_COLOR=\"#336699CC\"\n   VITE_TRAINING_AREAS_AOI_FILL_OPACITY=0.5\n   ```\n\n2. Rebuild the frontend application\n\n### Changing UI Content\n\nTo update instructional text for the model creation workflow:\n\n1. Modify `frontend/src/constants/ui-contents/models-content.ts`:\n   ```typescript\n   modelCreation: {\n     modelDetails: {\n       pageTitle: \"Create Custom AI Model\",\n       form: {\n         modelName: {\n           label: \"Model Name\",\n           helpText: \"Enter a descriptive name for your model (10-40 characters).\",\n           // ...\n         },\n       },\n       // ...\n     },\n     // ...\n   }\n   ```\n\n2. Rebuild the frontend application\n\nSources: [frontend/src/constants/ui-contents/models-content.ts:22-59]()\n\n## Conclusion\n\nThe fAIr system offers extensive configuration options through environment variables and UI content customization. This allows deployers to adapt the system to different environments, branding requirements, and use cases without modifying source code. Backend configuration focuses on system operations and model training parameters, while frontend configuration enables customization of user interface appearance, behavior, and content."])</script><script>self.__next_f.push([1,"2a:T41b4,"])</script><script>self.__next_f.push([1,"# Environment Variables\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.github/workflows/backend_build.yml](.github/workflows/backend_build.yml)\n- [backend/README.md](backend/README.md)\n- [backend/aiproject/settings.py](backend/aiproject/settings.py)\n- [backend/aiproject/urls.py](backend/aiproject/urls.py)\n- [backend/api-requirements.txt](backend/api-requirements.txt)\n- [backend/core/models.py](backend/core/models.py)\n- [backend/core/serializers.py](backend/core/serializers.py)\n- [backend/core/tasks.py](backend/core/tasks.py)\n- [backend/core/urls.py](backend/core/urls.py)\n- [backend/core/utils.py](backend/core/utils.py)\n- [backend/core/views.py](backend/core/views.py)\n- [backend/docker_sample_env](backend/docker_sample_env)\n- [backend/requirements.txt](backend/requirements.txt)\n- [backend/sample_env](backend/sample_env)\n- [backend/tests/__init__.py](backend/tests/__init__.py)\n- [backend/tests/test_endpoints.py](backend/tests/test_endpoints.py)\n- [backend/tests/test_views.py](backend/tests/test_views.py)\n- [frontend/.env.sample](frontend/.env.sample)\n- [frontend/src/config/env.ts](frontend/src/config/env.ts)\n- [frontend/src/config/index.ts](frontend/src/config/index.ts)\n\n\u003c/details\u003e\n\n\n\nThis document provides a comprehensive guide to environment variables in the fAIr system. Environment variables control configuration settings across both backend and frontend components, allowing for customization without code changes. This page focuses specifically on the environment variables themselves and how they're used throughout the system.\n\nFor information about the overall system architecture, please refer to [System Architecture](#1.1).\n\n## Environment Variable Loading Flow\n\nThe fAIr system handles environment variables differently in the backend versus the frontend. The following diagram illustrates how environment variables are loaded and used across the system:\n\n```mermaid\ngraph TD\n    subgraph \"Backend (Django)\"\n        EnvFile[\".env File\"] --\u003e|\"environ.Env.read_env()\"| EnvObject[\"Environment Object (env)\"]\n        EnvObject --\u003e|\"env('VARIABLE', default=value)\"| SettingsModule[\"Settings (settings.py)\"]\n        SettingsModule --\u003e BackendComponents[\"Django Components\"]\n        BackendComponents --\u003e API[\"API Endpoints\"]\n        BackendComponents --\u003e CeleryWorker[\"Celery Workers\"]\n        BackendComponents --\u003e Models[\"ML Models (RAMP/YOLO)\"]\n    end\n    \n    subgraph \"Frontend (React)\"\n        FrontendEnvFile[\".env File\"] --\u003e|\"import.meta.env\"| FrontendConfig[\"Config (env.ts)\"]\n        FrontendConfig --\u003e|\"parseEnv* functions\"| FrontendConstants[\"Constants (index.ts)\"]\n        FrontendConstants --\u003e FrontendComponents[\"React Components\"]\n    end\n    \n    API \u003c--\u003e FrontendComponents\n```\n\nSources: [backend/aiproject/settings.py:22-28](), [frontend/src/config/env.ts:1-92](), [frontend/src/config/index.ts:9-37]()\n\n### Backend Environment Variables\n\nIn the backend, environment variables are loaded using the `django-environ` package from a `.env` file in the project root. The Django settings module reads these variables and applies them to various configurations:\n\n```python\n# From backend/aiproject/settings.py\nimport environ\nenv = environ.Env()\nenviron.Env.read_env(os.path.join(BASE_DIR, \".env\"))\nDEBUG = env(\"DEBUG\", default=False)  # Example of loading with default value\n```\n\n### Frontend Environment Variables\n\nIn the frontend, environment variables are accessed using Vite's `import.meta.env`. These variables must be prefixed with `VITE_` to be accessible in the frontend code:\n\n```typescript\n// From frontend/src/config/env.ts\nexport const ENVS = {\n  BASE_API_URL: import.meta.env.VITE_BASE_API_URL,\n  // Additional environment variables\n}\n```\n\nThe frontend also includes helper functions to safely parse environment variables with default values:\n\n```typescript\n// From frontend/src/config/index.ts\nexport const parseIntEnv = (\n  value: string | undefined,\n  defaultValue: number,\n): number =\u003e\n  value !== undefined \u0026\u0026 !isNaN(parseInt(value, 10))\n    ? parseInt(value, 10)\n    : defaultValue;\n```\n\n## Environment Variable Categories\n\nThe following diagram shows how different categories of environment variables relate to system components:\n\n```mermaid\ngraph TD\n    subgraph \"Environment Variable Categories\"\n        CoreSettings[\"Django Core Settings\n        (DEBUG, SECRET_KEY, etc.)\"]\n        \n        OsmSettings[\"OSM Integration\n        (OSM_CLIENT_ID, OSM_SECRET_KEY, etc.)\"]\n        \n        S3Settings[\"AWS S3 Configuration\n        (BUCKET_NAME, AWS_REGION, etc.)\"]\n        \n        TrainingSettings[\"Model Training\n        (EPOCHS_LIMIT, RAMP_HOME, etc.)\"]\n        \n        CelerySettings[\"Celery and Tasks\n        (CELERY_BROKER_URL, LOG_PATH, etc.)\"]\n        \n        EmailSettings[\"Email Configuration\n        (EMAIL_HOST, EMAIL_PORT, etc.)\"]\n        \n        FrontendSettings[\"Frontend Settings\n        (VITE_BASE_API_URL, VITE_MAX_ZOOM_LEVEL, etc.)\"]\n    end\n    \n    subgraph \"System Components\"\n        DjangoCore[\"Django Core\"]\n        AuthSystem[\"Authentication\"]\n        StorageSystem[\"S3 Storage\"]\n        ModelTraining[\"Model Training\"]\n        TaskQueue[\"Celery Task Queue\"]\n        NotificationSystem[\"Notifications\"]\n        FrontendApp[\"React Frontend\"]\n    end\n    \n    CoreSettings --\u003e DjangoCore\n    OsmSettings --\u003e AuthSystem\n    S3Settings --\u003e StorageSystem\n    TrainingSettings --\u003e ModelTraining\n    CelerySettings --\u003e TaskQueue\n    EmailSettings --\u003e NotificationSystem\n    FrontendSettings --\u003e FrontendApp\n```\n\n## Core Django Settings\n\nThese environment variables configure the core Django settings:\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `DEBUG` | Enables/disables debug mode | `False` | `True` |\n| `SECRET_KEY` | Secret key for cryptographic signing | `\"default_secret_key\"` | `\"your-secret-key\"` |\n| `HOSTNAME` | Hostname where the application is running | `\"127.0.0.1\"` | `\"fair.hotosm.org\"` |\n| `FRONTEND_URL` | URL of the frontend application | `\"https://fair.hotosm.org\"` | `\"http://localhost:3000\"` |\n| `DATABASE_URL` | Database connection URL | `\"postgis://admin:password@localhost:5432/ai\"` | `\"postgis://user:pass@host:port/dbname\"` |\n| `DEFAULT_PAGINATION_SIZE` | Default number of items per page in API responses | `50` | `100` |\n\nSources: [backend/aiproject/settings.py:29-38](), [backend/aiproject/settings.py:122-133]()\n\n## OpenStreetMap (OSM) Integration\n\nThese environment variables configure OSM authentication and integration:\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `OSM_CLIENT_ID` | OAuth2 client ID for OSM authentication | Required | `\"your-client-id\"` |\n| `OSM_CLIENT_SECRET` | OAuth2 client secret for OSM authentication | Required | `\"your-client-secret\"` |\n| `OSM_URL` | Base URL for OSM API | `\"https://www.openstreetmap.org\"` | `\"https://www.openstreetmap.org\"` |\n| `OSM_SCOPE` | OAuth2 scope for OSM authentication | `\"read_prefs\"` | `\"read_prefs\"` |\n| `OSM_LOGIN_REDIRECT_URI` | Redirect URI for OSM authentication | `\"http://127.0.0.1:8000/api/v1/auth/callback/\"` | `\"https://your-app.com/api/v1/auth/callback/\"` |\n| `OSM_SECRET_KEY` | Secret key for OSM authentication | Required | `\"your-secret-key\"` |\n| `EXPORT_TOOL_API_URL` | URL for the OSM export tool API | `\"https://api-prod.raw-data.hotosm.org/v1\"` | `\"https://api-prod.raw-data.hotosm.org/v1\"` |\n\nSources: [backend/aiproject/settings.py:49-56](), [backend/core/views.py:85-86]()\n\n## AWS S3 Configuration\n\nThese environment variables configure AWS S3 storage for model artifacts:\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `BUCKET_NAME` | Name of the S3 bucket | `\"fair-dev\"` | `\"your-bucket-name\"` |\n| `PARENT_BUCKET_FOLDER` | Parent folder in the S3 bucket | `\"dev\"` | `\"prod\"` |\n| `AWS_REGION` | AWS region for the S3 bucket | `\"us-east-1\"` | `\"us-west-2\"` |\n| `AWS_ACCESS_KEY_ID` | AWS access key ID | None | `\"your-access-key-id\"` |\n| `AWS_SECRET_ACCESS_KEY` | AWS secret access key | None | `\"your-secret-access-key\"` |\n| `PRESIGNED_URL_EXPIRY` | Expiry time (in seconds) for S3 presigned URLs | `3600` | `86400` |\n\nSources: [backend/aiproject/settings.py:60-67](), [backend/core/utils.py:34-44](), [backend/core/utils.py:398-420]()\n\n## Model Training Configuration\n\nThese environment variables configure model training parameters and limits:\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `EPOCHS_LIMIT` | Maximum number of epochs for training | `20` | `30` |\n| `BATCH_SIZE_LIMIT` | Maximum batch size for training | `8` | `16` |\n| `YOLO_EPOCHS_LIMIT` | Maximum number of epochs for YOLO training | `200` | `300` |\n| `YOLO_BATCH_SIZE_LIMIT` | Maximum batch size for YOLO training | `8` | `16` |\n| `RAMP_EPOCHS_LIMIT` | Maximum number of epochs for RAMP training | `40` | `50` |\n| `RAMP_BATCH_SIZE_LIMIT` | Maximum batch size for RAMP training | `8` | `16` |\n| `TRAINING_WORKSPACE_DOWNLOAD_LIMIT` | Download limit for training workspace | `200` | `500` |\n| `RAMP_HOME` | Path to RAMP home directory | Required | `\"/path/to/ramp\"` |\n| `YOLO_HOME` | Path to YOLO home directory | `os.getcwd()` | `\"/path/to/yolo\"` |\n| `TRAINING_WORKSPACE` | Path to training workspace | `os.path.join(os.getcwd(), \"training\")` | `\"/path/to/training\"` |\n| `ENABLE_PREDICTION_API` | Enables/disables prediction API | `False` | `True` |\n\nSources: [backend/aiproject/settings.py:71-86](), [backend/aiproject/settings.py:243-254](), [backend/core/views.py:166-186](), [backend/core/tasks.py:381-464]()\n\n## Celery and Task Queue Configuration\n\nThese environment variables configure Celery and task queues for asynchronous processing:\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `CELERY_BROKER_URL` | URL for Celery broker | `\"redis://127.0.0.1:6379/0\"` | `\"redis://redis:6379/0\"` |\n| `CELERY_RESULT_BACKEND` | URL for Celery result backend | `\"redis://127.0.0.1:6379/0\"` | `\"redis://redis:6379/0\"` |\n| `LOG_PATH` | Path for log files | `os.path.join(os.getcwd(), \"log\")` | `\"/var/log/fair\"` |\n| `LOG_LINE_STREAM_TRUNCATE_VALUE` | Number of log lines to display in API responses | `10` | `20` |\n\nSources: [backend/aiproject/settings.py:217-232](), [backend/aiproject/settings.py:257](), [backend/core/tasks.py:43-82](), [backend/core/views.py:696]()\n\n## Email Configuration\n\nThese environment variables configure email notifications:\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `EMAIL_HOST` | SMTP host for sending emails | `\"smtp.gmail.com\"` | `\"smtp.mailgun.org\"` |\n| `EMAIL_PORT` | SMTP port for sending emails | `587` | `465` |\n| `EMAIL_USE_TLS` | Uses TLS for SMTP connection | `True` | `False` |\n| `EMAIL_USE_SSL` | Uses SSL for SMTP connection | `False` | `True` |\n| `EMAIL_HOST_USER` | Username for SMTP authentication | `\"example-email@example.com\"` | `\"your-email@gmail.com\"` |\n| `EMAIL_HOST_PASSWORD` | Password for SMTP authentication | `\"example-email-password\"` | `\"your-password\"` |\n| `DEFAULT_FROM_EMAIL` | Default sender email address | `\"no-reply@example.com\"` | `\"no-reply@your-domain.com\"` |\n| `FAIL_EMAIL_SILENTLY` | Silently fails when email sending fails | `\"True\"` | `\"False\"` |\n\nSources: [backend/aiproject/settings.py:263-271](), [backend/core/utils.py:463-510]()\n\n## CORS Configuration\n\nThese environment variables configure Cross-Origin Resource Sharing (CORS):\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `CORS_ORIGIN_ALLOW_ALL` | Allows all origins (in debug mode only) | `True` (in debug mode), `False` (in production) | `True` |\n| `CORS_ALLOWED_ORIGINS` | Comma-separated list of allowed origins | `\"http://127.0.0.1:8000\"` | `\"https://app.example.com,https://admin.example.com\"` |\n\nSources: [backend/aiproject/settings.py:276-302]()\n\n## Frontend Environment Variables\n\nThe frontend uses Vite environment variables, which must be prefixed with `VITE_`. Here are some key frontend environment variables:\n\n| Variable | Description | Default | Example |\n|----------|-------------|---------|---------|\n| `VITE_BASE_API_URL` | URL of the backend API | `\"http://localhost:8000/api/v1/\"` | `\"https://api.fair.hotosm.org/api/v1/\"` |\n| `VITE_MATOMO_ID` | Matomo analytics ID | `\"0\"` | `\"123\"` |\n| `VITE_MATOMO_APP_DOMAIN` | Matomo application domain | `\"fair.hotosm.org\"` | `\"your-domain.com\"` |\n| `VITE_MAX_TRAINING_AREA_SIZE` | Maximum size of training areas (sq meters) | `5000000` | `10000000` |\n| `VITE_MIN_TRAINING_AREA_SIZE` | Minimum size of training areas (sq meters) | `5797` | `1000` |\n| `VITE_MAX_ZOOM_LEVEL` | Maximum zoom level for the map | `21` | `22` |\n| `VITE_MIN_ZOOM_LEVEL_FOR_START_MAPPING_PREDICTION` | Minimum zoom level for prediction | `19` | `18` |\n| `VITE_FAIR_PREDICTOR_API_URL` | URL of the fAIr predictor API | `\"https://predictor-dev.fair.hotosm.org/predict/\"` | `\"https://predictor.fair.hotosm.org/predict/\"` |\n\nSources: [frontend/src/config/env.ts:1-92](), [frontend/src/config/index.ts:47-468](), [frontend/.env.sample:1-212]()\n\n## Environment Variables in the Model Training Process\n\nThe following diagram illustrates how environment variables are used in the model training process, a key workflow in the fAIr system:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant CeleryWorker\n    participant ModelTraining\n    participant S3Storage\n\n    User-\u003e\u003eFrontend: Create model \u0026 training\n    Frontend-\u003e\u003eBackend: POST /api/v1/training/\n    Note over Backend: Uses EPOCHS_LIMIT, BATCH_SIZE_LIMIT\n    Backend-\u003e\u003eCeleryWorker: Enqueue train_model task\n    Note over CeleryWorker: Uses CELERY_BROKER_URL\n    CeleryWorker-\u003e\u003eModelTraining: Run training\n    Note over ModelTraining: Uses RAMP_HOME, YOLO_HOME, TRAINING_WORKSPACE\n    ModelTraining-\u003e\u003eS3Storage: Upload model artifacts\n    Note over S3Storage: Uses BUCKET_NAME, AWS_ACCESS_KEY_ID\n    S3Storage--\u003e\u003eModelTraining: Upload success\n    ModelTraining--\u003e\u003eCeleryWorker: Training complete\n    CeleryWorker--\u003e\u003eBackend: Update training status\n    Backend-\u003e\u003eNotificationSystem: Send notification\n    Note over NotificationSystem: Uses EMAIL_* variables\n    Backend--\u003e\u003eFrontend: Return training status\n```\n\nSources: [backend/core/tasks.py:380-467](), [backend/core/utils.py:397-460](), [backend/core/views.py:124-232]()\n\n## Environment File Templates\n\n### Backend `.env` Template\n\n```\nDEBUG=True\nSECRET_KEY=your-secret-key\nDATABASE_URL=postgis://admin:password@localhost:5432/ai\nEXPORT_TOOL_API_URL=https://api-prod.raw-data.hotosm.org/v1\nCORS_ALLOWED_ORIGINS=http://localhost:3000\nHOSTNAME=\nOSM_CLIENT_ID=your-osm-client-id\nOSM_CLIENT_SECRET=your-osm-client-secret\nOSM_URL=https://www.openstreetmap.org\nOSM_SCOPE=read_prefs\nOSM_LOGIN_REDIRECT_URI=http://127.0.0.1:8000/api/v1/auth/callback/\nOSM_SECRET_KEY=your-osm-secret-key\nCELERY_BROKER_URL=redis://localhost:6379/0\nCELERY_RESULT_BACKEND=redis://localhost:6379/0\nRAMP_HOME=/path/to/ramp\nTRAINING_WORKSPACE=/path/to/training\nLOG_PATH=/path/to/log\nBUCKET_NAME=fair-dev\nPARENT_BUCKET_FOLDER=dev\nAWS_REGION=us-east-1\nAWS_ACCESS_KEY_ID=your-aws-access-key-id\nAWS_SECRET_ACCESS_KEY=your-aws-secret-access-key\nENABLE_PREDICTION_API=False\n```\n\nSources: [backend/sample_env:1-22]()\n\n### Frontend `.env` Template\n\n```\nVITE_BASE_API_URL=http://localhost:8000/api/v1/\nVITE_MATOMO_ID=0\nVITE_MATOMO_APP_DOMAIN=fair.hotosm.org\nVITE_MAX_TRAINING_AREA_SIZE=5000000\nVITE_MIN_TRAINING_AREA_SIZE=5797\nVITE_MAX_ZOOM_LEVEL=21\nVITE_MIN_ZOOM_LEVEL_FOR_START_MAPPING_PREDICTION=19\nVITE_FAIR_PREDICTOR_API_URL=https://predictor-dev.fair.hotosm.org/predict/\nVITE_OSM_DATABASE_STATUS_API_URL=https://api-prod.raw-data.hotosm.org/v1/status/\nVITE_OAM_TITILER_ENDPOINT=https://titiler.hotosm.org/\nVITE_OAM_S3_BUCKET_URL=https://oin-hotosm-temp.s3.us-east-1.amazonaws.com/\n```\n\nSources: [frontend/.env.sample:1-212]()\n\n## Importance of Environment Variables\n\nEnvironment variables are crucial for the fAIr system for several reasons:\n\n1. **Configuration without code changes**: System behavior can be modified without modifying the codebase\n2. **Environment-specific settings**: Different settings can be used across development, testing, and production environments\n3. **Security**: Sensitive information like API keys and passwords are kept out of the codebase\n4. **Deployment flexibility**: The system can be deployed in different environments with appropriate configurations\n5. **Feature toggling**: Features like the prediction API can be enabled or disabled using environment variables\n\nProper configuration of environment variables is essential for the correct functioning of the fAIr system across all environments."])</script><script>self.__next_f.push([1,"2b:T30bd,"])</script><script>self.__next_f.push([1,"# UI Content Configuration\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [frontend/.gitignore](frontend/.gitignore)\n- [frontend/README.md](frontend/README.md)\n- [frontend/eslint.config.js](frontend/eslint.config.js)\n- [frontend/index.html](frontend/index.html)\n- [frontend/src/components/ui/animated-beam/animated-beam.tsx](frontend/src/components/ui/animated-beam/animated-beam.tsx)\n- [frontend/src/components/ui/banner/banner.tsx](frontend/src/components/ui/banner/banner.tsx)\n- [frontend/src/components/ui/form/help-text/help-text.tsx](frontend/src/components/ui/form/help-text/help-text.tsx)\n- [frontend/src/constants/ui-contents/models-content.ts](frontend/src/constants/ui-contents/models-content.ts)\n- [frontend/src/constants/ui-contents/start-mapping-content.ts](frontend/src/constants/ui-contents/start-mapping-content.ts)\n- [frontend/src/features/model-creation/components/progress-bar.tsx](frontend/src/features/model-creation/components/progress-bar.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/create-new.tsx](frontend/src/features/model-creation/components/training-dataset/create-new.tsx)\n- [frontend/src/features/model-creation/components/training-dataset/select-existing.tsx](frontend/src/features/model-creation/components/training-dataset/select-existing.tsx)\n- [frontend/src/features/start-mapping/components/map/legend-control.tsx](frontend/src/features/start-mapping/components/map/legend-control.tsx)\n- [frontend/src/styles/index.css](frontend/src/styles/index.css)\n- [frontend/src/types/ui-contents.ts](frontend/src/types/ui-contents.ts)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains how to customize text content, labels, tooltips, and other UI elements in the fAIr system. The UI content configuration system provides a centralized approach to managing textual content throughout the application, making it easy to modify UI text without changing component code. This document covers the structure of content configuration files, how they are organized, and how to modify UI content.\n\n## UI Content Configuration Architecture\n\nThe fAIr application uses a structured approach to managing UI text content, separating it from component implementation. This architectural pattern improves maintainability and makes the system more adaptable to content changes.\n\n```mermaid\nflowchart TD\n    subgraph \"Type Definitions\"\n        TypeDefs[\"UI Content Types\\n/src/types/ui-contents.ts\"]\n    end\n\n    subgraph \"Content Definitions\"\n        ModelsContent[\"Models Content\\n/constants/ui-contents/models-content.ts\"]\n        StartMappingContent[\"Start Mapping Content\\n/constants/ui-contents/start-mapping-content.ts\"]\n        OtherContent[\"Other Content Files\\n/constants/ui-contents/*.ts\"]\n    end\n\n    subgraph \"Component Usage\"\n        Components[\"React Components\"]\n        ComponentsImport[\"Import Content Constants\"]\n        ComponentsRender[\"Render UI with Content\"]\n    end\n\n    TypeDefs --\u003e ModelsContent\n    TypeDefs --\u003e StartMappingContent\n    TypeDefs --\u003e OtherContent\n    \n    ModelsContent --\u003e ComponentsImport\n    StartMappingContent --\u003e ComponentsImport\n    OtherContent --\u003e ComponentsImport\n    \n    ComponentsImport --\u003e ComponentsRender\n    ComponentsRender --\u003e Components\n```\n\nSources:\n- [frontend/src/types/ui-contents.ts]()\n- [frontend/src/constants/ui-contents/models-content.ts]()\n- [frontend/src/constants/ui-contents/start-mapping-content.ts]()\n- [frontend/src/features/model-creation/components/training-dataset/create-new.tsx]()\n\n## Content Type System\n\nUI content is strictly typed using TypeScript interfaces. These types ensure consistency and help catch missing content during development.\n\n```mermaid\nclassDiagram\n    class TUIContentTypes {\n        TModelsContent\n        TSharedContent\n        TStartMappingPageContent\n        TAuthPageAndModalContent\n        TAboutPageContent\n        TLearnPageContent\n        TResourcesPageContent\n        TMapContent\n        TUserProfilePageContent\n    }\n    \n    class TModelsContent {\n        trainingArea\n        myModels\n        modelCreation\n        models\n    }\n    \n    class TSharedContent {\n        navbar\n        footer\n        homepage\n        pageNotFound\n        protectedPage\n        errorBoundary\n        construction\n        loginButtonLoading\n    }\n    \n    class TStartMappingPageContent {\n        pageTitle()\n        map\n        buttons\n        settings\n        mapData\n        actions\n        modelDetails\n        replicableModel\n    }\n    \n    TUIContentTypes --\u003e TModelsContent\n    TUIContentTypes --\u003e TSharedContent\n    TUIContentTypes --\u003e TStartMappingPageContent\n```\n\nSources:\n- [frontend/src/types/ui-contents.ts:1-776]()\n\n## Content Files Structure\n\nContent files are organized by feature area and exported as constants that conform to the type definitions.\n\n### Models Content Structure\n\nThe models content is one of the larger content modules, containing text for multiple sub-features related to model creation, management, and usage.\n\n```mermaid\nflowchart TD\n    MODELS_CONTENT[\"MODELS_CONTENT\"]\n    \n    MODELS_CONTENT --\u003e trainingArea[\"trainingArea\"]\n    MODELS_CONTENT --\u003e myModels[\"myModels\"]\n    MODELS_CONTENT --\u003e modelCreation[\"modelCreation\"]\n    MODELS_CONTENT --\u003e models[\"models\"]\n    \n    modelCreation --\u003e modelDetails[\"modelDetails\"]\n    modelCreation --\u003e trainingDataset[\"trainingDataset\"]\n    modelCreation --\u003e trainingSettings[\"trainingSettings\"]\n    modelCreation --\u003e confirmation[\"confirmation\"]\n    modelCreation --\u003e progressStepper[\"progressStepper\"]\n    \n    models --\u003e modelsList[\"modelsList\"]\n    models --\u003e modelsDetailsCard[\"modelsDetailsCard\"]\n    \n    modelsDetailsCard --\u003e properties[\"properties\"]\n    modelsDetailsCard --\u003e trainingHistoryTableHeader[\"trainingHistoryTableHeader\"]\n    modelsDetailsCard --\u003e modelEnhancement[\"modelEnhancement\"]\n```\n\nSources:\n- [frontend/src/constants/ui-contents/models-content.ts:1-349]()\n- [frontend/src/types/ui-contents.ts:4-305]()\n\n### Start Mapping Content Structure\n\nThe start mapping content provides text for the AI-assisted mapping interface where users interact with model predictions.\n\n```mermaid\nflowchart TD\n    START_MAPPING_PAGE_CONTENT[\"START_MAPPING_PAGE_CONTENT\"]\n    \n    START_MAPPING_PAGE_CONTENT --\u003e pageTitle[\"pageTitle(modelName)\"]\n    START_MAPPING_PAGE_CONTENT --\u003e map[\"map\"]\n    START_MAPPING_PAGE_CONTENT --\u003e buttons[\"buttons\"]\n    START_MAPPING_PAGE_CONTENT --\u003e settings[\"settings\"]\n    START_MAPPING_PAGE_CONTENT --\u003e mapData[\"mapData\"]\n    START_MAPPING_PAGE_CONTENT --\u003e actions[\"actions\"]\n    START_MAPPING_PAGE_CONTENT --\u003e modelDetails[\"modelDetails\"]\n    START_MAPPING_PAGE_CONTENT --\u003e replicableModel[\"replicableModel\"]\n    \n    map --\u003e controls[\"controls\"]\n    map --\u003e popup[\"popup\"]\n    \n    controls --\u003e fitToBoundsControl[\"fitToBoundsControl\"]\n    controls --\u003e legendControl[\"legendControl\"]\n    controls --\u003e layerControl[\"layerControl\"]\n```\n\nSources:\n- [frontend/src/constants/ui-contents/start-mapping-content.ts:1-104]()\n- [frontend/src/types/ui-contents.ts:467-567]()\n\n## Using UI Content in Components\n\nUI content is imported into components from the constants directory and used for labels, tooltips, placeholders, and other text elements.\n\n### Example: Form Component Using Content\n\n```typescript\n// Example based on create-new.tsx\nimport { Input } from \"@/components/ui/form\";\nimport { MODELS_CONTENT } from \"@/constants\";\n\nconst MyFormComponent = () =\u003e {\n  return (\n    \u003cInput\n      label={MODELS_CONTENT.modelCreation.trainingDataset.form.datasetName.label}\n      toolTipContent={MODELS_CONTENT.modelCreation.trainingDataset.form.datasetName.toolTip}\n      placeholder={MODELS_CONTENT.modelCreation.trainingDataset.form.datasetName.placeholder}\n      helpText={MODELS_CONTENT.modelCreation.trainingDataset.form.datasetName.helpText}\n    /\u003e\n  );\n};\n```\n\nThe component references text content directly from the content constants, avoiding hardcoded strings.\n\nSources:\n- [frontend/src/features/model-creation/components/training-dataset/create-new.tsx:91-123]()\n\n### Example: Legend Control Using Content\n\n```typescript\n// Example based on legend-control.tsx\nimport { START_MAPPING_PAGE_CONTENT } from \"@/constants\";\n\nconst LegendControl = () =\u003e {\n  return (\n    \u003cdiv\u003e\n      \u003cp\u003e{START_MAPPING_PAGE_CONTENT.map.controls.legendControl.title}\u003c/p\u003e\n      \u003cToolTip\n        content={\n          expandLegend\n            ? START_MAPPING_PAGE_CONTENT.map.controls.legendControl.toolTip.hide\n            : START_MAPPING_PAGE_CONTENT.map.controls.legendControl.toolTip.show\n        }\n      \u003e\n        \u003cLegendBookIcon className=\"icon\" /\u003e\n      \u003c/ToolTip\u003e\n    \u003c/div\u003e\n  );\n};\n```\n\nSources:\n- [frontend/src/features/start-mapping/components/map/legend-control.tsx:47-76]()\n\n## How to Modify UI Content\n\nTo modify UI text content in the application, follow these steps:\n\n1. Identify the content file containing the text you want to modify\n2. Locate the specific text entry in the content constant\n3. Update the text value while maintaining the structure\n4. Ensure your changes maintain compatibility with the TypeScript type definitions\n\n### Example Modification\n\nIf you want to change the tooltip text for model details, you would:\n\n1. Open [frontend/src/constants/ui-contents/models-content.ts]()\n2. Locate the section `modelDetails \u003e form \u003e modelName \u003e toolTip`\n3. Update the string value\n4. Save the file\n\n## Dynamic UI Content\n\nIn addition to static content defined in constants, the application also supports dynamic UI content through:\n\n### Banner Content\n\nThe application can display announcement banners with content fetched from the backend. The banner component renders content using Markdown for rich formatting.\n\n```typescript\n// Banner component renders dynamic content from the backend\n\u003cMarkdown\n  remarkPlugins={[remarkGfm]}\n  className=\"w-[90%] text-wrap xl:text-nowrap prose\"\n\u003e\n  {data?.[0]?.message}\n\u003c/Markdown\u003e\n```\n\nSources:\n- [frontend/src/components/ui/banner/banner.tsx:14-47]()\n\n### Content Functions\n\nSome content values are functions that generate text based on parameters:\n\n```typescript\n// Function-based content\npageTitle: (modelName: string) =\u003e `Start Mapping with ${modelName}`,\n\n// Usage in a component\nconst title = START_MAPPING_PAGE_CONTENT.pageTitle(model.name);\n```\n\nSources:\n- [frontend/src/constants/ui-contents/start-mapping-content.ts:4]()\n- [frontend/src/types/ui-contents.ts:468]()\n\n## UI Styling Configuration\n\nWhile not directly related to text content, UI styling is another customizable aspect of the application through CSS variables.\n\n```css\n:root,\n:host,\n.sl-theme-light {\n  /* Color tokens */\n  --hot-fair-color-primary: #d63f40;\n  --hot-fair-color-secondary: #ffeded;\n  --hot-fair-color-dark: #2c3038;\n  \n  /* Font sizes in rem */\n  --hot-fair-font-size-extra-large: 4.25rem;\n  --hot-fair-font-size-large-title: 3rem;\n  \n  /* Font weights */\n  --hot-fair-font-weight-regular: 400;\n  --hot-fair-font-weight-medium: 500;\n}\n```\n\nTo modify styling variables, edit the [frontend/src/styles/index.css]() file.\n\nSources:\n- [frontend/src/styles/index.css:10-71]()\n\n## UI Content and Component Relationship\n\nThe following diagram illustrates how UI content flows from configuration files to rendered components:\n\n```mermaid\nsequenceDiagram\n    participant ContentFile as \"Content File\u003cbr\u003e(models-content.ts)\"\n    participant Component as \"React Component\u003cbr\u003e(create-new.tsx)\"\n    participant DOM as \"Rendered DOM\"\n    \n    Component-\u003e\u003eContentFile: Import MODELS_CONTENT\n    ContentFile--\u003e\u003eComponent: Return content object\n    Component-\u003e\u003eComponent: Use content in JSX\n    Component-\u003e\u003eDOM: Render with content\n    \n    Note over ContentFile,Component: Content is structured\u003cbr\u003eaccording to TypeScript types\n    Note over Component,DOM: Components use content for\u003cbr\u003elabels, tooltips, help text, etc.\n```\n\nSources:\n- [frontend/src/features/model-creation/components/training-dataset/create-new.tsx]()\n- [frontend/src/constants/ui-contents/models-content.ts]()\n\n## Conclusion\n\nThe UI content configuration system in fAIr provides a centralized and type-safe approach to managing textual content in the application. By separating content from component implementation, it enables easy modification of UI text without changing component code. This approach enhances maintainability and supports localization efforts.\n\nFor information about environment configuration settings, see [Environment Variables](#5.1)."])</script><script>self.__next_f.push([1,"5:[\"$\",\"$L15\",null,{\"repoName\":\"hotosm/fAIr\",\"hasConfig\":false,\"canSteer\":true,\"children\":[\"$\",\"$L16\",null,{\"wiki\":{\"metadata\":{\"repo_name\":\"hotosm/fAIr\",\"commit_hash\":\"14c64164\",\"generated_at\":\"2025-04-30T15:26:23.772646\",\"config\":null,\"config_source\":null},\"pages\":[{\"page_plan\":{\"id\":\"1\",\"title\":\"Overview\"},\"content\":\"$17\"},{\"page_plan\":{\"id\":\"1.1\",\"title\":\"System Architecture\"},\"content\":\"$18\"},{\"page_plan\":{\"id\":\"1.2\",\"title\":\"Key Concepts and Terminology\"},\"content\":\"$19\"},{\"page_plan\":{\"id\":\"1.3\",\"title\":\"Installation and Setup\"},\"content\":\"$1a\"},{\"page_plan\":{\"id\":\"2\",\"title\":\"Backend System\"},\"content\":\"$1b\"},{\"page_plan\":{\"id\":\"2.1\",\"title\":\"API Endpoints\"},\"content\":\"$1c\"},{\"page_plan\":{\"id\":\"2.2\",\"title\":\"Data Models\"},\"content\":\"$1d\"},{\"page_plan\":{\"id\":\"2.3\",\"title\":\"Asynchronous Processing\"},\"content\":\"$1e\"},{\"page_plan\":{\"id\":\"3\",\"title\":\"Frontend System\"},\"content\":\"$1f\"},{\"page_plan\":{\"id\":\"3.1\",\"title\":\"Component Structure\"},\"content\":\"$20\"},{\"page_plan\":{\"id\":\"3.2\",\"title\":\"Start Mapping Feature\"},\"content\":\"$21\"},{\"page_plan\":{\"id\":\"3.3\",\"title\":\"Model Creation and Management\"},\"content\":\"$22\"},{\"page_plan\":{\"id\":\"3.4\",\"title\":\"Authentication and User Management\"},\"content\":\"$23\"},{\"page_plan\":{\"id\":\"3.5\",\"title\":\"Map Visualization Components\"},\"content\":\"$24\"},{\"page_plan\":{\"id\":\"4\",\"title\":\"Development and Deployment\"},\"content\":\"$25\"},{\"page_plan\":{\"id\":\"4.1\",\"title\":\"Development Environment Setup\"},\"content\":\"$26\"},{\"page_plan\":{\"id\":\"4.2\",\"title\":\"CI/CD Workflows\"},\"content\":\"$27\"},{\"page_plan\":{\"id\":\"4.3\",\"title\":\"Docker Containerization\"},\"content\":\"$28\"},{\"page_plan\":{\"id\":\"5\",\"title\":\"Configuration and Customization\"},\"content\":\"$29\"},{\"page_plan\":{\"id\":\"5.1\",\"title\":\"Environment Variables\"},\"content\":\"$2a\"},{\"page_plan\":{\"id\":\"5.2\",\"title\":\"UI Content Configuration\"},\"content\":\"$2b\"}]},\"children\":\"$L2c\"}]}]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"TechArticle\\\",\\\"headline\\\":\\\"Overview\\\",\\\"description\\\":\\\"The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi\\\",\\\"image\\\":\\\"https://deepwiki.com/hotosm/fAIr/og-image.png\\\",\\\"datePublished\\\":\\\"2025-04-30T15:26:23.772646\\\",\\\"dateModified\\\":\\\"2025-04-30T15:26:23.772646\\\",\\\"author\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"url\\\":\\\"https://deepwiki.com\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"logo\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://deepwiki.com/icon.png\\\"}},\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://deepwiki.com/hotosm/fAIr\\\"}}\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]\n"])</script><script>self.__next_f.push([1,"e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"hotosm/fAIr | DeepWiki\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi\"}],[\"$\",\"meta\",\"2\",{\"name\":\"keywords\",\"content\":\"hotosm/fAIr,hotosm,fAIr,documentation,wiki,codebase,AI documentation,Devin,Overview\"}],[\"$\",\"link\",\"3\",{\"rel\":\"canonical\",\"href\":\"https://deepwiki.com/hotosm/fAIr\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"hotosm/fAIr | DeepWiki\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://deepwiki.com/hotosm/fAIr\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"DeepWiki\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image\",\"content\":\"https://deepwiki.com/hotosm/fAIr/og-image.png?page=1\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:site\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:creator\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"hotosm/fAIr | DeepWiki\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"The fAIr project is an open AI-assisted mapping service developed by the Humanitarian OpenStreetMap Team (HOT) that enables accurate detection of geographic features from satellite imagery using machi\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image\",\"content\":\"https://deepwiki.com/hotosm/fAIr/og-image.png?page=1\"}],[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/icon.png?1ee4c6a68a73a205\",\"type\":\"image/png\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"18\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?a4f658907db0ab87\",\"type\":\"image/png\",\"sizes\":\"180x180\"}],[\"$\",\"$L2d\",\"19\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"13:\"$e:metadata\"\n"])</script><script>self.__next_f.push([1,"2c:[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]\n"])</script></body></html>