<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/de70bee13400563f.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7da0a892b4ad83db.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7"/><script src="/_next/static/chunks/87c73c54-dd8d81ac9604067c.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/18-2224119117d14cba.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/main-app-57aa1716f0d0f500.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/5462-08221e91030fd747.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/4429-943205658cbafffe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/9976-9250854d58eefaa3.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/1481-25d5bbc4f2d9524a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-6651f8cd8321a0db.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/25-9f305b682cea7558.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/7391-f64e18878e224268.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/6373-d56a493968555802.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><script src="/_next/static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" async=""></script><meta name="next-size-adjust" content=""/><title>kshitijrajsharma/opengeoaimodelshub | DeepWiki</title><meta name="description" content="This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr"/><meta name="keywords" content="kshitijrajsharma/opengeoaimodelshub,kshitijrajsharma,opengeoaimodelshub,documentation,wiki,codebase,AI documentation,Devin,Overview"/><link rel="canonical" href="https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub"/><meta property="og:title" content="kshitijrajsharma/opengeoaimodelshub | DeepWiki"/><meta property="og:description" content="This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr"/><meta property="og:url" content="https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub"/><meta property="og:site_name" content="DeepWiki"/><meta property="og:image" content="https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub/og-image.png?page=1"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@cognition"/><meta name="twitter:creator" content="@cognition"/><meta name="twitter:title" content="kshitijrajsharma/opengeoaimodelshub | DeepWiki"/><meta name="twitter:description" content="This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr"/><meta name="twitter:image" content="https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub/og-image.png?page=1"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="icon" href="/icon.png?1ee4c6a68a73a205" type="image/png" sizes="48x48"/><link rel="apple-touch-icon" href="/apple-icon.png?a4f658907db0ab87" type="image/png" sizes="180x180"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" noModule=""></script></head><body class="__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased"><div hidden=""><!--$--><!--/$--></div><section aria-label="Notifications alt+T" tabindex="-1" aria-live="polite" aria-relevant="additions text" aria-atomic="false"></section><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","light",null,["light","dark"],null,true,true)</script><!--$?--><template id="B:0"></template><div class="flex min-h-screen w-full flex-col text-white"><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div></div><!--/$--><script>requestAnimationFrame(function(){$RT=performance.now()});</script><script src="/_next/static/chunks/webpack-5400bf868d87f903.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7" id="_R_" async=""></script><div hidden id="S:0"><div class="flex min-h-screen w-full flex-col text-white" id="codebase-wiki-repo-page"><div class="bg-background border-b-border sticky top-0 z-30 border-b border-dashed"><div class="font-geist-mono relative flex h-8 items-center justify-center text-xs font-medium sm:hidden"><div class="powered-by-devin-gradient absolute inset-0 z-[-1] h-8 w-full"></div><button class="flex items-center gap-2"><svg class="size-3 [&amp;_path]:stroke-0 [&amp;_path]:animate-[custom-pulse_1.8s_infinite_var(--delay,0s)]" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="[--delay:0.6s]" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="[--delay:1.2s]" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg>Index your code with Devin</button></div><div class="container-wrapper"><div class="container mx-auto flex w-full flex-row items-center gap-2 py-4 md:py-6"><a class="flex items-center gap-3" href="/"><span class="text-base font-medium leading-none md:text-lg hidden sm:block">DeepWiki</span></a><div class="flex-1"><div class="flex flex-row items-center gap-2"><a class="block text-xs font-medium leading-none text-white sm:hidden md:text-lg" href="/">DeepWiki</a><p class="text-sm font-normal leading-none md:text-lg"><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub" target="_blank" rel="noopener noreferrer" title="Open repository" class="text-muted-foreground hover:text-muted-foreground/80 group inline-flex items-center gap-1 transition-colors">kshitijrajsharma/opengeoaimodelshub<!-- --> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="opacity-0 transition-opacity group-hover:opacity-100"><path d="M224,104a8,8,0,0,1-16,0V59.32l-66.33,66.34a8,8,0,0,1-11.32-11.32L196.68,48H152a8,8,0,0,1,0-16h64a8,8,0,0,1,8,8Zm-40,24a8,8,0,0,0-8,8v72H48V80h72a8,8,0,0,0,0-16H48A16,16,0,0,0,32,80V208a16,16,0,0,0,16,16H176a16,16,0,0,0,16-16V136A8,8,0,0,0,184,128Z"></path></svg></a></p></div></div><div class="flex items-center gap-4"><button class="group hidden items-center gap-1.5 md:flex"><div class="relative"><span class="text-foreground/70 group-hover:text-foreground text-xs font-light transition-colors">Index your code with</span><div class="bg-foreground/30 absolute bottom-0 left-0 h-[1px] w-0 transition-all duration-300 group-hover:w-full"></div></div><div class="flex items-center gap-1 transition-transform duration-300 group-hover:translate-x-0.5"><svg class="size-4 transform transition-transform duration-700 group-hover:rotate-180 [&amp;_path]:stroke-0" xmlns="http://www.w3.org/2000/svg" viewBox="110 110 460 500"><path style="fill:#21c19a" class="" d="M418.73,332.37c9.84-5.68,22.07-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.58,1.06.19.06.37.11.55.16.87.21,1.76.34,2.65.35.04,0,.08.02.13.02.1,0,.19-.03.29-.04.83-.02,1.64-.13,2.45-.32.14-.03.28-.05.42-.09.87-.24,1.7-.59,2.5-1.03.08-.04.17-.06.25-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.06,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.57-1.06-.19-.06-.37-.11-.56-.16-.88-.21-1.76-.34-2.65-.34-.13,0-.26.02-.4.02-.84.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.24.1.8.44,1.64.79,2.5,1.03.14.04.28.06.42.09.81.19,1.62.3,2.45.32.1,0,.19.04.29.04.04,0,.08-.02.13-.02.89,0,1.77-.13,2.65-.35.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.58-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-.08-.04-.16-.06-.24-.1-.8-.44-1.64-.8-2.51-1.04-.13-.04-.26-.05-.39-.09-.82-.2-1.65-.31-2.49-.33-.13,0-.25-.02-.38-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.75.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.07,5.68-31.9,0-9.84-5.68-15.95-16.27-15.95-27.63s6.11-21.95,15.95-27.63Z"></path><path style="fill:#3969ca" d="M141.09,317.65l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c.08-.04.13-.11.2-.16.78-.48,1.51-1.02,2.15-1.66.1-.1.18-.21.28-.31.57-.6,1.08-1.26,1.51-1.97.07-.12.15-.22.22-.34.44-.77.77-1.6,1.03-2.47.05-.19.1-.37.14-.56.22-.89.37-1.81.37-2.76v-29.43c0-11.36,6.11-21.95,15.96-27.63s22.06-5.68,31.91,0l25.49,14.71c.82.48,1.69.8,2.57,1.06.19.06.37.11.56.16.87.21,1.76.34,2.64.35.04,0,.09.02.13.02.1,0,.19-.04.29-.04.83-.02,1.65-.13,2.45-.32.14-.03.28-.05.41-.09.87-.24,1.71-.6,2.51-1.04.08-.04.16-.06.24-.1l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-.08.04-.13.11-.2.16-.78.48-1.51,1.02-2.15,1.66-.1.1-.18.21-.28.31-.57.6-1.08,1.26-1.51,1.97-.07.12-.15.22-.22.34-.44.77-.77,1.6-1.03,2.47-.05.19-.1.37-.14.56-.22.89-.37,1.81-.37,2.76v29.43c0,11.36-6.11,21.95-15.95,27.63-9.84,5.68-22.07,5.68-31.91,0l-25.49-14.71c-.82-.48-1.69-.8-2.58-1.06-.19-.06-.37-.11-.55-.16-.88-.21-1.76-.34-2.65-.35-.13,0-.26.02-.4.02-.83.02-1.66.13-2.47.32-.13.03-.27.05-.4.09-.87.24-1.71.6-2.51,1.04-.08.04-.16.06-.24.1l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22Z"></path><path style="fill:#0294de" class="" d="M396.88,484.35l-50.97-29.43c-.08-.04-.17-.06-.24-.1-.8-.44-1.64-.79-2.51-1.03-.14-.04-.27-.06-.41-.09-.81-.19-1.64-.3-2.47-.32-.13,0-.26-.02-.39-.02-.89,0-1.78.13-2.66.35-.18.04-.36.1-.54.15-.88.26-1.76.59-2.58,1.07l-25.49,14.72c-9.84,5.68-22.06,5.68-31.9,0-9.84-5.68-15.96-16.27-15.96-27.63v-29.43c0-.95-.15-1.87-.37-2.76-.05-.19-.09-.37-.14-.56-.25-.86-.59-1.69-1.03-2.47-.07-.12-.15-.22-.22-.34-.43-.71-.94-1.37-1.51-1.97-.1-.1-.18-.21-.28-.31-.65-.63-1.37-1.18-2.15-1.66-.07-.04-.13-.11-.2-.16l-50.97-29.43c-3.65-2.11-8.15-2.11-11.81,0l-50.97,29.43c-3.65,2.11-5.9,6.01-5.9,10.22v58.86c0,4.22,2.25,8.11,5.9,10.22l50.97,29.43c.08.04.17.06.25.1.8.44,1.63.79,2.5,1.03.14.04.29.06.43.09.8.19,1.61.3,2.43.32.1,0,.2.04.3.04.04,0,.09-.02.13-.02.88,0,1.77-.13,2.64-.34.19-.04.37-.1.56-.16.88-.26,1.75-.59,2.57-1.06l25.49-14.71c9.84-5.68,22.06-5.68,31.91,0,9.84,5.68,15.95,16.27,15.95,27.63v29.43c0,.95.15,1.87.37,2.76.05.19.09.37.14.56.25.86.59,1.69,1.03,2.47.07.12.15.22.22.34.43.71.94,1.37,1.51,1.97.1.1.18.21.28.31.65.63,1.37,1.18,2.15,1.66.07.04.13.11.2.16l50.97,29.43c1.83,1.05,3.86,1.58,5.9,1.58s4.08-.53,5.9-1.58l50.97-29.43c3.65-2.11,5.9-6.01,5.9-10.22v-58.86c0-4.22-2.25-8.11-5.9-10.22Z"></path></svg><span class="text-sm font-medium">Devin</span></div></button><button aria-label="Edit Wiki" class="flex items-center rounded-md cursor-pointer transition-all border border-border bg-surface hover:border-border-hover hover:bg-component disabled:cursor-default disabled:opacity-50 disabled:hover:border-border disabled:hover:bg-surface gap-2 px-3 py-1.5 text-sm"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="currentColor" viewBox="0 0 256 256"><path d="M227.32,73.37,182.63,28.69a16,16,0,0,0-22.63,0L36.69,152A15.86,15.86,0,0,0,32,163.31V208a16,16,0,0,0,16,16H216a8,8,0,0,0,0-16H115.32l112-112A16,16,0,0,0,227.32,73.37ZM92.69,208H48V163.31l88-88L180.69,120ZM192,108.69,147.32,64l24-24L216,84.69Z"></path></svg>Edit Wiki</button><button class="flex items-center rounded-md !text-white cursor-pointer transition-all border bg-blue-500 hover:bg-blue-600 border-blue-500 hover:border-blue-600 dark:bg-blue-900 dark:hover:bg-blue-800 dark:border-blue-900 dark:hover:border-blue-800 disabled:cursor-default disabled:opacity-50 disabled:hover:bg-blue-500 disabled:hover:border-blue-500 dark:disabled:hover:bg-blue-900 dark:disabled:hover:border-blue-900 gap-1.5 px-3 py-1.5 text-sm" aria-label="Share" data-state="closed" data-slot="tooltip-trigger"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="h-4 w-4"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg><span>Share</span></button><div class="h-8 w-8"></div></div></div></div></div><!--$?--><template id="B:1"></template><div class="container-wrapper flex flex-1 items-center justify-center px-4"><div class="inline-block bg-clip-text text-[#b5b5b5a4] animate-shine text-center text-lg" style="background-image:linear-gradient(120deg, rgba(255, 255, 255, 0) 40%, rgba(255, 255, 255, 0.8) 50%, rgba(255, 255, 255, 0) 60%);background-size:200% 100%;-webkit-background-clip:text;animation-duration:1s">Loading...</div></div><!--/$--></div></div><script>$RB=[];$RV=function(a){$RT=performance.now();for(var b=0;b<a.length;b+=2){var c=a[b],e=a[b+1];null!==e.parentNode&&e.parentNode.removeChild(e);var f=c.parentNode;if(f){var g=c.previousSibling,h=0;do{if(c&&8===c.nodeType){var d=c.data;if("/$"===d||"/&"===d)if(0===h)break;else h--;else"$"!==d&&"$?"!==d&&"$~"!==d&&"$!"!==d&&"&"!==d||h++}d=c.nextSibling;f.removeChild(c);c=d}while(c);for(;e.firstChild;)f.insertBefore(e.firstChild,c);g.data="$";g._reactRetry&&requestAnimationFrame(g._reactRetry)}}a.length=0};
$RC=function(a,b){if(b=document.getElementById(b))(a=document.getElementById(a))?(a.previousSibling.data="$~",$RB.push(a,b),2===$RB.length&&("number"!==typeof $RT?requestAnimationFrame($RV.bind(null,$RB)):(a=performance.now(),setTimeout($RV.bind(null,$RB),2300>a&&2E3<a?2300-a:$RT+300-a)))):b.parentNode.removeChild(b)};$RC("B:0","S:0")</script><div hidden id="S:1"><script type="application/ld+json">{"@context":"https://schema.org","@type":"TechArticle","headline":"Overview","description":"This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr","image":"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub/og-image.png","datePublished":"2026-01-13T11:23:12.622956","dateModified":"2026-01-13T11:23:12.622956","author":{"@type":"Organization","name":"DeepWiki","url":"https://deepwiki.com"},"publisher":{"@type":"Organization","name":"DeepWiki","logo":{"@type":"ImageObject","url":"https://deepwiki.com/icon.png"}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub"}}</script><div class="w-full flex-1"><div class="container-wrapper relative mx-auto h-full px-0"><div class="container relative mx-auto flex h-full w-full flex-col gap-0 max-md:!px-0 md:flex-row md:gap-6 lg:gap-10"><div class="border-r-border hidden max-h-screen border-r border-dashed py-6 pr-4 transition-[border-radius] md:sticky md:left-0 md:top-20 md:block md:h-[calc(100vh-82px)] md:w-64 md:flex-shrink-0 md:overflow-y-auto lg:py-9 xl:w-72"><div class="flex h-full w-full max-w-full flex-shrink-0 flex-col overflow-hidden" style="scrollbar-color:var(--color-border) transparent"><div class="flex-shrink-0 px-2"><div class="text-secondary pb-1 text-xs">Last indexed: <!-- -->13 January 2026<!-- --> (<a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/commits/393e3158" target="_blank" rel="noopener noreferrer" class="underline-offset-2 hover:underline">393e31</a>)</div></div><ul class="flex-1 flex-shrink-0 space-y-1 overflow-y-auto py-1" style="scrollbar-width:none"><li style="padding-left:0"><a data-selected="true" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/1-overview">Overview</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/2-getting-started">Getting Started</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/2.1-prerequisites-and-installation">Prerequisites and Installation</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/2.2-quick-start-guide">Quick Start Guide</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3-example-model-system">Example Model System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3.1-model-overview-and-architecture">Model Overview and Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3.2-training-pipeline">Training Pipeline</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3.3-inference-system">Inference System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3.4-esri-integration-and-dlpk-generation">ESRI Integration and DLPK Generation</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3.5-mlflow-project-structure">MLflow Project Structure</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3.6-dependencies-and-configuration">Dependencies and Configuration</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/3.7-utilities-and-helper-functions">Utilities and Helper Functions</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4-infrastructure-system">Infrastructure System</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4.1-service-architecture">Service Architecture</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4.2-setup-and-deployment">Setup and Deployment</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4.3-configuration-management">Configuration Management</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4.4-traefik-reverse-proxy">Traefik Reverse Proxy</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4.5-mlflow-tracking-server">MLflow Tracking Server</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4.6-storage-services-(minio-and-postgresql)">Storage Services (MinIO and PostgreSQL)</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/4.7-additional-services">Additional Services</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/5-cicd-pipelines">CI/CD Pipelines</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/5.1-docker-image-publishing-workflows">Docker Image Publishing Workflows</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/5.2-mlflow-custom-image-pipeline">MLflow Custom Image Pipeline</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/6-deployment-guide">Deployment Guide</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/6.1-infrastructure-deployment">Infrastructure Deployment</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/6.2-model-deployment-options">Model Deployment Options</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/7-development-guide">Development Guide</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/7.1-local-development-setup">Local Development Setup</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/7.2-working-with-the-training-pipeline">Working with the Training Pipeline</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/7.3-data-preparation-and-custom-datasets">Data Preparation and Custom Datasets</a></li><li style="padding-left:0"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/8-reference">Reference</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/8.1-mlproject-api-reference">MLproject API Reference</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/8.2-configuration-reference">Configuration Reference</a></li><li style="padding-left:12px"><a data-selected="false" class="hover:bg-hover block w-full rounded px-2 py-1.5 text-left text-sm transition-none text-secondary data-[selected=true]:bg-hover data-[selected=true]:text-primary font-normal data-[selected=true]:font-normal" href="/kshitijrajsharma/opengeoaimodelshub/8.3-research-and-background">Research and Background</a></li></ul></div></div><div class="flex h-full flex-1 flex-col overflow-hidden"><div class="bg-background border-b-border sticky top-0 z-10 border-b border-dashed md:hidden"><div class="flex cursor-pointer items-center gap-2 p-3"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 256 256" class="transition-transform"><path d="M184.49,136.49l-80,80a12,12,0,0,1-17-17L159,128,87.51,56.49a12,12,0,1,1,17-17l80,80A12,12,0,0,1,184.49,136.49Z"></path></svg><span class="truncate text-base font-normal">Menu</span></div></div><div class="relative flex-1 overflow-y-auto px-3 pt-3 md:rounded-md md:px-0 md:pt-0 [&amp;_::selection]:bg-purple-500/40" style="scrollbar-color:var(--color-night) transparent"><div class="pb-30 mx-auto max-w-2xl md:pb-40 md:pt-6 lg:pt-8"><div class="prose prose-invert dark:prose-invert prose-headings:text-inherit prose-p:text-inherit max-w-none"><div><div class="prose-custom prose-custom-md prose-custom-gray !max-w-none text-neutral-300 [overflow-wrap:anywhere]"><h1 id="overview" class="group" data-header="true">Overview<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h1>
<details>
<summary>Relevant source files</summary>
<ul>
<li><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/LICENSE" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>LICENSE</span></a></li>
<li><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/README.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span></a></li>
<li><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/docs/report.pdf" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>docs/report.pdf</span></a></li>
<li><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span></a></li>
<li><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/infra/Readme.md" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>infra/Readme.md</span></a></li>
</ul>
</details>
<p>This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a production-ready infrastructure stack. For detailed setup instructions, see <a href="/kshitijrajsharma/opengeoaimodelshub/2-getting-started" class="text-neutral-300 hover:text-neutral-200 hover:underline">Getting Started</a>. For infrastructure deployment specifics, see <a href="/kshitijrajsharma/opengeoaimodelshub/6.1-infrastructure-deployment" class="text-neutral-300 hover:text-neutral-200 hover:underline">Infrastructure Deployment</a>. For model training details, see <a href="/kshitijrajsharma/opengeoaimodelshub/3.2-training-pipeline" class="text-neutral-300 hover:text-neutral-200 hover:underline">Training Pipeline</a>.</p>
<h2 id="purpose-and-scope" class="group" data-header="true">Purpose and Scope<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>OpenGeoAIModelHub is a reference implementation demonstrating how to build reproducible, deployable, and scalable Geo-AI models using metadata standards and modern MLOps practices. The repository serves two interconnected purposes:</p>
<ol>
<li>
<p><strong>Example Model System</strong>: A complete refugee camp detection model built with PyTorch Lightning, demonstrating best practices for geospatial ML development including data acquisition from OpenAerialMap and OpenStreetMap, training orchestration via MLflow, and multi-format model deployment (PyTorch, ONNX, ESRI DLPK).</p>
</li>
<li>
<p><strong>Infrastructure System</strong>: A production-grade, self-hosted MLOps stack featuring Traefik reverse proxy, MLflow tracking server, MinIO object storage, PostgreSQL with PostGIS, and supporting services for experiment tracking, model registry, and deployment.</p>
</li>
</ol>
<p>The repository was developed as part of internship work with the Coastal Dynamics Lab (CDL) to explore standardized approaches for managing multiple GeoAI models within a unified platform.</p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/README.md#L1-L8" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-8</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md#L1-L3" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-3</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/infra/Readme.md#L1-L3" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>infra/Readme.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-3</span></a></p>
<h2 id="repository-structure" class="group" data-header="true">Repository Structure<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The repository is organized into two primary directories, each representing one of the major systems:</p>




















<table><thead><tr><th>Directory</th><th>Purpose</th><th>Key Files</th></tr></thead><tbody><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">examplemodel/</code></td><td>ML model implementation</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">MLproject</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">train.py</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">inference.py</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">model.py</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">pyproject.toml</code></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">infra/</code></td><td>Infrastructure stack</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">docker-compose.yml</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">setup.sh</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">.env.template</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">manage.sh</code></td></tr></tbody></table>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Diagram: Repository File Structure and Dependencies</strong></p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/README.md#L1-L9" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-9</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md#L1-L52" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-52</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/infra/Readme.md#L1-L83" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>infra/Readme.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-83</span></a></p>
<h2 id="system-architecture" class="group" data-header="true">System Architecture<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The repository implements a layered architecture where the Example Model System operates as a client to the Infrastructure System. The Infrastructure System provides core MLOps services (experiment tracking, artifact storage, model registry), while the Example Model System demonstrates how to build and deploy a geospatial ML model using these services.</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Diagram: System Architecture with Code Entity Mapping</strong></p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/README.md#L1-L9" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-9</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md#L1-L52" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-52</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/infra/Readme.md#L1-L83" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>infra/Readme.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-83</span></a></p>
<h2 id="major-components" class="group" data-header="true">Major Components<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<h3 id="example-model-system-components" class="group" data-header="true">Example Model System Components<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The Example Model System is located in the <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">examplemodel/</code> directory and implements a complete ML pipeline using MLflow Projects for orchestration.</p>








































<table><thead><tr><th>Component</th><th>File/Class</th><th>Purpose</th></tr></thead><tbody><tr><td><strong>MLflow Project Definition</strong></td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/MLproject" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/MLproject</span></a></td><td>Defines five entry points: <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">preprocess</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">train</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">inference</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">validate_stac</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">stac2esri</code></td></tr><tr><td><strong>Training Script</strong></td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/train.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/train.py</span></a></td><td>Implements training loop with PyTorch Lightning and MLflow logging</td></tr><tr><td><strong>Inference Script</strong></td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/inference.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/inference.py</span></a></td><td>Performs model inference and generates prediction overlays</td></tr><tr><td><strong>Model Architecture</strong></td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/src/model.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/src/model.py</span></a></td><td>Defines <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">LitRefugeeCamp</code> (U-Net) and <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">CampDataModule</code> classes</td></tr><tr><td><strong>ESRI Integration</strong></td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/src/esri/RefugeeCampDetector.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/src/esri/RefugeeCampDetector.py</span></a></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">RefugeeCampDetector</code> class for ArcGIS Deep Learning Package deployment</td></tr><tr><td><strong>Dependency Management</strong></td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/pyproject.toml" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/pyproject.toml</span></a></td><td>Declares dependencies using modern Python packaging standards</td></tr></tbody></table>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Diagram: Example Model Component Mapping (Natural Language to Code Entities)</strong></p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md#L1-L52" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-52</span></a></p>
<h3 id="infrastructure-system-components" class="group" data-header="true">Infrastructure System Components<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The Infrastructure System is located in the <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">infra/</code> directory and provides a complete MLOps stack managed by Docker Compose.</p>















































<table><thead><tr><th>Service</th><th>Docker Image</th><th>Purpose</th><th>Exposed Subdomain</th></tr></thead><tbody><tr><td><strong>traefik</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">traefik:v3.2</code></td><td>Reverse proxy with automatic SSL via Let&#x27;s Encrypt</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">traefik.yourdomain.com</code></td></tr><tr><td><strong>mlflow</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">ghcr.io/&lt;user&gt;/mlflow:latest</code></td><td>Experiment tracking and model registry</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">mlflow.yourdomain.com</code></td></tr><tr><td><strong>minio</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">minio/minio:latest</code></td><td>S3-compatible object storage for ML artifacts</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">minio.yourdomain.com</code>, <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">minio-api.yourdomain.com</code></td></tr><tr><td><strong>postgres</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">postgis/postgis:latest</code></td><td>PostgreSQL with PostGIS for metadata and geospatial data</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">postgres.yourdomain.com</code></td></tr><tr><td><strong>homepage</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">ghcr.io/gethomepage/homepage:latest</code></td><td>Service monitoring dashboard</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">yourdomain.com</code></td></tr><tr><td><strong>rustdesk</strong></td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">rustdesk/rustdesk-server:latest</code></td><td>Remote desktop server</td><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">rustdesk.yourdomain.com</code></td></tr></tbody></table>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Diagram: Infrastructure Component Mapping (Docker Services and Scripts)</strong></p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/infra/Readme.md#L1-L83" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>infra/Readme.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-83</span></a></p>
<h2 id="key-technologies-and-standards" class="group" data-header="true">Key Technologies and Standards<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The repository leverages modern tooling and industry standards to ensure reproducibility, interoperability, and production readiness:</p>
<h3 id="ml-pipeline-technologies" class="group" data-header="true">ML Pipeline Technologies<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<ul>
<li><strong>MLflow Projects</strong>: Declarative pipeline orchestration with reproducible entry points</li>
<li><strong>PyTorch Lightning</strong>: Structured deep learning framework with <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">LightningModule</code> and <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">LightningDataModule</code> abstractions</li>
<li><strong>ONNX</strong>: Model export format for cross-platform inference and deployment</li>
<li><strong>STAC-MLM</strong>: SpatioTemporal Asset Catalog Machine Learning Model extension for metadata standardization</li>
<li><strong>ESRI DLPK</strong>: Deep Learning Package format for ArcGIS Pro integration</li>
</ul>
<h3 id="infrastructure-technologies" class="group" data-header="true">Infrastructure Technologies<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<ul>
<li><strong>Docker Compose</strong>: Multi-container orchestration for service deployment</li>
<li><strong>Traefik</strong>: Modern reverse proxy with automatic HTTPS via Let&#x27;s Encrypt ACME protocol</li>
<li><strong>MinIO</strong>: High-performance S3-compatible object storage implementing the AWS S3 API</li>
<li><strong>PostgreSQL + PostGIS</strong>: Relational database with geospatial extensions supporting spatial queries</li>
<li><strong>uv</strong>: Fast Python package installer and resolver for dependency management</li>
</ul>
<h3 id="standards-compliance" class="group" data-header="true">Standards Compliance<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h3>
<p>The repository demonstrates compliance with geospatial AI standards:</p>
<ul>
<li><strong>STAC-MLM Extension</strong>: Model metadata follows the STAC Machine Learning Model extension specification</li>
<li><strong>OGC Standards</strong>: Integration with OpenStreetMap (OGC-compliant) for label acquisition</li>
<li><strong>S3 API</strong>: Artifact storage implements the Amazon S3 API via MinIO for cloud compatibility</li>
<li><strong>MLflow Tracking API</strong>: Experiment logging follows MLflow&#x27;s standard tracking protocol</li>
</ul>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md#L1-L52" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-52</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/infra/Readme.md#L1-L83" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>infra/Readme.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-83</span></a></p>
<h2 id="integration-workflow" class="group" data-header="true">Integration Workflow<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The Example Model System and Infrastructure System integrate through MLflow&#x27;s client-server architecture. During training, the model code (client) logs experiments to the MLflow server, which persists metadata in PostgreSQL and artifacts in MinIO.</p>
<pre class="px-2 py-1.5 has-[code]:rounded-md has-[code]:!bg-[#e5e5e5] has-[div]:bg-transparent has-[div]:!p-0 has-[code]:text-stone-900 dark:has-[code]:!bg-[#242424] has-[code]:dark:text-white [&amp;_code]:block [&amp;_code]:border-none [&amp;_code]:bg-transparent [&amp;_code]:p-0"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></pre>
<p><strong>Diagram: Integration Workflow Showing API Interactions</strong></p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md#L40-L48" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">40-48</span></a> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/infra/Readme.md#L5-L12" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>infra/Readme.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">5-12</span></a></p>
<h2 id="deployment-outputs" class="group" data-header="true">Deployment Outputs<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The training pipeline produces multiple deployment-ready artifacts, each targeting different deployment scenarios:</p>



































<table><thead><tr><th>Artifact</th><th>Format</th><th>Use Case</th><th>Generated By</th></tr></thead><tbody><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">best_model.pth</code></td><td>PyTorch checkpoint</td><td>Direct PyTorch inference, fine-tuning</td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/train.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>train.py</span></a></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">best_model.onnx</code></td><td>ONNX format</td><td>Cross-platform inference (ONNX Runtime)</td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/train.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>train.py</span></a> with <code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">torch.onnx.export()</code></td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">best_model.dlpk</code></td><td>ESRI Deep Learning Package</td><td>ArcGIS Pro deep learning tools</td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/stac2esri" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>stac2esri</span></a> entry point</td></tr><tr><td><code class="rounded-sm bg-[#e5e5e5] px-[0.25rem] py-[0.20rem] text-xs font-normal leading-[15px] before:hidden after:hidden dark:bg-[#484848]/30">stac_item.json</code></td><td>STAC-MLM JSON</td><td>Model discovery and metadata exchange</td><td><a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/train.py" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4] rounded-r"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>train.py</span></a></td></tr></tbody></table>
<p>These artifacts are stored in MinIO object storage and tracked in the MLflow Model Registry, enabling versioning, lineage tracking, and collaborative model management.</p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/examplemodel/README.md#L1-L52" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>examplemodel/README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-52</span></a></p>
<h2 id="license" class="group" data-header="true">License<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>The repository is released under the MIT License, permitting commercial and non-commercial use with minimal restrictions. See <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/LICENSE#L1-L22" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>LICENSE</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-22</span></a> for full terms.</p>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/LICENSE#L1-L22" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>LICENSE</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-22</span></a></p>
<h2 id="additional-resources" class="group" data-header="true">Additional Resources<button class="relative ml-2 cursor-pointer align-baseline text-gray-400 opacity-0 transition-opacity hover:text-blue-400 focus:opacity-100 group-hover:opacity-100" aria-label="Copy link to header"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M117.18,188.74a12,12,0,0,1,0,17l-5.12,5.12A58.26,58.26,0,0,1,70.6,228h0A58.62,58.62,0,0,1,29.14,127.92L63.89,93.17a58.64,58.64,0,0,1,98.56,28.11,12,12,0,1,1-23.37,5.44,34.65,34.65,0,0,0-58.22-16.58L46.11,144.89A34.62,34.62,0,0,0,70.57,204h0a34.41,34.41,0,0,0,24.49-10.14l5.11-5.12A12,12,0,0,1,117.18,188.74ZM226.83,45.17a58.65,58.65,0,0,0-82.93,0l-5.11,5.11a12,12,0,0,0,17,17l5.12-5.12a34.63,34.63,0,1,1,49,49L175.1,145.86A34.39,34.39,0,0,1,150.61,156h0a34.63,34.63,0,0,1-33.69-26.72,12,12,0,0,0-23.38,5.44A58.64,58.64,0,0,0,150.56,180h.05a58.28,58.28,0,0,0,41.47-17.17l34.75-34.75a58.62,58.62,0,0,0,0-82.91Z"></path></svg></button></h2>
<p>For more detailed information on specific subsystems:</p>
<ul>
<li><strong>Setting up the infrastructure</strong>: See <a href="/kshitijrajsharma/opengeoaimodelshub/6.1-infrastructure-deployment" class="text-neutral-300 hover:text-neutral-200 hover:underline">Infrastructure Deployment</a></li>
<li><strong>Training the example model</strong>: See <a href="/kshitijrajsharma/opengeoaimodelshub/3.2-training-pipeline" class="text-neutral-300 hover:text-neutral-200 hover:underline">Training Pipeline</a></li>
<li><strong>Deploying models to ArcGIS</strong>: See <a href="/kshitijrajsharma/opengeoaimodelshub/3.4-esri-integration-and-dlpk-generation" class="text-neutral-300 hover:text-neutral-200 hover:underline">ESRI Integration and DLPK Generation</a></li>
<li><strong>Understanding MLflow Projects</strong>: See <a href="/kshitijrajsharma/opengeoaimodelshub/3.5-mlflow-project-structure" class="text-neutral-300 hover:text-neutral-200 hover:underline">MLflow Project Structure</a></li>
<li><strong>Configuring services</strong>: See <a href="/kshitijrajsharma/opengeoaimodelshub/4.3-configuration-management" class="text-neutral-300 hover:text-neutral-200 hover:underline">Configuration Management</a></li>
<li><strong>Project background and research</strong>: See <a href="/kshitijrajsharma/opengeoaimodelshub/8.3-research-and-background" class="text-neutral-300 hover:text-neutral-200 hover:underline">Research and Background</a> and slides at <a href="https://gamma.app/docs/Internship-Presentation-3-min-xsuj64t3h0kyow4" class="text-neutral-300 hover:text-neutral-200 hover:underline">https://gamma.app/docs/Internship-Presentation-3-min-xsuj64t3h0kyow4</a></li>
</ul>
<p><strong>Sources:</strong> <a href="https://github.com/kshitijrajsharma/opengeoaimodelshub/blob/393e3158/README.md#L1-L9" target="_blank" rel="noopener noreferrer" class="mb-1 mr-1 inline-flex items-stretch font-mono text-xs !no-underline transition-opacity hover:opacity-75"><span class="flex items-center break-all rounded-l px-2 py-1.5 bg-[#e5e5e5] text-[#333333] dark:bg-[#252525] dark:text-[#e4e4e4]"><svg class="mr-1.5 hidden h-3.5 w-3.5 flex-shrink-0 opacity-40 md:block" fill="currentColor" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>README.md</span><span class="flex flex-shrink-0 items-center rounded-r border-l px-2 py-1.5 border-[#dddddd] bg-[#d8d8d8] text-[#666666] dark:border-[#333333] dark:bg-[#2a2a2a] dark:text-[#888888]">1-9</span></a></p></div></div></div></div></div></div><div class="hidden overflow-hidden transition-[border-radius] xl:sticky xl:right-0 xl:top-20 xl:block xl:h-[calc(100vh-82px)] xl:w-64 xl:flex-shrink-0 2xl:w-72" style="scrollbar-width:none"><div class="flex max-h-full w-full flex-shrink-0 flex-col py-6 pt-0 text-sm lg:pb-4 lg:pt-8 xl:w-64 2xl:w-72" style="scrollbar-color:var(--color-night) transparent"><div><div class="relative mx-4 my-4 rounded-md border border-neutral-200 bg-neutral-100 p-3 text-sm text-neutral-600 dark:border-neutral-800 dark:bg-neutral-900 dark:text-neutral-400"><button class="absolute right-2 top-2 rounded-sm p-1 opacity-70 transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-neutral-400 focus:ring-offset-2"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256" class="h-4 w-4"><path d="M205.66,194.34a8,8,0,0,1-11.32,11.32L128,139.31,61.66,205.66a8,8,0,0,1-11.32-11.32L116.69,128,50.34,61.66A8,8,0,0,1,61.66,50.34L128,116.69l66.34-66.35a8,8,0,0,1,11.32,11.32L139.31,128Z"></path></svg><span class="sr-only">Dismiss</span></button><p class="text-sm font-medium">Refresh this wiki</p><p class="mt-2 text-sm font-light text-neutral-500 dark:text-neutral-400">This wiki was recently refreshed. Please wait<!-- --> <!-- -->7<!-- --> day<!-- -->s<!-- --> to refresh again.</p></div></div><h3 class="px-4 pb-5 text-lg font-medium leading-none">On this page</h3><ul style="scrollbar-width:none" class="min-h-0 flex-1 space-y-3 overflow-y-auto p-4 pt-0"><li class=""><a href="#overview" class="hover:text-primary pr-1 transition-all text-primary font-medium">Overview</a></li><li class="ml-3"><a href="#purpose-and-scope" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Purpose and Scope</a></li><li class="ml-3"><a href="#repository-structure" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Repository Structure</a></li><li class="ml-3"><a href="#system-architecture" class="hover:text-primary pr-1 font-normal transition-all text-secondary">System Architecture</a></li><li class="ml-3"><a href="#major-components" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Major Components</a></li><li class="ml-6"><a href="#example-model-system-components" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Example Model System Components</a></li><li class="ml-6"><a href="#infrastructure-system-components" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Infrastructure System Components</a></li><li class="ml-3"><a href="#key-technologies-and-standards" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Key Technologies and Standards</a></li><li class="ml-6"><a href="#ml-pipeline-technologies" class="hover:text-primary pr-1 font-normal transition-all text-secondary">ML Pipeline Technologies</a></li><li class="ml-6"><a href="#infrastructure-technologies" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Infrastructure Technologies</a></li><li class="ml-6"><a href="#standards-compliance" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Standards Compliance</a></li><li class="ml-3"><a href="#integration-workflow" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Integration Workflow</a></li><li class="ml-3"><a href="#deployment-outputs" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Deployment Outputs</a></li><li class="ml-3"><a href="#license" class="hover:text-primary pr-1 font-normal transition-all text-secondary">License</a></li><li class="ml-3"><a href="#additional-resources" class="hover:text-primary pr-1 font-normal transition-all text-secondary">Additional Resources</a></li></ul></div></div><div class="pointer-events-none fixed bottom-2 left-2 right-2 mt-2 md:bottom-4 md:left-0 md:right-0"><div class="z-10 mx-auto max-w-3xl"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div></div></div></div><!--$--><!--/$--></div><script>$RC("B:1","S:1")</script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n"])</script><script>self.__next_f.push([1,"2:I[49138,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7177\",\"static/chunks/app/layout-0537c2076823e553.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"RootProvider\"]\n"])</script><script>self.__next_f.push([1,"3:I[85341,[],\"\"]\n4:I[90025,[],\"\"]\n7:I[41012,[],\"ClientPageRoot\"]\n"])</script><script>self.__next_f.push([1,"8:I[57456,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4129\",\"static/chunks/7bf36345-1ac10ec2f0e0c88f.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"2545\",\"static/chunks/c16f53c3-b390b6f98a69dcec.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9885\",\"static/chunks/9885-b57089f03806c3b8.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"659\",\"static/chunks/659-ee9e1e775e30dcef.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8461\",\"static/chunks/8461-369a9f0c48ea2626.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7198\",\"static/chunks/7198-985a49f2b6072d25.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"5462\",\"static/chunks/5462-08221e91030fd747.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4429\",\"static/chunks/4429-943205658cbafffe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9976\",\"static/chunks/9976-9250854d58eefaa3.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1481\",\"static/chunks/1481-25d5bbc4f2d9524a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"3285\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/%5B%5B...wikiRoutes%5D%5D/page-6651f8cd8321a0db.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"b:I[15104,[],\"OutletBoundary\"]\nd:I[94777,[],\"AsyncMetadataOutlet\"]\nf:I[15104,[],\"ViewportBoundary\"]\n11:I[15104,[],\"MetadataBoundary\"]\n12:\"$Sreact.suspense\"\n14:I[34431,[],\"\"]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/de70bee13400563f.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"style\"]\n:HL[\"/_next/static/css/7da0a892b4ad83db.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"SzzXrDjxzhDKxxD_1GShd\",\"p\":\"\",\"c\":[\"\",\"kshitijrajsharma\",\"opengeoaimodelshub\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"org\",\"kshitijrajsharma\",\"d\"],{\"children\":[[\"repo\",\"opengeoaimodelshub\",\"d\"],{\"children\":[[\"wikiRoutes\",\"\",\"oc\"],{\"children\":[\"__PAGE__\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/de70bee13400563f.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7da0a892b4ad83db.css?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{}],[\"$\",\"body\",null,{\"className\":\"__variable_188709 font-geist-sans relative min-h-screen __variable_9a8899 bg-background antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]]}],{\"children\":[[\"org\",\"kshitijrajsharma\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"repo\",\"opengeoaimodelshub\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L5\"]}],{\"children\":[[\"wikiRoutes\",\"\",\"oc\"],[\"$\",\"$1\",\"c\",{\"children\":[null,\"$L6\"]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L7\",null,{\"Component\":\"$8\",\"searchParams\":{},\"params\":{\"org\":\"kshitijrajsharma\",\"repo\":\"opengeoaimodelshub\"},\"promises\":[\"$@9\",\"$@a\"]}],null,[\"$\",\"$Lb\",null,{\"children\":[\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$L11\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$12\",null,{\"fallback\":null,\"children\":\"$L13\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"9:{}\na:\"$0:f:0:1:2:children:2:children:2:children:2:children:1:props:children:0:props:params\"\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nc:null\n"])</script><script>self.__next_f.push([1,"15:I[13550,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7391\",\"static/chunks/7391-f64e18878e224268.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6373\",\"static/chunks/6373-d56a493968555802.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6375\",\"static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9437\",\"static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"HeaderWrapperWithSuspense\"]\n"])</script><script>self.__next_f.push([1,"16:I[82188,[\"9453\",\"static/chunks/b1298b8d-5cac6cd7c8e952ff.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"8970\",\"static/chunks/378e5a93-860e027c5a5e0c0d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1585\",\"static/chunks/f7f68e2d-d8bf979db5ff4e9d.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7963\",\"static/chunks/7963-b29c27a8b53c3f2a.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"1265\",\"static/chunks/1265-fa8a95d3842768f5.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6212\",\"static/chunks/6212-505c2fc95d1a35ae.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"4336\",\"static/chunks/4336-624d5ae6f4cc1cc7.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"25\",\"static/chunks/25-9f305b682cea7558.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"7391\",\"static/chunks/7391-f64e18878e224268.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6373\",\"static/chunks/6373-d56a493968555802.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"6375\",\"static/chunks/6375-7e0e75eb09fc9abe.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"9437\",\"static/chunks/9437-be873d1907eef4d4.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\",\"2933\",\"static/chunks/app/%5Borg%5D/%5Brepo%5D/layout-77683f6369c5b39e.js?dpl=dpl_9vWZnGgB4PqEv2xwHjFeirVQfSL7\"],\"WikiContextProvider\"]\n"])</script><script>self.__next_f.push([1,"3a:I[36505,[],\"IconMark\"]\n17:T4590,"])</script><script>self.__next_f.push([1,"# Overview\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [LICENSE](LICENSE)\n- [README.md](README.md)\n- [docs/report.pdf](docs/report.pdf)\n- [examplemodel/README.md](examplemodel/README.md)\n- [infra/Readme.md](infra/Readme.md)\n\n\u003c/details\u003e\n\n\n\nThis document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a production-ready infrastructure stack. For detailed setup instructions, see [Getting Started](#2). For infrastructure deployment specifics, see [Infrastructure Deployment](#6.1). For model training details, see [Training Pipeline](#3.2).\n\n## Purpose and Scope\n\nOpenGeoAIModelHub is a reference implementation demonstrating how to build reproducible, deployable, and scalable Geo-AI models using metadata standards and modern MLOps practices. The repository serves two interconnected purposes:\n\n1. **Example Model System**: A complete refugee camp detection model built with PyTorch Lightning, demonstrating best practices for geospatial ML development including data acquisition from OpenAerialMap and OpenStreetMap, training orchestration via MLflow, and multi-format model deployment (PyTorch, ONNX, ESRI DLPK).\n\n2. **Infrastructure System**: A production-grade, self-hosted MLOps stack featuring Traefik reverse proxy, MLflow tracking server, MinIO object storage, PostgreSQL with PostGIS, and supporting services for experiment tracking, model registry, and deployment.\n\nThe repository was developed as part of internship work with the Coastal Dynamics Lab (CDL) to explore standardized approaches for managing multiple GeoAI models within a unified platform.\n\n**Sources:** [README.md:1-8](), [examplemodel/README.md:1-3](), [infra/Readme.md:1-3]()\n\n## Repository Structure\n\nThe repository is organized into two primary directories, each representing one of the major systems:\n\n| Directory | Purpose | Key Files |\n|-----------|---------|-----------|\n| `examplemodel/` | ML model implementation | `MLproject`, `train.py`, `inference.py`, `model.py`, `pyproject.toml` |\n| `infra/` | Infrastructure stack | `docker-compose.yml`, `setup.sh`, `.env.template`, `manage.sh` |\n\n```mermaid\ngraph TB\n    subgraph Repository[\"opengeoaimodelshub Repository\"]\n        subgraph ExampleModel[\"examplemodel/ Directory\"]\n            MLProject[\"MLproject\"]\n            TrainPy[\"train.py\"]\n            InferencePy[\"inference.py\"]\n            ModelPy[\"model.py\"]\n            PyProject[\"pyproject.toml\"]\n            SrcDir[\"src/ package\"]\n        end\n        \n        subgraph Infra[\"infra/ Directory\"]\n            DockerCompose[\"docker-compose.yml\"]\n            SetupSh[\"setup.sh\"]\n            EnvTemplate[\".env.template\"]\n            ManageSh[\"manage.sh\"]\n        end\n        \n        RootReadme[\"README.md\"]\n        License[\"LICENSE\"]\n    end\n    \n    MLProject --\u003e|\"defines entry points for\"| TrainPy\n    MLProject --\u003e|\"defines entry points for\"| InferencePy\n    TrainPy --\u003e|\"imports from\"| SrcDir\n    InferencePy --\u003e|\"imports from\"| SrcDir\n    PyProject --\u003e|\"defines dependencies for\"| SrcDir\n    \n    SetupSh --\u003e|\"deploys services in\"| DockerCompose\n    EnvTemplate --\u003e|\"provides configuration for\"| DockerCompose\n    ManageSh --\u003e|\"manages services in\"| DockerCompose\n    \n    RootReadme --\u003e|\"references\"| ExampleModel\n    RootReadme --\u003e|\"references\"| Infra\n```\n\n**Diagram: Repository File Structure and Dependencies**\n\n**Sources:** [README.md:1-9](), [examplemodel/README.md:1-52](), [infra/Readme.md:1-83]()\n\n## System Architecture\n\nThe repository implements a layered architecture where the Example Model System operates as a client to the Infrastructure System. The Infrastructure System provides core MLOps services (experiment tracking, artifact storage, model registry), while the Example Model System demonstrates how to build and deploy a geospatial ML model using these services.\n\n```mermaid\ngraph LR\n    subgraph External[\"External Data Sources\"]\n        OAM[\"OpenAerialMap API\u003cbr/\u003e(Satellite Imagery)\"]\n        OSM[\"OpenStreetMap API\u003cbr/\u003e(Building Labels)\"]\n    end\n    \n    subgraph ExampleModelSystem[\"Example Model System\"]\n        direction TB\n        MLProjectFile[\"MLproject\u003cbr/\u003e(Entry Point Definitions)\"]\n        \n        subgraph PipelineEntries[\"Pipeline Entry Points\"]\n            Preprocess[\"preprocess\u003cbr/\u003e(Data Acquisition)\"]\n            Train[\"train\u003cbr/\u003e(Model Training)\"]\n            Inference[\"inference\u003cbr/\u003e(Prediction)\"]\n            ValidateStac[\"validate_stac\u003cbr/\u003e(Metadata Validation)\"]\n            Stac2Esri[\"stac2esri\u003cbr/\u003e(DLPK Generation)\"]\n        end\n        \n        subgraph CoreModules[\"Core Modules (src/)\"]\n            CampDataModule[\"CampDataModule\u003cbr/\u003e(PyTorch Lightning DataModule)\"]\n            LitRefugeeCamp[\"LitRefugeeCamp\u003cbr/\u003e(PyTorch Lightning Model)\"]\n            RefugeeCampDetector[\"RefugeeCampDetector\u003cbr/\u003e(ESRI Inference Class)\"]\n        end\n    end\n    \n    subgraph InfrastructureSystem[\"Infrastructure System\"]\n        direction TB\n        Traefik[\"Traefik Service\u003cbr/\u003e(Reverse Proxy + SSL)\"]\n        \n        subgraph CoreServices[\"Core Services\"]\n            MLflowSvc[\"mlflow Service\u003cbr/\u003e(Tracking Server)\"]\n            MinioSvc[\"minio Service\u003cbr/\u003e(S3 Object Storage)\"]\n            PostgresSvc[\"postgres Service\u003cbr/\u003e(Metadata Database)\"]\n        end\n        \n        subgraph SupportServices[\"Support Services\"]\n            HomepageSvc[\"homepage Service\u003cbr/\u003e(Dashboard)\"]\n            RustdeskSvc[\"rustdesk Service\u003cbr/\u003e(Remote Desktop)\"]\n        end\n    end\n    \n    MLProjectFile -.-\u003e|\"orchestrates\"| PipelineEntries\n    Train --\u003e|\"uses\"| CampDataModule\n    Train --\u003e|\"uses\"| LitRefugeeCamp\n    Stac2Esri --\u003e|\"packages\"| RefugeeCampDetector\n    \n    Preprocess --\u003e|\"fetches data\"| OAM\n    Preprocess --\u003e|\"fetches labels\"| OSM\n    \n    Train --\u003e|\"logs experiments to\"| MLflowSvc\n    Inference --\u003e|\"logs results to\"| MLflowSvc\n    \n    MLflowSvc --\u003e|\"stores metadata in\"| PostgresSvc\n    MLflowSvc --\u003e|\"stores artifacts in\"| MinioSvc\n    \n    Traefik --\u003e|\"routes traffic to\"| MLflowSvc\n    Traefik --\u003e|\"routes traffic to\"| MinioSvc\n    Traefik --\u003e|\"routes traffic to\"| PostgresSvc\n    \n    HomepageSvc -.-\u003e|\"monitors\"| CoreServices\n```\n\n**Diagram: System Architecture with Code Entity Mapping**\n\n**Sources:** [README.md:1-9](), [examplemodel/README.md:1-52](), [infra/Readme.md:1-83]()\n\n## Major Components\n\n### Example Model System Components\n\nThe Example Model System is located in the `examplemodel/` directory and implements a complete ML pipeline using MLflow Projects for orchestration.\n\n| Component | File/Class | Purpose |\n|-----------|------------|---------|\n| **MLflow Project Definition** | [examplemodel/MLproject]() | Defines five entry points: `preprocess`, `train`, `inference`, `validate_stac`, `stac2esri` |\n| **Training Script** | [examplemodel/train.py]() | Implements training loop with PyTorch Lightning and MLflow logging |\n| **Inference Script** | [examplemodel/inference.py]() | Performs model inference and generates prediction overlays |\n| **Model Architecture** | [examplemodel/src/model.py]() | Defines `LitRefugeeCamp` (U-Net) and `CampDataModule` classes |\n| **ESRI Integration** | [examplemodel/src/esri/RefugeeCampDetector.py]() | `RefugeeCampDetector` class for ArcGIS Deep Learning Package deployment |\n| **Dependency Management** | [examplemodel/pyproject.toml]() | Declares dependencies using modern Python packaging standards |\n\n```mermaid\ngraph TB\n    subgraph MLProjectEntryPoints[\"MLproject Entry Points\"]\n        EP_Preprocess[\"preprocess\"]\n        EP_Train[\"train\"]\n        EP_Inference[\"inference\"]\n        EP_ValidateStac[\"validate_stac\"]\n        EP_Stac2Esri[\"stac2esri\"]\n    end\n    \n    subgraph PythonModules[\"Python Modules\"]\n        TrainPy[\"train.py\u003cbr/\u003emain training logic\"]\n        InferencePy[\"inference.py\u003cbr/\u003eprediction logic\"]\n        ModelPy[\"src/model.py\"]\n        EsriDir[\"src/esri/\"]\n        \n        subgraph ModelPyClasses[\"model.py Classes\"]\n            LitRefugeeCamp[\"class LitRefugeeCamp\u003cbr/\u003e(LightningModule)\"]\n            CampDataModule[\"class CampDataModule\u003cbr/\u003e(LightningDataModule)\"]\n        end\n        \n        subgraph EsriClasses[\"esri/ Classes\"]\n            RefugeeCampDetector[\"class RefugeeCampDetector\u003cbr/\u003e(ArcGIS inference)\"]\n        end\n    end\n    \n    subgraph Artifacts[\"Generated Artifacts\"]\n        CheckpointFile[\"best_model.pth\u003cbr/\u003e(PyTorch checkpoint)\"]\n        OnnxFile[\"best_model.onnx\u003cbr/\u003e(ONNX format)\"]\n        StacFile[\"stac_item.json\u003cbr/\u003e(STAC-MLM metadata)\"]\n        DlpkFile[\"best_model.dlpk\u003cbr/\u003e(ESRI package)\"]\n    end\n    \n    EP_Preprocess -.-\u003e|\"executes\"| TrainPy\n    EP_Train -.-\u003e|\"executes\"| TrainPy\n    EP_Inference -.-\u003e|\"executes\"| InferencePy\n    EP_ValidateStac -.-\u003e|\"validates\"| StacFile\n    EP_Stac2Esri -.-\u003e|\"packages\"| DlpkFile\n    \n    TrainPy --\u003e|\"instantiates\"| LitRefugeeCamp\n    TrainPy --\u003e|\"instantiates\"| CampDataModule\n    TrainPy --\u003e|\"produces\"| CheckpointFile\n    TrainPy --\u003e|\"exports to\"| OnnxFile\n    TrainPy --\u003e|\"generates\"| StacFile\n    \n    EP_Stac2Esri --\u003e|\"bundles\"| RefugeeCampDetector\n    EP_Stac2Esri --\u003e|\"bundles\"| OnnxFile\n    EP_Stac2Esri --\u003e|\"bundles\"| StacFile\n    EP_Stac2Esri --\u003e|\"into\"| DlpkFile\n```\n\n**Diagram: Example Model Component Mapping (Natural Language to Code Entities)**\n\n**Sources:** [examplemodel/README.md:1-52]()\n\n### Infrastructure System Components\n\nThe Infrastructure System is located in the `infra/` directory and provides a complete MLOps stack managed by Docker Compose.\n\n| Service | Docker Image | Purpose | Exposed Subdomain |\n|---------|--------------|---------|-------------------|\n| **traefik** | `traefik:v3.2` | Reverse proxy with automatic SSL via Let's Encrypt | `traefik.yourdomain.com` |\n| **mlflow** | `ghcr.io/\u003cuser\u003e/mlflow:latest` | Experiment tracking and model registry | `mlflow.yourdomain.com` |\n| **minio** | `minio/minio:latest` | S3-compatible object storage for ML artifacts | `minio.yourdomain.com`, `minio-api.yourdomain.com` |\n| **postgres** | `postgis/postgis:latest` | PostgreSQL with PostGIS for metadata and geospatial data | `postgres.yourdomain.com` |\n| **homepage** | `ghcr.io/gethomepage/homepage:latest` | Service monitoring dashboard | `yourdomain.com` |\n| **rustdesk** | `rustdesk/rustdesk-server:latest` | Remote desktop server | `rustdesk.yourdomain.com` |\n\n```mermaid\ngraph TB\n    subgraph DockerComposeServices[\"docker-compose.yml Services\"]\n        TraefikService[\"service: traefik\u003cbr/\u003eimage: traefik:v3.2\u003cbr/\u003eports: 80, 443\"]\n        MlflowService[\"service: mlflow\u003cbr/\u003eimage: ghcr.io/.../mlflow:latest\u003cbr/\u003edepends_on: postgres, minio\"]\n        MinioService[\"service: minio\u003cbr/\u003eimage: minio/minio:latest\u003cbr/\u003ecommand: server /data --console-address :9001\"]\n        PostgresService[\"service: postgres\u003cbr/\u003eimage: postgis/postgis:latest\u003cbr/\u003eenvironment: POSTGRES_DB=mlflow\"]\n        HomepageService[\"service: homepage\u003cbr/\u003eimage: ghcr.io/gethomepage/homepage:latest\"]\n        RustdeskService[\"service: rustdesk\u003cbr/\u003eimage: rustdesk/rustdesk-server:latest\"]\n    end\n    \n    subgraph DockerVolumes[\"Docker Volumes\"]\n        TraefikData[\"traefik-data\u003cbr/\u003e(SSL certificates)\"]\n        MinioData[\"minio\u003cbr/\u003e(ML artifacts)\"]\n        PostgresData[\"postgres\u003cbr/\u003e(experiment metadata)\"]\n        RustdeskData[\"rustdesk\u003cbr/\u003e(desktop sessions)\"]\n    end\n    \n    subgraph ManagementScripts[\"Management Scripts\"]\n        SetupScript[\"setup.sh\u003cbr/\u003e(initial deployment)\"]\n        ManageScript[\"manage.sh\u003cbr/\u003e(operations)\"]\n        EnvFile[\".env\u003cbr/\u003e(configuration)\"]\n    end\n    \n    TraefikService --\u003e|\"routes to\"| MlflowService\n    TraefikService --\u003e|\"routes to\"| MinioService\n    TraefikService --\u003e|\"routes to\"| PostgresService\n    TraefikService --\u003e|\"routes to\"| HomepageService\n    \n    MlflowService --\u003e|\"reads/writes metadata\"| PostgresService\n    MlflowService --\u003e|\"stores artifacts\"| MinioService\n    \n    TraefikService -.-\u003e|\"persists to\"| TraefikData\n    MinioService -.-\u003e|\"persists to\"| MinioData\n    PostgresService -.-\u003e|\"persists to\"| PostgresData\n    RustdeskService -.-\u003e|\"persists to\"| RustdeskData\n    \n    SetupScript --\u003e|\"reads config from\"| EnvFile\n    SetupScript --\u003e|\"starts\"| DockerComposeServices\n    ManageScript --\u003e|\"controls\"| DockerComposeServices\n```\n\n**Diagram: Infrastructure Component Mapping (Docker Services and Scripts)**\n\n**Sources:** [infra/Readme.md:1-83]()\n\n## Key Technologies and Standards\n\nThe repository leverages modern tooling and industry standards to ensure reproducibility, interoperability, and production readiness:\n\n### ML Pipeline Technologies\n\n- **MLflow Projects**: Declarative pipeline orchestration with reproducible entry points\n- **PyTorch Lightning**: Structured deep learning framework with `LightningModule` and `LightningDataModule` abstractions\n- **ONNX**: Model export format for cross-platform inference and deployment\n- **STAC-MLM**: SpatioTemporal Asset Catalog Machine Learning Model extension for metadata standardization\n- **ESRI DLPK**: Deep Learning Package format for ArcGIS Pro integration\n\n### Infrastructure Technologies\n\n- **Docker Compose**: Multi-container orchestration for service deployment\n- **Traefik**: Modern reverse proxy with automatic HTTPS via Let's Encrypt ACME protocol\n- **MinIO**: High-performance S3-compatible object storage implementing the AWS S3 API\n- **PostgreSQL + PostGIS**: Relational database with geospatial extensions supporting spatial queries\n- **uv**: Fast Python package installer and resolver for dependency management\n\n### Standards Compliance\n\nThe repository demonstrates compliance with geospatial AI standards:\n\n- **STAC-MLM Extension**: Model metadata follows the STAC Machine Learning Model extension specification\n- **OGC Standards**: Integration with OpenStreetMap (OGC-compliant) for label acquisition\n- **S3 API**: Artifact storage implements the Amazon S3 API via MinIO for cloud compatibility\n- **MLflow Tracking API**: Experiment logging follows MLflow's standard tracking protocol\n\n**Sources:** [examplemodel/README.md:1-52](), [infra/Readme.md:1-83]()\n\n## Integration Workflow\n\nThe Example Model System and Infrastructure System integrate through MLflow's client-server architecture. During training, the model code (client) logs experiments to the MLflow server, which persists metadata in PostgreSQL and artifacts in MinIO.\n\n```mermaid\nsequenceDiagram\n    participant Dev as \"Developer\"\n    participant UV as \"uv (Package Manager)\"\n    participant MLProject as \"MLproject\"\n    participant Train as \"train.py\"\n    participant MLflowClient as \"MLflow Client API\"\n    participant MLflowServer as \"mlflow Service\"\n    participant Postgres as \"postgres Service\"\n    participant Minio as \"minio Service\"\n    \n    Dev-\u003e\u003eUV: \"uv sync\"\n    UV--\u003e\u003eDev: \"Dependencies installed\"\n    \n    Dev-\u003e\u003eMLProject: \"uv run mlflow run . -e train\"\n    MLProject-\u003e\u003eTrain: \"Execute training entry point\"\n    \n    Train-\u003e\u003eMLflowClient: \"mlflow.start_run()\"\n    MLflowClient-\u003e\u003eMLflowServer: \"POST /api/2.0/mlflow/runs/create\"\n    MLflowServer-\u003e\u003ePostgres: \"INSERT run metadata\"\n    MLflowServer--\u003e\u003eMLflowClient: \"run_id\"\n    \n    Train-\u003e\u003eTrain: \"Training loop (epochs)\"\n    \n    loop \"Each epoch\"\n        Train-\u003e\u003eMLflowClient: \"mlflow.log_metrics()\"\n        MLflowClient-\u003e\u003eMLflowServer: \"POST /api/2.0/mlflow/runs/log-metric\"\n        MLflowServer-\u003e\u003ePostgres: \"INSERT metrics\"\n    end\n    \n    Train-\u003e\u003eTrain: \"Save best_model.pth\"\n    Train-\u003e\u003eMLflowClient: \"mlflow.log_artifact()\"\n    MLflowClient-\u003e\u003eMLflowServer: \"POST /api/2.0/mlflow/artifacts\"\n    MLflowServer-\u003e\u003eMinio: \"PUT s3://mlflow/artifacts/...\"\n    \n    Train-\u003e\u003eMLflowClient: \"mlflow.pytorch.log_model()\"\n    MLflowClient-\u003e\u003eMLflowServer: \"POST /api/2.0/mlflow/model-versions/create\"\n    MLflowServer-\u003e\u003ePostgres: \"INSERT model version\"\n    MLflowServer-\u003e\u003eMinio: \"PUT s3://mlflow/models/...\"\n    \n    MLflowServer--\u003e\u003eDev: \"Training complete (via logs)\"\n```\n\n**Diagram: Integration Workflow Showing API Interactions**\n\n**Sources:** [examplemodel/README.md:40-48](), [infra/Readme.md:5-12]()\n\n## Deployment Outputs\n\nThe training pipeline produces multiple deployment-ready artifacts, each targeting different deployment scenarios:\n\n| Artifact | Format | Use Case | Generated By |\n|----------|--------|----------|--------------|\n| `best_model.pth` | PyTorch checkpoint | Direct PyTorch inference, fine-tuning | [train.py]() |\n| `best_model.onnx` | ONNX format | Cross-platform inference (ONNX Runtime) | [train.py]() with `torch.onnx.export()` |\n| `best_model.dlpk` | ESRI Deep Learning Package | ArcGIS Pro deep learning tools | [stac2esri]() entry point |\n| `stac_item.json` | STAC-MLM JSON | Model discovery and metadata exchange | [train.py]() |\n\nThese artifacts are stored in MinIO object storage and tracked in the MLflow Model Registry, enabling versioning, lineage tracking, and collaborative model management.\n\n**Sources:** [examplemodel/README.md:1-52]()\n\n## License\n\nThe repository is released under the MIT License, permitting commercial and non-commercial use with minimal restrictions. See [LICENSE:1-22]() for full terms.\n\n**Sources:** [LICENSE:1-22]()\n\n## Additional Resources\n\nFor more detailed information on specific subsystems:\n\n- **Setting up the infrastructure**: See [Infrastructure Deployment](#6.1)\n- **Training the example model**: See [Training Pipeline](#3.2)\n- **Deploying models to ArcGIS**: See [ESRI Integration and DLPK Generation](#3.4)\n- **Understanding MLflow Projects**: See [MLflow Project Structure](#3.5)\n- **Configuring services**: See [Configuration Management](#4.3)\n- **Project background and research**: See [Research and Background](#8.3) and slides at https://gamma.app/docs/Internship-Presentation-3-min-xsuj64t3h0kyow4\n\n**Sources:** [README.md:1-9]()"])</script><script>self.__next_f.push([1,"18:T3293,"])</script><script>self.__next_f.push([1,"# Getting Started\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/README.md](examplemodel/README.md)\n- [infra/Readme.md](infra/Readme.md)\n- [infra/install-docker.sh](infra/install-docker.sh)\n\n\u003c/details\u003e\n\n\n\nThis page provides an overview of initial setup procedures for the OpenGeoAIModelHub repository. The repository contains two independent but complementary systems: an **Example Model System** for refugee camp detection and an **Infrastructure System** for production MLOps deployment. This guide introduces the setup requirements and workflows for both systems at a high level.\n\nFor detailed installation instructions, see [Prerequisites and Installation](#2.1). For step-by-step execution procedures, see [Quick Start Guide](#2.2). For comprehensive documentation of the example model, see [Example Model System](#3). For infrastructure deployment details, see [Infrastructure System](#4).\n\n## Repository Structure\n\nThe repository is organized into two primary directories, each containing a complete system with its own dependencies, configuration, and deployment procedures:\n\n```mermaid\ngraph TB\n    ROOT[\"OpenGeoAIModelHub Repository\"]\n    \n    ROOT --\u003e EXAMPLE[\"examplemodel/\u003cbr/\u003eExample Model System\"]\n    ROOT --\u003e INFRA[\"infra/\u003cbr/\u003eInfrastructure System\"]\n    ROOT --\u003e ROOT_FILES[\"Root Configuration\"]\n    \n    EXAMPLE --\u003e EM_SRC[\"src/\u003cbr/\u003eSource Code\"]\n    EXAMPLE --\u003e EM_MLPROJ[\"MLproject\u003cbr/\u003eEntry Points\"]\n    EXAMPLE --\u003e EM_PYPROJ[\"pyproject.toml\u003cbr/\u003eDependencies\"]\n    EXAMPLE --\u003e EM_README[\"README.md\u003cbr/\u003eModel Docs\"]\n    \n    EM_SRC --\u003e TRAIN_PY[\"train.py\u003cbr/\u003eTraining Pipeline\"]\n    EM_SRC --\u003e INFERENCE_PY[\"inference.py\u003cbr/\u003eInference System\"]\n    EM_SRC --\u003e MODEL_PY[\"model.py\u003cbr/\u003eU-Net Architecture\"]\n    EM_SRC --\u003e ESRI_DIR[\"esri/\u003cbr/\u003eDLPK Generation\"]\n    \n    INFRA --\u003e COMPOSE[\"docker-compose.yml\u003cbr/\u003eService Stack\"]\n    INFRA --\u003e SETUP[\"setup.sh\u003cbr/\u003eDeployment Script\"]\n    INFRA --\u003e ENV_TMPL[\".env.template\u003cbr/\u003eConfiguration\"]\n    INFRA --\u003e MANAGE[\"manage.sh\u003cbr/\u003eOperations\"]\n    INFRA --\u003e INFRA_README[\"Readme.md\u003cbr/\u003eInfra Docs\"]\n    INFRA --\u003e INSTALL_DOCKER[\"install-docker.sh\u003cbr/\u003eDocker Setup\"]\n    \n    ROOT_FILES --\u003e README[\"README.md\u003cbr/\u003eMain Documentation\"]\n    ROOT_FILES --\u003e GIT_DIR[\".github/\u003cbr/\u003eCI/CD Workflows\"]\n```\n\n**Sources:** [infra/Readme.md:1-83](), [examplemodel/README.md:1-52]()\n\n## System Architecture Overview\n\nThe two systems serve different but complementary purposes and can be used independently or together:\n\n```mermaid\ngraph LR\n    subgraph \"Example Model System\"\n        EM_TRAIN[\"train.py\u003cbr/\u003eTraining Pipeline\"]\n        EM_INFER[\"inference.py\u003cbr/\u003ePrediction System\"]\n        EM_MLPROJ[\"MLproject\u003cbr/\u003eOrchestration\"]\n    end\n    \n    subgraph \"Infrastructure System\"\n        INFRA_MLFLOW[\"MLflow Service\u003cbr/\u003eExperiment Tracking\"]\n        INFRA_MINIO[\"MinIO Service\u003cbr/\u003eArtifact Storage\"]\n        INFRA_PG[\"PostgreSQL Service\u003cbr/\u003eMetadata Store\"]\n    end\n    \n    subgraph \"Local Development\"\n        UV[\"uv Package Manager\u003cbr/\u003eDependency Install\"]\n        MLFLOW_LOCAL[\"mlflow ui\u003cbr/\u003eLocal Tracking\"]\n    end\n    \n    subgraph \"External Data\"\n        OAM[\"OpenAerialMap\u003cbr/\u003eSatellite Imagery\"]\n        OSM[\"OpenStreetMap\u003cbr/\u003eLabel Data\"]\n    end\n    \n    UV --\u003e EM_TRAIN\n    EM_MLPROJ --\u003e EM_TRAIN\n    EM_MLPROJ --\u003e EM_INFER\n    \n    EM_TRAIN -.-\u003e|\"can log to\"| MLFLOW_LOCAL\n    EM_TRAIN -.-\u003e|\"or log to\"| INFRA_MLFLOW\n    \n    INFRA_MLFLOW --\u003e INFRA_PG\n    INFRA_MLFLOW --\u003e INFRA_MINIO\n    \n    EM_TRAIN --\u003e OAM\n    EM_TRAIN --\u003e OSM\n```\n\n| System | Purpose | Primary Use Case | Deployment Target |\n|--------|---------|-----------------|-------------------|\n| Example Model | ML model development and training | Local experimentation, model development | Standalone training environment |\n| Infrastructure | Production MLOps platform | Multi-user experiment tracking, model registry | Server/cloud deployment |\n\n**Sources:** [infra/Readme.md:1-83](), [examplemodel/README.md:1-52]()\n\n## Setup Workflows\n\nThere are two primary setup workflows depending on your objectives:\n\n### Workflow Diagram: Setup Decision Tree\n\n```mermaid\ngraph TD\n    START[\"Clone Repository\"]\n    \n    START --\u003e CHOOSE{\"What do you\u003cbr/\u003ewant to set up?\"}\n    \n    CHOOSE --\u003e|\"Train model locally\"| MODEL_PATH[\"Example Model Setup\"]\n    CHOOSE --\u003e|\"Deploy infrastructure\"| INFRA_PATH[\"Infrastructure Setup\"]\n    CHOOSE --\u003e|\"Both\"| BOTH[\"Sequential Setup\"]\n    \n    MODEL_PATH --\u003e UV_INSTALL[\"Install uv\u003cbr/\u003ecurl -LsSf astral.sh/uv/install.sh\"]\n    UV_INSTALL --\u003e UV_SYNC[\"uv sync\u003cbr/\u003ein examplemodel/\"]\n    UV_SYNC --\u003e MLFLOW_UI[\"uv run mlflow ui\u003cbr/\u003eLocal tracking\"]\n    MLFLOW_UI --\u003e PREPROCESS[\"uv run mlflow run . -e preprocess\"]\n    PREPROCESS --\u003e TRAIN[\"uv run mlflow run . -e train\"]\n    \n    INFRA_PATH --\u003e DOCKER_CHECK{\"Docker\u003cbr/\u003einstalled?\"}\n    DOCKER_CHECK --\u003e|\"No\"| DOCKER_INSTALL[\"./infra/install-docker.sh\"]\n    DOCKER_CHECK --\u003e|\"Yes\"| ENV_CONFIG\n    DOCKER_INSTALL --\u003e ENV_CONFIG[\"cp .env.template .env\u003cbr/\u003eConfigure domain/credentials\"]\n    ENV_CONFIG --\u003e DNS_SETUP[\"Configure DNS A records\"]\n    DNS_SETUP --\u003e RUN_SETUP[\"./infra/setup.sh\"]\n    \n    BOTH --\u003e MODEL_PATH\n    BOTH --\u003e INFRA_PATH\n```\n\n**Sources:** [infra/Readme.md:14-30](), [examplemodel/README.md:7-38](), [infra/install-docker.sh:1-50]()\n\n### Example Model Setup Path\n\nThe example model system runs entirely on your local machine using `uv` for dependency management and can log experiments either locally or to a remote MLflow server.\n\n**Key Steps:**\n1. Install `uv` package manager\n2. Run `uv sync` in `examplemodel/` directory to install dependencies from `pyproject.toml`\n3. Execute MLflow entry points defined in `MLproject` file\n4. Access results via local MLflow UI or remote infrastructure\n\n**Key Files:**\n- [examplemodel/pyproject.toml]() - Python dependencies\n- [examplemodel/MLproject]() - Entry point definitions\n- [examplemodel/src/train.py]() - Training pipeline\n- [examplemodel/src/inference.py]() - Inference pipeline\n\n**Sources:** [examplemodel/README.md:7-38]()\n\n### Infrastructure Setup Path\n\nThe infrastructure system deploys a multi-service Docker Compose stack on a server with automatic SSL certificates, requiring domain name configuration and Docker installation.\n\n**Key Steps:**\n1. Install Docker and Docker Compose (via `install-docker.sh` if needed)\n2. Configure `.env` file from `.env.template` with domain and credentials\n3. Set up DNS A records for all service subdomains\n4. Run `setup.sh` to deploy the stack\n5. Access services via configured subdomains\n\n**Key Files:**\n- [infra/docker-compose.yml]() - Service definitions\n- [infra/.env.template]() - Configuration template\n- [infra/setup.sh]() - Automated deployment\n- [infra/manage.sh]() - Operations management\n\n**Sources:** [infra/Readme.md:14-30]()\n\n## Prerequisites Summary\n\n### For Example Model System\n\n| Requirement | Purpose | Installation Method |\n|------------|---------|-------------------|\n| `uv` package manager | Python dependency management | `curl -LsSf https://astral.sh/uv/install.sh \\| sh` |\n| Python 3.10+ | Runtime environment | System package manager |\n| Git | Repository cloning | System package manager |\n\n### For Infrastructure System\n\n| Requirement | Purpose | Installation Method |\n|------------|---------|-------------------|\n| Docker Engine | Container runtime | [infra/install-docker.sh]() or manual |\n| Docker Compose | Multi-container orchestration | Included with Docker Engine |\n| Domain name | SSL certificates and routing | DNS provider |\n| Server with public IP | Service hosting | Cloud provider or on-premise |\n\n**Detailed installation instructions are provided in [Prerequisites and Installation](#2.1).**\n\n**Sources:** [examplemodel/README.md:7-12](), [infra/Readme.md:14-30](), [infra/install-docker.sh:1-50]()\n\n## Configuration Overview\n\n### Example Model Configuration\n\nThe example model uses environment variables for remote tracking server configuration:\n\n```mermaid\ngraph LR\n    ENV_VARS[\"Environment Variables\"]\n    PYPROJECT[\"pyproject.toml\u003cbr/\u003eDependencies\"]\n    MLPROJECT[\"MLproject\u003cbr/\u003eEntry Points\"]\n    \n    ENV_VARS --\u003e TRAIN[\"train.py\"]\n    PYPROJECT --\u003e UV[\"uv sync\"]\n    UV --\u003e DEPS[\"Installed Dependencies\"]\n    DEPS --\u003e TRAIN\n    MLPROJECT --\u003e TRAIN\n    \n    ENV_VARS -.-\u003e|\"AWS_ACCESS_KEY_ID\"| MINIO_AUTH[\"MinIO Authentication\"]\n    ENV_VARS -.-\u003e|\"AWS_SECRET_ACCESS_KEY\"| MINIO_AUTH\n    ENV_VARS -.-\u003e|\"MLFLOW_S3_ENDPOINT_URL\"| MINIO_AUTH\n    ENV_VARS -.-\u003e|\"MLFLOW_TRACKING_URI\"| MLFLOW_CONN[\"MLflow Connection\"]\n```\n\n**Key Environment Variables:**\n- `AWS_ACCESS_KEY_ID` - MinIO/S3 access key for artifact storage\n- `AWS_SECRET_ACCESS_KEY` - MinIO/S3 secret key\n- `MLFLOW_S3_ENDPOINT_URL` - MinIO endpoint URL (e.g., `http://minio.yourdomain.com:9000`)\n- `MLFLOW_TRACKING_URI` - MLflow server URL (optional for remote logging)\n\n**Sources:** [examplemodel/README.md:40-48]()\n\n### Infrastructure Configuration\n\nThe infrastructure stack uses a single `.env` file to configure all services:\n\n```mermaid\ngraph TB\n    ENV_TEMPLATE[\".env.template\u003cbr/\u003eConfiguration Template\"]\n    ENV_FILE[\".env\u003cbr/\u003eActual Configuration\"]\n    \n    ENV_TEMPLATE --\u003e|\"cp command\"| ENV_FILE\n    \n    ENV_FILE --\u003e DOMAIN[\"DOMAIN\u003cbr/\u003eyourdomain.com\"]\n    ENV_FILE --\u003e EMAIL[\"EMAIL\u003cbr/\u003eletsencrypt@email.com\"]\n    ENV_FILE --\u003e CREDS[\"Service Credentials\u003cbr/\u003ePasswords, Keys\"]\n    \n    DOMAIN --\u003e TRAEFIK[\"traefik service\u003cbr/\u003eRouting Rules\"]\n    EMAIL --\u003e TRAEFIK\n    CREDS --\u003e MLFLOW[\"mlflow service\"]\n    CREDS --\u003e MINIO[\"minio service\"]\n    CREDS --\u003e POSTGRES[\"postgres service\"]\n    \n    TRAEFIK --\u003e SSL[\"Let's Encrypt\u003cbr/\u003eSSL Certificates\"]\n```\n\n**Required DNS Records:**\n- `yourdomain.com`  Homepage dashboard\n- `mlflow.yourdomain.com`  MLflow tracking server\n- `minio.yourdomain.com`  MinIO console\n- `minio-api.yourdomain.com`  MinIO S3 API\n- `postgres.yourdomain.com`  PostgreSQL database\n- `rustdesk.yourdomain.com`  RustDesk server\n- `traefik.yourdomain.com`  Traefik dashboard\n\n**Sources:** [infra/Readme.md:32-41]()\n\n## Service Management\n\nOnce the infrastructure is deployed, services are managed through two mechanisms:\n\n### Management Script\n\nThe `manage.sh` script provides operational commands:\n\n```mermaid\ngraph LR\n    MANAGE[\"./manage.sh\"]\n    \n    MANAGE --\u003e STATUS[\"status\u003cbr/\u003eCheck service health\"]\n    MANAGE --\u003e LOGS[\"logs \u003cservice\u003e\u003cbr/\u003eView service logs\"]\n    MANAGE --\u003e RESTART[\"restart \u003cservice\u003e\u003cbr/\u003eRestart specific service\"]\n    MANAGE --\u003e UPDATE[\"update\u003cbr/\u003ePull latest images\"]\n    MANAGE --\u003e BACKUP[\"backup\u003cbr/\u003eCreate data backup\"]\n```\n\n**Sources:** [infra/Readme.md:54-60]()\n\n### Systemd Integration\n\nThe `setup.sh` script optionally creates a systemd service for automatic startup:\n\n| Command | Function |\n|---------|----------|\n| `sudo systemctl start tech-infra` | Start all services |\n| `sudo systemctl stop tech-infra` | Stop all services |\n| `sudo systemctl status tech-infra` | Check service status |\n| `sudo systemctl enable tech-infra` | Enable auto-start on boot |\n\n**Sources:** [infra/Readme.md:62-68]()\n\n## Integration Patterns\n\n### Local Development with Remote Infrastructure\n\nThe example model can be trained locally while logging to remote infrastructure:\n\n```mermaid\nsequenceDiagram\n    participant DEV as \"Local Machine\"\n    participant TRAIN as \"train.py\"\n    participant MLFLOW as \"mlflow.yourdomain.com\"\n    participant MINIO as \"minio-api.yourdomain.com\"\n    participant PG as \"postgres.yourdomain.com\"\n    \n    DEV-\u003e\u003eDEV: \"export MLFLOW_TRACKING_URI=https://mlflow.yourdomain.com\"\n    DEV-\u003e\u003eDEV: \"export MLFLOW_S3_ENDPOINT_URL=http://minio-api.yourdomain.com:9000\"\n    DEV-\u003e\u003eDEV: \"uv run mlflow run . -e train\"\n    \n    TRAIN-\u003e\u003eMLFLOW: \"mlflow.start_run()\"\n    MLFLOW-\u003e\u003ePG: \"Create run entry in metadata store\"\n    \n    loop \"Training Loop\"\n        TRAIN-\u003e\u003eMLFLOW: \"mlflow.log_metric()\"\n        MLFLOW-\u003e\u003ePG: \"Store metrics\"\n    end\n    \n    TRAIN-\u003e\u003eMLFLOW: \"mlflow.log_artifact()\"\n    MLFLOW-\u003e\u003eMINIO: \"Upload artifacts to S3 storage\"\n    \n    DEV-\u003e\u003eMLFLOW: \"View experiments in browser\"\n```\n\n**Sources:** [examplemodel/README.md:40-48](), [infra/Readme.md:5-12]()\n\n## Next Steps\n\nAfter understanding the repository structure and setup overview:\n\n1. **Install Prerequisites** - Follow [Prerequisites and Installation](#2.1) for detailed installation instructions for both `uv` and Docker\n2. **Execute Quick Start** - Follow [Quick Start Guide](#2.2) for step-by-step commands to run your first training or deploy infrastructure\n3. **Explore the Model** - Read [Example Model System](#3) for comprehensive documentation of the ML pipeline\n4. **Deploy Infrastructure** - Read [Infrastructure System](#4) for detailed service configuration and management\n5. **Customize** - See [Development Guide](#7) for instructions on extending functionality and working with custom datasets\n\n**Sources:** [infra/Readme.md:1-83](), [examplemodel/README.md:1-52]()"])</script><script>self.__next_f.push([1,"19:T3c55,"])</script><script>self.__next_f.push([1,"# Prerequisites and Installation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/pyproject.toml](examplemodel/pyproject.toml)\n- [examplemodel/uv.lock](examplemodel/uv.lock)\n- [infra/install-docker.sh](infra/install-docker.sh)\n\n\u003c/details\u003e\n\n\n\nThis document details the system requirements and installation procedures for the OpenGeoAIModelHub repository. It covers infrastructure prerequisites (Docker, Docker Compose), development environment setup (Python, uv package manager), and dependency installation for both the example model and infrastructure stack.\n\nFor instructions on running the example model training or deploying the infrastructure stack after installation, see [Quick Start Guide](#2.2).\n\n---\n\n## Purpose and Scope\n\nThis page covers:\n- **System requirements**: Operating system, hardware, and software prerequisites\n- **Docker installation**: Installing Docker Engine and Docker Compose for infrastructure deployment\n- **Python environment setup**: Installing Python and the uv package manager\n- **Dependency installation**: Installing Python dependencies for the example model\n\nThis page does NOT cover:\n- Running the example model training (see [Quick Start Guide](#2.2))\n- Infrastructure deployment procedures (see [Infrastructure Deployment](#6.1))\n- Development workflows (see [Development Guide](#7))\n\n---\n\n## System Requirements\n\n### Operating System\n\nThe OpenGeoAIModelHub supports the following operating systems:\n\n| Component | Supported OS | Notes |\n|-----------|--------------|-------|\n| Infrastructure Stack | Linux (Debian-based recommended) | Docker Compose stack tested on Debian/Ubuntu |\n| Example Model | Linux, macOS, Windows | PyTorch supports all major platforms |\n| Docker | Linux, macOS, Windows | WSL2 required for Windows |\n\n### Hardware Requirements\n\n| Component | Minimum | Recommended |\n|-----------|---------|-------------|\n| CPU | 4 cores | 8+ cores |\n| RAM | 8 GB | 16+ GB (32+ GB for training) |\n| Storage | 20 GB | 50+ GB (SSD recommended) |\n| GPU | None (CPU training supported) | NVIDIA GPU with CUDA support for training |\n\n### Software Prerequisites\n\nThe following software must be installed before proceeding:\n\n| Software | Version | Purpose |\n|----------|---------|---------|\n| Docker Engine | 20.10+ | Container runtime for infrastructure stack |\n| Docker Compose | 2.0+ | Multi-container orchestration |\n| Python | 3.10+ | Example model runtime |\n| Git | Any recent | Repository cloning |\n| curl | Any recent | Downloading installation scripts |\n\n**Sources:** [examplemodel/pyproject.toml:6]()\n\n---\n\n## Installation Workflow Overview\n\n```mermaid\nflowchart TB\n    START[\"Start Installation\"]\n    CHECK_OS{\"Operating System?\"}\n    \n    INSTALL_DOCKER[\"Install Docker Engine\u003cbr/\u003e+ Docker Compose\"]\n    INSTALL_PYTHON[\"Install Python 3.10+\"]\n    INSTALL_UV[\"Install uv Package Manager\"]\n    CLONE_REPO[\"Clone Repository\u003cbr/\u003egit clone \u003crepo-url\u003e\"]\n    \n    SYNC_DEPS[\"Install Dependencies\u003cbr/\u003euv sync\"]\n    VERIFY[\"Verify Installation\u003cbr/\u003edocker --version\u003cbr/\u003euv --version\u003cbr/\u003epython --version\"]\n    \n    COMPLETE[\"Installation Complete\"]\n    \n    START --\u003e CHECK_OS\n    CHECK_OS --\u003e|\"Linux (Debian)\"| INSTALL_DOCKER\n    CHECK_OS --\u003e|\"Other OS\"| INSTALL_DOCKER\n    \n    INSTALL_DOCKER --\u003e INSTALL_PYTHON\n    INSTALL_PYTHON --\u003e INSTALL_UV\n    INSTALL_UV --\u003e CLONE_REPO\n    CLONE_REPO --\u003e SYNC_DEPS\n    SYNC_DEPS --\u003e VERIFY\n    VERIFY --\u003e COMPLETE\n    \n    style INSTALL_DOCKER fill:#f0f0f0\n    style SYNC_DEPS fill:#f0f0f0\n    style COMPLETE fill:#e8f5e9\n```\n\n**Sources:** [infra/install-docker.sh:1-50](), [examplemodel/pyproject.toml:1-31]()\n\n---\n\n## Docker Installation\n\nDocker and Docker Compose are required to run the infrastructure stack (MLflow, MinIO, PostgreSQL, Traefik).\n\n### Automated Installation (Debian/Ubuntu)\n\nThe repository provides an automated installation script for Debian-based systems:\n\n```bash\n# Download and execute the installation script\ncurl -fsSL https://raw.githubusercontent.com/kshitijrajsharma/opengeoaimodelshub/main/infra/install-docker.sh | bash\n\n# Or clone the repository first and run locally\ngit clone https://github.com/kshitijrajsharma/opengeoaimodelshub.git\ncd opengeoaimodelshub\nchmod +x infra/install-docker.sh\n./infra/install-docker.sh\n```\n\nThe script performs the following operations:\n\n1. Updates system packages (`apt update \u0026\u0026 apt upgrade`)\n2. Installs Docker prerequisites (apt-transport-https, ca-certificates, curl, gnupg, lsb-release)\n3. Adds Docker's official GPG key to `/etc/apt/keyrings/docker.gpg`\n4. Configures Docker's apt repository\n5. Installs `docker-ce`, `docker-ce-cli`, `containerd.io`, `docker-buildx-plugin`, `docker-compose-plugin`\n6. Enables and starts the Docker service\n7. Adds the current user to the `docker` group\n\n**Important:** After installation, you must log out and log back in (or restart) for Docker group permissions to take effect.\n\n**Sources:** [infra/install-docker.sh:1-50]()\n\n### Manual Installation\n\nFor other operating systems, follow the official Docker documentation:\n- **Linux**: https://docs.docker.com/engine/install/\n- **macOS**: Install Docker Desktop from https://www.docker.com/products/docker-desktop\n- **Windows**: Install Docker Desktop with WSL2 backend from https://docs.docker.com/desktop/install/windows-install/\n\n### Verification\n\nVerify Docker and Docker Compose installation:\n\n```bash\n# Check Docker version (should be 20.10+)\ndocker --version\n\n# Check Docker Compose version (should be 2.0+)\ndocker compose version\n\n# Test Docker installation\ndocker run hello-world\n```\n\n**Sources:** [infra/install-docker.sh:35-48]()\n\n---\n\n## Python Environment Setup\n\n### Python Installation\n\nThe example model requires **Python 3.10 or later**. Check your Python version:\n\n```bash\npython3 --version\n```\n\nIf Python 3.10+ is not installed:\n\n**Debian/Ubuntu:**\n```bash\nsudo apt update\nsudo apt install python3.10 python3.10-venv python3-pip\n```\n\n**macOS:**\n```bash\nbrew install python@3.10\n```\n\n**Windows:**\nDownload and install from https://www.python.org/downloads/\n\n**Sources:** [examplemodel/pyproject.toml:6]()\n\n### uv Package Manager Installation\n\nThe repository uses `uv` for fast, reliable Python dependency management. Install uv:\n\n```bash\n# Install uv using the official installer\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Or using pip\npip install uv\n```\n\nVerify installation:\n\n```bash\nuv --version\n```\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [examplemodel/uv.lock:1-10]()\n\n---\n\n## Dependency Installation\n\n### Repository Clone\n\nClone the OpenGeoAIModelHub repository:\n\n```bash\ngit clone https://github.com/kshitijrajsharma/opengeoaimodelshub.git\ncd opengeoaimodelshub\n```\n\n### Example Model Dependencies\n\nNavigate to the example model directory and install dependencies using uv:\n\n```bash\ncd examplemodel\n\n# Install all dependencies from pyproject.toml\nuv sync\n\n# Or install in development mode with validation tools\nuv sync --group validation\n```\n\nThis command reads `pyproject.toml` and `uv.lock` to install exact package versions.\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [examplemodel/uv.lock:1-10]()\n\n### Dependency Structure\n\n```mermaid\nflowchart TB\n    PYPROJECT[\"pyproject.toml\u003cbr/\u003eProject Definition\"]\n    UVLOCK[\"uv.lock\u003cbr/\u003eLocked Dependencies\"]\n    \n    CORE[\"Core Dependencies\"]\n    ML[\"ML Framework\"]\n    GEO[\"Geospatial Tools\"]\n    MLOPS[\"MLOps\"]\n    DEPLOYMENT[\"Deployment\"]\n    VALIDATION[\"Validation Group\"]\n    \n    PYPROJECT --\u003e UVLOCK\n    UVLOCK --\u003e CORE\n    UVLOCK --\u003e ML\n    UVLOCK --\u003e GEO\n    UVLOCK --\u003e MLOPS\n    UVLOCK --\u003e DEPLOYMENT\n    PYPROJECT -.-\u003e|\"dependency-groups\"| VALIDATION\n    \n    CORE --\u003e PSUTIL[\"psutil\u003e=7.0.0\"]\n    CORE --\u003e PYNVML[\"pynvml\u003e=12.0.0\"]\n    \n    ML --\u003e TORCH[\"torch\u003e=2.7.1\"]\n    ML --\u003e TORCHVISION[\"torchvision\u003e=0.22.1\"]\n    ML --\u003e LIGHTNING[\"pytorch-lightning\u003e=2.5.2\"]\n    \n    GEO --\u003e GEOMLTOOLKITS[\"geomltoolkits\u003e=0.3.9\"]\n    GEO --\u003e OPENCV[\"opencv-python\u003e=4.8.0\"]\n    \n    MLOPS --\u003e MLFLOW[\"mlflow\u003e=3.1.1\"]\n    MLOPS --\u003e BOTO3[\"boto3\u003e=1.39.12\"]\n    \n    DEPLOYMENT --\u003e ONNX[\"onnx\u003e=1.18.0\"]\n    DEPLOYMENT --\u003e ONNXSCRIPT[\"onnxscript\u003e=0.3.2\"]\n    DEPLOYMENT --\u003e STACMODEL[\"stac-model\u003e=0.3.0\"]\n    DEPLOYMENT --\u003e PYSTAC[\"pystac\u003e=1.8.0\"]\n    \n    VALIDATION --\u003e JSONSCHEMA[\"jsonschema\u003e=4.0.0\"]\n    VALIDATION --\u003e REQUESTS[\"requests\u003e=2.28.0\"]\n    \n    style PYPROJECT fill:#f0f0f0\n    style UVLOCK fill:#f0f0f0\n```\n\n**Sources:** [examplemodel/pyproject.toml:7-24](), [examplemodel/pyproject.toml:26-30]()\n\n### Core Dependencies by Category\n\nThe `pyproject.toml` specifies the following dependency categories:\n\n| Category | Packages | Purpose |\n|----------|----------|---------|\n| **ML Framework** | `torch`, `torchvision`, `pytorch-lightning` | Model training and inference |\n| **Geospatial** | `geomltoolkits`, `opencv-python` | Geospatial data processing and image manipulation |\n| **MLOps** | `mlflow`, `boto3` | Experiment tracking and artifact storage |\n| **Deployment** | `onnx`, `onnxscript`, `stac-model`, `pystac` | Model export and metadata generation |\n| **Monitoring** | `psutil`, `pynvml` | System resource monitoring |\n| **Validation** | `jsonschema`, `requests` | STAC metadata validation (optional group) |\n| **Configuration** | `dotenv` | Environment variable management |\n\n**Sources:** [examplemodel/pyproject.toml:7-24]()\n\n### Locked Dependencies\n\nThe `uv.lock` file pins exact versions and includes transitive dependencies. Key locked packages include:\n\n```mermaid\nflowchart LR\n    subgraph \"Primary Dependencies\"\n        TORCH[\"torch 2.7.1+\"]\n        MLFLOW[\"mlflow 3.1.1+\"]\n        GEOMLTK[\"geomltoolkits 0.3.9+\"]\n    end\n    \n    subgraph \"Transitive: ML Stack\"\n        NUMPY[\"numpy\"]\n        PYTORCH_LIGHTNING[\"pytorch-lightning\"]\n        TORCHVISION[\"torchvision\"]\n    end\n    \n    subgraph \"Transitive: MLflow Stack\"\n        ALEMBIC[\"alembic 1.16.3\"]\n        SQLALCHEMY[\"sqlalchemy\"]\n        FLASK[\"flask\"]\n        BOTO3[\"boto3 1.39.12\"]\n    end\n    \n    subgraph \"Transitive: Geo Stack\"\n        RASTERIO[\"rasterio\"]\n        FIONA[\"fiona\"]\n        SHAPELY[\"shapely\"]\n        GDAL[\"gdal bindings\"]\n    end\n    \n    TORCH --\u003e NUMPY\n    TORCH --\u003e PYTORCH_LIGHTNING\n    TORCH --\u003e TORCHVISION\n    \n    MLFLOW --\u003e ALEMBIC\n    MLFLOW --\u003e SQLALCHEMY\n    MLFLOW --\u003e FLASK\n    MLFLOW --\u003e BOTO3\n    \n    GEOMLTK --\u003e RASTERIO\n    GEOMLTK --\u003e FIONA\n    GEOMLTK --\u003e SHAPELY\n    GEOMLTK --\u003e GDAL\n    \n    style TORCH fill:#f0f0f0\n    style MLFLOW fill:#f0f0f0\n    style GEOMLTK fill:#f0f0f0\n```\n\n**Sources:** [examplemodel/uv.lock:1-10](), [examplemodel/uv.lock:129-141](), [examplemodel/uv.lock:195-206]()\n\n### Platform-Specific Wheel Resolution\n\nThe `uv.lock` file contains platform-specific wheel URLs for different Python versions and operating systems:\n\n```\nresolution-markers = [\n    \"python_full_version \u003e= '3.13'\",\n    \"python_full_version == '3.12.*'\",\n    \"python_full_version == '3.11.*'\",\n    \"python_full_version \u003c '3.11'\",\n]\n```\n\nThis ensures consistent installations across:\n- **Linux**: manylinux wheels for x86_64, aarch64, ppc64le, s390x\n- **macOS**: Universal2 and arm64 wheels\n- **Windows**: win32 and win_amd64 wheels\n- **musllinux**: Alpine Linux compatibility\n\n**Sources:** [examplemodel/uv.lock:4-9]()\n\n---\n\n## Verification\n\nAfter completing all installation steps, verify your environment:\n\n### Docker Verification\n\n```bash\n# Check Docker is running\ndocker ps\n\n# Check Docker Compose is available\ndocker compose version\n\n# Test with hello-world container\ndocker run hello-world\n```\n\n### Python Environment Verification\n\n```bash\n# Check Python version (should be 3.10+)\npython3 --version\n\n# Check uv is installed\nuv --version\n\n# Check installed packages\ncd examplemodel\nuv pip list\n```\n\n### Dependency Import Test\n\n```bash\ncd examplemodel\n\n# Test core imports\npython3 -c \"import torch; print(f'PyTorch: {torch.__version__}')\"\npython3 -c \"import mlflow; print(f'MLflow: {mlflow.__version__}')\"\npython3 -c \"import pytorch_lightning; print(f'Lightning: {pytorch_lightning.__version__}')\"\npython3 -c \"import geomltoolkits; print('geomltoolkits imported successfully')\"\n```\n\nIf all imports succeed, your environment is ready.\n\n**Sources:** [examplemodel/pyproject.toml:7-24]()\n\n---\n\n## Installation Decision Tree\n\n```mermaid\nflowchart TB\n    START[\"Start\"]\n    \n    Q1{\"Infrastructure\u003cbr/\u003estack needed?\"}\n    Q2{\"Linux OS?\"}\n    Q3{\"Model training\u003cbr/\u003eneeded?\"}\n    Q4{\"Python 3.10+\u003cbr/\u003einstalled?\"}\n    Q5{\"uv package\u003cbr/\u003emanager installed?\"}\n    \n    DOCKER_AUTO[\"Run install-docker.sh\"]\n    DOCKER_MANUAL[\"Install Docker manually\"]\n    INSTALL_PYTHON[\"Install Python 3.10+\"]\n    INSTALL_UV[\"Install uv\"]\n    UV_SYNC[\"Run uv sync\"]\n    \n    DOCKER_DONE[\"Docker Ready\"]\n    PYTHON_DONE[\"Python Ready\"]\n    COMPLETE[\"Installation Complete\"]\n    \n    START --\u003e Q1\n    Q1 --\u003e|\"Yes\"| Q2\n    Q1 --\u003e|\"No\"| Q3\n    \n    Q2 --\u003e|\"Yes (Debian/Ubuntu)\"| DOCKER_AUTO\n    Q2 --\u003e|\"No\"| DOCKER_MANUAL\n    \n    DOCKER_AUTO --\u003e DOCKER_DONE\n    DOCKER_MANUAL --\u003e DOCKER_DONE\n    DOCKER_DONE --\u003e Q3\n    \n    Q3 --\u003e|\"Yes\"| Q4\n    Q3 --\u003e|\"No\"| COMPLETE\n    \n    Q4 --\u003e|\"No\"| INSTALL_PYTHON\n    Q4 --\u003e|\"Yes\"| Q5\n    INSTALL_PYTHON --\u003e Q5\n    \n    Q5 --\u003e|\"No\"| INSTALL_UV\n    Q5 --\u003e|\"Yes\"| UV_SYNC\n    INSTALL_UV --\u003e UV_SYNC\n    \n    UV_SYNC --\u003e PYTHON_DONE\n    PYTHON_DONE --\u003e COMPLETE\n    \n    style DOCKER_AUTO fill:#f0f0f0\n    style UV_SYNC fill:#f0f0f0\n    style COMPLETE fill:#e8f5e9\n```\n\n**Sources:** [infra/install-docker.sh:1-50](), [examplemodel/pyproject.toml:1-31]()\n\n---\n\n## Common Installation Issues\n\n### Docker Permission Denied\n\n**Symptom:** `permission denied while trying to connect to the Docker daemon socket`\n\n**Solution:**\n```bash\n# Add user to docker group (already done by install-docker.sh)\nsudo usermod -aG docker $USER\n\n# Log out and log back in, or run:\nnewgrp docker\n\n# Verify\ndocker ps\n```\n\n**Sources:** [infra/install-docker.sh:32-33]()\n\n### Python Version Mismatch\n\n**Symptom:** `requires-python = \"\u003e=3.10\"` error during `uv sync`\n\n**Solution:**\n```bash\n# Check Python version\npython3 --version\n\n# If \u003c 3.10, install newer Python version\n# Then explicitly specify Python for uv\nuv sync --python python3.10\n```\n\n**Sources:** [examplemodel/pyproject.toml:6]()\n\n### Missing System Dependencies\n\n**Symptom:** Build errors for `gdal`, `rasterio`, or other geospatial packages\n\n**Solution:**\n```bash\n# Install system dependencies (Debian/Ubuntu)\nsudo apt install -y \\\n    gdal-bin \\\n    libgdal-dev \\\n    python3-dev \\\n    build-essential\n\n# Then retry uv sync\ncd examplemodel\nuv sync\n```\n\n### Network/Proxy Issues\n\n**Symptom:** Download failures during `uv sync` or Docker installation\n\n**Solution:**\n```bash\n# Configure uv proxy\nexport HTTP_PROXY=http://proxy.example.com:8080\nexport HTTPS_PROXY=http://proxy.example.com:8080\n\n# Configure Docker proxy (edit /etc/systemd/system/docker.service.d/http-proxy.conf)\n```\n\n---\n\n## Next Steps\n\nAfter completing installation:\n\n1. **Start infrastructure services:** See [Quick Start Guide](#2.2) to deploy the MLflow stack using Docker Compose\n2. **Run example model training:** See [Quick Start Guide](#2.2) for training the refugee camp detection model\n3. **Configure environment variables:** See [Configuration Management](#4.3) for `.env` file setup\n4. **Explore development workflows:** See [Development Guide](#7) for local development setup\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [infra/install-docker.sh:1-50]()"])</script><script>self.__next_f.push([1,"1a:T36cb,"])</script><script>self.__next_f.push([1,"# Quick Start Guide\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/MLproject](examplemodel/MLproject)\n- [examplemodel/README.md](examplemodel/README.md)\n- [infra/Readme.md](infra/Readme.md)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page provides step-by-step instructions for getting started with OpenGeoAIModelHub in under 10 minutes. It covers two quick start workflows: (1) running the example refugee camp detection model training pipeline, and (2) deploying the production infrastructure stack. \n\nFor detailed prerequisites and installation instructions, see [Prerequisites and Installation](#2.1). For comprehensive model documentation, see [Example Model System](#3). For infrastructure deployment details, see [Infrastructure System](#4).\n\n## Quick Start Decision Tree\n\n```mermaid\nflowchart TD\n    START[\"User starts here\"]\n    GOAL{\"What do you want to do?\"}\n    MODEL[\"Train the example ML model\"]\n    INFRA[\"Set up MLOps infrastructure\"]\n    \n    MODEL_STEPS[\"Follow Model Quick Start\u003cbr/\u003euv sync  mlflow run preprocess  mlflow run train\"]\n    INFRA_STEPS[\"Follow Infrastructure Quick Start\u003cbr/\u003eConfigure .env  ./setup.sh  Access services\"]\n    \n    MODEL_OUTPUT[\"Output: Trained model artifacts\u003cbr/\u003ebest_model.pt, best_model.onnx, best_model.dlpk\"]\n    INFRA_OUTPUT[\"Output: Running services\u003cbr/\u003eMLflow, MinIO, PostgreSQL, Traefik\"]\n    \n    BOTH[\"Need both?\u003cbr/\u003eSet up infrastructure first,\u003cbr/\u003ethen configure model to use it\"]\n    \n    START --\u003e GOAL\n    GOAL --\u003e|\"Experiment with ML\"| MODEL\n    GOAL --\u003e|\"Deploy production stack\"| INFRA\n    GOAL --\u003e|\"Both\"| BOTH\n    \n    MODEL --\u003e MODEL_STEPS\n    INFRA --\u003e INFRA_STEPS\n    BOTH --\u003e INFRA_STEPS\n    INFRA_STEPS --\u003e MODEL_STEPS\n    \n    MODEL_STEPS --\u003e MODEL_OUTPUT\n    INFRA_STEPS --\u003e INFRA_OUTPUT\n```\n\n**Quick Start Workflow Selection**\n\nSources: [infra/Readme.md:1-83](), [examplemodel/README.md:1-52]()\n\n## Example Model Quick Start\n\nThis workflow demonstrates training the refugee camp detection model using local MLflow tracking. The model uses satellite imagery from OpenAerialMap and labels from OpenStreetMap.\n\n### Step 1: Install UV Package Manager\n\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\nSources: [examplemodel/README.md:7-12]()\n\n### Step 2: Install Project Dependencies\n\n```bash\ncd examplemodel\nuv sync\n```\n\nThe `uv sync` command reads [examplemodel/pyproject.toml]() and installs all required dependencies including PyTorch, PyTorch Lightning, MLflow, and geospatial libraries.\n\nSources: [examplemodel/README.md:14-20]()\n\n### Step 3: Start Local MLflow UI (Optional)\n\n```bash\nuv run mlflow ui\n```\n\nThis launches the MLflow tracking server at `http://localhost:5000` where you can monitor experiments in real-time.\n\nSources: [examplemodel/README.md:22-26]()\n\n### Step 4: Preprocess Data\n\n```bash\nuv run mlflow run . -e preprocess --env-manager local\n```\n\nThis executes the `preprocess` entry point defined in [examplemodel/MLproject:6-17](), which:\n- Downloads satellite imagery tiles from OpenAerialMap\n- Fetches building footprint labels from OpenStreetMap\n- Generates training chips and labels in `data/train/sample/`\n\n**Default Parameters:**\n| Parameter | Default Value | Description |\n|-----------|---------------|-------------|\n| `zoom` | 19 | Tile zoom level |\n| `bbox` | \"85.5199...,27.6288...\" | Geographic bounding box (lon_min, lat_min, lon_max, lat_max) |\n| `tms` | \"https://tiles.openaerialmap.org/...\" | Tile Map Service URL |\n| `train_dir` | \"data/train/sample\" | Output directory for training data |\n\nSources: [examplemodel/MLproject:6-17]()\n\n### Step 5: Train Model\n\n```bash\nuv run mlflow run . -e train --env-manager local\n```\n\nThis executes the `train` entry point defined in [examplemodel/MLproject:19-32](), which:\n- Loads training chips and labels\n- Trains a U-Net model using PyTorch Lightning\n- Logs metrics to MLflow\n- Generates model artifacts: `best_model.pt`, `best_model.onnx`, `best_model.dlpk`, `stac_item.json`\n\n**Default Parameters:**\n| Parameter | Default Value | Description |\n|-----------|---------------|-------------|\n| `epochs` | 1 | Number of training epochs |\n| `batch_size` | 32 | Training batch size |\n| `chips_dir` | \"data/train/sample/chips\" | Input chips directory |\n| `labels_dir` | \"data/train/sample/labels\" | Input labels directory |\n| `lr` | 1e-3 | Learning rate |\n\nSources: [examplemodel/MLproject:19-32]()\n\n### Step 6: Run Inference (Optional)\n\n```bash\nuv run mlflow run . -e inference --env-manager local -P image_path=path/to/image.jpg\n```\n\nThis executes the `inference` entry point defined in [examplemodel/MLproject:34-44]() to perform prediction on a single image.\n\nSources: [examplemodel/MLproject:34-44]()\n\n### Example Model Workflow Diagram\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant UV[\"uv Package Manager\"]\n    participant MLflow[\"MLflow CLI\"]\n    participant Preprocess[\"src/preprocess.py\"]\n    participant Train[\"src/train.py\"]\n    participant Inference[\"src/inference.py\"]\n    participant FS[\"File System\"]\n    participant OAM[\"OpenAerialMap API\"]\n    participant OSM[\"OpenStreetMap API\"]\n    \n    User-\u003e\u003eUV: uv sync\n    UV-\u003e\u003eFS: Install dependencies from pyproject.toml\n    \n    User-\u003e\u003eMLflow: mlflow run . -e preprocess\n    MLflow-\u003e\u003ePreprocess: Execute preprocess entry point\n    Preprocess-\u003e\u003eOAM: Fetch satellite imagery tiles\n    Preprocess-\u003e\u003eOSM: Fetch building footprint labels\n    Preprocess-\u003e\u003eFS: Save to data/train/sample/chips/ and labels/\n    \n    User-\u003e\u003eMLflow: mlflow run . -e train\n    MLflow-\u003e\u003eTrain: Execute train entry point\n    Train-\u003e\u003eFS: Load from data/train/sample/\n    Train-\u003e\u003eTrain: Train U-Net model with PyTorch Lightning\n    Train-\u003e\u003eFS: Save best_model.pt, best_model.onnx, best_model.dlpk\n    Train-\u003e\u003eMLflow: Log metrics and artifacts\n    \n    User-\u003e\u003eMLflow: mlflow run . -e inference\n    MLflow-\u003e\u003eInference: Execute inference entry point\n    Inference-\u003e\u003eFS: Load best_model.pt\n    Inference-\u003e\u003eFS: Load input image\n    Inference-\u003e\u003eFS: Save prediction to output/\n```\n\n**Example Model Execution Sequence**\n\nSources: [examplemodel/MLproject:1-63](), [examplemodel/README.md:1-52]()\n\n### Output Artifacts\n\nAfter successful training, the following artifacts are generated in the `meta/` directory:\n\n| Artifact | Description | Use Case |\n|----------|-------------|----------|\n| `best_model.pt` | PyTorch checkpoint | Direct inference, fine-tuning |\n| `best_model.onnx` | ONNX format model | Cross-platform deployment |\n| `best_model.dlpk` | ESRI Deep Learning Package | ArcGIS deployment |\n| `stac_item.json` | STAC-MLM metadata | Model discovery and cataloging |\n\nSources: [examplemodel/MLproject:19-62]()\n\n## Infrastructure Stack Quick Start\n\nThis workflow deploys a production-ready MLOps infrastructure stack with Traefik, MLflow, MinIO, PostgreSQL, and monitoring services.\n\n### Step 1: Clone Repository and Configure Environment\n\n```bash\ngit clone \u003crepository-url\u003e\ncd infra\ncp .env.template .env\nnano .env  # Edit with your domain and credentials\n```\n\n**Required `.env` Variables:**\n- `DOMAIN` - Your base domain (e.g., `example.com`)\n- `POSTGRES_USER`, `POSTGRES_PASSWORD` - PostgreSQL credentials\n- `MINIO_ROOT_USER`, `MINIO_ROOT_PASSWORD` - MinIO credentials\n- `MLFLOW_TRACKING_USERNAME`, `MLFLOW_TRACKING_PASSWORD` - MLflow authentication\n\nSources: [infra/Readme.md:16-25]()\n\n### Step 2: Configure DNS Records\n\nPoint the following A records to your server's IP address:\n\n| Subdomain | Service | Purpose |\n|-----------|---------|---------|\n| `yourdomain.com` | Homepage | Service dashboard |\n| `mlflow.yourdomain.com` | MLflow | Experiment tracking |\n| `minio.yourdomain.com` | MinIO Console | Object storage UI |\n| `minio-api.yourdomain.com` | MinIO API | S3-compatible API |\n| `postgres.yourdomain.com` | PostgreSQL | Database access |\n| `rustdesk.yourdomain.com` | RustDesk | Remote desktop |\n| `traefik.yourdomain.com` | Traefik | Proxy dashboard |\n\nSources: [infra/Readme.md:32-42]()\n\n### Step 3: Run Setup Script\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n\nThe `setup.sh` script performs the following operations:\n1. Validates `.env` configuration\n2. Pulls Docker images from GitHub Container Registry\n3. Starts all services via `docker-compose`\n4. Configures systemd service for auto-start\n5. Generates SSL certificates via Let's Encrypt\n\nSources: [infra/Readme.md:26-30]()\n\n### Step 4: Verify Services\n\nAccess the services via their respective URLs:\n\n```bash\n# Homepage dashboard\nhttps://yourdomain.com\n\n# MLflow tracking server\nhttps://mlflow.yourdomain.com\n\n# MinIO console\nhttps://minio.yourdomain.com\n```\n\nSources: [infra/Readme.md:5-12]()\n\n### Infrastructure Deployment Flow\n\n```mermaid\nflowchart LR\n    subgraph \"Preparation\"\n        CLONE[\"git clone repository\"]\n        ENV[\"Configure .env file\"]\n        DNS[\"Configure DNS records\"]\n    end\n    \n    subgraph \"Setup Script Execution\"\n        VALIDATE[\"Validate configuration\"]\n        PULL[\"Pull Docker images\u003cbr/\u003efrom ghcr.io\"]\n        COMPOSE[\"docker-compose up -d\"]\n        SYSTEMD[\"Create systemd service\u003cbr/\u003etech-infra.service\"]\n    end\n    \n    subgraph \"Running Services\"\n        TRAEFIK[\"traefik\u003cbr/\u003eReverse proxy :80, :443\"]\n        HOMEPAGE[\"homepage\u003cbr/\u003eDashboard monitoring\"]\n        MLFLOW[\"mlflow\u003cbr/\u003eTracking server\"]\n        MINIO[\"minio\u003cbr/\u003eObject storage\"]\n        POSTGRES[\"postgres\u003cbr/\u003eDatabase + PostGIS\"]\n        RUSTDESK[\"rustdesk\u003cbr/\u003eRemote desktop\"]\n    end\n    \n    subgraph \"SSL Automation\"\n        LETSENCRYPT[\"Let's Encrypt\u003cbr/\u003eAutomatic certificates\"]\n    end\n    \n    CLONE --\u003e ENV\n    ENV --\u003e DNS\n    DNS --\u003e VALIDATE\n    \n    VALIDATE --\u003e PULL\n    PULL --\u003e COMPOSE\n    COMPOSE --\u003e SYSTEMD\n    \n    COMPOSE --\u003e TRAEFIK\n    COMPOSE --\u003e HOMEPAGE\n    COMPOSE --\u003e MLFLOW\n    COMPOSE --\u003e MINIO\n    COMPOSE --\u003e POSTGRES\n    COMPOSE --\u003e RUSTDESK\n    \n    TRAEFIK --\u003e LETSENCRYPT\n    LETSENCRYPT -.-\u003e|\"TLS certs\"| TRAEFIK\n    \n    TRAEFIK --\u003e|\"Routes to\"| HOMEPAGE\n    TRAEFIK --\u003e|\"Routes to\"| MLFLOW\n    TRAEFIK --\u003e|\"Routes to\"| MINIO\n    TRAEFIK --\u003e|\"Routes to\"| POSTGRES\n    TRAEFIK --\u003e|\"Routes to\"| RUSTDESK\n```\n\n**Infrastructure Stack Deployment Workflow**\n\nSources: [infra/Readme.md:14-30]()\n\n### Service Management Commands\n\nAfter deployment, manage services using the `manage.sh` script:\n\n```bash\n# Check service status\n./manage.sh status\n\n# View service logs\n./manage.sh logs mlflow\n./manage.sh logs minio\n\n# Restart a service\n./manage.sh restart postgres\n\n# Update images\n./manage.sh update\n\n# Create backup\n./manage.sh backup\n```\n\nAlternatively, use systemd commands:\n\n```bash\n# Start all services\nsudo systemctl start tech-infra\n\n# Stop all services\nsudo systemctl stop tech-infra\n\n# Check status\nsudo systemctl status tech-infra\n```\n\nSources: [infra/Readme.md:53-68]()\n\n## Connecting Example Model to Infrastructure\n\nTo use the deployed infrastructure for experiment tracking, configure the example model with the following environment variables:\n\n```bash\nexport AWS_ACCESS_KEY_ID=\u003cyour_minio_root_user\u003e\nexport AWS_SECRET_ACCESS_KEY=\u003cyour_minio_root_password\u003e\nexport MLFLOW_S3_ENDPOINT_URL=https://minio-api.yourdomain.com\nexport MLFLOW_TRACKING_URI=https://mlflow.yourdomain.com\n```\n\nThen run training as usual:\n\n```bash\ncd examplemodel\nuv run mlflow run . -e train --env-manager local\n```\n\nThe training pipeline will now log all experiments and artifacts to your deployed infrastructure instead of locally.\n\nSources: [examplemodel/README.md:40-48]()\n\n## Service Access Summary\n\nAfter successful deployment, services are accessible at the following endpoints:\n\n| Service | URL | Authentication | Purpose |\n|---------|-----|----------------|---------|\n| Homepage | `https://yourdomain.com` | None | Service monitoring dashboard |\n| MLflow | `https://mlflow.yourdomain.com` | Basic Auth | Experiment tracking and model registry |\n| MinIO Console | `https://minio.yourdomain.com` | MinIO credentials | Object storage management |\n| MinIO API | `https://minio-api.yourdomain.com` | S3 credentials | S3-compatible API endpoint |\n| PostgreSQL | `postgres.yourdomain.com:5432` | PostgreSQL credentials | Database access via clients |\n| RustDesk | `https://rustdesk.yourdomain.com` | RustDesk credentials | Remote desktop access |\n| Traefik Dashboard | `https://traefik.yourdomain.com` | None | Proxy routing and metrics |\n\nSources: [infra/Readme.md:5-42]()\n\n## Troubleshooting Common Issues\n\n### Model Training Issues\n\n**Issue:** `ModuleNotFoundError` when running `mlflow run`\n\n**Solution:** Ensure `uv sync` completed successfully and you're using `uv run` prefix:\n```bash\nuv sync\nuv run mlflow run . -e train --env-manager local\n```\n\n**Issue:** Preprocessing fails to download imagery\n\n**Solution:** Check network connectivity and verify the TMS URL is accessible. You can override the default TMS:\n```bash\nuv run mlflow run . -e preprocess --env-manager local -P tms=\"\u003cyour-tms-url\u003e\"\n```\n\nSources: [examplemodel/MLproject:6-17](), [examplemodel/README.md:14-18]()\n\n### Infrastructure Deployment Issues\n\n**Issue:** Services fail to start with SSL errors\n\n**Solution:** Ensure DNS records are properly configured and propagated before running `./setup.sh`. Traefik requires valid DNS resolution to obtain Let's Encrypt certificates.\n\n**Issue:** Cannot access services via domain\n\n**Solution:** Check that ports 80 and 443 are open in your firewall and that Traefik is running:\n```bash\n./manage.sh logs traefik\n```\n\nSources: [infra/Readme.md:32-42](), [infra/Readme.md:54-56]()\n\n## Next Steps\n\nAfter completing the quick start:\n\n- For detailed model architecture and training pipeline documentation, see [Example Model System](#3)\n- For infrastructure customization and advanced configuration, see [Infrastructure System](#4)  \n- For model deployment options (ONNX, DLPK, PyTorch), see [Model Deployment Options](#6.2)\n- For local development setup and extending functionality, see [Development Guide](#7)\n\nSources: [infra/Readme.md:1-83](), [examplemodel/README.md:1-52](), [examplemodel/MLproject:1-63]()"])</script><script>self.__next_f.push([1,"1b:T6e5a,"])</script><script>self.__next_f.push([1,"# Example Model System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/MLproject](examplemodel/MLproject)\n- [examplemodel/README.md](examplemodel/README.md)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Example Model System is a complete end-to-end machine learning pipeline for refugee camp detection in satellite imagery. This system demonstrates a production-ready implementation of a geospatial AI model with standardized metadata, multiple deployment formats, and reproducible training workflows.\n\nThis document covers the ML pipeline components, training and inference workflows, model architecture, output artifact generation, and MLflow orchestration. For infrastructure setup and deployment, see [Infrastructure System](#4). For specific deployment strategies to different platforms, see [Model Deployment Options](#6.2).\n\n**Sources:** [examplemodel/README.md:1-52](), [examplemodel/src/train.py:1-519](), [examplemodel/MLproject:1-63]()\n\n---\n\n## System Overview\n\nThe Example Model System implements a semantic segmentation pipeline using U-Net architecture to identify refugee camps in satellite imagery. The system is structured around five distinct entry points orchestrated by MLflow, each handling a specific phase of the ML lifecycle.\n\n### High-Level Architecture\n\n```mermaid\ngraph TB\n    subgraph \"MLflow Entry Points\"\n        EP_PREPROCESS[\"preprocess\u003cbr/\u003eEntry Point\"]\n        EP_TRAIN[\"train\u003cbr/\u003eEntry Point\"]\n        EP_INFERENCE[\"inference\u003cbr/\u003eEntry Point\"]\n        EP_VALIDATE[\"validate_stac\u003cbr/\u003eEntry Point\"]\n        EP_STAC2ESRI[\"stac2esri\u003cbr/\u003eEntry Point\"]\n    end\n    \n    subgraph \"Core Python Modules\"\n        PREPROCESS_PY[\"preprocess.py\"]\n        TRAIN_PY[\"train.py\"]\n        INFERENCE_PY[\"inference.py\"]\n        MODEL_PY[\"model.py\"]\n        STAC2ESRI_PY[\"stac2esri.py\"]\n        VALIDATE_PY[\"validate_stac_mlm.py\"]\n    end\n    \n    subgraph \"Model Components\"\n        CAMP_DATA[\"CampDataModule\u003cbr/\u003ePyTorch Lightning DataModule\"]\n        LIT_MODEL[\"LitRefugeeCamp\u003cbr/\u003ePyTorch Lightning Module\"]\n        UNET[\"U-Net Neural Network\"]\n    end\n    \n    subgraph \"ESRI Integration\"\n        DETECTOR[\"RefugeeCampDetector\u003cbr/\u003eInference Class\"]\n        EMD[\"model.emd\u003cbr/\u003eEsri Model Definition\"]\n    end\n    \n    subgraph \"Output Artifacts\"\n        PTH[\"best_model.pth\u003cbr/\u003eState Dictionary\"]\n        PT[\"best_model.pt\u003cbr/\u003eTorchScript Traced\"]\n        ONNX[\"best_model.onnx\u003cbr/\u003eONNX Format\"]\n        DLPK[\"best_model.dlpk\u003cbr/\u003eESRI Package\"]\n        STAC[\"stac_item.json\u003cbr/\u003eSTAC-MLM Metadata\"]\n    end\n    \n    EP_PREPROCESS --\u003e|executes| PREPROCESS_PY\n    EP_TRAIN --\u003e|executes| TRAIN_PY\n    EP_INFERENCE --\u003e|executes| INFERENCE_PY\n    EP_VALIDATE --\u003e|executes| VALIDATE_PY\n    EP_STAC2ESRI --\u003e|executes| STAC2ESRI_PY\n    \n    TRAIN_PY --\u003e|imports| MODEL_PY\n    TRAIN_PY --\u003e|imports| INFERENCE_PY\n    TRAIN_PY --\u003e|imports| STAC2ESRI_PY\n    \n    MODEL_PY --\u003e|defines| CAMP_DATA\n    MODEL_PY --\u003e|defines| LIT_MODEL\n    LIT_MODEL --\u003e|contains| UNET\n    \n    TRAIN_PY --\u003e|instantiates| CAMP_DATA\n    TRAIN_PY --\u003e|trains| LIT_MODEL\n    \n    TRAIN_PY --\u003e|produces| PTH\n    TRAIN_PY --\u003e|produces| PT\n    TRAIN_PY --\u003e|produces| ONNX\n    TRAIN_PY --\u003e|produces| STAC\n    TRAIN_PY --\u003e|produces| DLPK\n    \n    STAC2ESRI_PY --\u003e|creates| DLPK\n    STAC2ESRI_PY --\u003e|uses| DETECTOR\n    STAC2ESRI_PY --\u003e|generates| EMD\n```\n\n**Sources:** [examplemodel/MLproject:1-63](), [examplemodel/src/train.py:1-519]()\n\n---\n\n## MLflow Entry Points\n\nThe system defines five entry points in the `MLproject` file, each representing a distinct phase of the ML workflow.\n\n### Entry Point Configuration\n\n| Entry Point | Module | Purpose | Key Parameters |\n|-------------|--------|---------|----------------|\n| `preprocess` | `src/preprocess.py` | Data acquisition from TMS and OSM | `zoom`, `bbox`, `tms`, `train_dir` |\n| `train` | `src/train.py` | Model training and artifact generation | `epochs`, `batch_size`, `chips_dir`, `labels_dir`, `lr` |\n| `inference` | `src/inference.py` | Model prediction on new images | `image_path`, `model_path`, `output_dir`, `mlflow_tracking` |\n| `validate_stac` | `validate_stac_mlm.py` | STAC-MLM metadata validation | `stac_file` |\n| `stac2esri` | `src/stac2esri.py` | DLPK package generation for ArcGIS | `stac_path`, `onnx_path`, `out_dir`, `dlpk_name` |\n\n**Sources:** [examplemodel/MLproject:5-63]()\n\n### Entry Point Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant MLflow as \"MLflow CLI\"\n    participant UV as \"uv Package Manager\"\n    participant Module as \"Python Module\"\n    participant Artifacts as \"Artifact Storage\"\n    \n    User-\u003e\u003eMLflow: \"mlflow run . -e preprocess\"\n    MLflow-\u003e\u003eUV: \"uv run python src/preprocess.py --zoom 19 --bbox ...\"\n    UV-\u003e\u003eModule: \"Execute with parameters\"\n    Module-\u003e\u003eModule: \"Fetch TMS imagery\"\n    Module-\u003e\u003eModule: \"Fetch OSM labels\"\n    Module-\u003e\u003eArtifacts: \"Save chips/ and labels/\"\n    \n    User-\u003e\u003eMLflow: \"mlflow run . -e train\"\n    MLflow-\u003e\u003eUV: \"uv run python src/train.py --epochs 1 ...\"\n    UV-\u003e\u003eModule: \"Execute train_model()\"\n    Module-\u003e\u003eModule: \"Call get_system_info()\"\n    Module-\u003e\u003eModule: \"Call calculate_dataset_statistics()\"\n    Module-\u003e\u003eModule: \"Initialize CampDataModule\"\n    Module-\u003e\u003eModule: \"Train LitRefugeeCamp\"\n    Module-\u003e\u003eModule: \"Call create_stac_mlm_item()\"\n    Module-\u003e\u003eModule: \"Call create_dlpk()\"\n    Module-\u003e\u003eArtifacts: \"Save .pth, .pt, .onnx, .dlpk, stac_item.json\"\n    \n    User-\u003e\u003eMLflow: \"mlflow run . -e inference --image_path=test.jpg\"\n    MLflow-\u003e\u003eUV: \"uv run python src/inference.py test.jpg\"\n    UV-\u003e\u003eModule: \"Load model and predict\"\n    Module-\u003e\u003eArtifacts: \"Save prediction outputs\"\n```\n\n**Sources:** [examplemodel/MLproject:5-63](), [examplemodel/src/train.py:370-519]()\n\n---\n\n## Training Pipeline Components\n\nThe training pipeline is orchestrated by the `train_model()` function in `train.py`, which coordinates multiple subsystems.\n\n### Training Pipeline Architecture\n\n```mermaid\ngraph LR\n    subgraph \"train_model Function\"\n        START[\"train_model(args)\"]\n        SYSINFO[\"get_system_info()\"]\n        STATS[\"calculate_dataset_statistics()\"]\n        DATAMOD[\"CampDataModule instantiation\"]\n        TRAINER[\"pl.Trainer configuration\"]\n        TRAINING[\"trainer.fit()\"]\n        CKPT[\"ModelCheckpoint callback\"]\n        CONVERT[\"Model format conversions\"]\n        STAC[\"create_stac_mlm_item()\"]\n        DLPK_CREATE[\"create_dlpk()\"]\n        LOG[\"MLflow logging\"]\n    end\n    \n    subgraph \"PyTorch Lightning\"\n        LITMODEL[\"LitRefugeeCamp\"]\n        UNET_MODEL[\"self.model\u003cbr/\u003eU-Net\"]\n    end\n    \n    subgraph \"Artifacts Generated\"\n        PTH_FILE[\"best_model.pth\"]\n        PT_FILE[\"best_model.pt\u003cbr/\u003eTorchScript\"]\n        ONNX_FILE[\"best_model.onnx\"]\n        STAC_FILE[\"stac_item.json\"]\n        DLPK_FILE[\"best_model.dlpk\"]\n        EMD_FILE[\"model.emd\"]\n    end\n    \n    START --\u003e SYSINFO\n    START --\u003e STATS\n    START --\u003e DATAMOD\n    START --\u003e TRAINER\n    TRAINER --\u003e CKPT\n    TRAINER --\u003e TRAINING\n    TRAINING --\u003e LITMODEL\n    LITMODEL --\u003e UNET_MODEL\n    TRAINING --\u003e CONVERT\n    CONVERT --\u003e PTH_FILE\n    CONVERT --\u003e PT_FILE\n    CONVERT --\u003e ONNX_FILE\n    START --\u003e STAC\n    START --\u003e DLPK_CREATE\n    STAC --\u003e STAC_FILE\n    DLPK_CREATE --\u003e DLPK_FILE\n    DLPK_CREATE --\u003e EMD_FILE\n    CONVERT --\u003e LOG\n    STAC --\u003e LOG\n    DLPK_CREATE --\u003e LOG\n```\n\n**Sources:** [examplemodel/src/train.py:370-519]()\n\n### Key Functions in train.py\n\n#### get_system_info()\n\nCollects comprehensive system and hardware information for reproducibility tracking.\n\n**Function signature:** `def get_system_info() -\u003e Dict[str, Any]:`\n\n**Returns:** Dictionary containing:\n- Platform information (`platform`, `architecture`, `processor`)\n- Python version\n- Hardware specs (`cpu_count`, `memory_total`, `memory_available`)\n- PyTorch and CUDA versions\n- GPU information via `pynvml` (name, memory stats)\n\n**Sources:** [examplemodel/src/train.py:25-61]()\n\n#### calculate_dataset_statistics()\n\nComputes pixel-level statistics across the training dataset for normalization.\n\n**Function signature:** `def calculate_dataset_statistics(data_module: CampDataModule) -\u003e Dict[str, Any]:`\n\n**Process:**\n1. Iterates through training dataloader\n2. Accumulates channel sums and squared sums\n3. Calculates mean and standard deviation per channel\n4. Counts samples in train/val/test splits\n\n**Returns:** Dictionary with dataset statistics including `train_samples`, `val_samples`, `test_samples`, `pixel_mean`, `pixel_std`\n\n**Sources:** [examplemodel/src/train.py:63-99]()\n\n#### create_stac_mlm_item()\n\nGenerates STAC-MLM compliant metadata describing the trained model.\n\n**Function signature:** `def create_stac_mlm_item(model, dataset_stats, system_info, model_performance, checkpoint_path) -\u003e Dict[str, Any]:`\n\n**STAC Item Structure:**\n- **Core properties:** `id`, `geometry`, `bbox`, `datetime`\n- **MLM properties:** Model architecture, framework, parameters, I/O specifications\n- **Processing properties:** Training environment details\n- **Assets:** Links to all model artifacts (PyTorch, ONNX, DLPK, etc.)\n\n**Key STAC Extensions Used:**\n- `mlm/v1.5.0` - Machine Learning Model extension\n- `file/v1.0.0` - File information\n- `processing/v1.1.0` - Processing provenance\n\n**Sources:** [examplemodel/src/train.py:101-368]()\n\n#### train_model()\n\nMain training orchestration function that coordinates all pipeline components.\n\n**Function signature:** `def train_model(args):`\n\n**Execution Flow:**\n1. Start MLflow run\n2. Collect system info\n3. Initialize `CampDataModule` with data directories\n4. Calculate dataset statistics\n5. Configure `ModelCheckpoint` callback (monitors `val_loss`)\n6. Initialize `pl.Trainer` with auto-accelerator detection\n7. Train `LitRefugeeCamp` model\n8. Convert best checkpoint to multiple formats:\n   - `.pth` - Raw state dictionary\n   - `.pt` - TorchScript traced model\n   - `.onnx` - ONNX format\n9. Generate STAC-MLM metadata\n10. Create ESRI DLPK package via `create_dlpk()`\n11. Log all artifacts to MLflow\n12. Log model with signature for MLflow Model Registry\n\n**Sources:** [examplemodel/src/train.py:370-519]()\n\n---\n\n## Model Format Conversions\n\nThe training pipeline produces four distinct model formats for different deployment scenarios.\n\n### Conversion Process\n\n```mermaid\ngraph TB\n    subgraph \"Source Model\"\n        CHECKPOINT[\"best_model_path\u003cbr/\u003ePyTorch Lightning Checkpoint\"]\n    end\n    \n    subgraph \"Loaded Model\"\n        LOADED[\"LitRefugeeCamp.load_from_checkpoint()\"]\n        STATE_DICT[\"model.state_dict()\"]\n        CLEAN_MODEL[\"clean_model = LitRefugeeCamp()\"]\n        TORCH_MODEL[\"torch_model = clean_model.model\"]\n    end\n    \n    subgraph \"Format Conversions\"\n        SAVE_PTH[\"torch.save(state_dict)\"]\n        TRACE[\"torch.jit.trace()\"]\n        ONNX_EXPORT[\"torch.onnx.export()\"]\n    end\n    \n    subgraph \"Output Files\"\n        PTH[\"meta/best_model.pth\u003cbr/\u003eState Dictionary\"]\n        PT[\"meta/best_model.pt\u003cbr/\u003eTorchScript Traced\"]\n        ONNX[\"meta/best_model.onnx\u003cbr/\u003eONNX Format\"]\n    end\n    \n    subgraph \"ESRI Package Creation\"\n        DLPK_FUNC[\"create_dlpk()\"]\n        DLPK[\"meta/best_model.dlpk\u003cbr/\u003eDeep Learning Package\"]\n    end\n    \n    CHECKPOINT --\u003e LOADED\n    LOADED --\u003e STATE_DICT\n    STATE_DICT --\u003e CLEAN_MODEL\n    CLEAN_MODEL --\u003e TORCH_MODEL\n    \n    STATE_DICT --\u003e SAVE_PTH\n    SAVE_PTH --\u003e PTH\n    \n    TORCH_MODEL --\u003e TRACE\n    TRACE --\u003e PT\n    \n    TORCH_MODEL --\u003e ONNX_EXPORT\n    ONNX_EXPORT --\u003e ONNX\n    \n    PT --\u003e DLPK_FUNC\n    ONNX --\u003e DLPK_FUNC\n    DLPK_FUNC --\u003e DLPK\n```\n\n**Sources:** [examplemodel/src/train.py:433-469]()\n\n### Format Specifications\n\n| Format | File Extension | Purpose | Generation Code |\n|--------|---------------|---------|-----------------|\n| State Dictionary | `.pth` | Raw PyTorch weights for flexible loading | `torch.save(model.state_dict(), \"meta/best_model.pth\")` |\n| TorchScript | `.pt` | Traced model for optimized inference | `torch.jit.trace(torch_model, torch.randn(1,3,256,256))` |\n| ONNX | `.onnx` | Cross-platform interoperability | `torch.onnx.export(torch_model, ...)` with opset 11 |\n| ESRI DLPK | `.dlpk` | ArcGIS Pro deployment package | `create_dlpk(emd_path, pt_path, esri_inference_path, dlpk_path)` |\n\n**Key Implementation Details:**\n\n**TorchScript Export:** Uses `torch.jit.trace()` on the inner neural network (`clean_model.model`) rather than the Lightning wrapper to ensure compatibility.\n\n**ONNX Export Parameters:**\n- `opset_version=11` for broad compatibility\n- Dynamic axes for batch dimension: `{\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}`\n- Input/output names: `[\"input\"]` and `[\"output\"]`\n\n**Sources:** [examplemodel/src/train.py:438-469]()\n\n---\n\n## STAC-MLM Metadata Generation\n\nThe system generates comprehensive STAC (SpatioTemporal Asset Catalog) metadata following the Machine Learning Model (MLM) extension specification v1.5.0.\n\n### STAC Item Structure\n\n```mermaid\ngraph TB\n    subgraph \"STAC Item Core\"\n        TYPE[\"type: Feature\"]\n        VERSION[\"stac_version: 1.1.0\"]\n        ID[\"id: refugee-camp-detector-v1.0.0\"]\n        GEOMETRY[\"geometry: Polygon\"]\n    end\n    \n    subgraph \"STAC Extensions\"\n        MLM_EXT[\"mlm/v1.5.0\u003cbr/\u003eMachine Learning Model\"]\n        FILE_EXT[\"file/v1.0.0\u003cbr/\u003eFile Information\"]\n        PROC_EXT[\"processing/v1.1.0\u003cbr/\u003eProcessing Provenance\"]\n    end\n    \n    subgraph \"Properties\"\n        META[\"Descriptive metadata\u003cbr/\u003etitle, description, keywords\"]\n        MLM_PROPS[\"mlm:* properties\u003cbr/\u003earchitecture, tasks, framework\"]\n        INPUT_SPEC[\"mlm:input\u003cbr/\u003eshape, normalization, bands\"]\n        OUTPUT_SPEC[\"mlm:output\u003cbr/\u003eshape, classes, data_type\"]\n        PROC_PROPS[\"processing:* properties\u003cbr/\u003efacility, software, expression\"]\n    end\n    \n    subgraph \"Assets\"\n        PYTORCH_CKPT[\"pytorch-checkpoint\u003cbr/\u003eLightning checkpoint\"]\n        PYTORCH_SD[\"pytorch-state-dict\u003cbr/\u003eTorchScript model\"]\n        PYTORCH_RAW[\"pytorch-state-dict-raw\u003cbr/\u003eRaw .pth file\"]\n        ONNX_ASSET[\"onnx-model\u003cbr/\u003eONNX format\"]\n        DLPK_ASSET[\"esri-package\u003cbr/\u003eDLPK file\"]\n        CODE_ASSETS[\"source-code, inference-script,\u003cbr/\u003etraining-script\"]\n        VIZ_ASSETS[\"confusion-matrix, example-input,\u003cbr/\u003eexample-prediction\"]\n    end\n    \n    TYPE --\u003e GEOMETRY\n    VERSION --\u003e MLM_EXT\n    MLM_EXT --\u003e MLM_PROPS\n    MLM_PROPS --\u003e INPUT_SPEC\n    MLM_PROPS --\u003e OUTPUT_SPEC\n    PROC_EXT --\u003e PROC_PROPS\n    \n    ID --\u003e PYTORCH_CKPT\n    ID --\u003e PYTORCH_SD\n    ID --\u003e PYTORCH_RAW\n    ID --\u003e ONNX_ASSET\n    ID --\u003e DLPK_ASSET\n    ID --\u003e CODE_ASSETS\n    ID --\u003e VIZ_ASSETS\n```\n\n**Sources:** [examplemodel/src/train.py:101-368]()\n\n### Key STAC-MLM Properties\n\n#### Model Architecture Properties\n\n```\nmlm:name: \"RefugeeCampDetector\"\nmlm:architecture: \"U-Net\"\nmlm:tasks: [\"semantic-segmentation\"]\nmlm:framework: \"PyTorch\"\nmlm:framework_version: torch.__version__\nmlm:total_parameters: sum(p.numel() for p in model.parameters())\nmlm:memory_size: sum(p.numel() * p.element_size() for p in model.parameters())\n```\n\n**Sources:** [examplemodel/src/train.py:172-180]()\n\n#### Input Specification\n\nThe input specification defines expected image format and normalization:\n\n```json\n{\n  \"name\": \"satellite_image\",\n  \"bands\": [\"red\", \"green\", \"blue\"],\n  \"input\": {\n    \"shape\": [-1, 3, 256, 256],\n    \"dim_order\": [\"batch\", \"bands\", \"height\", \"width\"],\n    \"data_type\": \"float32\"\n  },\n  \"value_scaling\": [\n    {\"type\": \"z-score\", \"mean\": 0.485, \"stddev\": 0.229},\n    {\"type\": \"z-score\", \"mean\": 0.456, \"stddev\": 0.224},\n    {\"type\": \"z-score\", \"mean\": 0.406, \"stddev\": 0.225}\n  ]\n}\n```\n\nUses ImageNet normalization statistics for transfer learning compatibility.\n\n**Sources:** [examplemodel/src/train.py:190-206]()\n\n#### Output Specification\n\nDefines the segmentation mask output format:\n\n```json\n{\n  \"name\": \"segmentation_mask\",\n  \"result\": {\n    \"shape\": [-1, 1, 256, 256],\n    \"dim_order\": [\"batch\", \"channel\", \"height\", \"width\"],\n    \"data_type\": \"float32\"\n  },\n  \"classification:classes\": [\n    {\"value\": 0, \"name\": \"background\"},\n    {\"value\": 1, \"name\": \"refugee_camp\"}\n  ]\n}\n```\n\n**Sources:** [examplemodel/src/train.py:207-230]()\n\n### Asset Catalog\n\nThe STAC item catalogs 15 distinct assets across multiple categories:\n\n| Asset Key | Type | Role | File Path |\n|-----------|------|------|-----------|\n| `pytorch-checkpoint` | Lightning checkpoint | `mlm:model`, `mlm:checkpoint` | Path from checkpoint callback |\n| `pytorch-state-dict` | TorchScript | `mlm:model`, `mlm:weights` | `meta/best_model.pt` |\n| `pytorch-state-dict-raw` | State dict | `mlm:model`, `mlm:weights` | `meta/best_model.pth` |\n| `onnx-model` | ONNX | `mlm:model`, `mlm:inference` | `meta/best_model.onnx` |\n| `esri-package` | DLPK ZIP | `mlm:model` | `meta/best_model.dlpk` |\n| `source-code` | Repository link | `mlm:source_code`, `code` | GitHub URL |\n| `inference-script` | Python script | `mlm:source_code` | `src/inference.py` |\n| `training-script` | Python script | `mlm:training`, `code` | `src/train.py` |\n| `container-image` | OCI image | `mlm:container`, `runtime` | GHCR URL |\n| `confusion-matrix` | PNG | `metadata`, `overview` | `meta/confusion_matrix.png` |\n| `example-input` | PNG | `metadata`, `overview` | `meta/example_input.png` |\n| `example-prediction` | PNG | `metadata`, `overview` | `meta/example_pred.png` |\n| `example-target` | PNG | `metadata`, `overview` | `meta/example_target.png` |\n| `model-metadata` | EMD JSON | `metadata` | `meta/model.emd` |\n| `requirements` | Text file | `runtime`, `metadata` | `requirements.txt` |\n\n**Sources:** [examplemodel/src/train.py:246-364]()\n\n---\n\n## MLflow Integration\n\nThe training pipeline integrates with MLflow for experiment tracking, artifact logging, and model registry.\n\n### MLflow Logging Architecture\n\n```mermaid\ngraph LR\n    subgraph \"Training Process\"\n        RUN_START[\"mlflow.start_run()\"]\n        LOG_PARAMS[\"mlflow.log_params()\"]\n        LOG_METRICS[\"mlflow.log_metrics()\"]\n        LOG_ARTIFACTS[\"mlflow.log_artifact()\"]\n        LOG_MODEL[\"mlflow.pytorch.log_model()\"]\n    end\n    \n    subgraph \"Logged Parameters\"\n        DATASET_PARAMS[\"train_samples, val_samples,\u003cbr/\u003etest_samples, num_classes\"]\n    end\n    \n    subgraph \"Logged Metrics\"\n        PERFORMANCE[\"best_val_loss,\u003cbr/\u003eepochs_trained\"]\n    end\n    \n    subgraph \"Logged Artifacts\"\n        METADATA_DIR[\"metadata/\u003cbr/\u003estac_item.json\"]\n        MODELS_DIR[\"models/\u003cbr/\u003epth, pt, onnx, dlpk\"]\n        CHECKPOINTS_DIR[\"checkpoints/\u003cbr/\u003eLightning checkpoint\"]\n        DATASETS_DIR[\"datasets/train/\u003cbr/\u003echips/, labels/\"]\n        ESRI_DIR[\"esri/\u003cbr/\u003edlpk, emd, pt, RefugeeCampDetector.py\"]\n    end\n    \n    subgraph \"Model Registry\"\n        MODEL_ARTIFACT[\"model/\u003cbr/\u003ePyTorch model with signature\"]\n        SIGNATURE[\"infer_signature()\"]\n    end\n    \n    RUN_START --\u003e LOG_PARAMS\n    LOG_PARAMS --\u003e DATASET_PARAMS\n    RUN_START --\u003e LOG_METRICS\n    LOG_METRICS --\u003e PERFORMANCE\n    RUN_START --\u003e LOG_ARTIFACTS\n    LOG_ARTIFACTS --\u003e METADATA_DIR\n    LOG_ARTIFACTS --\u003e MODELS_DIR\n    LOG_ARTIFACTS --\u003e CHECKPOINTS_DIR\n    LOG_ARTIFACTS --\u003e DATASETS_DIR\n    LOG_ARTIFACTS --\u003e ESRI_DIR\n    RUN_START --\u003e LOG_MODEL\n    LOG_MODEL --\u003e SIGNATURE\n    LOG_MODEL --\u003e MODEL_ARTIFACT\n```\n\n**Sources:** [examplemodel/src/train.py:373-507]()\n\n### Artifact Organization\n\nThe system logs artifacts in a structured directory hierarchy:\n\n```\nmlflow_run/\n metadata/\n    stac_item.json\n models/\n    best_model.pth\n    best_model.pt\n    best_model.onnx\n    best_model.dlpk\n checkpoints/\n    epoch={epoch}-step={step}.ckpt\n datasets/\n    train/\n        chips/\n        labels/\n esri/\n    best_model.dlpk\n    model.emd\n    best_model.pt\n    RefugeeCampDetector.py\n model/\n     [PyTorch model with signature]\n```\n\n**Implementation:**\n```python\nmlflow.log_artifact(stac_output_path, artifact_path=\"metadata\")\nmlflow.log_artifact(\"meta/best_model.pth\", artifact_path=\"models\")\nmlflow.log_artifact(\"meta/best_model.pt\", artifact_path=\"models\")\nmlflow.log_artifact(\"meta/best_model.onnx\", artifact_path=\"models\")\nmlflow.log_artifact(\"meta/best_model.dlpk\", artifact_path=\"models\")\nmlflow.log_artifact(best_checkpoint, artifact_path=\"checkpoints\")\nmlflow.log_artifact(args.chips_dir, artifact_path=\"datasets/train/chips\")\nmlflow.log_artifact(args.labels_dir, artifact_path=\"datasets/train/labels\")\n```\n\n**Sources:** [examplemodel/src/train.py:479-496]()\n\n### Model Signature\n\nThe system uses `infer_signature()` to capture input/output schemas:\n\n```python\ndummy_input = torch.randn(1, 3, 256, 256)\nsignature = infer_signature(\n    dummy_input.numpy(), \n    clean_model(dummy_input).detach().numpy()\n)\nmlflow.pytorch.log_model(\n    clean_model, \n    \"model\", \n    signature=signature, \n    extra_files=[stac_output_path]\n)\n```\n\nThis enables MLflow's model serving capabilities with schema validation.\n\n**Sources:** [examplemodel/src/train.py:498-503]()\n\n---\n\n## System Dependencies and Configuration\n\n### Dependency Management\n\nThe system uses `uv` package manager with dependencies defined in `pyproject.toml`. All entry points execute via `uv run` to ensure consistent dependency resolution.\n\n**Example from MLproject:**\n```\ncommand: \u003e\n  uv run python src/train.py \n  --epochs {epochs}\n  --batch_size {batch_size}\n  --chips_dir {chips_dir}\n  --labels_dir {labels_dir}\n  --lr {lr}\n```\n\n**Sources:** [examplemodel/MLproject:26-32]()\n\n### Training Parameters\n\nThe training entry point accepts five configurable parameters:\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `epochs` | int | 1 | Number of training epochs |\n| `batch_size` | int | 32 | Batch size for training |\n| `chips_dir` | str | `data/train/sample/chips` | Directory containing image chips |\n| `labels_dir` | str | `data/train/sample/labels` | Directory containing label masks |\n| `lr` | float | 1e-3 | Learning rate for optimizer |\n\n**Command-line usage:**\n```bash\nuv run mlflow run . -e train \\\n  -P epochs=10 \\\n  -P batch_size=16 \\\n  -P chips_dir=data/custom/chips \\\n  -P labels_dir=data/custom/labels \\\n  -P lr=0.001\n```\n\n**Sources:** [examplemodel/MLproject:19-32](), [examplemodel/src/train.py:510-518]()\n\n---\n\n## Data Flow Through the System\n\n```mermaid\nflowchart TD\n    subgraph \"External Data Sources\"\n        TMS[\"OpenAerialMap TMS\u003cbr/\u003eSatellite Imagery\"]\n        OSM[\"OpenStreetMap\u003cbr/\u003eLabel Polygons\"]\n    end\n    \n    subgraph \"Preprocessing\"\n        PREPROCESS[\"preprocess.py\"]\n        CHIPS[\"chips/\u003cbr/\u003e256x256 image tiles\"]\n        LABELS[\"labels/\u003cbr/\u003eBinary masks\"]\n    end\n    \n    subgraph \"Data Loading\"\n        DATAMODULE[\"CampDataModule\"]\n        TRAIN_LOADER[\"train_dataloader()\"]\n        VAL_LOADER[\"val_dataloader()\"]\n        TEST_LOADER[\"test_dataloader()\"]\n    end\n    \n    subgraph \"Training\"\n        TRAINER[\"pl.Trainer\"]\n        LITMODEL[\"LitRefugeeCamp\"]\n        FORWARD[\"forward() pass\"]\n        LOSS[\"Loss computation\"]\n        BACKWARD[\"Backpropagation\"]\n    end\n    \n    subgraph \"Evaluation\"\n        VAL_STEP[\"validation_step()\"]\n        TEST_STEP[\"test_step()\"]\n        CONFUSION[\"log_confusion_matrix()\"]\n        INFERENCE_EX[\"log_inference_example()\"]\n    end\n    \n    subgraph \"Model Export\"\n        CHECKPOINT[\"Best checkpoint\"]\n        CONVERSIONS[\"Format conversions\"]\n        PTH_OUT[\"best_model.pth\"]\n        PT_OUT[\"best_model.pt\"]\n        ONNX_OUT[\"best_model.onnx\"]\n    end\n    \n    subgraph \"Metadata \u0026 Packaging\"\n        STAC_GEN[\"create_stac_mlm_item()\"]\n        DLPK_GEN[\"create_dlpk()\"]\n        STAC_JSON[\"stac_item.json\"]\n        DLPK_OUT[\"best_model.dlpk\"]\n    end\n    \n    subgraph \"MLflow Tracking\"\n        MLFLOW_LOG[\"mlflow.log_*()\"]\n        MINIO[\"MinIO Storage\"]\n        POSTGRES[\"PostgreSQL Metadata\"]\n    end\n    \n    TMS --\u003e PREPROCESS\n    OSM --\u003e PREPROCESS\n    PREPROCESS --\u003e CHIPS\n    PREPROCESS --\u003e LABELS\n    \n    CHIPS --\u003e DATAMODULE\n    LABELS --\u003e DATAMODULE\n    DATAMODULE --\u003e TRAIN_LOADER\n    DATAMODULE --\u003e VAL_LOADER\n    DATAMODULE --\u003e TEST_LOADER\n    \n    TRAIN_LOADER --\u003e TRAINER\n    VAL_LOADER --\u003e TRAINER\n    TRAINER --\u003e LITMODEL\n    LITMODEL --\u003e FORWARD\n    FORWARD --\u003e LOSS\n    LOSS --\u003e BACKWARD\n    \n    VAL_LOADER --\u003e VAL_STEP\n    TEST_LOADER --\u003e TEST_STEP\n    TEST_STEP --\u003e CONFUSION\n    TEST_LOADER --\u003e INFERENCE_EX\n    \n    TRAINER --\u003e CHECKPOINT\n    CHECKPOINT --\u003e CONVERSIONS\n    CONVERSIONS --\u003e PTH_OUT\n    CONVERSIONS --\u003e PT_OUT\n    CONVERSIONS --\u003e ONNX_OUT\n    \n    LITMODEL --\u003e STAC_GEN\n    CHECKPOINT --\u003e STAC_GEN\n    STAC_GEN --\u003e STAC_JSON\n    PT_OUT --\u003e DLPK_GEN\n    ONNX_OUT --\u003e DLPK_GEN\n    DLPK_GEN --\u003e DLPK_OUT\n    \n    PTH_OUT --\u003e MLFLOW_LOG\n    PT_OUT --\u003e MLFLOW_LOG\n    ONNX_OUT --\u003e MLFLOW_LOG\n    DLPK_OUT --\u003e MLFLOW_LOG\n    STAC_JSON --\u003e MLFLOW_LOG\n    CONFUSION --\u003e MLFLOW_LOG\n    INFERENCE_EX --\u003e MLFLOW_LOG\n    \n    MLFLOW_LOG --\u003e MINIO\n    MLFLOW_LOG --\u003e POSTGRES\n```\n\n**Sources:** [examplemodel/src/train.py:1-519](), [examplemodel/MLproject:1-63]()\n\n---\n\n## Output Artifacts Summary\n\nThe training pipeline generates a comprehensive set of outputs for different use cases:\n\n### Model Artifacts\n\n1. **best_model.pth** - Raw PyTorch state dictionary for flexible loading and fine-tuning\n2. **best_model.pt** - TorchScript traced model for production inference with JIT compilation\n3. **best_model.onnx** - ONNX format for cross-platform deployment (TensorRT, ONNX Runtime, etc.)\n4. **best_model.dlpk** - ESRI Deep Learning Package for ArcGIS Pro integration\n\n### Metadata and Documentation\n\n1. **stac_item.json** - STAC-MLM compliant metadata describing model capabilities, requirements, and provenance\n2. **model.emd** - ESRI Model Definition file with ArcGIS-specific configuration\n\n### Evaluation Artifacts\n\n1. **confusion_matrix.png** - Model performance visualization on test set\n2. **example_input.png** - Sample input satellite imagery\n3. **example_pred.png** - Model prediction on sample\n4. **example_target.png** - Ground truth for sample\n\n### Source Code\n\n1. **RefugeeCampDetector.py** - ESRI inference class bundled in DLPK\n2. Links to training and inference scripts in STAC metadata\n\n**Sources:** [examplemodel/src/train.py:436-496]()\n\n---\n\n## Integration with Infrastructure\n\nThe Example Model System integrates with the Infrastructure System through MLflow tracking:\n\n```mermaid\ngraph LR\n    subgraph \"Example Model System\"\n        TRAIN[\"train.py\"]\n        INFERENCE[\"inference.py\"]\n    end\n    \n    subgraph \"Infrastructure System\"\n        MLFLOW_SERVER[\"MLflow Tracking Server\"]\n        POSTGRES_DB[\"PostgreSQL Database\"]\n        MINIO_S3[\"MinIO Object Storage\"]\n    end\n    \n    subgraph \"Environment Variables\"\n        MLFLOW_URI[\"MLFLOW_TRACKING_URI\"]\n        AWS_KEY[\"AWS_ACCESS_KEY_ID\"]\n        AWS_SECRET[\"AWS_SECRET_ACCESS_KEY\"]\n        S3_ENDPOINT[\"MLFLOW_S3_ENDPOINT_URL\"]\n    end\n    \n    TRAIN --\u003e|\"mlflow.start_run()\"| MLFLOW_SERVER\n    INFERENCE --\u003e|\"mlflow.log_*()\"| MLFLOW_SERVER\n    \n    MLFLOW_SERVER --\u003e|\"stores metrics\"| POSTGRES_DB\n    MLFLOW_SERVER --\u003e|\"stores artifacts\"| MINIO_S3\n    \n    MLFLOW_URI -.-\u003e|\"configures\"| TRAIN\n    AWS_KEY -.-\u003e|\"authenticates\"| MINIO_S3\n    AWS_SECRET -.-\u003e|\"authenticates\"| MINIO_S3\n    S3_ENDPOINT -.-\u003e|\"routes to\"| MINIO_S3\n```\n\n**Configuration Example (from README):**\n```bash\nexport AWS_ACCESS_KEY_ID=mlflow\nexport AWS_SECRET_ACCESS_KEY=mlflow123\nexport MLFLOW_S3_ENDPOINT_URL=http://your_remote_server_minio:9000\n```\n\n**Sources:** [examplemodel/README.md:40-48](), [examplemodel/src/train.py:8-22]()\n\n---\n\n## Related Documentation\n\n- For model architecture details and PyTorch Lightning components, see [Model Overview and Architecture](#3.1)\n- For detailed training process and hyperparameters, see [Training Pipeline](#3.2)\n- For inference workflows and prediction generation, see [Inference System](#3.3)\n- For ESRI package creation and ArcGIS deployment, see [ESRI Integration and DLPK Generation](#3.4)\n- For MLproject entry point specifications, see [MLflow Project Structure](#3.5)\n- For dependency management and configuration, see [Dependencies and Configuration](#3.6)\n- For infrastructure setup and MLflow server deployment, see [Infrastructure System](#4)"])</script><script>self.__next_f.push([1,"1c:T3b39,"])</script><script>self.__next_f.push([1,"# Model Overview and Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/README.md](examplemodel/README.md)\n- [examplemodel/playground.ipynb](examplemodel/playground.ipynb)\n- [examplemodel/src/model.py](examplemodel/src/model.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a detailed technical overview of the refugee camp detection model architecture implemented in the OpenGeoAIModelHub example model system. It covers the U-Net based segmentation model (`RefugeeCampDetector`), the PyTorch Lightning training wrapper (`LitRefugeeCamp`), and the data handling components (`CampDataModule` and `CampDataset`).\n\nFor information about the training pipeline that uses these components, see [Training Pipeline](#3.2). For details on the inference system, see [Inference System](#3.3). For MLflow project orchestration, see [MLflow Project Structure](#3.5).\n\n---\n\n## Architecture Overview\n\nThe example model implements a semantic segmentation task for detecting refugee camps (or buildings) in satellite imagery. The architecture follows a simplified U-Net design without skip connections, implemented as a standalone PyTorch model (`RefugeeCampDetector`) and wrapped in a PyTorch Lightning module (`LitRefugeeCamp`) for training orchestration.\n\n### High-Level Component Diagram\n\n```mermaid\ngraph TB\n    subgraph \"Data Layer\"\n        CampDataset[\"CampDataset\u003cbr/\u003e(torch.utils.data.Dataset)\"]\n        CampDataModule[\"CampDataModule\u003cbr/\u003e(pl.LightningDataModule)\"]\n    end\n    \n    subgraph \"Model Layer\"\n        RefugeeCampDetector[\"RefugeeCampDetector\u003cbr/\u003e(nn.Module)\u003cbr/\u003eU-Net Architecture\"]\n        LitRefugeeCamp[\"LitRefugeeCamp\u003cbr/\u003e(pl.LightningModule)\u003cbr/\u003eTraining Wrapper\"]\n    end\n    \n    subgraph \"Training Components\"\n        BCELoss[\"nn.BCELoss()\"]\n        AdamOptimizer[\"torch.optim.Adam\"]\n        pixel_acc[\"pixel_acc()\u003cbr/\u003eAccuracy Metric\"]\n    end\n    \n    CampDataModule --\u003e|\"creates\"| CampDataset\n    CampDataModule --\u003e|\"provides train/val/test\"| LitRefugeeCamp\n    \n    LitRefugeeCamp --\u003e|\"contains\"| RefugeeCampDetector\n    LitRefugeeCamp --\u003e|\"uses\"| BCELoss\n    LitRefugeeCamp --\u003e|\"optimizes with\"| AdamOptimizer\n    LitRefugeeCamp --\u003e|\"evaluates with\"| pixel_acc\n```\n\nSources: [examplemodel/src/model.py:1-175]()\n\n---\n\n## RefugeeCampDetector: U-Net Model Architecture\n\nThe `RefugeeCampDetector` class implements a simplified U-Net architecture for binary semantic segmentation. The model consists of an encoder path that progressively downsamples the input, a bottleneck layer, and a decoder path that upsamples back to the original resolution.\n\n### Network Structure Diagram\n\n```mermaid\ngraph LR\n    subgraph \"Encoder Path\"\n        Input[\"Input\u003cbr/\u003e3x256x256\"]\n        conv1[\"conv1\u003cbr/\u003eConv2d(3,64)\u003cbr/\u003eReLU\"]\n        pool1[\"pool1\u003cbr/\u003eMaxPool2d(2)\"]\n        conv2[\"conv2\u003cbr/\u003eConv2d(64,128)\u003cbr/\u003eReLU\"]\n        pool2[\"pool2\u003cbr/\u003eMaxPool2d(2)\"]\n        conv3[\"conv3\u003cbr/\u003eConv2d(128,256)\u003cbr/\u003eReLU\"]\n        pool3[\"pool3\u003cbr/\u003eMaxPool2d(2)\"]\n    end\n    \n    subgraph \"Bottleneck\"\n        conv4[\"conv4\u003cbr/\u003eConv2d(256,512)\u003cbr/\u003eReLU\"]\n    end\n    \n    subgraph \"Decoder Path\"\n        up1[\"up1\u003cbr/\u003eConvTranspose2d(512,256)\"]\n        up2[\"up2\u003cbr/\u003eConvTranspose2d(256,128)\"]\n        up3[\"up3\u003cbr/\u003eConvTranspose2d(128,64)\"]\n        outc[\"outc\u003cbr/\u003eConv2d(64,1)\"]\n        sigmoid[\"Sigmoid\"]\n        Output[\"Output\u003cbr/\u003e1x256x256\"]\n    end\n    \n    Input --\u003e conv1 --\u003e pool1 --\u003e conv2 --\u003e pool2 --\u003e conv3 --\u003e pool3\n    pool3 --\u003e conv4\n    conv4 --\u003e up1 --\u003e up2 --\u003e up3 --\u003e outc --\u003e sigmoid --\u003e Output\n```\n\nSources: [examplemodel/src/model.py:103-137]()\n\n### Layer Configuration\n\nThe following table details the layer-by-layer configuration of the `RefugeeCampDetector` model:\n\n| Layer Name | Layer Type | Input Channels | Output Channels | Kernel/Stride | Output Size (HxW) | Purpose |\n|------------|------------|----------------|-----------------|---------------|-------------------|---------|\n| `conv1` | Conv2d | 3 | 64 | 3x3, padding=1 | 256x256 | Initial feature extraction |\n| `pool1` | MaxPool2d | 64 | 64 | 2x2 | 128x128 | Spatial downsampling |\n| `conv2` | Conv2d | 64 | 128 | 3x3, padding=1 | 128x128 | Feature expansion |\n| `pool2` | MaxPool2d | 128 | 128 | 2x2 | 64x64 | Spatial downsampling |\n| `conv3` | Conv2d | 128 | 256 | 3x3, padding=1 | 64x64 | Feature expansion |\n| `pool3` | MaxPool2d | 256 | 256 | 2x2 | 32x32 | Spatial downsampling |\n| `conv4` | Conv2d | 256 | 512 | 3x3, padding=1 | 32x32 | Bottleneck features |\n| `up1` | ConvTranspose2d | 512 | 256 | 2x2, stride=2 | 64x64 | Spatial upsampling |\n| `up2` | ConvTranspose2d | 256 | 128 | 2x2, stride=2 | 128x128 | Spatial upsampling |\n| `up3` | ConvTranspose2d | 128 | 64 | 2x2, stride=2 | 256x256 | Spatial upsampling |\n| `outc` | Conv2d | 64 | 1 | 1x1 | 256x256 | Final prediction layer |\n| (activation) | Sigmoid | 1 | 1 | - | 256x256 | Probability output |\n\nSources: [examplemodel/src/model.py:104-116](), [examplemodel/src/model.py:118-137]()\n\n### Design Characteristics\n\nThe model has the following notable characteristics:\n\n- **No Skip Connections**: Unlike the canonical U-Net architecture, this implementation does not include skip connections between encoder and decoder paths. This simplifies the architecture but may limit feature reuse.\n- **Progressive Channel Expansion**: Encoder path doubles channels at each stage (364128256512), capturing increasingly complex features.\n- **Symmetric Decoder**: Decoder mirrors the encoder structure, progressively halving channels (512256128641).\n- **Fixed Input Size**: Model expects 256x256 pixel RGB images as input, producing 256x256 single-channel probability masks.\n- **Binary Segmentation**: Output is a single-channel probability map for the \"refugee camp/building\" class.\n\nSources: [examplemodel/src/model.py:103-137](), [examplemodel/README.md:50-52]()\n\n---\n\n## LitRefugeeCamp: PyTorch Lightning Training Module\n\nThe `LitRefugeeCamp` class wraps the `RefugeeCampDetector` model in a PyTorch Lightning module, providing a standardized training interface with automatic logging, checkpointing, and distributed training support.\n\n### Training Loop Flow\n\n```mermaid\nsequenceDiagram\n    participant Trainer as \"pl.Trainer\"\n    participant LitRefugeeCamp\n    participant RefugeeCampDetector\n    participant BCELoss\n    participant pixel_acc\n    participant Logger as \"MLflow Logger\"\n    \n    Trainer-\u003e\u003eLitRefugeeCamp: training_step(batch, batch_idx)\n    LitRefugeeCamp-\u003e\u003eLitRefugeeCamp: x, y = batch\n    LitRefugeeCamp-\u003e\u003eRefugeeCampDetector: forward(x)\n    RefugeeCampDetector--\u003e\u003eLitRefugeeCamp: y_hat\n    LitRefugeeCamp-\u003e\u003eBCELoss: criterion(y_hat, y)\n    BCELoss--\u003e\u003eLitRefugeeCamp: loss\n    LitRefugeeCamp-\u003e\u003epixel_acc: pixel_acc(y_hat, y)\n    pixel_acc--\u003e\u003eLitRefugeeCamp: accuracy\n    LitRefugeeCamp-\u003e\u003eLogger: log(\"train_loss\", loss)\n    LitRefugeeCamp-\u003e\u003eLogger: log(\"train_acc\", accuracy)\n    LitRefugeeCamp--\u003e\u003eTrainer: {\"loss\": loss, \"acc\": accuracy}\n    \n    Note over Trainer,Logger: Same flow for validation_step and test_step\n```\n\nSources: [examplemodel/src/model.py:145-174]()\n\n### Component Details\n\n| Component | Implementation | Configuration | Purpose |\n|-----------|---------------|---------------|---------|\n| Base Model | `RefugeeCampDetector()` | Initialized in `__init__` | Core U-Net architecture |\n| Loss Function | `nn.BCELoss()` | Binary Cross-Entropy | Measures pixel-wise prediction error |\n| Optimizer | `torch.optim.Adam` | Learning rate configurable (default: 1e-3) | Parameter updates |\n| Metrics | `pixel_acc()` function | Threshold at 0.5 | Binary pixel accuracy |\n| Logging | `self.log()` | Automatic MLflow integration | Tracks loss and accuracy |\n\nSources: [examplemodel/src/model.py:145-174]()\n\n### Training Step Implementation\n\nThe `_shared_step` method [examplemodel/src/model.py:155-162]() implements a unified training/validation/test logic:\n\n1. **Forward Pass**: Input images `x` are passed through the model to generate predictions `y_hat`\n2. **Loss Computation**: Binary Cross-Entropy loss is calculated between predictions and ground truth `y`\n3. **Accuracy Calculation**: `pixel_acc()` function computes percentage of correctly classified pixels (threshold=0.5)\n4. **Logging**: Both loss and accuracy are logged with appropriate stage prefix (`train_`, `val_`, `test_`)\n5. **Return**: Dictionary containing loss and accuracy for optional aggregation\n\nThe optimizer is configured in `configure_optimizers()` [examplemodel/src/model.py:173-174]() using Adam with the learning rate specified during initialization.\n\nSources: [examplemodel/src/model.py:145-174]()\n\n---\n\n## Data Handling Components\n\n### CampDataset: Custom PyTorch Dataset\n\nThe `CampDataset` class [examplemodel/src/model.py:12-30]() implements a custom PyTorch `Dataset` for loading paired image-label data from disk.\n\n**Key Features:**\n- **Paired Loading**: Loads matching image and label files from separate directories\n- **Format Conversion**: Images converted to RGB, labels to grayscale\n- **Transform Support**: Applies optional transforms to images and labels independently\n- **File Matching**: Assumes image and label files have identical filenames\n\n**Constructor Parameters:**\n- `image_dir`: Path to directory containing RGB image chips\n- `label_dir`: Path to directory containing label masks\n- `transform`: Optional transform pipeline for images\n- `target_transform`: Optional transform pipeline for labels\n\nSources: [examplemodel/src/model.py:12-30]()\n\n### CampDataModule: PyTorch Lightning DataModule\n\nThe `CampDataModule` class [examplemodel/src/model.py:33-100]() encapsulates all data-related logic in a reusable PyTorch Lightning component.\n\n```mermaid\ngraph TB\n    subgraph \"CampDataModule Setup\"\n        Init[\"__init__()\u003cbr/\u003eStore paths \u0026 config\"]\n        Setup[\"setup()\u003cbr/\u003eCreate datasets\"]\n        Transform[\"Create Transforms\"]\n        FullDataset[\"Create CampDataset\u003cbr/\u003e(full dataset)\"]\n        Split[\"random_split()\u003cbr/\u003eTrain/Val/Test\"]\n    end\n    \n    subgraph \"Data Splits\"\n        TrainDS[\"train_ds\u003cbr/\u003e(70% default)\"]\n        ValDS[\"val_ds\u003cbr/\u003e(15% default)\"]\n        TestDS[\"test_ds\u003cbr/\u003e(15% default)\"]\n    end\n    \n    subgraph \"DataLoaders\"\n        TrainLoader[\"train_dataloader()\u003cbr/\u003eshuffle=True\u003cbr/\u003eworkers=4\"]\n        ValLoader[\"val_dataloader()\u003cbr/\u003eshuffle=False\u003cbr/\u003eworkers=4\"]\n        TestLoader[\"test_dataloader()\u003cbr/\u003eshuffle=False\u003cbr/\u003eworkers=4\"]\n    end\n    \n    Init --\u003e Setup\n    Setup --\u003e Transform\n    Transform --\u003e FullDataset\n    FullDataset --\u003e Split\n    \n    Split --\u003e TrainDS\n    Split --\u003e ValDS\n    Split --\u003e TestDS\n    \n    TrainDS --\u003e TrainLoader\n    ValDS --\u003e ValLoader\n    TestDS --\u003e TestLoader\n```\n\nSources: [examplemodel/src/model.py:33-100]()\n\n### Data Transformation Pipeline\n\nThe `CampDataModule.setup()` method [examplemodel/src/model.py:52-73]() defines standard transformation pipelines:\n\n**Image Transform:**\n```python\ntransforms.Compose([\n    transforms.Resize((256, 256)),                          # Resize to fixed size\n    transforms.ToTensor(),                                  # Convert to [0,1] tensor\n    transforms.Normalize([0.485, 0.456, 0.406],            # ImageNet mean\n                        [0.229, 0.224, 0.225])              # ImageNet std\n])\n```\n\n**Label Transform:**\n```python\ntransforms.Compose([\n    transforms.Resize((256, 256), \n                     transforms.InterpolationMode.NEAREST),  # Preserve discrete values\n    transforms.ToTensor()                                    # Convert to [0,1] tensor\n])\n```\n\nThe use of ImageNet normalization statistics suggests the model may benefit from transfer learning in future iterations, though the current implementation trains from scratch [examplemodel/README.md:50-52]().\n\nSources: [examplemodel/src/model.py:53-65]()\n\n### Configuration Parameters\n\n| Parameter | Default Value | Description |\n|-----------|---------------|-------------|\n| `batch_size` | 32 | Number of samples per batch |\n| `val_ratio` | 0.15 | Fraction of data for validation |\n| `test_ratio` | 0.15 | Fraction of data for testing |\n| `seed` | 62 | Random seed for reproducible splits |\n| `num_workers` | 4 | Parallel data loading workers |\n| `pin_memory` | True | Pin memory for faster GPU transfer |\n\nSources: [examplemodel/src/model.py:34-50](), [examplemodel/src/model.py:75-100]()\n\n---\n\n## Input/Output Specifications\n\n### Model Input Requirements\n\n| Property | Specification | Notes |\n|----------|--------------|-------|\n| **Format** | PyTorch Tensor | `torch.Tensor` with dtype float32 |\n| **Shape** | (B, 3, 256, 256) | Batch size  RGB channels  Height  Width |\n| **Color Space** | RGB | Red, Green, Blue channels |\n| **Value Range** | Normalized | Mean=[0.485, 0.456, 0.406], Std=[0.229, 0.224, 0.225] |\n| **Source** | OpenAerialMap | Satellite imagery tiles |\n\n### Model Output Format\n\n| Property | Specification | Notes |\n|----------|--------------|-------|\n| **Format** | PyTorch Tensor | `torch.Tensor` with dtype float32 |\n| **Shape** | (B, 1, 256, 256) | Batch size  1 channel  Height  Width |\n| **Value Range** | [0.0, 1.0] | Probability per pixel (sigmoid activation) |\n| **Interpretation** | Binary segmentation | Values \u003e 0.5 typically classified as \"building/camp\" |\n| **Ground Truth Source** | OpenStreetMap | Building footprint polygons |\n\nSources: [examplemodel/src/model.py:118-137](), [examplemodel/README.md:50-52](), [examplemodel/playground.ipynb:55-78]()\n\n### Inference Example\n\nThe [examplemodel/playground.ipynb]() demonstrates the complete inference flow:\n\n1. **Load Model**: `torch.jit.load()` loads serialized model [examplemodel/playground.ipynb:44-46]()\n2. **Prepare Input**: Create or load tensor with shape (1, 3, 256, 256) [examplemodel/playground.ipynb:55-56]()\n3. **Forward Pass**: `model(input)` produces raw probability output [examplemodel/playground.ipynb:67-72]()\n4. **Thresholding**: Apply threshold (e.g., 0.5) to convert to binary mask [examplemodel/playground.ipynb:75-78]()\n\n---\n\n## Model Design Rationale\n\nThe architecture balances several design objectives:\n\n1. **Simplicity**: Simplified U-Net without skip connections reduces complexity, making it easier to understand and modify as a template model.\n\n2. **Self-Contained**: No pretrained backbone dependency allows the model to serve as a complete example without external model dependencies [examplemodel/README.md:50-52]().\n\n3. **Reproducibility**: Fixed random seeds, deterministic data splits, and comprehensive logging ensure reproducible experiments.\n\n4. **Extensibility**: Clear separation between model architecture (`RefugeeCampDetector`), training logic (`LitRefugeeCamp`), and data handling (`CampDataModule`) facilitates modifications.\n\n5. **Standards Compliance**: PyTorch Lightning integration provides automatic support for distributed training, mixed precision, and checkpoint management without custom code.\n\nThe model serves as a reference implementation demonstrating best practices for GeoAI model development rather than achieving state-of-the-art performance.\n\nSources: [examplemodel/README.md:1-52](), [examplemodel/src/model.py:1-175]()"])</script><script>self.__next_f.push([1,"1d:T6f1f,"])</script><script>self.__next_f.push([1,"# Training Pipeline\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/playground.ipynb](examplemodel/playground.ipynb)\n- [examplemodel/src/model.py](examplemodel/src/model.py)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the training pipeline for the refugee camp detection model, covering the end-to-end process from data loading through model training to artifact generation and MLflow logging. The pipeline is implemented in [examplemodel/src/train.py]() and orchestrates all components required for reproducible model training.\n\nFor information about the model architecture itself, see [Model Overview and Architecture](#3.1). For details on using trained models for prediction, see [Inference System](#3.3). For MLflow orchestration and entry points, see [MLflow Project Structure](#3.5).\n\n**Sources:** [examplemodel/src/train.py:1-519]()\n\n---\n\n## Training Pipeline Architecture\n\nThe training pipeline follows a structured workflow that integrates PyTorch Lightning for model training, MLflow for experiment tracking, and multiple export formats for deployment flexibility.\n\n```mermaid\nflowchart TD\n    START[\"train_model() Entry Point\"]\n    MLFLOW_START[\"mlflow.start_run()\"]\n    SYSINFO[\"get_system_info()\"]\n    DATAMOD[\"CampDataModule initialization\"]\n    STATS[\"calculate_dataset_statistics()\"]\n    CHECKPOINT[\"ModelCheckpoint callback\"]\n    TRAINER[\"pl.Trainer creation\"]\n    TRAINING[\"trainer.fit()\"]\n    BEST[\"Load best checkpoint\"]\n    \n    subgraph \"Artifact Generation\"\n        PTH[\"torch.save()  best_model.pth\"]\n        TORCHSCRIPT[\"torch.jit.trace()  best_model.pt\"]\n        ONNX[\"torch.onnx.export()  best_model.onnx\"]\n        STAC[\"create_stac_mlm_item()  stac_item.json\"]\n        DLPK[\"create_dlpk()  best_model.dlpk\"]\n    end\n    \n    subgraph \"MLflow Logging\"\n        LOG_ARTIFACTS[\"mlflow.log_artifact()\"]\n        LOG_MODEL[\"mlflow.pytorch.log_model()\"]\n        LOG_INFERENCE[\"log_inference_example()\"]\n        LOG_CONFUSION[\"log_confusion_matrix()\"]\n    end\n    \n    START --\u003e MLFLOW_START\n    MLFLOW_START --\u003e SYSINFO\n    SYSINFO --\u003e DATAMOD\n    DATAMOD --\u003e STATS\n    STATS --\u003e CHECKPOINT\n    CHECKPOINT --\u003e TRAINER\n    TRAINER --\u003e TRAINING\n    TRAINING --\u003e BEST\n    \n    BEST --\u003e PTH\n    BEST --\u003e TORCHSCRIPT\n    BEST --\u003e ONNX\n    BEST --\u003e STAC\n    BEST --\u003e DLPK\n    \n    PTH --\u003e LOG_ARTIFACTS\n    TORCHSCRIPT --\u003e LOG_ARTIFACTS\n    ONNX --\u003e LOG_ARTIFACTS\n    STAC --\u003e LOG_ARTIFACTS\n    DLPK --\u003e LOG_ARTIFACTS\n    \n    LOG_ARTIFACTS --\u003e LOG_MODEL\n    LOG_MODEL --\u003e LOG_INFERENCE\n    LOG_INFERENCE --\u003e LOG_CONFUSION\n```\n\n**Sources:** [examplemodel/src/train.py:370-507]()\n\n---\n\n## System Information Collection\n\nThe `get_system_info()` function captures comprehensive hardware and software configuration details for reproducibility and debugging. This information is logged to MLflow and embedded in STAC-MLM metadata.\n\n### Collected Metadata\n\n| Category | Information Collected |\n|----------|----------------------|\n| **Platform** | OS type, architecture, processor |\n| **Python** | Version, virtual environment details |\n| **PyTorch** | PyTorch version, TorchVision version |\n| **CUDA** | Availability, version, GPU count |\n| **GPU Details** | Name, total memory, free memory, used memory per device |\n| **CPU** | Core count |\n| **Memory** | Total and available system memory |\n\n### GPU Detection\n\nThe function attempts to import `pynvml` (NVIDIA Management Library) to query GPU information. If unavailable or an error occurs, it falls back to an empty GPU list, ensuring the training pipeline continues on CPU-only systems.\n\n```mermaid\nflowchart LR\n    SYSINFO[\"get_system_info()\"]\n    PYNVML[\"Import pynvml\"]\n    INIT[\"pynvml.nvmlInit()\"]\n    COUNT[\"nvmlDeviceGetCount()\"]\n    LOOP[\"Loop through GPUs\"]\n    HANDLE[\"nvmlDeviceGetHandleByIndex(i)\"]\n    INFO[\"Collect name and memory_info\"]\n    FALLBACK[\"Return empty gpu_info list\"]\n    RETURN[\"Return system_info dict\"]\n    \n    SYSINFO --\u003e PYNVML\n    PYNVML --\u003e|Success| INIT\n    PYNVML --\u003e|Exception| FALLBACK\n    INIT --\u003e COUNT\n    COUNT --\u003e LOOP\n    LOOP --\u003e HANDLE\n    HANDLE --\u003e INFO\n    INFO --\u003e LOOP\n    LOOP --\u003e|Complete| RETURN\n    FALLBACK --\u003e RETURN\n```\n\n**Sources:** [examplemodel/src/train.py:25-60]()\n\n---\n\n## Data Module Initialization\n\nThe training pipeline uses `CampDataModule`, a PyTorch Lightning `LightningDataModule` that handles data loading, preprocessing, and splitting.\n\n### CampDataModule Configuration\n\nThe data module is instantiated with the following parameters:\n\n```python\ndata_module = CampDataModule(\n    image_dir=args.chips_dir,\n    label_dir=args.labels_dir,\n    batch_size=args.batch_size,\n)\n```\n\n### Internal Data Splitting\n\n| Split | Default Ratio | Purpose |\n|-------|---------------|---------|\n| **Training** | 70% | Model parameter optimization |\n| **Validation** | 15% | Hyperparameter tuning, checkpoint selection |\n| **Test** | 15% | Final performance evaluation |\n\nThe splits are created using `torch.utils.data.random_split()` with a fixed seed (62) for reproducibility.\n\n### Transformations\n\n**Image Transformations:**\n- Resize to 256256\n- Convert to tensor\n- Normalize with ImageNet statistics: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n\n**Label Transformations:**\n- Resize to 256256 using NEAREST interpolation (preserves discrete values)\n- Convert to tensor\n\n**Sources:** [examplemodel/src/model.py:33-100](), [examplemodel/src/train.py:389-393]()\n\n---\n\n## Dataset Statistics Calculation\n\nThe `calculate_dataset_statistics()` function computes dataset-level statistics by iterating through the training data loader. These statistics are logged to MLflow for dataset provenance tracking.\n\n### Computed Statistics\n\n```mermaid\ngraph TB\n    FUNC[\"calculate_dataset_statistics()\"]\n    SETUP[\"data_module.setup()\"]\n    LOADER[\"train_dataloader()\"]\n    ITERATE[\"Iterate batches\"]\n    \n    subgraph \"Per-Batch Accumulation\"\n        SUM[\"Accumulate channel_sum\"]\n        SQ[\"Accumulate channel_squared_sum\"]\n        COUNT[\"Increment total_samples\"]\n    end\n    \n    subgraph \"Final Calculations\"\n        MEAN[\"mean = channel_sum / total_samples\"]\n        STD[\"std = sqrt(variance)\"]\n    end\n    \n    subgraph \"Returned Statistics\"\n        COUNTS[\"train/val/test sample counts\"]\n        DIMS[\"image dimensions: 2562563\"]\n        STATS[\"pixel mean and std per channel\"]\n        CLASSES[\"num_classes: 2\"]\n    end\n    \n    FUNC --\u003e SETUP\n    SETUP --\u003e LOADER\n    LOADER --\u003e ITERATE\n    ITERATE --\u003e SUM\n    ITERATE --\u003e SQ\n    ITERATE --\u003e COUNT\n    SUM --\u003e MEAN\n    SQ --\u003e STD\n    MEAN --\u003e STATS\n    STD --\u003e STATS\n    ITERATE --\u003e COUNTS\n    COUNTS --\u003e DIMS\n    DIMS --\u003e STATS\n    STATS --\u003e CLASSES\n```\n\n### Output Schema\n\n```python\n{\n    \"train_samples\": int,\n    \"val_samples\": int,\n    \"test_samples\": int,\n    \"total_samples\": int,\n    \"image_channels\": 3,\n    \"image_height\": 256,\n    \"image_width\": 256,\n    \"pixel_mean\": [float, float, float],  # RGB channels\n    \"pixel_std\": [float, float, float],   # RGB channels\n    \"num_classes\": 2\n}\n```\n\n**Sources:** [examplemodel/src/train.py:63-98]()\n\n---\n\n## Training Configuration\n\n### Command-Line Arguments\n\nThe training script accepts the following command-line arguments:\n\n| Argument | Type | Default | Description |\n|----------|------|---------|-------------|\n| `--epochs` | int | 1 | Number of training epochs |\n| `--batch_size` | int | 32 | Batch size for training |\n| `--chips_dir` | str | `data/train/sample/chips` | Path to input image chips |\n| `--labels_dir` | str | `data/train/sample/labels` | Path to label masks |\n| `--lr` | float | 1e-3 | Learning rate (passed to LitRefugeeCamp) |\n\n### ModelCheckpoint Callback\n\nThe training pipeline uses PyTorch Lightning's `ModelCheckpoint` callback to save the best model based on validation loss:\n\n```python\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"checkpoints\",\n    filename=\"epoch={epoch}-step={step}\",\n    save_top_k=1,\n    verbose=True,\n    monitor=\"val_loss\",\n    mode=\"min\",\n)\n```\n\n**Configuration Details:**\n- **dirpath:** Checkpoints saved to `checkpoints/` directory\n- **save_top_k=1:** Only the best checkpoint is retained\n- **monitor=\"val_loss\":** Selection criterion is validation loss\n- **mode=\"min\":** Lower validation loss is better\n\n### PyTorch Lightning Trainer\n\n```python\ntrainer = pl.Trainer(\n    max_epochs=args.epochs,\n    accelerator=\"auto\",\n    callbacks=[checkpoint_callback],\n    logger=False,\n)\n```\n\n**Trainer Configuration:**\n- **accelerator=\"auto\":** Automatically selects GPU if available, otherwise CPU\n- **logger=False:** MLflow logging is handled manually for greater control\n\n**Sources:** [examplemodel/src/train.py:405-419](), [examplemodel/src/train.py:510-516]()\n\n---\n\n## Model Training Execution\n\n### Training Process\n\n```mermaid\nsequenceDiagram\n    participant Script as \"train.py\"\n    participant Trainer as \"pl.Trainer\"\n    participant Model as \"LitRefugeeCamp\"\n    participant DataMod as \"CampDataModule\"\n    participant Checkpoint as \"ModelCheckpoint\"\n    \n    Script-\u003e\u003eModel: LitRefugeeCamp()\n    Script-\u003e\u003eTrainer: trainer.fit(model, data_module)\n    \n    loop For each epoch\n        Trainer-\u003e\u003eDataMod: train_dataloader()\n        DataMod--\u003e\u003eTrainer: batch iterator\n        \n        loop For each batch\n            Trainer-\u003e\u003eModel: training_step(batch, batch_idx)\n            Model-\u003e\u003eModel: Forward pass\n            Model-\u003e\u003eModel: Compute loss (BCELoss)\n            Model-\u003e\u003eModel: Compute pixel_acc\n            Model-\u003e\u003eModel: self.log(\"train_loss\", loss)\n            Model-\u003e\u003eModel: self.log(\"train_acc\", acc)\n            Model--\u003e\u003eTrainer: {\"loss\": loss, \"acc\": acc}\n        end\n        \n        Trainer-\u003e\u003eDataMod: val_dataloader()\n        DataMod--\u003e\u003eTrainer: batch iterator\n        \n        loop For each validation batch\n            Trainer-\u003e\u003eModel: validation_step(batch, batch_idx)\n            Model-\u003e\u003eModel: Forward pass (no grad)\n            Model-\u003e\u003eModel: Compute val_loss\n            Model-\u003e\u003eModel: self.log(\"val_loss\", loss)\n            Model--\u003e\u003eTrainer: {\"loss\": loss, \"acc\": acc}\n        end\n        \n        Trainer-\u003e\u003eCheckpoint: Check if best model\n        Checkpoint-\u003e\u003eCheckpoint: Compare val_loss\n        Checkpoint-\u003e\u003eCheckpoint: Save if improved\n    end\n    \n    Trainer--\u003e\u003eScript: Training complete\n    Script-\u003e\u003eCheckpoint: checkpoint_callback.best_model_path\n    Checkpoint--\u003e\u003eScript: \"checkpoints/epoch=X-step=Y.ckpt\"\n```\n\n### Loss Function and Metrics\n\nThe `LitRefugeeCamp` model uses:\n- **Loss Function:** Binary Cross-Entropy Loss (`nn.BCELoss`)\n- **Metric:** Pixel accuracy computed by `pixel_acc()` function\n  - Applies threshold of 0.5 to predictions\n  - Computes percentage of correctly classified pixels\n\n### Optimizer\n\nThe model uses Adam optimizer with the specified learning rate:\n```python\ntorch.optim.Adam(self.parameters(), lr=self.lr)\n```\n\n**Sources:** [examplemodel/src/train.py:414-422](), [examplemodel/src/model.py:145-174]()\n\n---\n\n## Artifact Generation\n\nAfter training completes, the pipeline generates multiple artifact formats to support different deployment scenarios.\n\n### Artifact Types and Purposes\n\n```mermaid\ngraph TB\n    BEST[\"Best Checkpoint\u003cbr/\u003eepoch=X-step=Y.ckpt\"]\n    \n    subgraph \"Exported Artifacts\"\n        PTH[\"best_model.pth\u003cbr/\u003eRaw state dict\"]\n        PT[\"best_model.pt\u003cbr/\u003eTorchScript traced\"]\n        ONNX[\"best_model.onnx\u003cbr/\u003eONNX format\"]\n        DLPK[\"best_model.dlpk\u003cbr/\u003eESRI package\"]\n        STAC[\"stac_item.json\u003cbr/\u003eSTAC-MLM metadata\"]\n        EMD[\"model.emd\u003cbr/\u003eESRI model definition\"]\n    end\n    \n    subgraph \"Use Cases\"\n        PTH_USE[\"PyTorch model loading\u003cbr/\u003eFurther training\"]\n        PT_USE[\"Optimized PyTorch inference\u003cbr/\u003eTorchServe deployment\"]\n        ONNX_USE[\"Cross-platform inference\u003cbr/\u003eONNX Runtime\"]\n        DLPK_USE[\"ArcGIS Pro integration\u003cbr/\u003eGeospatial analysis\"]\n        STAC_USE[\"Model discovery\u003cbr/\u003eMetadata standardization\"]\n    end\n    \n    BEST --\u003e PTH\n    BEST --\u003e PT\n    BEST --\u003e ONNX\n    BEST --\u003e DLPK\n    BEST --\u003e STAC\n    \n    PTH --\u003e PTH_USE\n    PT --\u003e PT_USE\n    ONNX --\u003e ONNX_USE\n    DLPK --\u003e DLPK_USE\n    STAC --\u003e STAC_USE\n    \n    PT --\u003e DLPK\n    STAC --\u003e DLPK\n```\n\n### PyTorch State Dictionary (.pth)\n\nRaw PyTorch state dictionary containing model weights:\n```python\ntorch.save(model.state_dict(), \"meta/best_model.pth\")\n```\n\n**Use Case:** Loading weights into the same model architecture for fine-tuning or continued training.\n\n### TorchScript Model (.pt)\n\nTraced model using TorchScript for optimized inference:\n```python\ntorch_model = clean_model.model\ntorch_model.eval()\ntraced_model = torch.jit.trace(torch_model, torch.randn(1, 3, 256, 256))\ntorch.jit.save(traced_model, \"meta/best_model.pt\")\n```\n\n**Key Details:**\n- Extracts the underlying `RefugeeCampDetector` module (not the Lightning wrapper)\n- Traces with a dummy input of shape (1, 3, 256, 256)\n- Enables deployment without PyTorch Lightning dependency\n\n### ONNX Model (.onnx)\n\nCross-platform model format:\n```python\ntorch.onnx.export(\n    torch_model,\n    dummy_input,\n    \"meta/best_model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n)\n```\n\n**Configuration:**\n- **opset_version=11:** Compatible with most ONNX runtimes\n- **dynamic_axes:** Batch dimension is dynamic, allowing variable batch sizes\n- **input/output names:** Named tensors for clarity\n\n### ESRI Deep Learning Package (.dlpk)\n\nPackage for ArcGIS integration:\n```python\ncreate_dlpk(emd_path, pt_path, esri_inference_path, dlpk_path)\n```\n\nThe DLPK includes:\n- TorchScript model (.pt)\n- ESRI model definition (model.emd)\n- Inference script (RefugeeCampDetector.py)\n- STAC-MLM metadata\n\nFor details on DLPK creation, see [ESRI Integration and DLPK Generation](#3.4).\n\n**Sources:** [examplemodel/src/train.py:436-468]()\n\n---\n\n## STAC-MLM Metadata Generation\n\nThe `create_stac_mlm_item()` function generates comprehensive metadata following the STAC (SpatioTemporal Asset Catalog) Machine Learning Model (MLM) extension specification.\n\n### STAC Item Structure\n\n```mermaid\ngraph TB\n    STAC[\"STAC Item\u003cbr/\u003estac_item.json\"]\n    \n    subgraph \"Core STAC Fields\"\n        TYPE[\"type: Feature\"]\n        VERSION[\"stac_version: 1.1.0\"]\n        ID[\"id: refugee-camp-detector-v1.0.0\"]\n        GEOM[\"geometry: Polygon\"]\n    end\n    \n    subgraph \"STAC Extensions\"\n        MLM[\"MLM Extension\u003cbr/\u003ev1.5.0\"]\n        FILE[\"File Extension\u003cbr/\u003ev1.0.0\"]\n        PROC[\"Processing Extension\u003cbr/\u003ev1.1.0\"]\n    end\n    \n    subgraph \"Properties\"\n        BASIC[\"datetime, title, description\"]\n        KEYWORDS[\"keywords, license, mission\"]\n        BANDS[\"bands: red, green, blue\"]\n        MLM_PROPS[\"mlm:* fields\"]\n        PROC_PROPS[\"processing:* fields\"]\n    end\n    \n    subgraph \"Assets\"\n        MODELS[\"Model artifacts\u003cbr/\u003epth, pt, onnx, dlpk\"]\n        CODE[\"Source code\u003cbr/\u003etrain.py, inference.py\"]\n        IMAGES[\"Visualization assets\u003cbr/\u003econfusion matrix, examples\"]\n        META[\"Metadata files\u003cbr/\u003emodel.emd, requirements.txt\"]\n    end\n    \n    STAC --\u003e TYPE\n    STAC --\u003e VERSION\n    STAC --\u003e ID\n    STAC --\u003e GEOM\n    STAC --\u003e MLM\n    STAC --\u003e FILE\n    STAC --\u003e PROC\n    STAC --\u003e BASIC\n    STAC --\u003e KEYWORDS\n    STAC --\u003e BANDS\n    STAC --\u003e MLM_PROPS\n    STAC --\u003e PROC_PROPS\n    STAC --\u003e MODELS\n    STAC --\u003e CODE\n    STAC --\u003e IMAGES\n    STAC --\u003e META\n```\n\n### MLM Extension Properties\n\nThe metadata includes extensive MLM-specific fields:\n\n| Property | Example Value | Description |\n|----------|--------------|-------------|\n| `mlm:name` | \"RefugeeCampDetector\" | Model name |\n| `mlm:architecture` | \"U-Net\" | Neural network architecture |\n| `mlm:tasks` | [\"semantic-segmentation\"] | ML task types |\n| `mlm:framework` | \"PyTorch\" | Deep learning framework |\n| `mlm:framework_version` | torch.__version__ | Framework version |\n| `mlm:total_parameters` | Computed from model | Total parameter count |\n| `mlm:memory_size` | Computed from model | Memory footprint in bytes |\n| `mlm:batch_size_suggestion` | 32 | Recommended batch size |\n| `mlm:accelerator` | \"cuda\" or \"cpu\" | Required accelerator type |\n| `mlm:accelerator_count` | GPU count | Number of accelerators used |\n\n### Input/Output Specifications\n\n**Input Specification:**\n```python\n\"mlm:input\": [\n    {\n        \"name\": \"satellite_image\",\n        \"bands\": [\"red\", \"green\", \"blue\"],\n        \"input\": {\n            \"shape\": [-1, 3, 256, 256],\n            \"dim_order\": [\"batch\", \"bands\", \"height\", \"width\"],\n            \"data_type\": \"float32\",\n        },\n        \"value_scaling\": [\n            {\"type\": \"z-score\", \"mean\": 0.485, \"stddev\": 0.229},  # Red\n            {\"type\": \"z-score\", \"mean\": 0.456, \"stddev\": 0.224},  # Green\n            {\"type\": \"z-score\", \"mean\": 0.406, \"stddev\": 0.225},  # Blue\n        ],\n    }\n]\n```\n\n**Output Specification:**\n```python\n\"mlm:output\": [\n    {\n        \"name\": \"segmentation_mask\",\n        \"result\": {\n            \"shape\": [-1, 1, 256, 256],\n            \"dim_order\": [\"batch\", \"channel\", \"height\", \"width\"],\n            \"data_type\": \"float32\",\n        },\n        \"classification:classes\": [\n            {\"value\": 0, \"name\": \"background\"},\n            {\"value\": 1, \"name\": \"refugee_camp\"},\n        ],\n    }\n]\n```\n\n### Asset Catalog\n\nThe STAC item catalogs 15 different assets:\n\n| Asset Role | Files | Purpose |\n|------------|-------|---------|\n| **mlm:model** | best_model.pth, best_model.pt, best_model.onnx, best_model.dlpk | Model artifacts in various formats |\n| **mlm:checkpoint** | epoch=X-step=Y.ckpt | Full training checkpoint |\n| **mlm:source_code** | train.py, inference.py | Training and inference code |\n| **mlm:container** | ghcr.io/*/opengeoaimodelshub | Docker image |\n| **metadata** | model.emd, stac_item.json, confusion_matrix.png | Metadata and evaluation |\n| **overview** | example_input.png, example_pred.png, example_target.png | Example visualizations |\n\n**Sources:** [examplemodel/src/train.py:101-367]()\n\n---\n\n## MLflow Integration\n\nThe training pipeline integrates with MLflow for comprehensive experiment tracking and artifact management.\n\n### MLflow Run Structure\n\n```mermaid\ngraph TB\n    RUN[\"mlflow.start_run()\"]\n    \n    subgraph \"Parameters Logged\"\n        SYS_PARAMS[\"System parameters\u003cbr/\u003eplatform, GPU info\"]\n        DATA_PARAMS[\"Dataset parameters\u003cbr/\u003etrain/val/test counts\"]\n    end\n    \n    subgraph \"Metrics Logged\"\n        TRAIN_METRICS[\"Training metrics\u003cbr/\u003etrain_loss, train_acc\"]\n        VAL_METRICS[\"Validation metrics\u003cbr/\u003eval_loss, val_acc\"]\n        PERF_METRICS[\"Performance metrics\u003cbr/\u003ebest_val_loss, epochs_trained\"]\n    end\n    \n    subgraph \"Artifacts Logged\"\n        META_DIR[\"metadata/\u003cbr/\u003estac_item.json\"]\n        MODEL_DIR[\"models/\u003cbr/\u003epth, pt, onnx, dlpk\"]\n        CKPT_DIR[\"checkpoints/\u003cbr/\u003ebest checkpoint\"]\n        DATASET_DIR[\"datasets/train/\u003cbr/\u003echips/, labels/\"]\n        ESRI_DIR[\"esri/\u003cbr/\u003eDLPK components\"]\n        VIZ_DIR[\"Visualizations\u003cbr/\u003econfusion matrix, examples\"]\n    end\n    \n    subgraph \"Model Registry\"\n        PYTORCH_MODEL[\"mlflow.pytorch.log_model()\u003cbr/\u003ewith signature\"]\n    end\n    \n    RUN --\u003e SYS_PARAMS\n    RUN --\u003e DATA_PARAMS\n    RUN --\u003e TRAIN_METRICS\n    RUN --\u003e VAL_METRICS\n    RUN --\u003e PERF_METRICS\n    RUN --\u003e META_DIR\n    RUN --\u003e MODEL_DIR\n    RUN --\u003e CKPT_DIR\n    RUN --\u003e DATASET_DIR\n    RUN --\u003e ESRI_DIR\n    RUN --\u003e VIZ_DIR\n    RUN --\u003e PYTORCH_MODEL\n```\n\n### Parameters Logged\n\nThe pipeline logs dataset statistics as MLflow parameters:\n```python\nmlflow.log_params({\n    \"train_samples\": dataset_stats[\"train_samples\"],\n    \"val_samples\": dataset_stats[\"val_samples\"],\n    \"test_samples\": dataset_stats[\"test_samples\"],\n    \"num_classes\": dataset_stats[\"num_classes\"],\n})\n```\n\n### Metrics Logged\n\nPerformance metrics are logged at the end of training:\n```python\nmodel_performance = {\n    \"best_val_loss\": float(checkpoint_callback.best_model_score),\n    \"epochs_trained\": trainer.current_epoch + 1,\n}\nmlflow.log_metrics(model_performance)\n```\n\nNote: Per-epoch metrics (train_loss, val_loss, etc.) are logged automatically by PyTorch Lightning through the `self.log()` calls in `LitRefugeeCamp`.\n\n### Artifact Organization\n\nArtifacts are organized into logical directories:\n\n```python\n# Metadata artifacts\nmlflow.log_artifact(stac_output_path, artifact_path=\"metadata\")\n\n# Model artifacts\nmlflow.log_artifact(\"meta/best_model.pth\", artifact_path=\"models\")\nmlflow.log_artifact(\"meta/best_model.pt\", artifact_path=\"models\")\nmlflow.log_artifact(\"meta/best_model.onnx\", artifact_path=\"models\")\nmlflow.log_artifact(\"meta/best_model.dlpk\", artifact_path=\"models\")\n\n# Checkpoint\nmlflow.log_artifact(best_checkpoint, artifact_path=\"checkpoints\")\n\n# Training data\nmlflow.log_artifact(args.chips_dir, artifact_path=\"datasets/train/chips\")\nmlflow.log_artifact(args.labels_dir, artifact_path=\"datasets/train/labels\")\n\n# ESRI components\nmlflow.log_artifact(dlpk_path, artifact_path=\"esri\")\nmlflow.log_artifact(emd_path, artifact_path=\"esri\")\nmlflow.log_artifact(pt_path, artifact_path=\"esri\")\nmlflow.log_artifact(esri_inference_path, artifact_path=\"esri\")\n```\n\n### Model Registry Integration\n\nThe pipeline registers the model with MLflow's model registry, including input/output signature:\n```python\nsignature = infer_signature(\n    dummy_input.numpy(), \n    clean_model(dummy_input).detach().numpy()\n)\nmlflow.pytorch.log_model(\n    clean_model, \n    \"model\", \n    signature=signature, \n    extra_files=[stac_output_path]\n)\n```\n\n**Signature Format:**\n- Input: (1, 3, 256, 256) float32 array\n- Output: (1, 1, 256, 256) float32 array\n\n**Sources:** [examplemodel/src/train.py:373-506]()\n\n---\n\n## Evaluation and Visualization\n\n### Inference Example Logging\n\nThe `log_inference_example()` function creates visual examples of model predictions:\n\n```python\nlog_inference_example(model, data_module)\n```\n\nThis function:\n1. Loads a sample from the test set\n2. Runs inference\n3. Saves three images:\n   - `meta/example_input.png` - Input satellite image\n   - `meta/example_pred.png` - Model prediction mask\n   - `meta/example_target.png` - Ground truth mask\n4. Logs all three to MLflow\n\n### Confusion Matrix Logging\n\nThe `log_confusion_matrix()` function generates and logs a confusion matrix visualization:\n\n```python\nlog_confusion_matrix(model, data_module, run)\n```\n\nThis utility:\n1. Iterates through the test dataset\n2. Collects all predictions and ground truth labels\n3. Computes a confusion matrix\n4. Generates a heatmap visualization\n5. Logs to MLflow as `meta/confusion_matrix.png`\n\nFor implementation details, see [Utilities and Helper Functions](#3.7).\n\n**Sources:** [examplemodel/src/train.py:490-491]()\n\n---\n\n## Command-Line Usage\n\n### Basic Training\n\nTrain the model with default parameters:\n```bash\npython src/train.py\n```\n\n### Custom Configuration\n\nTrain with custom parameters:\n```bash\npython src/train.py \\\n  --epochs 10 \\\n  --batch_size 16 \\\n  --lr 5e-4 \\\n  --chips_dir /path/to/chips \\\n  --labels_dir /path/to/labels\n```\n\n### Via MLflow Project\n\nThe training pipeline is typically invoked through the MLflow project interface:\n```bash\nmlflow run . -e train -P epochs=10 -P batch_size=16\n```\n\nFor details on MLflow entry points, see [MLflow Project Structure](#3.5).\n\n### Output Files\n\nAfter successful training, the following files are generated:\n\n```\ncheckpoints/\n epoch=X-step=Y.ckpt\n\nmeta/\n best_model.pth\n best_model.pt\n best_model.onnx\n best_model.dlpk\n model.emd\n stac_item.json\n confusion_matrix.png\n example_input.png\n example_pred.png\n example_target.png\n```\n\n**Sources:** [examplemodel/src/train.py:509-518]()\n\n---\n\n## Training Pipeline Flow Diagram\n\nThe complete training pipeline with all components and their interactions:\n\n```mermaid\ngraph TB\n    START[\"python src/train.py\u003cbr/\u003e--epochs 10\u003cbr/\u003e--batch_size 32\"]\n    \n    subgraph \"Initialization Phase\"\n        ARGS[\"argparse\u003cbr/\u003eParse arguments\"]\n        MLFLOW[\"mlflow.start_run()\"]\n        SYSINFO[\"get_system_info()\u003cbr/\u003eCollect hardware info\"]\n        DATAMOD[\"CampDataModule()\u003cbr/\u003eInitialize data pipeline\"]\n        STATS[\"calculate_dataset_statistics()\u003cbr/\u003eCompute dataset stats\"]\n        LOGPARAMS[\"mlflow.log_params()\u003cbr/\u003eLog dataset info\"]\n    end\n    \n    subgraph \"Training Phase\"\n        CKPT[\"ModelCheckpoint\u003cbr/\u003emonitor=val_loss\"]\n        TRAINER[\"pl.Trainer\u003cbr/\u003eaccelerator=auto\"]\n        MODEL[\"LitRefugeeCamp\u003cbr/\u003eU-Net model\"]\n        FIT[\"trainer.fit()\u003cbr/\u003eTrain model\"]\n        BEST[\"Load best_model_path\u003cbr/\u003efrom checkpoint\"]\n    end\n    \n    subgraph \"Export Phase\"\n        SAVEPTH[\"torch.save()\u003cbr/\u003ebest_model.pth\"]\n        TRACEPT[\"torch.jit.trace()\u003cbr/\u003ebest_model.pt\"]\n        ONNX[\"torch.onnx.export()\u003cbr/\u003ebest_model.onnx\"]\n        STACGEN[\"create_stac_mlm_item()\u003cbr/\u003estac_item.json\"]\n        DLPKGEN[\"create_dlpk()\u003cbr/\u003ebest_model.dlpk\"]\n    end\n    \n    subgraph \"Logging Phase\"\n        LOGARTIFACTS[\"mlflow.log_artifact()\u003cbr/\u003eLog all artifacts\"]\n        LOGINFER[\"log_inference_example()\u003cbr/\u003eVisual examples\"]\n        LOGCONF[\"log_confusion_matrix()\u003cbr/\u003eEvaluation metrics\"]\n        LOGMODEL[\"mlflow.pytorch.log_model()\u003cbr/\u003eModel registry\"]\n    end\n    \n    START --\u003e ARGS\n    ARGS --\u003e MLFLOW\n    MLFLOW --\u003e SYSINFO\n    SYSINFO --\u003e DATAMOD\n    DATAMOD --\u003e STATS\n    STATS --\u003e LOGPARAMS\n    \n    LOGPARAMS --\u003e CKPT\n    CKPT --\u003e TRAINER\n    TRAINER --\u003e MODEL\n    MODEL --\u003e FIT\n    FIT --\u003e BEST\n    \n    BEST --\u003e SAVEPTH\n    BEST --\u003e TRACEPT\n    BEST --\u003e ONNX\n    BEST --\u003e STACGEN\n    BEST --\u003e DLPKGEN\n    \n    SAVEPTH --\u003e LOGARTIFACTS\n    TRACEPT --\u003e LOGARTIFACTS\n    ONNX --\u003e LOGARTIFACTS\n    STACGEN --\u003e LOGARTIFACTS\n    DLPKGEN --\u003e LOGARTIFACTS\n    \n    LOGARTIFACTS --\u003e LOGINFER\n    LOGINFER --\u003e LOGCONF\n    LOGCONF --\u003e LOGMODEL\n```\n\n**Sources:** [examplemodel/src/train.py:370-507]()\n\n---\n\n## Key Classes and Functions Reference\n\n### Primary Functions\n\n| Function | Location | Purpose |\n|----------|----------|---------|\n| `train_model()` | [examplemodel/src/train.py:370-507]() | Main training orchestration function |\n| `get_system_info()` | [examplemodel/src/train.py:25-60]() | Collects system and hardware information |\n| `calculate_dataset_statistics()` | [examplemodel/src/train.py:63-98]() | Computes dataset-level statistics |\n| `create_stac_mlm_item()` | [examplemodel/src/train.py:101-367]() | Generates STAC-MLM metadata |\n\n### Classes Used\n\n| Class | Location | Purpose |\n|-------|----------|---------|\n| `CampDataModule` | [examplemodel/src/model.py:33-100]() | PyTorch Lightning data module for dataset management |\n| `LitRefugeeCamp` | [examplemodel/src/model.py:145-174]() | PyTorch Lightning module wrapping the model |\n| `RefugeeCampDetector` | [examplemodel/src/model.py:103-137]() | Core U-Net neural network architecture |\n| `ModelCheckpoint` | PyTorch Lightning | Callback for saving best model checkpoints |\n| `Trainer` | PyTorch Lightning | Training loop orchestration |\n\n### External Utilities\n\n| Function | Source Module | Purpose |\n|----------|---------------|---------|\n| `log_inference_example()` | [examplemodel/src/inference.py]() | Generates and logs example predictions |\n| `log_confusion_matrix()` | [examplemodel/src/utils.py]() | Generates and logs confusion matrix |\n| `create_dlpk()` | [examplemodel/src/stac2esri.py]() | Creates ESRI Deep Learning Package |\n\n**Sources:** [examplemodel/src/train.py:1-22](), [examplemodel/src/model.py:1-175]()\n\n---\n\n## Performance Considerations\n\n### GPU Acceleration\n\nThe training pipeline automatically detects and uses CUDA-enabled GPUs when available. GPU information is collected via `pynvml` and logged to MLflow for reproducibility.\n\n**Fallback Behavior:** If GPU is unavailable, training proceeds on CPU without manual intervention.\n\n### DataLoader Configuration\n\nThe `CampDataModule` configures data loaders with:\n- **num_workers=4:** Parallel data loading with 4 worker processes\n- **pin_memory=True:** Faster CPU-to-GPU memory transfer when CUDA is available\n- **shuffle=True:** Training data is shuffled each epoch\n\n### Batch Size Selection\n\nThe default batch size of 32 is a reasonable starting point. Adjust based on:\n- Available GPU memory\n- Dataset size\n- Model architecture complexity\n\n**Recommendation:** Start with 32 and reduce by factors of 2 if OOM errors occur.\n\n**Sources:** [examplemodel/src/model.py:75-100](), [examplemodel/src/train.py:25-60]()"])</script><script>self.__next_f.push([1,"1e:T3785,"])</script><script>self.__next_f.push([1,"# Inference System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/playground.ipynb](examplemodel/playground.ipynb)\n- [examplemodel/src/inference.py](examplemodel/src/inference.py)\n- [examplemodel/src/model.py](examplemodel/src/model.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThe Inference System provides the capability to apply trained refugee camp detection models to new satellite imagery. This system handles image preprocessing, model execution, prediction postprocessing, and metadata generation. It supports both simple inference workflows and enhanced workflows that produce comprehensive output artifacts including visualizations and structured metadata.\n\nFor information about the model architecture used during inference, see [Model Overview and Architecture](#3.1). For details on how models are trained and exported, see [Training Pipeline](#3.2). For ESRI-specific inference capabilities, see [ESRI Integration and DLPK Generation](#3.4).\n\n## Inference Pipeline Architecture\n\nThe inference system implements a multi-stage pipeline that transforms raw satellite imagery into segmentation predictions and associated metadata.\n\n```mermaid\nflowchart TD\n    Input[\"Input Image\u003cbr/\u003e(arbitrary size)\"]\n    LoadModel[\"Load Model\u003cbr/\u003etorch.jit.load() or\u003cbr/\u003eLitRefugeeCamp()\"]\n    PreprocessFunc[\"preprocess_image()\u003cbr/\u003eexamplemodel/src/inference.py:52-60\"]\n    Resize[\"Resize to 256x256\u003cbr/\u003etransforms.Resize()\"]\n    ToTensor[\"Convert to Tensor\u003cbr/\u003etransforms.ToTensor()\"]\n    Normalize[\"Normalize\u003cbr/\u003emean=[0.485,0.456,0.406]\u003cbr/\u003estd=[0.229,0.224,0.225]\"]\n    AddBatch[\"Add batch dimension\u003cbr/\u003eunsqueeze(0)\"]\n    ModelInference[\"Model Forward Pass\u003cbr/\u003emodel(input_tensor)\"]\n    Sigmoid[\"Apply Sigmoid\u003cbr/\u003etorch.sigmoid(output)\"]\n    Threshold[\"Binary Threshold\u003cbr/\u003eprediction \u003e 0.5\"]\n    GenMetadata[\"create_inference_metadata()\u003cbr/\u003eexamplemodel/src/inference.py:23-49\"]\n    SaveOutputs[\"Save Outputs\u003cbr/\u003e- prediction_raw.npy\u003cbr/\u003e- prediction_mask.png\u003cbr/\u003e- prediction_overlay.png\u003cbr/\u003e- inference_metadata.json\"]\n    \n    Input --\u003e LoadModel\n    LoadModel --\u003e PreprocessFunc\n    PreprocessFunc --\u003e Resize\n    Resize --\u003e ToTensor\n    ToTensor --\u003e Normalize\n    Normalize --\u003e AddBatch\n    AddBatch --\u003e ModelInference\n    ModelInference --\u003e Sigmoid\n    Sigmoid --\u003e Threshold\n    Threshold --\u003e GenMetadata\n    GenMetadata --\u003e SaveOutputs\n```\n\n**Sources:** [examplemodel/src/inference.py:52-143]()\n\n## Image Preprocessing\n\nThe `preprocess_image()` function standardizes input images to the format expected by the model. This function is located at [examplemodel/src/inference.py:52-60]() and implements the following transformation pipeline:\n\n| Step | Operation | Parameters |\n|------|-----------|------------|\n| 1. Load | `Image.open().convert('RGB')` | Ensures 3-channel RGB format |\n| 2. Resize | `transforms.Resize((256, 256))` | Fixed input size for model |\n| 3. Tensor Conversion | `transforms.ToTensor()` | Converts to [0,1] range |\n| 4. Normalization | `transforms.Normalize()` | mean=[0.485, 0.456, 0.406]\u003cbr/\u003estd=[0.229, 0.224, 0.225] |\n| 5. Batch Dimension | `.unsqueeze(0)` | Shape: [1, 3, 256, 256] |\n\nThe normalization parameters follow ImageNet statistics, which are standard for models pretrained on natural images. The output tensor has shape `[1, 3, 256, 256]` and is ready for direct input to the model.\n\n**Sources:** [examplemodel/src/inference.py:52-60]()\n\n## Prediction Functions\n\n### Basic Inference: `predict_image()`\n\nThe `predict_image()` function at [examplemodel/src/inference.py:127-142]() provides a lightweight inference interface that returns a binary mask:\n\n```mermaid\ngraph LR\n    Input[\"image_path\u003cbr/\u003emodel_path\"]\n    Load[\"Load Model\u003cbr/\u003etorch.jit.load() or\u003cbr/\u003efrom checkpoint\"]\n    Preprocess[\"preprocess_image()\"]\n    Forward[\"model(input_tensor)\"]\n    Sigmoid[\"torch.sigmoid()\"]\n    Threshold[\"prediction \u003e 0.5\"]\n    Output[\"np.ndarray\u003cbr/\u003ebinary mask\"]\n    \n    Input --\u003e Load\n    Load --\u003e Preprocess\n    Preprocess --\u003e Forward\n    Forward --\u003e Sigmoid\n    Sigmoid --\u003e Threshold\n    Threshold --\u003e Output\n```\n\nThis function:\n- Accepts either TorchScript models (`.pt` files) or PyTorch state dicts (`.pth` files)\n- Performs inference with `model.eval()` and `torch.no_grad()`\n- Applies sigmoid activation to convert logits to probabilities\n- Applies threshold of 0.5 to produce binary predictions\n- Returns a NumPy array with dtype `uint8` containing 0s and 1s\n\n**Sources:** [examplemodel/src/inference.py:127-142]()\n\n### Enhanced Inference: `predict_image_enhanced()`\n\nThe `predict_image_enhanced()` function at [examplemodel/src/inference.py:63-124]() provides comprehensive inference with metadata generation and multiple output formats:\n\n```mermaid\nflowchart TD\n    subgraph \"Inputs\"\n        Model[\"model: torch.nn.Module\"]\n        ImagePath[\"image_path: str\"]\n        STACMeta[\"stac_metadata: Dict\u003cbr/\u003e(optional)\"]\n        OutputDir[\"output_dir: str\"]\n    end\n    \n    subgraph \"Processing\"\n        CreateDir[\"Create output directory\u003cbr/\u003ePath(output_dir).mkdir()\"]\n        Preprocess[\"preprocess_image()\"]\n        TimedInference[\"Timed model forward pass\u003cbr/\u003etime.time() wrapper\"]\n        Sigmoid[\"torch.sigmoid()\"]\n        Binary[\"Binary threshold \u003e 0.5\"]\n        CreateMeta[\"create_inference_metadata()\"]\n    end\n    \n    subgraph \"Outputs\"\n        RawNPY[\"prediction_raw.npy\u003cbr/\u003efloat32 probabilities\"]\n        MaskPNG[\"prediction_mask.png\u003cbr/\u003ebinary 0/255\"]\n        OverlayPNG[\"prediction_overlay.png\u003cbr/\u003eoriginal + red overlay\"]\n        MetaJSON[\"inference_metadata.json\u003cbr/\u003eSTAC-compliant metadata\"]\n    end\n    \n    Model --\u003e TimedInference\n    ImagePath --\u003e Preprocess\n    STACMeta --\u003e CreateMeta\n    OutputDir --\u003e CreateDir\n    \n    Preprocess --\u003e TimedInference\n    TimedInference --\u003e Sigmoid\n    Sigmoid --\u003e Binary\n    Binary --\u003e CreateMeta\n    \n    Binary --\u003e RawNPY\n    Binary --\u003e MaskPNG\n    Binary --\u003e OverlayPNG\n    CreateMeta --\u003e MetaJSON\n```\n\nThe function produces four output files:\n\n| File | Format | Content | Line Reference |\n|------|--------|---------|----------------|\n| `prediction_raw.npy` | NumPy | Float32 probability map [0,1] | [examplemodel/src/inference.py:93-95]() |\n| `prediction_mask.png` | PNG | Binary mask (0=black, 255=white) | [examplemodel/src/inference.py:97-99]() |\n| `prediction_overlay.png` | PNG | Original image with red overlay on detections | [examplemodel/src/inference.py:101-109]() |\n| `inference_metadata.json` | JSON | STAC-compliant inference metadata | [examplemodel/src/inference.py:111-114]() |\n\n**Sources:** [examplemodel/src/inference.py:63-124]()\n\n## Inference Metadata Generation\n\nThe `create_inference_metadata()` function at [examplemodel/src/inference.py:23-49]() generates structured metadata for each inference run. This metadata follows STAC-MLM conventions and includes:\n\n```mermaid\ngraph TD\n    subgraph \"Input Parameters\"\n        ImagePath[\"image_path\"]\n        ModelInfo[\"model_info\u003cbr/\u003e(from STAC item)\"]\n        InferTime[\"inference_time\"]\n        InputShape[\"input_shape\"]\n        OutputShape[\"output_shape\"]\n    end\n    \n    subgraph \"Metadata Structure\"\n        Timestamp[\"inference_timestamp\u003cbr/\u003eISO 8601 format\"]\n        ModelName[\"model_name\u003cbr/\u003emlm:name\"]\n        ModelVersion[\"model_version\"]\n        Framework[\"model_framework\"]\n        Timing[\"inference_time_seconds\"]\n        Shapes[\"input_shape \u0026 output_shape\"]\n        Preprocessing[\"preprocessing config\u003cbr/\u003eresize, normalization\"]\n        Postprocessing[\"postprocessing config\u003cbr/\u003ethreshold, output_type\"]\n    end\n    \n    ImagePath --\u003e Timestamp\n    ModelInfo --\u003e ModelName\n    ModelInfo --\u003e ModelVersion\n    ModelInfo --\u003e Framework\n    InferTime --\u003e Timing\n    InputShape --\u003e Shapes\n    OutputShape --\u003e Shapes\n    \n    Timestamp --\u003e MetadataDict[\"Dict[str, Any]\u003cbr/\u003eJSON-serializable\"]\n    ModelName --\u003e MetadataDict\n    Timing --\u003e MetadataDict\n    Shapes --\u003e MetadataDict\n    Preprocessing --\u003e MetadataDict\n    Postprocessing --\u003e MetadataDict\n```\n\nThe preprocessing section documents:\n- Resize dimensions: `[256, 256]`\n- Normalization scheme: `\"ImageNet\"`\n- Mean values: `[0.485, 0.456, 0.406]`\n- Std values: `[0.229, 0.224, 0.225]`\n\nThe postprocessing section documents:\n- Threshold value: `0.5`\n- Output type: `\"binary_mask\"`\n\n**Sources:** [examplemodel/src/inference.py:23-49]()\n\n## MLflow Integration\n\n### Inference Run Tracking\n\nWhen invoked with the `--mlflow_tracking` flag, the inference system logs results to MLflow:\n\n```mermaid\nsequenceDiagram\n    participant CLI as \"Command Line\"\n    participant Main as \"main()\u003cbr/\u003einference.py:211-247\"\n    participant Enhanced as \"predict_image_enhanced()\"\n    participant MLflow as \"MLflow Server\"\n    participant MinIO as \"MinIO Storage\"\n    \n    CLI-\u003e\u003eMain: \"mlflow run . -e inference\u003cbr/\u003e--mlflow_tracking\"\n    Main-\u003e\u003eEnhanced: \"predict_image_enhanced()\"\n    Enhanced--\u003e\u003eMain: \"results dict\"\n    \n    Main-\u003e\u003eMLflow: \"mlflow.start_run()\"\n    MLflow--\u003e\u003eMain: \"run context\"\n    \n    Main-\u003e\u003eMLflow: \"mlflow.log_params(metadata)\"\n    Main-\u003e\u003eMLflow: \"mlflow.log_metric('inference_time')\"\n    Main-\u003e\u003eMLflow: \"mlflow.log_metric('refugee_camp_detected')\"\n    \n    loop \"For each output file\"\n        Main-\u003e\u003eMLflow: \"mlflow.log_artifact(file_path)\"\n        MLflow-\u003e\u003eMinIO: \"Store artifact\"\n    end\n    \n    Main-\u003e\u003eMLflow: \"End run\"\n```\n\nLogged parameters include all fields from `inference_metadata.json`, such as model name, version, framework, input/output shapes, and preprocessing configuration. Logged metrics include `inference_time` (seconds) and `refugee_camp_detected` (0 or 1).\n\n**Sources:** [examplemodel/src/inference.py:236-243]()\n\n### Training-Time Inference Examples\n\nThe `log_inference_example()` function at [examplemodel/src/inference.py:145-208]() is called during training to log example predictions:\n\n```mermaid\nflowchart TD\n    Trainer[\"PyTorch Lightning Trainer\u003cbr/\u003e(during training)\"]\n    LogFunc[\"log_inference_example()\u003cbr/\u003einference.py:145-208\"]\n    GetBatch[\"Get test batch\u003cbr/\u003etest_dataloader()\"]\n    DeviceMove[\"Move to model device\u003cbr/\u003e.to(device)\"]\n    Forward[\"Model forward pass\u003cbr/\u003emodel(images[:1])\"]\n    Denorm[\"Denormalize input image\"]\n    BinarySigmoid[\"Apply sigmoid + threshold\"]\n    CreateFigs[\"Create matplotlib figures\u003cbr/\u003e- Input, Prediction, Ground Truth\u003cbr/\u003e- Individual prediction\u003cbr/\u003e- Individual ground truth\"]\n    SaveLocal[\"Save to meta/ directory\u003cbr/\u003eexample_input.png\u003cbr/\u003eexample_pred.png\u003cbr/\u003eexample_target.png\"]\n    LogMLflow[\"mlflow.log_artifact()\u003cbr/\u003eartifact_path='examples'\"]\n    \n    Trainer --\u003e LogFunc\n    LogFunc --\u003e GetBatch\n    GetBatch --\u003e DeviceMove\n    DeviceMove --\u003e Forward\n    Forward --\u003e Denorm\n    Forward --\u003e BinarySigmoid\n    Denorm --\u003e CreateFigs\n    BinarySigmoid --\u003e CreateFigs\n    CreateFigs --\u003e SaveLocal\n    SaveLocal --\u003e LogMLflow\n```\n\nThis function:\n- Takes the first image from the test dataset\n- Denormalizes the input for visualization\n- Converts predictions to binary format (0 or 1)\n- Creates three visualization files\n- Logs them to MLflow under the `examples` artifact path\n\n**Sources:** [examplemodel/src/inference.py:145-208]()\n\n## Command-Line Interface\n\nThe inference system can be invoked via the command line with the following interface:\n\n| Argument | Type | Required | Description | Default |\n|----------|------|----------|-------------|---------|\n| `image_path` | Positional | Yes | Path to input image file | - |\n| `--model_path` | Optional | No | Path to model file (.pt or .pth) | `meta/best_model.pth` |\n| `--stac_path` | Optional | No | Path to STAC metadata file | None |\n| `--output_dir` | Optional | No | Directory for output files | `output` |\n| `--mlflow_tracking` | Flag | No | Enable MLflow logging | False |\n\nExample usage:\n```bash\n# Basic inference\npython src/inference.py satellite_image.jpg\n\n# With custom model and MLflow tracking\npython src/inference.py satellite_image.jpg \\\n  --model_path models/custom_model.pt \\\n  --mlflow_tracking\n\n# With STAC metadata\npython src/inference.py satellite_image.jpg \\\n  --stac_path meta/stac_item.json \\\n  --output_dir results/\n```\n\nThe main function at [examplemodel/src/inference.py:211-247]() orchestrates the complete inference workflow, including argument parsing, model loading, prediction generation, and optional MLflow logging.\n\n**Sources:** [examplemodel/src/inference.py:211-247]()\n\n## Output Artifacts\n\n### Overlay Visualization\n\nThe overlay visualization ([examplemodel/src/inference.py:101-109]()) composites the binary prediction mask onto the original image:\n\n1. Load and resize original image to 256256\n2. Create overlay with red color `[0, 0, 255]` where mask equals 1\n3. Blend using `cv2.addWeighted()` with weights 0.7 (original) and 0.3 (overlay)\n4. Save as `prediction_overlay.png`\n\nThis provides immediate visual feedback on model predictions, highlighting detected refugee camp areas in red.\n\n**Sources:** [examplemodel/src/inference.py:101-109]()\n\n### Return Value Structure\n\nThe `predict_image_enhanced()` function returns a dictionary with the following structure:\n\n```python\n{\n    \"prediction\": np.ndarray,           # Float probabilities [0,1]\n    \"binary_mask\": np.ndarray,          # Binary mask {0,1}\n    \"metadata\": dict,                   # Inference metadata\n    \"output_files\": {\n        \"raw_prediction\": str,          # Path to .npy file\n        \"binary_mask\": str,             # Path to mask .png\n        \"overlay\": str,                 # Path to overlay .png\n        \"metadata\": str                 # Path to .json file\n    },\n    \"refugee_camp_detected\": bool       # True if any pixel is positive\n}\n```\n\nThe `refugee_camp_detected` field is computed as `bool(np.any(binary_mask))` at [examplemodel/src/inference.py:121](), indicating whether any pixels in the image were classified as refugee camp structures.\n\n**Sources:** [examplemodel/src/inference.py:116-124]()\n\n## Integration with MLflow Project\n\nThe inference system is exposed as an MLflow entry point, allowing it to be invoked via MLflow's orchestration layer. For details on the MLflow project configuration and entry point parameters, see [MLflow Project Structure](#3.5).\n\n**Sources:** [examplemodel/src/inference.py:1-251]()"])</script><script>self.__next_f.push([1,"1f:T54a0,"])</script><script>self.__next_f.push([1,"# ESRI Integration and DLPK Generation\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/src/esri/RefugeeCampDetector.py](examplemodel/src/esri/RefugeeCampDetector.py)\n- [examplemodel/src/stac2esri.py](examplemodel/src/stac2esri.py)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the ESRI integration components of the OpenGeoAIModelHub, specifically the creation and structure of Deep Learning Package (DLPK) files for deployment in ArcGIS Pro and ArcGIS Enterprise environments. The system converts trained PyTorch models into ESRI-compatible packages that can be directly loaded and used for raster inference within ESRI's geospatial tooling ecosystem.\n\nFor information about the model training process that produces the artifacts packaged into DLPKs, see [Training Pipeline](#3.2). For details on the STAC-MLM metadata that serves as the source for EMD generation, see [Training Pipeline](#3.2). For deployment strategies using the generated DLPK files, see [Model Deployment Options](#6.2).\n\n---\n\n## DLPK Package Format\n\nA DLPK (Deep Learning Package) is a ZIP archive containing model files and metadata required for ArcGIS deep learning inference. The package structure follows ESRI's specification for custom deep learning models.\n\n### Package Structure\n\n```mermaid\ngraph TB\n    DLPK[\"best_model.dlpk\u003cbr/\u003e(ZIP Archive)\"]\n    \n    subgraph \"Package Contents\"\n        EMD[\"model.emd\u003cbr/\u003e(JSON Metadata)\"]\n        PT[\"model.pt\u003cbr/\u003e(TorchScript Model)\"]\n        INFERENCE[\"RefugeeCampDetector.py\u003cbr/\u003e(Inference Script)\"]\n    end\n    \n    DLPK --\u003e|\"contains\"| EMD\n    DLPK --\u003e|\"contains\"| PT\n    DLPK --\u003e|\"contains\"| INFERENCE\n    \n    subgraph \"EMD Structure\"\n        FRAMEWORK[\"Framework: PyTorch\"]\n        MODELTYPE[\"ModelType: ImageClassification\"]\n        MODELFILE[\"ModelFile: model.pt\"]\n        INFERFN[\"InferenceFunction: RefugeeCampDetector.py\"]\n        DIMS[\"ImageHeight/Width: 256x256\"]\n        CLASSES[\"Classes: Background, Refugee Camp\"]\n        PARAMS[\"ModelParameters: mean, std\"]\n    end\n    \n    EMD --\u003e|\"defines\"| FRAMEWORK\n    EMD --\u003e|\"defines\"| MODELTYPE\n    EMD --\u003e|\"references\"| MODELFILE\n    EMD --\u003e|\"references\"| INFERFN\n    EMD --\u003e|\"specifies\"| DIMS\n    EMD --\u003e|\"specifies\"| CLASSES\n    EMD --\u003e|\"specifies\"| PARAMS\n    \n    PT -.-\u003e|\"loaded by\"| INFERENCE\n```\n\n**Sources:** [examplemodel/src/stac2esri.py:23-46](), [examplemodel/src/train.py:462-468]()\n\n### EMD File Format\n\nThe Esri Model Definition (EMD) file is a JSON document that describes the model architecture, input/output specifications, and inference parameters.\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `Framework` | string | Deep learning framework (e.g., \"PyTorch\") |\n| `ModelType` | string | Type of model task (e.g., \"ImageClassification\") |\n| `ModelName` | string | Human-readable model identifier |\n| `Architecture` | string | Neural network architecture (e.g., \"U-Net\") |\n| `ModelFile` | string | Relative path to model weights file |\n| `InferenceFunction` | string | Relative path to Python inference script |\n| `ImageHeight` | integer | Expected input tile height in pixels |\n| `ImageWidth` | integer | Expected input tile width in pixels |\n| `ImageSpaceUsed` | string | Coordinate system (\"MAP_SPACE\" or \"PIXEL_SPACE\") |\n| `ExtractBands` | array | Zero-indexed band indices to extract from input |\n| `DataRange` | array | Expected data value range [min, max] |\n| `BatchSize` | integer | Number of tiles to process simultaneously |\n| `Classes` | array | Class definitions with Value, Name, and Color |\n| `ModelParameters` | object | Preprocessing parameters (mean, std, etc.) |\n| `Threshold` | float | Classification confidence threshold |\n\n**Sources:** [examplemodel/src/stac2esri.py:23-46]()\n\n---\n\n## RefugeeCampDetector Class\n\nThe `RefugeeCampDetector` class implements ESRI's Python Raster Function interface, enabling custom deep learning inference within ArcGIS workflows. This class is packaged into the DLPK and instantiated by ArcGIS at runtime.\n\n### Class Interface\n\n```mermaid\nclassDiagram\n    class RefugeeCampDetector {\n        +name: str\n        +description: str\n        +json_info: dict\n        +model_path: str\n        +model: torch.jit.ScriptModule\n        +model_is_loaded: bool\n        \n        +initialize(**kwargs) void\n        +load_model() void\n        +getParameterInfo() list\n        +getConfiguration(**scalars) dict\n        +updateRasterInfo(**kwargs) dict\n        +updatePixels(tlc, shape, props, **pixelBlocks) dict\n    }\n    \n    class ArcGISRuntime {\n        \u003c\u003cESRI System\u003e\u003e\n    }\n    \n    class TorchScriptModel {\n        \u003c\u003ctorch.jit.ScriptModule\u003e\u003e\n    }\n    \n    ArcGISRuntime --\u003e RefugeeCampDetector : \"instantiates and calls\"\n    RefugeeCampDetector --\u003e TorchScriptModel : \"loads via torch.jit.load()\"\n```\n\n**Sources:** [examplemodel/src/esri/RefugeeCampDetector.py:25-184]()\n\n### Method Responsibilities\n\n#### initialize()\n\nParses the EMD file and extracts model configuration parameters.\n\n- **Input:** `model` parameter containing path to EMD file or JSON string\n- **Output:** Populates `self.json_info` and `self.model_path`\n- **Location:** [examplemodel/src/esri/RefugeeCampDetector.py:31-51]()\n\nKey operations:\n1. Loads EMD file as JSON dictionary\n2. Extracts `ModelFile` path and resolves to absolute path\n3. Sets `model_is_loaded` flag to `False`\n\n#### load_model()\n\nLazy-loads the TorchScript model file into memory.\n\n- **Input:** Uses `self.model_path` set during initialization\n- **Output:** Populates `self.model` with loaded `torch.jit.ScriptModule`\n- **Location:** [examplemodel/src/esri/RefugeeCampDetector.py:53-65]()\n\nThe model is loaded with `map_location=\"cpu\"` to ensure compatibility across systems, even if trained with GPU acceleration.\n\n#### getParameterInfo()\n\nDefines the parameters that ArcGIS users can configure in the UI.\n\n- **Returns:** List of parameter dictionaries with name, dataType, and default values\n- **Location:** [examplemodel/src/esri/RefugeeCampDetector.py:67-83]()\n\nExposed parameters:\n- `raster`: Input raster dataset (required)\n- `model`: Path to EMD file (required)\n- `BatchSize`: Number of tiles to process together (optional, default from EMD)\n- `Threshold`: Classification confidence threshold (optional, default from EMD)\n\n#### getConfiguration()\n\nConfigures the raster processing pipeline based on user parameters and EMD metadata.\n\n- **Input:** Scalar parameters from `getParameterInfo()`\n- **Returns:** Dictionary with processing configuration\n- **Location:** [examplemodel/src/esri/RefugeeCampDetector.py:85-100]()\n\nConfiguration keys:\n- `BatchSize`: Tiles per batch\n- `tx`, `ty`: Tile dimensions\n- `extractBands`: Band indices to extract\n- `dataRange`: Expected pixel value range\n- `inputMask`: Whether to respect input mask\n- `inheritProperties`: Bitfield for metadata inheritance\n\n#### updateRasterInfo()\n\nModifies output raster metadata to match the segmentation mask format.\n\n- **Input:** Keyword arguments including `output_info` dictionary\n- **Returns:** Modified kwargs with updated `output_info`\n- **Location:** [examplemodel/src/esri/RefugeeCampDetector.py:102-105]()\n\nSets output to single-band, 8-bit unsigned integer (pixel type `u1`).\n\n#### updatePixels()\n\nCore inference method called for each raster tile.\n\n- **Input:** Tile coordinates, shape, properties, and pixel blocks\n- **Returns:** Modified `pixelBlocks` with `output_pixels` populated\n- **Location:** [examplemodel/src/esri/RefugeeCampDetector.py:107-183]()\n\n### Inference Pipeline\n\n```mermaid\nflowchart TD\n    START[\"updatePixels() called\"]\n    \n    EXTRACT[\"Extract raster_pixels (C, H, W)\"]\n    MASK[\"Apply raster_mask\"]\n    NORMALIZE[\"Normalize to [0, 1] range\"]\n    BATCH[\"Add batch dimension (1, C, H, W)\"]\n    BANDS[\"Select configured bands\"]\n    \n    ZSCORE[\"Apply Z-score normalization\u003cbr/\u003emean=[0.485, 0.456, 0.406]\u003cbr/\u003estd=[0.229, 0.224, 0.225]\"]\n    \n    LOAD_CHECK{\"model_is_loaded?\"}\n    LOAD[\"Call load_model()\"]\n    \n    TENSOR[\"Convert to torch.Tensor\"]\n    FORWARD[\"Forward pass: model(tensor)\"]\n    \n    TUPLE_CHECK{\"Output is tuple?\"}\n    INTERPOLATE[\"Interpolate mask to 256x256\"]\n    SKIP[\"Use output directly\"]\n    \n    SIGMOID[\"Apply sigmoid activation\"]\n    THRESHOLD[\"Apply threshold \u003e 0.95\"]\n    CONVERT[\"Convert to uint8 [0, 255]\"]\n    OUTPUT[\"Set output_pixels\"]\n    \n    START --\u003e EXTRACT\n    EXTRACT --\u003e MASK\n    MASK --\u003e NORMALIZE\n    NORMALIZE --\u003e BATCH\n    BATCH --\u003e BANDS\n    BANDS --\u003e ZSCORE\n    ZSCORE --\u003e LOAD_CHECK\n    \n    LOAD_CHECK --\u003e|\"No\"| LOAD\n    LOAD_CHECK --\u003e|\"Yes\"| TENSOR\n    LOAD --\u003e TENSOR\n    \n    TENSOR --\u003e FORWARD\n    FORWARD --\u003e TUPLE_CHECK\n    \n    TUPLE_CHECK --\u003e|\"Yes\"| INTERPOLATE\n    TUPLE_CHECK --\u003e|\"No\"| SKIP\n    \n    INTERPOLATE --\u003e SIGMOID\n    SKIP --\u003e SIGMOID\n    \n    SIGMOID --\u003e THRESHOLD\n    THRESHOLD --\u003e CONVERT\n    CONVERT --\u003e OUTPUT\n```\n\n**Sources:** [examplemodel/src/esri/RefugeeCampDetector.py:107-183]()\n\nKey operations in the pipeline:\n\n1. **Masking**: Applies `raster_mask` to handle NoData regions [line 112]()\n2. **Type conversion**: Converts to float32 for computation [line 116]()\n3. **Band extraction**: Selects configured bands (typically RGB) [lines 121-123]()\n4. **Normalization**: Scales to [0, 1] if input exceeds 1.0 [lines 125-126]()\n5. **Z-score standardization**: Applies ImageNet statistics [lines 134-141]()\n6. **Model inference**: Forward pass with `torch.no_grad()` [lines 147-148]()\n7. **Post-processing**: Sigmoid activation and thresholding [lines 172-174]()\n8. **Output formatting**: Converts to uint8 binary mask [line 180]()\n\n---\n\n## STAC-MLM to EMD Conversion\n\nThe `stacmlm_to_emd()` function converts STAC-MLM metadata to ESRI's EMD format, bridging the standardized machine learning model metadata with ESRI-specific requirements.\n\n### Conversion Flow\n\n```mermaid\nflowchart LR\n    STAC[\"stac_item.json\u003cbr/\u003e(STAC-MLM format)\"]\n    \n    subgraph \"stacmlm_to_emd()\"\n        PARSE[\"Parse STAC JSON\"]\n        EXTRACT_INPUT[\"Extract mlm:input[0]\"]\n        EXTRACT_OUTPUT[\"Extract mlm:output[0]\"]\n        EXTRACT_PROPS[\"Extract properties\"]\n        \n        MAP_FRAMEWORK[\"Map mlm:framework to Framework\"]\n        MAP_ARCH[\"Map mlm:architecture to Architecture\"]\n        MAP_NAME[\"Map mlm:name to ModelName\"]\n        MAP_DIMS[\"Extract shape to ImageHeight/Width\"]\n        MAP_CLASSES[\"Map classification:classes to Classes\"]\n        MAP_PARAMS[\"Extract value_scaling to ModelParameters\"]\n    end\n    \n    EMD[\"model.emd\u003cbr/\u003e(ESRI EMD format)\"]\n    \n    STAC --\u003e PARSE\n    PARSE --\u003e EXTRACT_INPUT\n    PARSE --\u003e EXTRACT_OUTPUT\n    PARSE --\u003e EXTRACT_PROPS\n    \n    EXTRACT_INPUT --\u003e MAP_DIMS\n    EXTRACT_INPUT --\u003e MAP_PARAMS\n    EXTRACT_OUTPUT --\u003e MAP_CLASSES\n    EXTRACT_PROPS --\u003e MAP_FRAMEWORK\n    EXTRACT_PROPS --\u003e MAP_ARCH\n    EXTRACT_PROPS --\u003e MAP_NAME\n    \n    MAP_FRAMEWORK --\u003e EMD\n    MAP_ARCH --\u003e EMD\n    MAP_NAME --\u003e EMD\n    MAP_DIMS --\u003e EMD\n    MAP_CLASSES --\u003e EMD\n    MAP_PARAMS --\u003e EMD\n```\n\n**Sources:** [examplemodel/src/stac2esri.py:7-51]()\n\n### Field Mapping Table\n\n| STAC-MLM Path | EMD Field | Transformation |\n|---------------|-----------|----------------|\n| `properties.mlm:framework` | `Framework` | Direct copy (e.g., \"PyTorch\") |\n| `properties.mlm:architecture` | `Architecture` | Direct copy (e.g., \"U-Net\") |\n| `properties.mlm:name` | `ModelName` | Direct copy or default \"RefugeeCampDetector\" |\n| `properties.mlm:input[0].input.shape[2]` | `ImageHeight` | Extract dimension |\n| `properties.mlm:input[0].input.shape[3]` | `ImageWidth` | Extract dimension |\n| `properties.mlm:input[0].value_scaling` | `ModelParameters.mean/std` | Extract z-score parameters |\n| `properties.mlm:output[0].classification:classes` | `Classes` | Convert to EMD class format |\n| (hardcoded) | `ModelFile` | Always \"model.pt\" |\n| (hardcoded) | `InferenceFunction` | Always \"RefugeeCampDetector.py\" |\n\n**Sources:** [examplemodel/src/stac2esri.py:7-51]()\n\nThe function performs these steps:\n\n1. **Load STAC file**: Reads JSON from `stac_path` [line 8]()\n2. **Extract properties**: Accesses `properties` dictionary [line 9]()\n3. **Find PyTorch asset**: Searches for asset with `mlm:artifact_type == \"pytorch\"` [lines 11-16]()\n4. **Build EMD structure**: Constructs EMD dictionary with required fields [lines 23-46]()\n5. **Write EMD file**: Saves JSON to `output_dir/model.emd` [lines 48-50]()\n\n---\n\n## DLPK Creation Process\n\nThe `create_dlpk()` function packages the EMD file, TorchScript model, and inference script into a single ZIP archive.\n\n### Packaging Sequence\n\n```mermaid\nsequenceDiagram\n    participant Caller as \"Training Script\"\n    participant CreateDLPK as \"create_dlpk()\"\n    participant ZipFile as \"zipfile.ZipFile\"\n    participant FS as \"File System\"\n    \n    Caller-\u003e\u003eCreateDLPK: emd_path, pt_path,\u003cbr/\u003einference_path, output_dlpk\n    \n    CreateDLPK-\u003e\u003eZipFile: Open with ZIP_DEFLATED compression\n    activate ZipFile\n    \n    CreateDLPK-\u003e\u003eFS: Read emd_path\n    FS--\u003e\u003eCreateDLPK: EMD content\n    CreateDLPK-\u003e\u003eZipFile: write(emd_path, arcname=\"model.emd\")\n    \n    CreateDLPK-\u003e\u003eFS: Read pt_path\n    FS--\u003e\u003eCreateDLPK: TorchScript model\n    CreateDLPK-\u003e\u003eZipFile: write(pt_path, arcname=\"model.pt\")\n    \n    CreateDLPK-\u003e\u003eFS: Read inference_path\n    FS--\u003e\u003eCreateDLPK: Python inference script\n    CreateDLPK-\u003e\u003eZipFile: write(inference_path, arcname=\"RefugeeCampDetector.py\")\n    \n    CreateDLPK-\u003e\u003eZipFile: Close archive\n    deactivate ZipFile\n    \n    CreateDLPK--\u003e\u003eCaller: DLPK created at output_dlpk\n```\n\n**Sources:** [examplemodel/src/stac2esri.py:54-58]()\n\nFunction signature:\n```python\ndef create_dlpk(emd_path: Path, pt_path: Path, inference_path: Path, output_dlpk: Path)\n```\n\nParameters:\n- `emd_path`: Path to the EMD JSON file\n- `pt_path`: Path to the TorchScript `.pt` model file\n- `inference_path`: Path to `RefugeeCampDetector.py` inference script\n- `output_dlpk`: Destination path for the `.dlpk` output file\n\nThe function uses `zipfile.ZIP_DEFLATED` compression to reduce package size while maintaining fast decompression performance.\n\n---\n\n## Integration with Training Pipeline\n\nThe DLPK generation is integrated into the main training pipeline and executed automatically after model training completes.\n\n### Training Pipeline Integration\n\n```mermaid\nflowchart TD\n    TRAIN_START[\"Training completes\"]\n    SAVE_PTH[\"Save state_dict to best_model.pth\"]\n    TRACE[\"Create TorchScript trace\"]\n    SAVE_PT[\"Save traced model to best_model.pt\"]\n    EXPORT_ONNX[\"Export to ONNX format\"]\n    \n    CREATE_STAC[\"Create STAC-MLM metadata\"]\n    \n    LOCATE_INFERENCE[\"Locate RefugeeCampDetector.py\"]\n    CONVERT_EMD[\"Call stacmlm_to_emd()\"]\n    CREATE_DLPK[\"Call create_dlpk()\"]\n    \n    LOG_ARTIFACTS[\"Log all artifacts to MLflow\"]\n    \n    TRAIN_START --\u003e SAVE_PTH\n    SAVE_PTH --\u003e TRACE\n    TRACE --\u003e SAVE_PT\n    SAVE_PT --\u003e EXPORT_ONNX\n    EXPORT_ONNX --\u003e CREATE_STAC\n    CREATE_STAC --\u003e LOCATE_INFERENCE\n    LOCATE_INFERENCE --\u003e CONVERT_EMD\n    CONVERT_EMD --\u003e CREATE_DLPK\n    CREATE_DLPK --\u003e LOG_ARTIFACTS\n```\n\n**Sources:** [examplemodel/src/train.py:436-496]()\n\n### Code Flow in train.py\n\nThe DLPK creation occurs in the `train_model()` function after model training:\n\n1. **Save PyTorch state dict** [line 438]():\n   ```python\n   torch.save(model.state_dict(), \"meta/best_model.pth\")\n   ```\n\n2. **Create TorchScript model** [lines 444-448]():\n   ```python\n   torch_model = clean_model.model\n   torch_model.eval()\n   traced_model = torch.jit.trace(torch_model, torch.randn(1, 3, 256, 256))\n   torch.jit.save(traced_model, \"meta/best_model.pt\")\n   ```\n\n3. **Locate inference script** [lines 463-464]():\n   ```python\n   current_dir = Path(__file__).parent.absolute()\n   esri_inference_path = current_dir / \"esri\" / \"RefugeeCampDetector.py\"\n   ```\n\n4. **Define paths** [lines 465-467]():\n   ```python\n   emd_path = Path(\"meta/model.emd\")\n   pt_path = Path(\"meta/best_model.pt\")\n   dlpk_path = Path(\"meta/best_model.dlpk\")\n   ```\n\n5. **Create DLPK** [line 468]():\n   ```python\n   create_dlpk(emd_path, pt_path, esri_inference_path, dlpk_path)\n   ```\n\n6. **Log to MLflow** [lines 493-496]():\n   ```python\n   mlflow.log_artifact(dlpk_path, artifact_path=\"esri\")\n   mlflow.log_artifact(emd_path, artifact_path=\"esri\")\n   mlflow.log_artifact(pt_path, artifact_path=\"esri\")\n   mlflow.log_artifact(esri_inference_path, artifact_path=\"esri\")\n   ```\n\n**Sources:** [examplemodel/src/train.py:462-496]()\n\n### Artifact Organization\n\nAfter training, the DLPK and related artifacts are logged to MLflow in a structured hierarchy:\n\n```\nmlflow_run/\n models/\n    best_model.pth\n    best_model.pt\n    best_model.onnx\n    best_model.dlpk\n metadata/\n    stac_item.json\n esri/\n    best_model.dlpk\n    model.emd\n    model.pt\n    RefugeeCampDetector.py\n checkpoints/\n     epoch=X-step=Y.ckpt\n```\n\nThe `esri/` directory contains all files needed for ESRI deployment, making it easy to download and deploy the complete package.\n\n**Sources:** [examplemodel/src/train.py:479-496]()\n\n---\n\n## Standalone Usage\n\nThe `stac2esri.py` script can be executed standalone for converting existing STAC-MLM files to DLPK packages without re-running training.\n\n### Command-Line Interface\n\n```bash\npython stac2esri.py \\\n  --stac path/to/stac_item.json \\\n  --pt path/to/model.pt \\\n  --out-dir output/ \\\n  --dlpk-name custom_model.dlpk\n```\n\nParameters:\n\n| Flag | Required | Default | Description |\n|------|----------|---------|-------------|\n| `--stac` | Yes | - | Path to STAC-MLM JSON file |\n| `--pt` | No | (inferred) | Path to PyTorch model; if omitted, inferred from STAC assets |\n| `--out-dir` | No | `output` | Output directory for EMD and DLPK |\n| `--dlpk-name` | No | `model.dlpk` | Filename for output DLPK package |\n\n**Sources:** [examplemodel/src/stac2esri.py:61-114]()\n\n### Standalone Execution Flow\n\n```mermaid\nflowchart TD\n    START[\"python stac2esri.py\"]\n    PARSE_ARGS[\"Parse command-line arguments\"]\n    \n    CHECK_PT{\"--pt provided?\"}\n    USE_PROVIDED[\"Use provided pt_path\"]\n    INFER_PT[\"Parse STAC file\u003cbr/\u003eFind pytorch asset\u003cbr/\u003eResolve relative path\"]\n    \n    LOCATE_INFERENCE[\"Locate inference.py\u003cbr/\u003ein script directory\"]\n    \n    CONVERT[\"Call stacmlm_to_emd()\"]\n    CREATE[\"Call create_dlpk()\"]\n    \n    PRINT[\"Print success message\"]\n    \n    START --\u003e PARSE_ARGS\n    PARSE_ARGS --\u003e CHECK_PT\n    \n    CHECK_PT --\u003e|\"Yes\"| USE_PROVIDED\n    CHECK_PT --\u003e|\"No\"| INFER_PT\n    \n    USE_PROVIDED --\u003e LOCATE_INFERENCE\n    INFER_PT --\u003e LOCATE_INFERENCE\n    \n    LOCATE_INFERENCE --\u003e CONVERT\n    CONVERT --\u003e CREATE\n    CREATE --\u003e PRINT\n```\n\n**Sources:** [examplemodel/src/stac2esri.py:61-114]()\n\nThe standalone mode is useful for:\n- Converting STAC-MLM files from external sources\n- Re-packaging models with updated inference scripts\n- Batch processing multiple models for deployment\n- Testing DLPK creation without full training runs\n\n---\n\n## Deployment in ArcGIS\n\nOnce created, DLPK packages can be deployed in ArcGIS Pro or ArcGIS Enterprise for raster inference operations.\n\n### ArcGIS Pro Integration\n\n1. **Load DLPK**: In ArcGIS Pro, use the \"Add Deep Learning Model\" tool\n2. **Configure Inference**: Open the \"Classify Pixels Using Deep Learning\" tool\n3. **Select DLPK**: Point to the `.dlpk` file location\n4. **Set Parameters**: Configure BatchSize and Threshold parameters\n5. **Run Inference**: Process raster datasets to generate segmentation masks\n\n### Runtime Behavior\n\nWhen ArcGIS loads the DLPK:\n\n1. Extracts the ZIP archive to a temporary directory\n2. Parses `model.emd` to understand model requirements\n3. Imports `RefugeeCampDetector.py` as a Python module\n4. Instantiates `RefugeeCampDetector` class\n5. Calls `initialize()` with path to `model.emd`\n6. Divides input raster into tiles matching `ImageHeight` x `ImageWidth`\n7. Calls `updatePixels()` for each tile or batch of tiles\n8. Mosaics output tiles into final segmentation raster\n\n### Performance Considerations\n\nThe DLPK inference performance in ArcGIS depends on:\n\n- **Tile size**: Larger tiles (e.g., 256x256) reduce overhead but increase memory usage\n- **Batch size**: Higher batch sizes improve GPU utilization but require more VRAM\n- **Model size**: TorchScript models are optimized but still larger than ONNX alternatives\n- **CPU vs GPU**: The `RefugeeCampDetector` loads models with `map_location=\"cpu\"` by default; GPU acceleration requires ArcGIS environment configuration\n\n**Sources:** [examplemodel/src/esri/RefugeeCampDetector.py:59]()\n\n---\n\n## Debugging and Logging\n\nThe `RefugeeCampDetector` class includes extensive logging for debugging DLPK inference issues in ArcGIS environments.\n\n### Log File Location\n\nAll debug messages are written to `rcd_debug.log` in the same directory as the inference script:\n\n```python\nSCRIPT_DIR = Path(__file__).parent\nLOG_PATH = SCRIPT_DIR / \"rcd_debug.log\"\n```\n\n**Sources:** [examplemodel/src/esri/RefugeeCampDetector.py:16-17]()\n\n### Logged Information\n\nThe inference script logs:\n\n- Initialization parameters [line 32]()\n- Model loading success/failure [lines 54, 62, 64]()\n- Tile coordinates and shapes [line 108]()\n- Mask statistics [line 110]()\n- Array shapes at each processing step [lines 117, 120, 124, 142]()\n- Input/output value ranges [lines 138, 141, 173]()\n- Model output types and shapes [lines 149, 157, 163, 170]()\n\nThis logging is essential for diagnosing issues when deploying DLPKs in production ArcGIS environments where interactive debugging is not available.\n\n**Sources:** [examplemodel/src/esri/RefugeeCampDetector.py:20-184]()"])</script><script>self.__next_f.push([1,"20:T422d,"])</script><script>self.__next_f.push([1,"# MLflow Project Structure\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/MLproject](examplemodel/MLproject)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document details the MLflow Project configuration defined in the `MLproject` file, which provides a declarative interface for orchestrating the machine learning pipeline in the OpenGeoAIModelHub repository. The `MLproject` file defines five entry points (preprocess, train, inference, validate_stac, stac2esri) that encapsulate distinct phases of the ML workflow, each with configurable parameters and command specifications.\n\nFor information about the actual implementation of the training pipeline, see [Training Pipeline](#3.2). For details on dependency management and package configuration, see [Dependencies and Configuration](#3.6).\n\n**Sources:** [examplemodel/MLproject:1-63]()\n\n---\n\n## MLproject File Overview\n\nThe `MLproject` file is a YAML-formatted configuration that defines the project name and entry points for execution via the MLflow CLI. It serves as the orchestration layer between user commands and the underlying Python implementation scripts.\n\n### Project Metadata\n\n```yaml\nname: refugee-camp-detector\n```\n\nThe project is named `refugee-camp-detector`, which serves as the identifier in MLflow tracking and experiments.\n\n**Sources:** [examplemodel/MLproject:1]()\n\n### Commented Docker Environment\n\n```yaml\n# docker_env:\n#   image: ghcr.io/kshitijrajsharma/opengeoaimodelshub:master\n```\n\nThe file includes a commented-out Docker environment specification, indicating that the project can optionally run in a containerized environment using the pre-built image from GitHub Container Registry. When uncommented, this would ensure consistent execution environments across different machines.\n\n**Sources:** [examplemodel/MLproject:2-3]()\n\n---\n\n## Entry Point Architecture\n\nThe MLproject defines five distinct entry points, each handling a specific phase of the ML pipeline. These entry points are invoked using the MLflow CLI with the syntax: `mlflow run . -e \u003centry_point_name\u003e`.\n\n```mermaid\ngraph TB\n    subgraph \"MLproject Entry Points\"\n        PREPROCESS[\"preprocess\u003cbr/\u003eData Acquisition\"]\n        TRAIN[\"train\u003cbr/\u003eModel Training\"]\n        INFERENCE[\"inference\u003cbr/\u003eModel Application\"]\n        VALIDATE[\"validate_stac\u003cbr/\u003eMetadata Validation\"]\n        CONVERT[\"stac2esri\u003cbr/\u003eDLPK Generation\"]\n    end\n    \n    subgraph \"Python Implementation Scripts\"\n        PREP_PY[\"src/preprocess.py\"]\n        TRAIN_PY[\"src/train.py\"]\n        INFER_PY[\"src/inference.py\"]\n        VAL_PY[\"validate_stac_mlm.py\"]\n        CONV_PY[\"src/stac2esri.py\"]\n    end\n    \n    subgraph \"Data Artifacts\"\n        CHIPS[\"chips/ directory\u003cbr/\u003eImage tiles\"]\n        LABELS[\"labels/ directory\u003cbr/\u003eSegmentation masks\"]\n    end\n    \n    subgraph \"Model Artifacts\"\n        CKPT[\"best_model.pth\u003cbr/\u003ePyTorch checkpoint\"]\n        ONNX[\"best_model.onnx\u003cbr/\u003eONNX model\"]\n        STAC[\"stac_item.json\u003cbr/\u003eSTAC-MLM metadata\"]\n        DLPK[\"best_model.dlpk\u003cbr/\u003eESRI package\"]\n    end\n    \n    PREPROCESS --\u003e|executes| PREP_PY\n    TRAIN --\u003e|executes| TRAIN_PY\n    INFERENCE --\u003e|executes| INFER_PY\n    VALIDATE --\u003e|executes| VAL_PY\n    CONVERT --\u003e|executes| CONV_PY\n    \n    PREP_PY --\u003e|produces| CHIPS\n    PREP_PY --\u003e|produces| LABELS\n    \n    CHIPS --\u003e|consumed by| TRAIN_PY\n    LABELS --\u003e|consumed by| TRAIN_PY\n    \n    TRAIN_PY --\u003e|produces| CKPT\n    TRAIN_PY --\u003e|produces| ONNX\n    TRAIN_PY --\u003e|produces| STAC\n    TRAIN_PY --\u003e|produces| DLPK\n    \n    CKPT --\u003e|loaded by| INFER_PY\n    STAC --\u003e|validated by| VAL_PY\n    STAC --\u003e|consumed by| CONV_PY\n    ONNX --\u003e|consumed by| CONV_PY\n    CONV_PY --\u003e|produces| DLPK\n```\n\n**Diagram: MLproject Entry Points and Their Relationships**\n\n**Sources:** [examplemodel/MLproject:5-63]()\n\n---\n\n## Entry Point: preprocess\n\nThe `preprocess` entry point downloads satellite imagery from a Tile Map Service (TMS) and generates training data chips with corresponding label masks.\n\n### Parameters\n\n| Parameter | Type | Default Value | Description |\n|-----------|------|---------------|-------------|\n| `zoom` | int | 19 | Tile zoom level for image resolution |\n| `bbox` | str | \"85.51991979758662,27.628837632373674,85.52736620395387,27.633394557789373\" | Bounding box coordinates (minlon,minlat,maxlon,maxlat) |\n| `tms` | str | \"https://tiles.openaerialmap.org/...\" | Tile Map Service URL template with {z}/{x}/{y} placeholders |\n| `train_dir` | str | \"data/train/sample\" | Output directory for generated training data |\n\n### Command Specification\n\n```yaml\ncommand: \u003e\n  uv run python src/preprocess.py\n  --zoom {zoom}\n  --bbox {bbox}\n  --tms {tms}\n  --train-dir    {train_dir}\n```\n\nThe command uses `uv run python` to execute the preprocessing script within the project's dependency environment. MLflow performs parameter substitution using curly brace syntax `{parameter_name}`.\n\n### Invocation Example\n\n```bash\nmlflow run . -e preprocess \\\n  -P zoom=19 \\\n  -P bbox=\"85.5199,27.6288,85.5274,27.6334\" \\\n  -P tms=\"https://tiles.openaerialmap.org/62d85d11d8499800053796c1/0/62d85d11d8499800053796c2/{z}/{x}/{y}\"\n```\n\n**Sources:** [examplemodel/MLproject:6-17]()\n\n---\n\n## Entry Point: train\n\nThe `train` entry point executes the model training pipeline using PyTorch Lightning, logs metrics to MLflow, and generates multiple model artifacts.\n\n### Parameters\n\n| Parameter | Type | Default Value | Description |\n|-----------|------|---------------|-------------|\n| `epochs` | int | 1 | Number of training epochs |\n| `batch_size` | int | 32 | Batch size for training |\n| `chips_dir` | str | \"data/train/sample/chips\" | Directory containing image chips |\n| `labels_dir` | str | \"data/train/sample/labels\" | Directory containing label masks |\n| `lr` | float | 1e-3 | Learning rate for optimizer |\n\n### Command Specification\n\n```yaml\ncommand: \u003e\n  uv run python src/train.py \n  --epochs {epochs}\n  --batch_size {batch_size}\n  --chips_dir {chips_dir}\n  --labels_dir {labels_dir}\n  --lr {lr}\n```\n\n### Parameter Flow to Python Implementation\n\n```mermaid\ngraph LR\n    subgraph \"MLproject Entry Point\"\n        TRAIN_EP[\"train entry point\"]\n        PARAMS[\"Parameters:\u003cbr/\u003eepochs: 1\u003cbr/\u003ebatch_size: 32\u003cbr/\u003echips_dir: str\u003cbr/\u003elabels_dir: str\u003cbr/\u003elr: 1e-3\"]\n    end\n    \n    subgraph \"Command Execution\"\n        CMD[\"uv run python src/train.py\"]\n        ARGPARSE[\"argparse.ArgumentParser\"]\n    end\n    \n    subgraph \"Python Script (train.py)\"\n        PARSER[\"parser.add_argument()\"]\n        TRAIN_FUNC[\"train_model(args)\"]\n        DATA_MODULE[\"CampDataModule\"]\n        TRAINER[\"pl.Trainer\"]\n    end\n    \n    TRAIN_EP --\u003e PARAMS\n    PARAMS --\u003e CMD\n    CMD --\u003e ARGPARSE\n    ARGPARSE --\u003e PARSER\n    PARSER --\u003e TRAIN_FUNC\n    TRAIN_FUNC --\u003e DATA_MODULE\n    TRAIN_FUNC --\u003e TRAINER\n```\n\n**Diagram: Parameter Flow from MLproject to Training Implementation**\n\nThe training script receives parameters via argparse at [examplemodel/src/train.py:510-516]() and uses them to configure the `CampDataModule` [examplemodel/src/train.py:389-393]() and `pl.Trainer` [examplemodel/src/train.py:414-419]().\n\n### Invocation Example\n\n```bash\nmlflow run . -e train \\\n  -P epochs=10 \\\n  -P batch_size=64 \\\n  -P lr=0.001\n```\n\n**Sources:** [examplemodel/MLproject:19-32](), [examplemodel/src/train.py:509-518]()\n\n---\n\n## Entry Point: inference\n\nThe `inference` entry point performs model inference on a single image and generates prediction overlays.\n\n### Parameters\n\n| Parameter | Type | Default Value | Description |\n|-----------|------|---------------|-------------|\n| `image_path` | str | (required) | Path to input image for inference |\n| `model_path` | str | \"meta/best_model.pt\" | Path to TorchScript model file |\n| `output_dir` | str | \"output\" | Directory for saving prediction results |\n| `mlflow_tracking` | bool | false | Enable MLflow tracking for inference run |\n\n### Command Specification\n\n```yaml\ncommand: \u003e\n  uv run python src/inference.py {image_path}\n  --model_path {model_path}\n  --output_dir {output_dir}\n  {{--mlflow_tracking if mlflow_tracking}}\n```\n\nThe command uses conditional parameter syntax `{{--mlflow_tracking if mlflow_tracking}}`, which only includes the `--mlflow_tracking` flag when the parameter is `true`.\n\n### Invocation Example\n\n```bash\nmlflow run . -e inference \\\n  -P image_path=test.jpg \\\n  -P model_path=meta/best_model.pt \\\n  -P mlflow_tracking=true\n```\n\n**Sources:** [examplemodel/MLproject:34-44]()\n\n---\n\n## Entry Point: validate_stac\n\nThe `validate_stac` entry point validates STAC-MLM metadata against the official schema to ensure compliance.\n\n### Parameters\n\n| Parameter | Type | Default Value | Description |\n|-----------|------|---------------|-------------|\n| `stac_file` | str | \"meta/stac_item.json\" | Path to STAC-MLM JSON file for validation |\n\n### Command Specification\n\n```yaml\ncommand: \"uv run python validate_stac_mlm.py {stac_file}\"\n```\n\n### Invocation Example\n\n```bash\nmlflow run . -e validate_stac -P stac_file=meta/stac_item.json\n```\n\n**Sources:** [examplemodel/MLproject:46-49]()\n\n---\n\n## Entry Point: stac2esri\n\nThe `stac2esri` entry point converts STAC-MLM metadata and ONNX models into ESRI Deep Learning Package (DLPK) format for ArcGIS deployment.\n\n### Parameters\n\n| Parameter | Type | Default Value | Description |\n|-----------|------|---------------|-------------|\n| `stac_path` | str | \"meta/stac_item.json\" | Path to STAC-MLM metadata file |\n| `onnx_path` | str | \"meta/best_model.onnx\" | Path to ONNX model file |\n| `out_dir` | str | \"meta\" | Output directory for generated DLPK |\n| `dlpk_name` | str | \"refugee-camp-detector.dlpk\" | Filename for the DLPK package |\n\n### Command Specification\n\n```yaml\ncommand: \u003e\n  uv run python src/stac2esri.py\n  --stac {stac_path}\n  --onnx {onnx_path}\n  --out-dir {out_dir}\n  --dlpk-name {dlpk_name}\n```\n\n### Invocation Example\n\n```bash\nmlflow run . -e stac2esri \\\n  -P stac_path=meta/stac_item.json \\\n  -P onnx_path=meta/best_model.onnx \\\n  -P dlpk_name=my_model.dlpk\n```\n\n**Sources:** [examplemodel/MLproject:51-62]()\n\n---\n\n## Parameter Type System\n\nThe MLproject file uses a type system to validate parameters passed via the MLflow CLI. Each parameter declaration includes a type specification that enforces data type constraints.\n\n### Supported Parameter Types\n\n| Type | Description | Example Values |\n|------|-------------|----------------|\n| `int` | Integer values | 1, 10, 100 |\n| `float` | Floating-point numbers | 1e-3, 0.001, 3.14 |\n| `str` | String values | \"path/to/file\", \"85.5199,27.6288\" |\n| `bool` | Boolean flags | true, false |\n\n### Parameter Declaration Syntax\n\n```yaml\nparameters:\n  parameter_name: {type: \u003ctype\u003e, default: \u003cvalue\u003e}\n```\n\nFor required parameters without defaults:\n\n```yaml\nparameters:\n  parameter_name: {type: \u003ctype\u003e}\n```\n\nExample from the `inference` entry point showing a required string parameter:\n\n```yaml\nimage_path: {type: str}\n```\n\n**Sources:** [examplemodel/MLproject:7-56]()\n\n---\n\n## Command Execution Pattern\n\nAll entry points use the `uv run python` command pattern, which ensures execution within the project's isolated dependency environment managed by the `uv` package manager.\n\n### Command Structure\n\n```\nuv run python \u003cscript_path\u003e [positional_args] [--flag value]\n```\n\n### Parameter Substitution\n\nMLflow performs string substitution for parameters using `{parameter_name}` syntax:\n\n```yaml\ncommand: \u003e\n  uv run python src/train.py \n  --epochs {epochs}\n  --batch_size {batch_size}\n```\n\nAt runtime, if invoked with `-P epochs=10 -P batch_size=64`, this becomes:\n\n```bash\nuv run python src/train.py --epochs 10 --batch_size 64\n```\n\n### Conditional Parameters\n\nThe `inference` entry point demonstrates conditional parameter inclusion:\n\n```yaml\n{{--mlflow_tracking if mlflow_tracking}}\n```\n\nThis syntax only includes `--mlflow_tracking` in the command when the boolean parameter is `true`.\n\n**Sources:** [examplemodel/MLproject:12-62]()\n\n---\n\n## Entry Point Dependency Graph\n\nThe following diagram shows the typical execution order and data dependencies between entry points:\n\n```mermaid\ngraph TD\n    START[\"Start: Fresh Project\"]\n    \n    PREPROCESS[\"mlflow run . -e preprocess\"]\n    TRAIN[\"mlflow run . -e train\"]\n    VALIDATE[\"mlflow run . -e validate_stac\"]\n    STAC2ESRI[\"mlflow run . -e stac2esri\"]\n    INFERENCE[\"mlflow run . -e inference\"]\n    \n    DATA_CHIPS[\"data/train/sample/chips/\"]\n    DATA_LABELS[\"data/train/sample/labels/\"]\n    MODEL_PTH[\"meta/best_model.pth\"]\n    MODEL_PT[\"meta/best_model.pt\"]\n    MODEL_ONNX[\"meta/best_model.onnx\"]\n    STAC_JSON[\"meta/stac_item.json\"]\n    MODEL_DLPK[\"meta/best_model.dlpk\"]\n    \n    START --\u003e PREPROCESS\n    PREPROCESS --\u003e|produces| DATA_CHIPS\n    PREPROCESS --\u003e|produces| DATA_LABELS\n    \n    DATA_CHIPS --\u003e TRAIN\n    DATA_LABELS --\u003e TRAIN\n    \n    TRAIN --\u003e|produces| MODEL_PTH\n    TRAIN --\u003e|produces| MODEL_PT\n    TRAIN --\u003e|produces| MODEL_ONNX\n    TRAIN --\u003e|produces| STAC_JSON\n    \n    STAC_JSON --\u003e VALIDATE\n    \n    STAC_JSON --\u003e STAC2ESRI\n    MODEL_ONNX --\u003e STAC2ESRI\n    STAC2ESRI --\u003e|produces| MODEL_DLPK\n    \n    MODEL_PT --\u003e INFERENCE\n    \n    VALIDATE -.-\u003e|optional check| STAC2ESRI\n```\n\n**Diagram: Entry Point Execution Dependencies**\n\n### Typical Workflow Sequence\n\n1. **Data Preparation**: Run `preprocess` to download and prepare training data\n2. **Model Training**: Run `train` to train the model and generate artifacts\n3. **Metadata Validation** (optional): Run `validate_stac` to verify STAC-MLM compliance\n4. **ESRI Package Creation** (optional): Run `stac2esri` to generate DLPK for ArcGIS deployment\n5. **Inference** (as needed): Run `inference` to apply the model to new images\n\n**Sources:** [examplemodel/MLproject:5-63]()\n\n---\n\n## Integration with MLflow Tracking\n\nWhen entry points are executed via `mlflow run`, MLflow automatically creates a run in the tracking server and logs the following information:\n\n### Automatically Tracked Information\n\n- **Parameters**: All entry point parameters are logged as MLflow parameters\n- **Source**: Git commit hash (if running from a Git repository)\n- **Entry Point**: Name of the entry point executed\n- **Start/End Time**: Execution timing information\n- **Status**: Success or failure status\n\n### Additional Tracking in train.py\n\nThe training script explicitly uses MLflow tracking API to log additional information:\n\n```mermaid\ngraph TB\n    subgraph \"MLflow Run Context\"\n        START_RUN[\"mlflow.start_run()\"]\n        LOG_PARAMS[\"mlflow.log_params()\"]\n        LOG_METRICS[\"mlflow.log_metrics()\"]\n        LOG_ARTIFACT[\"mlflow.log_artifact()\"]\n        LOG_MODEL[\"mlflow.pytorch.log_model()\"]\n    end\n    \n    subgraph \"Logged Information\"\n        SYS_INFO[\"System information\"]\n        DATA_STATS[\"Dataset statistics\"]\n        TRAIN_METRICS[\"Training metrics\"]\n        MODEL_PERF[\"Model performance\"]\n        ARTIFACTS[\"Model artifacts\"]\n    end\n    \n    START_RUN --\u003e|line 373| LOG_PARAMS\n    LOG_PARAMS --\u003e|line 396-403| DATA_STATS\n    LOG_PARAMS --\u003e LOG_METRICS\n    LOG_METRICS --\u003e|line 431| MODEL_PERF\n    LOG_METRICS --\u003e LOG_ARTIFACT\n    LOG_ARTIFACT --\u003e|lines 479-496| ARTIFACTS\n    LOG_ARTIFACT --\u003e LOG_MODEL\n    LOG_MODEL --\u003e|lines 501-503| ARTIFACTS\n    \n    SYS_INFO -.-\u003e|captured at line 374| LOG_PARAMS\n    TRAIN_METRICS -.-\u003e|logged during training| LOG_METRICS\n```\n\n**Diagram: MLflow Tracking Integration in Training Pipeline**\n\n**Sources:** [examplemodel/src/train.py:373](), [examplemodel/src/train.py:396-403](), [examplemodel/src/train.py:431](), [examplemodel/src/train.py:479-503]()\n\n---\n\n## Extending the MLproject\n\nTo add a new entry point to the MLproject file:\n\n1. **Define the entry point** with a unique name\n2. **Specify parameters** with types and optional defaults\n3. **Write the command** using `uv run python` and parameter substitution\n4. **Implement the Python script** that handles the specified arguments\n5. **Test the entry point** using `mlflow run . -e \u003centry_point_name\u003e`\n\n### Example: Adding a New Entry Point\n\n```yaml\nevaluate:\n  parameters:\n    model_path: {type: str, default: \"meta/best_model.pt\"}\n    test_dir: {type: str, default: \"data/test\"}\n    metrics_output: {type: str, default: \"meta/metrics.json\"}\n  command: \u003e\n    uv run python src/evaluate.py\n    --model_path {model_path}\n    --test_dir {test_dir}\n    --metrics_output {metrics_output}\n```\n\n**Sources:** [examplemodel/MLproject:5-63]()\n\n---\n\n## Summary\n\nThe `MLproject` file provides a declarative, standardized interface for executing the refugee camp detection ML pipeline. It defines five entry points that orchestrate data preprocessing, model training, inference, metadata validation, and ESRI package generation. Each entry point specifies typed parameters with defaults and uses `uv run python` for dependency-isolated execution. This structure enables reproducible ML workflows that can be tracked, versioned, and shared via MLflow.\n\n**Sources:** [examplemodel/MLproject:1-63]()"])</script><script>self.__next_f.push([1,"21:T43c9,"])</script><script>self.__next_f.push([1,"# Dependencies and Configuration\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/.env_sample](examplemodel/.env_sample)\n- [examplemodel/pyproject.toml](examplemodel/pyproject.toml)\n- [examplemodel/uv.lock](examplemodel/uv.lock)\n\n\u003c/details\u003e\n\n\n\nThis page documents the dependency management system and configuration requirements for the example model in OpenGeoAIModelHub. It covers the Python package dependencies defined in `pyproject.toml`, the lock file mechanism via `uv.lock`, and the environment variables required for MLflow and S3 integration.\n\nFor information about the MLflow Project structure and entry points, see [MLflow Project Structure](#3.5). For infrastructure deployment configuration, see [Configuration Management](#4.3).\n\n## Purpose and Scope\n\nThe example model system uses modern Python dependency management with the `uv` package manager, which provides fast dependency resolution and reproducible builds. This document explains:\n\n- The `pyproject.toml` dependency specification\n- The `uv.lock` lock file structure and purpose\n- Environment variable configuration for MLflow tracking and artifact storage\n- Dependency categories and their roles in the ML pipeline\n\n## Package Management Architecture\n\nThe project uses `uv` as its package manager, which reads dependency specifications from `pyproject.toml` and generates a lock file (`uv.lock`) that pins all transitive dependencies to specific versions.\n\n**Dependency Management Flow**\n\n```mermaid\ngraph TB\n    PYPROJECT[\"pyproject.toml\u003cbr/\u003e[project] section\"]\n    DEPGROUPS[\"dependency-groups\u003cbr/\u003e[dependency-groups]\"]\n    UV[\"uv package manager\"]\n    UVLOCK[\"uv.lock\u003cbr/\u003eVersion pins\"]\n    VENV[\"Virtual Environment\u003cbr/\u003e.venv/\"]\n    \n    PYPROJECT --\u003e|\"defines core deps\"| UV\n    DEPGROUPS --\u003e|\"defines optional deps\"| UV\n    UV --\u003e|\"resolves versions\"| UVLOCK\n    UVLOCK --\u003e|\"uv sync\"| VENV\n    \n    ENVVARS[\".env_sample\u003cbr/\u003eEnvironment Variables\"]\n    RUNTIME[\"Runtime Configuration\u003cbr/\u003eMLflow + S3\"]\n    \n    ENVVARS -.-\u003e|\"export vars\"| RUNTIME\n    VENV --\u003e|\"imports packages\"| RUNTIME\n```\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [examplemodel/uv.lock:1-10](), [examplemodel/.env_sample:1-5]()\n\n## Project Dependencies (pyproject.toml)\n\nThe `pyproject.toml` file defines the project metadata and dependencies using PEP 621 standards.\n\n### Project Metadata\n\n| Field | Value | Description |\n|-------|-------|-------------|\n| `name` | `examplemodel` | Package name |\n| `version` | `0.0.1` | Semantic version |\n| `requires-python` | `\u003e=3.10` | Minimum Python version |\n| `description` | Enhanced refugee camp detection model with production-ready STAC-MLM metadata | Project description |\n\n**Sources:** [examplemodel/pyproject.toml:1-6]()\n\n### Core Dependencies\n\nThe project depends on several categories of packages that support different aspects of the ML pipeline:\n\n**ML Framework Dependencies**\n\n```mermaid\ngraph LR\n    subgraph \"ML Training Stack\"\n        TORCH[\"torch\u003e=2.7.1\u003cbr/\u003eDeep Learning Framework\"]\n        TORCHVISION[\"torchvision\u003e=0.22.1\u003cbr/\u003eVision Utilities\"]\n        LIGHTNING[\"pytorch-lightning\u003e=2.5.2\u003cbr/\u003eTraining Abstraction\"]\n    end\n    \n    subgraph \"Model Export\"\n        ONNX[\"onnx\u003e=1.18.0\u003cbr/\u003eModel Serialization\"]\n        ONNXSCRIPT[\"onnxscript\u003e=0.3.2\u003cbr/\u003eONNX Utilities\"]\n    end\n    \n    subgraph \"Geospatial ML\"\n        GEOMLTK[\"geomltoolkits\u003e=0.3.9\u003cbr/\u003eGeospatial Data Processing\"]\n        OPENCV[\"opencv-python\u003e=4.8.0\u003cbr/\u003eImage Processing\"]\n    end\n    \n    subgraph \"MLOps Stack\"\n        MLFLOW[\"mlflow\u003e=3.1.1\u003cbr/\u003eExperiment Tracking\"]\n        BOTO3[\"boto3\u003e=1.39.12\u003cbr/\u003eS3 Client\"]\n    end\n    \n    subgraph \"Metadata Standards\"\n        STACMODEL[\"stac-model\u003e=0.3.0\u003cbr/\u003eML Model Metadata\"]\n        PYSTAC[\"pystac\u003e=1.8.0\u003cbr/\u003eSTAC Catalog\"]\n    end\n    \n    LIGHTNING --\u003e TORCH\n    TORCHVISION --\u003e TORCH\n    ONNX --\u003e TORCH\n    GEOMLTK --\u003e OPENCV\n    MLFLOW --\u003e BOTO3\n    STACMODEL --\u003e PYSTAC\n```\n\n**Sources:** [examplemodel/pyproject.toml:7-24]()\n\n### Dependency Table\n\nThe following table lists all direct dependencies and their purposes:\n\n| Package | Version Constraint | Purpose |\n|---------|-------------------|---------|\n| `torch` | `\u003e=2.7.1` | Core PyTorch deep learning framework |\n| `torchvision` | `\u003e=0.22.1` | Computer vision utilities and transforms |\n| `pytorch-lightning` | `\u003e=2.5.2` | High-level training abstraction (LitRefugeeCamp) |\n| `mlflow` | `\u003e=3.1.1` | Experiment tracking and model registry |\n| `onnx` | `\u003e=1.18.0` | Model export to ONNX format |\n| `onnxscript` | `\u003e=0.3.2` | ONNX graph manipulation utilities |\n| `geomltoolkits` | `\u003e=0.3.9` | Geospatial ML data processing (TMS, OSM integration) |\n| `stac-model` | `\u003e=0.3.0` | STAC-MLM metadata generation |\n| `pystac` | `\u003e=1.8.0` | STAC catalog manipulation |\n| `boto3` | `\u003e=1.39.12` | AWS S3 / MinIO object storage client |\n| `opencv-python` | `\u003e=4.8.0` | Image processing and visualization |\n| `psutil` | `\u003e=7.0.0` | System resource monitoring |\n| `pynvml` | `\u003e=12.0.0` | NVIDIA GPU monitoring |\n| `jsonschema` | `\u003e=4.0.0` | JSON schema validation for STAC |\n| `requests` | `\u003e=2.28.0` | HTTP requests for external APIs |\n| `dotenv` | `\u003e=0.9.9` | Environment variable loading |\n\n**Sources:** [examplemodel/pyproject.toml:7-24]()\n\n### Dependency Groups\n\nThe project defines optional dependency groups for specific use cases:\n\n```toml\n[dependency-groups]\nvalidation = [\n    \"jsonschema\u003e=4.0.0\",\n    \"requests\u003e=2.28.0\",\n]\n```\n\nThe `validation` group includes dependencies needed for the STAC metadata validation entry point. These can be installed separately with `uv sync --group validation`.\n\n**Sources:** [examplemodel/pyproject.toml:26-30]()\n\n## Lock File Structure (uv.lock)\n\nThe `uv.lock` file contains the complete dependency graph with pinned versions for reproducible installations. It uses a custom format that records:\n\n- Exact versions of all direct and transitive dependencies\n- Multiple resolution markers for different Python versions\n- Source URLs and checksums for verification\n- Wheel and source distribution metadata\n\n### Lock File Metadata\n\n```\nversion = 1\nrevision = 2\nrequires-python = \"\u003e=3.10\"\nresolution-markers = [\n    \"python_full_version \u003e= '3.13'\",\n    \"python_full_version == '3.12.*'\",\n    \"python_full_version == '3.11.*'\",\n    \"python_full_version \u003c '3.11'\",\n]\n```\n\nThe lock file supports multiple Python version resolution strategies, ensuring compatibility across Python 3.10-3.13.\n\n**Sources:** [examplemodel/uv.lock:1-9]()\n\n### Key Transitive Dependencies\n\nThe lock file resolves hundreds of transitive dependencies. Key categories include:\n\n| Category | Example Packages | Purpose |\n|----------|-----------------|---------|\n| **HTTP/Async** | `aiohttp`, `aiosignal`, `frozenlist`, `multidict`, `yarl` | Async HTTP client for MLflow |\n| **Data Processing** | `numpy`, `pandas`, `scipy` | Scientific computing (via PyTorch) |\n| **Visualization** | `matplotlib`, `contourpy`, `pillow` | Plot generation and image handling |\n| **Geospatial** | `rasterio`, `affine`, `gdal` | Raster data processing (via geomltoolkits) |\n| **ML Frameworks** | `torch`, `torchvision`, `pytorch-lightning` | Core ML stack |\n| **MLOps** | `mlflow`, `sqlalchemy`, `alembic` | Experiment tracking backend |\n| **Cloud Storage** | `boto3`, `botocore`, `s3transfer` | S3/MinIO integration |\n\n**Sources:** [examplemodel/uv.lock:11-230]()\n\n### Package Resolution Example\n\nEach package in the lock file includes detailed metadata:\n\n```\n[[package]]\nname = \"torch\"\nversion = \"2.7.1\"\nsource = { registry = \"https://pypi.org/simple\" }\ndependencies = [\n    { name = \"filelock\" },\n    { name = \"typing-extensions\" },\n    { name = \"sympy\" },\n    { name = \"networkx\" },\n    { name = \"jinja2\" },\n    { name = \"fsspec\" },\n]\n```\n\nThis structure allows `uv` to:\n- Verify package integrity via checksums\n- Resolve version conflicts deterministically\n- Support multiple platform-specific wheels\n- Cache downloads for fast installation\n\n**Sources:** [examplemodel/uv.lock:1-10]()\n\n## Environment Configuration\n\nThe example model requires environment variables for MLflow tracking and S3 artifact storage. These are documented in the `.env_sample` template file.\n\n### Environment Variable Structure\n\n**Environment Configuration Flow**\n\n```mermaid\ngraph TB\n    ENVSAMPLE[\".env_sample\u003cbr/\u003eTemplate File\"]\n    ENVFILE[\".env\u003cbr/\u003eLocal Configuration\"]\n    EXPORT[\"export Commands\u003cbr/\u003eShell Environment\"]\n    \n    subgraph \"MLflow Configuration\"\n        MLFLOW_URI[\"MLFLOW_TRACKING_URI\u003cbr/\u003eServer URL\"]\n        S3_ENDPOINT[\"MLFLOW_S3_ENDPOINT_URL\u003cbr/\u003eMinIO/S3 URL\"]\n    end\n    \n    subgraph \"S3 Credentials\"\n        AWS_KEY[\"AWS_ACCESS_KEY_ID\u003cbr/\u003eAccess Key\"]\n        AWS_SECRET[\"AWS_SECRET_ACCESS_KEY\u003cbr/\u003eSecret Key\"]\n    end\n    \n    subgraph \"Runtime Usage\"\n        TRAIN[\"train.py\u003cbr/\u003emlflow.start_run()\"]\n        ARTIFACTS[\"Artifact Storage\u003cbr/\u003emodel files\"]\n        METRICS[\"Metrics Storage\u003cbr/\u003ePostgreSQL\"]\n    end\n    \n    ENVSAMPLE -.-\u003e|\"copy to\"| ENVFILE\n    ENVFILE --\u003e|\"source .env\"| EXPORT\n    EXPORT --\u003e MLFLOW_URI\n    EXPORT --\u003e S3_ENDPOINT\n    EXPORT --\u003e AWS_KEY\n    EXPORT --\u003e AWS_SECRET\n    \n    MLFLOW_URI --\u003e|\"connects to\"| TRAIN\n    MLFLOW_URI --\u003e|\"logs to\"| METRICS\n    S3_ENDPOINT --\u003e|\"stores to\"| ARTIFACTS\n    AWS_KEY --\u003e|\"authenticates\"| ARTIFACTS\n    AWS_SECRET --\u003e|\"authenticates\"| ARTIFACTS\n```\n\n**Sources:** [examplemodel/.env_sample:1-5]()\n\n### Required Environment Variables\n\n| Variable | Example Value | Purpose |\n|----------|--------------|---------|\n| `AWS_ACCESS_KEY_ID` | `key_key` | MinIO/S3 access key for artifact storage |\n| `AWS_SECRET_ACCESS_KEY` | `secret_secret` | MinIO/S3 secret key for authentication |\n| `MLFLOW_S3_ENDPOINT_URL` | `https://minio-api.krschap.tech` | S3-compatible endpoint URL (MinIO) |\n| `MLFLOW_TRACKING_URI` | `http://mlflow.krschap.tech` | MLflow tracking server URL |\n\n**Sources:** [examplemodel/.env_sample:1-4]()\n\n### Configuration Usage\n\nThese environment variables are consumed by:\n\n1. **MLflow Python Client**: `mlflow.set_tracking_uri()` reads `MLFLOW_TRACKING_URI` to connect to the tracking server\n2. **Boto3 S3 Client**: MLflow's artifact repository uses `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `MLFLOW_S3_ENDPOINT_URL` to store artifacts in MinIO\n3. **Training Pipeline**: The `train.py` script automatically uses these variables when calling `mlflow.start_run()` and `mlflow.log_artifact()`\n\n**Usage Pattern:**\n\n```bash\n# Copy template and edit with your credentials\ncp .env_sample .env\nnano .env\n\n# Source environment variables\nsource .env\n\n# Run training with MLflow integration\nmlflow run . -e train\n```\n\nThe MLflow client library automatically detects these environment variables and configures the appropriate backends for metadata storage (PostgreSQL via tracking server) and artifact storage (MinIO via S3 protocol).\n\n**Sources:** [examplemodel/.env_sample:1-5]()\n\n## Dependency Installation Workflow\n\nThe complete dependency installation process uses `uv` for fast, reproducible builds:\n\n**Installation Process**\n\n```mermaid\ngraph TB\n    START[\"Developer Clones Repo\"]\n    INSTALL_UV[\"Install uv Package Manager\u003cbr/\u003epip install uv\"]\n    SYNC[\"uv sync\u003cbr/\u003eRead pyproject.toml + uv.lock\"]\n    \n    subgraph \"Dependency Resolution\"\n        READ_PYPROJECT[\"Parse pyproject.toml\u003cbr/\u003eExtract dependencies\"]\n        READ_LOCK[\"Parse uv.lock\u003cbr/\u003eGet pinned versions\"]\n        CHECK_CACHE[\"Check Local Cache\u003cbr/\u003e~/.cache/uv/\"]\n        DOWNLOAD[\"Download Missing Packages\u003cbr/\u003eFrom PyPI\"]\n        VERIFY[\"Verify Checksums\u003cbr/\u003eAgainst uv.lock\"]\n    end\n    \n    subgraph \"Environment Setup\"\n        CREATE_VENV[\"Create Virtual Environment\u003cbr/\u003e.venv/\"]\n        INSTALL_PKGS[\"Install Packages\u003cbr/\u003eInto .venv/\"]\n        LINK_BIN[\"Link Executables\u003cbr/\u003e.venv/bin/\"]\n    end\n    \n    READY[\"Environment Ready\u003cbr/\u003emlflow run .\"]\n    \n    START --\u003e INSTALL_UV\n    INSTALL_UV --\u003e SYNC\n    SYNC --\u003e READ_PYPROJECT\n    SYNC --\u003e READ_LOCK\n    READ_PYPROJECT --\u003e CHECK_CACHE\n    READ_LOCK --\u003e CHECK_CACHE\n    CHECK_CACHE --\u003e|\"cache miss\"| DOWNLOAD\n    CHECK_CACHE --\u003e|\"cache hit\"| VERIFY\n    DOWNLOAD --\u003e VERIFY\n    VERIFY --\u003e CREATE_VENV\n    CREATE_VENV --\u003e INSTALL_PKGS\n    INSTALL_PKGS --\u003e LINK_BIN\n    LINK_BIN --\u003e READY\n```\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [examplemodel/uv.lock:1-10]()\n\n### Installation Commands\n\n| Command | Purpose |\n|---------|---------|\n| `uv sync` | Install all dependencies from `uv.lock` |\n| `uv sync --group validation` | Install with optional dependency groups |\n| `uv add \u003cpackage\u003e` | Add a new dependency and update `uv.lock` |\n| `uv lock` | Regenerate `uv.lock` from `pyproject.toml` |\n| `uv pip list` | List installed packages |\n\nThe `uv sync` command is idempotent and fast, typically completing in seconds due to aggressive caching and parallel downloads.\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [examplemodel/uv.lock:1-10]()\n\n## Dependency Categories and Their Roles\n\nUnderstanding the dependency categories helps clarify their usage in the codebase:\n\n**Dependency Role Mapping**\n\n```mermaid\ngraph TB\n    subgraph \"Data Processing Layer\"\n        GEOMLTK[\"geomltoolkits\u003cbr/\u003eData Module\"]\n        OPENCV[\"opencv-python\u003cbr/\u003eImage Processing\"]\n        RASTERIO[\"rasterio\u003cbr/\u003eGeospatial Raster\"]\n    end\n    \n    subgraph \"Model Layer\"\n        TORCH[\"torch\u003cbr/\u003eNeural Networks\"]\n        LIGHTNING[\"pytorch-lightning\u003cbr/\u003eLitRefugeeCamp\"]\n        TORCHVISION[\"torchvision\u003cbr/\u003eTransforms\"]\n    end\n    \n    subgraph \"Training Layer\"\n        MLFLOW[\"mlflow\u003cbr/\u003eExperiment Tracking\"]\n        PSUTIL[\"psutil\u003cbr/\u003eResource Monitoring\"]\n        PYNVML[\"pynvml\u003cbr/\u003eGPU Monitoring\"]\n    end\n    \n    subgraph \"Export Layer\"\n        ONNX[\"onnx\u003cbr/\u003eModel Serialization\"]\n        STACMODEL[\"stac-model\u003cbr/\u003eMetadata Generation\"]\n        PYSTAC[\"pystac\u003cbr/\u003eSTAC Items\"]\n    end\n    \n    subgraph \"Storage Layer\"\n        BOTO3[\"boto3\u003cbr/\u003eS3 Client\"]\n        REQUESTS[\"requests\u003cbr/\u003eHTTP Client\"]\n    end\n    \n    GEOMLTK --\u003e|\"provides data to\"| LIGHTNING\n    OPENCV --\u003e|\"preprocesses for\"| LIGHTNING\n    LIGHTNING --\u003e|\"uses\"| TORCH\n    LIGHTNING --\u003e|\"uses\"| TORCHVISION\n    LIGHTNING --\u003e|\"logs to\"| MLFLOW\n    PSUTIL --\u003e|\"monitors for\"| MLFLOW\n    PYNVML --\u003e|\"monitors for\"| MLFLOW\n    TORCH --\u003e|\"exports to\"| ONNX\n    ONNX --\u003e|\"described by\"| STACMODEL\n    STACMODEL --\u003e|\"creates\"| PYSTAC\n    MLFLOW --\u003e|\"stores with\"| BOTO3\n    GEOMLTK --\u003e|\"fetches with\"| REQUESTS\n```\n\n**Sources:** [examplemodel/pyproject.toml:7-24]()\n\n### PyTorch Lightning Dependencies\n\nThe `pytorch-lightning` package provides the high-level abstractions used throughout the training pipeline:\n\n- **Used in**: `src/model.py` for the `LitRefugeeCamp` class\n- **Provides**: `LightningModule`, `Trainer`, `LightningDataModule`\n- **Integrates with**: MLflow via automatic logging callbacks\n\n### MLflow Dependencies\n\nThe `mlflow` package handles experiment tracking and model versioning:\n\n- **Used in**: `src/train.py` for `mlflow.start_run()` and `mlflow.pytorch.log_model()`\n- **Requires**: Environment variables for tracking URI and S3 endpoint\n- **Stores**: Metrics in PostgreSQL, artifacts in MinIO\n\n### Geospatial Dependencies\n\nThe `geomltoolkits` package provides utilities for working with geospatial data:\n\n- **Used in**: Data preprocessing for fetching imagery from TMS servers\n- **Integrates with**: OpenAerialMap and OpenStreetMap APIs\n- **Provides**: Chip extraction and label generation\n\n### STAC/ONNX Dependencies\n\nThe `stac-model` and `onnx` packages support model export and metadata:\n\n- **Used in**: `src/esri/RefugeeCampDetector.py` for DLPK generation\n- **Creates**: STAC-MLM compliant metadata for model discoverability\n- **Exports**: PyTorch models to ONNX format for cross-platform deployment\n\n**Sources:** [examplemodel/pyproject.toml:7-24]()\n\n## Version Constraints and Compatibility\n\nThe project uses minimum version constraints (`\u003e=`) rather than exact pins in `pyproject.toml`, allowing flexibility while maintaining compatibility:\n\n| Constraint Type | Example | Rationale |\n|----------------|---------|-----------|\n| Minimum version | `torch\u003e=2.7.1` | Ensures required features are available |\n| Python requirement | `requires-python = \"\u003e=3.10\"` | Leverages modern Python features |\n| Lock file pins | Exact versions in `uv.lock` | Ensures reproducibility |\n\nThis approach provides:\n- **Flexibility**: Allows patch version updates automatically\n- **Reproducibility**: `uv.lock` pins exact versions for consistent builds\n- **Security**: `uv lock` can be run to update to latest compatible versions with security patches\n\n**Sources:** [examplemodel/pyproject.toml:6-24](), [examplemodel/uv.lock:1-9]()\n\n## Summary\n\nThe dependency management system uses:\n- **pyproject.toml**: PEP 621 compliant project metadata and dependency specifications\n- **uv.lock**: Complete dependency graph with pinned versions for reproducibility\n- **uv package manager**: Fast, modern dependency resolution and installation\n- **Environment variables**: Runtime configuration for MLflow and S3 integration\n\nThis configuration supports the complete ML pipeline from data ingestion through model training, export, and deployment with reproducible builds and standardized metadata.\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [examplemodel/uv.lock:1-10](), [examplemodel/.env_sample:1-5]()"])</script><script>self.__next_f.push([1,"22:T4bd1,"])</script><script>self.__next_f.push([1,"# Utilities and Helper Functions\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/src/inference.py](examplemodel/src/inference.py)\n- [examplemodel/src/utils.py](examplemodel/src/utils.py)\n\n\u003c/details\u003e\n\n\n\nThis page documents the utility functions and helper modules that support the example model system's training, evaluation, and inference workflows. These utilities provide reusable functionality for model evaluation metrics, image preprocessing, prediction generation, metadata management, and MLflow integration.\n\nFor information about the main training pipeline that uses these utilities, see [Training Pipeline](#3.2). For details on the inference system architecture, see [Inference System](#3.3).\n\n## Overview\n\nThe utility functions are distributed across two primary modules:\n\n| Module | Purpose | Key Functions |\n|--------|---------|---------------|\n| `examplemodel/src/utils.py` | Model evaluation and metrics logging | `log_confusion_matrix()` |\n| `examplemodel/src/inference.py` | Inference support and preprocessing | `preprocess_image()`, `create_inference_metadata()`, `load_stac_item()`, `log_inference_example()` |\n\nThese utilities are designed to be called from both the training pipeline ([train.py](https://github.com/kshitijrajsharma/opengeoaimodelshub)) and the inference pipeline ([inference.py](https://github.com/kshitijrajsharma/opengeoaimodelshub)), providing consistent functionality across the ML lifecycle.\n\n## Utility Module Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Training Pipeline\"\n        TRAIN[\"train.py\"]\n        TRAINER[\"PyTorch Lightning Trainer\"]\n    end\n    \n    subgraph \"Inference Pipeline\"\n        INFER[\"inference.py::main()\"]\n        PREDICT[\"inference.py::predict_image_enhanced()\"]\n    end\n    \n    subgraph \"Utility Modules\"\n        UTILS[\"utils.py\"]\n        CONFMAT[\"log_confusion_matrix()\"]\n        \n        INFER_UTILS[\"inference.py (utilities)\"]\n        PREPROCESS[\"preprocess_image()\"]\n        CREATE_META[\"create_inference_metadata()\"]\n        LOAD_STAC[\"load_stac_item()\"]\n        LOG_EXAMPLE[\"log_inference_example()\"]\n    end\n    \n    subgraph \"MLflow Integration\"\n        MLFLOW[\"MLflow Tracking\"]\n        ARTIFACTS[\"Artifact Logging\"]\n        METRICS[\"Metric Logging\"]\n    end\n    \n    TRAIN --\u003e CONFMAT\n    TRAIN --\u003e LOG_EXAMPLE\n    \n    INFER --\u003e PREPROCESS\n    INFER --\u003e CREATE_META\n    INFER --\u003e LOAD_STAC\n    PREDICT --\u003e PREPROCESS\n    \n    CONFMAT --\u003e MLFLOW\n    CONFMAT --\u003e ARTIFACTS\n    LOG_EXAMPLE --\u003e MLFLOW\n    LOG_EXAMPLE --\u003e ARTIFACTS\n    \n    CREATE_META --\u003e METRICS\n    \n    TRAINER -.-\u003e|\"after training\"| CONFMAT\n    TRAINER -.-\u003e|\"after training\"| LOG_EXAMPLE\n```\n\n**Diagram: Utility Function Integration**\n\nThis diagram shows how utility functions integrate with the training and inference pipelines, and their connections to MLflow tracking.\n\nSources: [examplemodel/src/utils.py:1-32](), [examplemodel/src/inference.py:1-250]()\n\n## Evaluation Utilities\n\n### Confusion Matrix Logging\n\nThe `log_confusion_matrix()` function provides pixel-wise evaluation metrics for segmentation tasks. It computes a confusion matrix across all test samples and logs the visualization to MLflow.\n\n```mermaid\nsequenceDiagram\n    participant Train as \"train.py\"\n    participant LogCM as \"log_confusion_matrix()\"\n    participant Model as \"PyTorch Model\"\n    participant DM as \"DataModule\"\n    participant SKL as \"sklearn.metrics\"\n    participant PLT as \"matplotlib.pyplot\"\n    participant MLflow as \"MLflow\"\n    \n    Train-\u003e\u003eLogCM: Call after training\n    LogCM-\u003e\u003eModel: Set to eval mode\n    LogCM-\u003e\u003eDM: Get test_dataloader()\n    \n    loop For each test batch\n        LogCM-\u003e\u003eModel: Forward pass (x)\n        Model--\u003e\u003eLogCM: predictions\n        LogCM-\u003e\u003eLogCM: Threshold at 0.5\n        LogCM-\u003e\u003eLogCM: Accumulate y_true, y_pred\n    end\n    \n    LogCM-\u003e\u003eSKL: confusion_matrix(y_true, y_pred)\n    SKL--\u003e\u003eLogCM: Confusion matrix array\n    \n    LogCM-\u003e\u003ePLT: ConfusionMatrixDisplay.plot()\n    LogCM-\u003e\u003ePLT: Save to \"meta/confusion_matrix.png\"\n    \n    LogCM-\u003e\u003eMLflow: log_artifact(\"meta/confusion_matrix.png\")\n    MLflow--\u003e\u003eTrain: Artifact logged\n```\n\n**Diagram: Confusion Matrix Generation Flow**\n\nSources: [examplemodel/src/utils.py:8-31]()\n\n#### Function Signature\n\n```python\ndef log_confusion_matrix(model, datamodule, mlflow_run, device=None)\n```\n\n**Parameters:**\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `model` | `torch.nn.Module` | The trained model to evaluate |\n| `datamodule` | `LightningDataModule` | Data module providing `test_dataloader()` |\n| `mlflow_run` | MLflow Run | Active MLflow run context (not actively used in function) |\n| `device` | `torch.device` | Optional device; defaults to model's device |\n\n**Implementation Details:**\n\nThe function performs the following steps:\n\n1. **Device Detection**: Automatically detects the model's device if not specified ([utils.py:9-10]())\n2. **Evaluation Mode**: Sets model to evaluation mode ([utils.py:11]())\n3. **Prediction Collection**: Iterates through test dataloader, collecting predictions and ground truth ([utils.py:14-22]())\n4. **Thresholding**: Applies 0.5 threshold to convert probabilities to binary predictions ([utils.py:19]())\n5. **Confusion Matrix Computation**: Uses `sklearn.metrics.confusion_matrix()` for pixel-wise accuracy ([utils.py:23]())\n6. **Visualization**: Creates confusion matrix display using `ConfusionMatrixDisplay` ([utils.py:24-27]())\n7. **MLflow Logging**: Saves to `meta/confusion_matrix.png` and logs as artifact ([utils.py:27-29]())\n\n**Usage Context:**\n\nThis function is typically called from [train.py](https://github.com/kshitijrajsharma/opengeoaimodelshub) after training completes, as part of the evaluation phase. It provides a visual representation of the model's classification performance at the pixel level.\n\nSources: [examplemodel/src/utils.py:8-31]()\n\n## Inference Utilities\n\nThe inference utilities provide a comprehensive set of functions for image preprocessing, prediction generation, and metadata management during model inference.\n\n### Image Preprocessing\n\n#### `preprocess_image()`\n\nTransforms input images into model-ready tensors using standard ImageNet normalization.\n\n```python\ndef preprocess_image(image_path: str) -\u003e torch.Tensor\n```\n\n**Preprocessing Pipeline:**\n\n```mermaid\nflowchart LR\n    INPUT[\"Input Image\u003cbr/\u003e(PIL.Image)\"]\n    RESIZE[\"Resize\u003cbr/\u003e256x256\"]\n    TOTENSOR[\"ToTensor\u003cbr/\u003eNormalize [0,1]\"]\n    NORMALIZE[\"Normalize\u003cbr/\u003emean=[0.485,0.456,0.406]\u003cbr/\u003estd=[0.229,0.224,0.225]\"]\n    UNSQUEEZE[\"Unsqueeze\u003cbr/\u003eAdd batch dimension\"]\n    OUTPUT[\"Tensor\u003cbr/\u003eshape: [1,3,256,256]\"]\n    \n    INPUT --\u003e RESIZE\n    RESIZE --\u003e TOTENSOR\n    TOTENSOR --\u003e NORMALIZE\n    NORMALIZE --\u003e UNSQUEEZE\n    UNSQUEEZE --\u003e OUTPUT\n```\n\n**Diagram: Image Preprocessing Transform Chain**\n\n**Transform Specifications:**\n\n| Step | Operation | Parameters | Purpose |\n|------|-----------|------------|---------|\n| 1 | `transforms.Resize()` | (256, 256) | Standardize input size |\n| 2 | `transforms.ToTensor()` | - | Convert PIL to tensor, scale to [0,1] |\n| 3 | `transforms.Normalize()` | ImageNet mean/std | Align with training distribution |\n| 4 | `.unsqueeze(0)` | - | Add batch dimension for model input |\n\nSources: [examplemodel/src/inference.py:52-60]()\n\n### Metadata Management\n\n#### `load_stac_item()`\n\nLoads STAC-MLM metadata from JSON files, providing model provenance information for inference.\n\n```python\ndef load_stac_item(stac_path: str) -\u003e Dict[str, Any]\n```\n\nThis simple utility reads STAC metadata files generated during training ([stac_item.json](https://github.com/kshitijrajsharma/opengeoaimodelshub)) and returns them as Python dictionaries for downstream processing.\n\nSources: [examplemodel/src/inference.py:18-20]()\n\n#### `create_inference_metadata()`\n\nGenerates comprehensive metadata for each inference run, capturing input/output specifications, timing information, and preprocessing details.\n\n```python\ndef create_inference_metadata(\n    image_path: str,\n    model_info: Dict[str, Any],\n    inference_time: float,\n    input_shape: Tuple[int, ...],\n    output_shape: Tuple[int, ...]\n) -\u003e Dict[str, Any]\n```\n\n**Metadata Schema:**\n\n```mermaid\nclassDiagram\n    class InferenceMetadata {\n        +string inference_timestamp\n        +string input_image_path\n        +string model_name\n        +string model_version\n        +string model_framework\n        +float inference_time_seconds\n        +list input_shape\n        +list output_shape\n        +dict preprocessing\n        +dict postprocessing\n    }\n    \n    class Preprocessing {\n        +list resize\n        +string normalization\n        +list mean\n        +list std\n    }\n    \n    class Postprocessing {\n        +float threshold\n        +string output_type\n    }\n    \n    InferenceMetadata *-- Preprocessing\n    InferenceMetadata *-- Postprocessing\n```\n\n**Diagram: Inference Metadata Structure**\n\n**Metadata Fields:**\n\n| Category | Field | Type | Description |\n|----------|-------|------|-------------|\n| Temporal | `inference_timestamp` | ISO 8601 string | Inference execution time |\n| Input | `input_image_path` | string | Source image path |\n| Input | `input_shape` | list | Tensor shape [batch, channels, H, W] |\n| Model | `model_name` | string | From STAC metadata or \"unknown\" |\n| Model | `model_version` | string | Model version identifier |\n| Model | `model_framework` | string | ML framework (e.g., \"PyTorch\") |\n| Performance | `inference_time_seconds` | float | Inference duration |\n| Output | `output_shape` | list | Output tensor shape |\n| Preprocessing | `preprocessing` | dict | Transformation details |\n| Postprocessing | `postprocessing` | dict | Threshold and output type |\n\nThe preprocessing dictionary captures the exact transformations applied ([inference.py:39-44]()), while postprocessing documents the threshold (0.5) and output type (\"binary_mask\") used to convert model outputs to final predictions.\n\nSources: [examplemodel/src/inference.py:23-49]()\n\n### Prediction Functions\n\n#### `predict_image_enhanced()`\n\nThe primary inference function that provides a complete prediction pipeline with visualization, metadata generation, and artifact management.\n\n```mermaid\nflowchart TB\n    START[\"predict_image_enhanced()\"]\n    SETUP[\"Create output directory\"]\n    PREPROC[\"preprocess_image()\"]\n    INFERENCE[\"Model forward pass\u003cbr/\u003etorch.no_grad()\"]\n    SIGMOID[\"Apply sigmoid activation\"]\n    THRESHOLD[\"Threshold at 0.5\"]\n    METADATA[\"create_inference_metadata()\"]\n    \n    subgraph \"Artifact Generation\"\n        RAW[\"Save raw prediction\u003cbr/\u003e.npy\"]\n        MASK[\"Save binary mask\u003cbr/\u003e.png\"]\n        OVERLAY[\"Create overlay visualization\u003cbr/\u003ecv2.addWeighted()\"]\n        META[\"Save metadata\u003cbr/\u003e.json\"]\n    end\n    \n    RESULT[\"Return results dict\"]\n    \n    START --\u003e SETUP\n    SETUP --\u003e PREPROC\n    PREPROC --\u003e INFERENCE\n    INFERENCE --\u003e SIGMOID\n    SIGMOID --\u003e THRESHOLD\n    INFERENCE --\u003e METADATA\n    \n    THRESHOLD --\u003e RAW\n    THRESHOLD --\u003e MASK\n    THRESHOLD --\u003e OVERLAY\n    METADATA --\u003e META\n    \n    RAW --\u003e RESULT\n    MASK --\u003e RESULT\n    OVERLAY --\u003e RESULT\n    META --\u003e RESULT\n```\n\n**Diagram: Enhanced Prediction Pipeline**\n\n**Function Signature:**\n\n```python\ndef predict_image_enhanced(\n    model: torch.nn.Module,\n    image_path: str,\n    stac_metadata: Dict[str, Any] = None,\n    output_dir: str = \"output\"\n) -\u003e Dict[str, Any]\n```\n\n**Output Artifacts:**\n\n| Artifact | File Name | Format | Description |\n|----------|-----------|--------|-------------|\n| Raw Prediction | `prediction_raw.npy` | NumPy binary | Continuous probability values [0,1] |\n| Binary Mask | `prediction_mask.png` | PNG image | Thresholded binary mask (0 or 255) |\n| Overlay | `prediction_overlay.png` | PNG image | Original image with red overlay on predicted regions |\n| Metadata | `inference_metadata.json` | JSON | Complete inference metadata |\n\n**Overlay Generation:**\n\nThe overlay visualization is created using OpenCV ([inference.py:101-109]()):\n1. Loads and resizes original image to 256x256\n2. Creates red overlay ([0, 0, 255] in BGR) on predicted regions\n3. Blends original (70%) with overlay (30%) using `cv2.addWeighted()`\n4. Saves result to output directory\n\n**Return Dictionary:**\n\n```python\n{\n    \"prediction\": np.ndarray,           # Raw probability map\n    \"binary_mask\": np.ndarray,          # Binary prediction\n    \"metadata\": dict,                   # Inference metadata\n    \"output_files\": dict,               # Paths to saved artifacts\n    \"refugee_camp_detected\": bool       # Whether any pixels were classified positive\n}\n```\n\nSources: [examplemodel/src/inference.py:63-124]()\n\n#### `predict_image()`\n\nA simplified prediction function for basic inference without metadata or visualization.\n\n```python\ndef predict_image(image_path: str, model_path: str = None) -\u003e np.ndarray\n```\n\nThis function provides a minimal interface that:\n1. Loads model from path or defaults to `meta/best_model.pth`\n2. Preprocesses input image\n3. Performs inference\n4. Returns thresholded binary mask\n\nIt is useful for quick predictions or integration into external systems where only the binary prediction is needed.\n\nSources: [examplemodel/src/inference.py:127-142]()\n\n## MLflow Integration Utilities\n\n### `log_inference_example()`\n\nGenerates and logs example predictions to MLflow during or after training, providing visual verification of model performance.\n\n```mermaid\nsequenceDiagram\n    participant Caller as \"train.py\"\n    participant LogEx as \"log_inference_example()\"\n    participant DM as \"DataModule\"\n    participant Model as \"Model\"\n    participant PLT as \"matplotlib\"\n    participant MLflow as \"MLflow\"\n    \n    Caller-\u003e\u003eLogEx: Call with model, datamodule\n    LogEx-\u003e\u003eDM: setup() + test_dataloader()\n    LogEx-\u003e\u003eModel: Set to eval mode\n    \n    LogEx-\u003e\u003eDM: Get first batch\n    DM--\u003e\u003eLogEx: images, targets\n    \n    LogEx-\u003e\u003eModel: Forward pass (images[:1])\n    Model--\u003e\u003eLogEx: outputs\n    \n    LogEx-\u003e\u003eLogEx: Denormalize input image\n    LogEx-\u003e\u003eLogEx: Apply sigmoid + threshold\n    \n    rect rgb(240,240,240)\n        Note over LogEx,PLT: Generate 3 visualizations\n        LogEx-\u003e\u003ePLT: Create 3-panel figure\u003cbr/\u003eInput | Prediction | Ground Truth\n        LogEx-\u003e\u003ePLT: Save \"meta/example_input.png\"\n        \n        LogEx-\u003e\u003ePLT: Create prediction figure\n        LogEx-\u003e\u003ePLT: Save \"meta/example_pred.png\"\n        \n        LogEx-\u003e\u003ePLT: Create ground truth figure\n        LogEx-\u003e\u003ePLT: Save \"meta/example_target.png\"\n    end\n    \n    LogEx-\u003e\u003eMLflow: log_artifact()  3\n    MLflow--\u003e\u003eCaller: Examples logged\n```\n\n**Diagram: Inference Example Logging Flow**\n\n**Function Signature:**\n\n```python\ndef log_inference_example(model, data_module)\n```\n\n**Implementation Details:**\n\n1. **Data Extraction**: Retrieves first batch from test dataloader ([inference.py:146-156]())\n2. **Device Handling**: Automatically moves tensors to model's device ([inference.py:154-155]())\n3. **Image Denormalization**: Reverses ImageNet normalization for visualization ([inference.py:158-160]())\n4. **Binary Conversion**: Thresholds predictions and targets at 0.5 ([inference.py:166-167]())\n5. **Visualization Generation**: Creates three separate visualizations:\n   - Combined 3-panel figure ([inference.py:169-186]())\n   - Individual prediction ([inference.py:188-193]())\n   - Individual ground truth ([inference.py:195-200]())\n6. **MLflow Logging**: Logs all artifacts to `examples/` path ([inference.py:203-206]())\n\n**Visualization Specifications:**\n\n| Figure | File Name | Panels | DPI | Purpose |\n|--------|-----------|--------|-----|---------|\n| Combined | `example_input.png` | 3 (155) | 300 | Side-by-side comparison |\n| Prediction | `example_pred.png` | 1 (55) | 300 | Isolated model output |\n| Ground Truth | `example_target.png` | 1 (55) | 300 | Isolated reference |\n\nAll visualizations use grayscale colormap with `vmin=0, vmax=1` for consistent binary representation ([inference.py:176-182]()).\n\nSources: [examplemodel/src/inference.py:145-208]()\n\n## Usage Patterns\n\n### Training Pipeline Integration\n\nThe utilities are called from [train.py](https://github.com/kshitijrajsharma/opengeoaimodelshub) after model training completes:\n\n```python\n# After training\nfrom utils import log_confusion_matrix\nfrom inference import log_inference_example\n\nlog_confusion_matrix(model, data_module, run)\nlog_inference_example(model, data_module)\n```\n\nThis provides comprehensive evaluation metrics and visual examples in the MLflow tracking UI.\n\n### Inference Pipeline Integration\n\nThe main inference script ([inference.py:211-250]()) demonstrates command-line usage:\n\n```bash\n# Basic inference\npython inference.py /path/to/image.jpg\n\n# With STAC metadata\npython inference.py /path/to/image.jpg --stac_path meta/stac_item.json\n\n# With MLflow tracking\npython inference.py /path/to/image.jpg --mlflow_tracking --output_dir results/\n```\n\nThe `main()` function orchestrates the utilities ([inference.py:211-247]()):\n1. Parses command-line arguments\n2. Loads STAC metadata if provided\n3. Loads model (JIT or PyTorch checkpoint)\n4. Calls `predict_image_enhanced()`\n5. Optionally logs results to MLflow\n6. Prints detection status\n\n### Direct Function Calls\n\nUtilities can also be imported and called directly from Python scripts:\n\n```python\nfrom examplemodel.src.inference import preprocess_image, predict_image_enhanced\nfrom examplemodel.src.utils import log_confusion_matrix\n\n# Preprocess only\ntensor = preprocess_image(\"input.jpg\")\n\n# Full inference with artifacts\nresults = predict_image_enhanced(model, \"input.jpg\", output_dir=\"results/\")\nprint(f\"Camp detected: {results['refugee_camp_detected']}\")\n```\n\nSources: [examplemodel/src/inference.py:211-250](), [examplemodel/src/utils.py:1-32]()\n\n## Error Handling and Device Management\n\n### Automatic Device Detection\n\nBoth `log_confusion_matrix()` and `log_inference_example()` implement automatic device detection:\n\n```python\ndevice = next(model.parameters()).device\n```\n\nThis ensures utilities work correctly regardless of whether the model is on CPU or GPU, eliminating the need for explicit device management by callers.\n\n### File System Operations\n\nThe utilities create necessary directories and handle file I/O:\n\n- `predict_image_enhanced()` creates output directory if it doesn't exist ([inference.py:70-71]())\n- All artifact generation uses `Path` objects for cross-platform compatibility\n- MLflow artifact logging handles path resolution automatically\n\nSources: [examplemodel/src/utils.py:9-10](), [examplemodel/src/inference.py:150](), [examplemodel/src/inference.py:70-71]()\n\n## Summary\n\nThe utility functions provide essential support infrastructure for the example model system:\n\n- **Evaluation**: `log_confusion_matrix()` computes pixel-wise accuracy metrics\n- **Preprocessing**: `preprocess_image()` standardizes input images\n- **Metadata**: `create_inference_metadata()` and `load_stac_item()` manage provenance\n- **Prediction**: `predict_image_enhanced()` and `predict_image()` generate predictions\n- **Visualization**: `log_inference_example()` creates visual examples for MLflow\n\nThese utilities are designed for modularity and reusability, with consistent interfaces and automatic device management. They integrate seamlessly with MLflow for experiment tracking and support both training and inference workflows.\n\nSources: [examplemodel/src/utils.py:1-32](), [examplemodel/src/inference.py:1-250]()"])</script><script>self.__next_f.push([1,"23:T5d55,"])</script><script>self.__next_f.push([1,"# Infrastructure System\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/Readme.md](infra/Readme.md)\n- [infra/docker-compose.yml](infra/docker-compose.yml)\n- [infra/setup.sh](infra/setup.sh)\n\n\u003c/details\u003e\n\n\n\nThe Infrastructure System provides a production-ready MLOps stack for GeoAI model development and deployment. It consists of a containerized service architecture orchestrated by Docker Compose, including experiment tracking (MLflow), object storage (MinIO), database services (PostgreSQL with PostGIS), reverse proxy with automatic SSL (Traefik), system monitoring (Homepage), and remote access (RustDesk). This page covers the overall infrastructure architecture, service configuration, and operational aspects.\n\nFor detailed information about individual services, see sections [4.1](#4.1) through [4.7](#4.7). For deployment procedures, see [6.1](#6.1). For the ML training pipeline that utilizes this infrastructure, see [3](#3).\n\n## Stack Architecture\n\nThe infrastructure stack is defined in [infra/docker-compose.yml:1-184]() as a collection of seven containerized services orchestrated through Docker Compose. All services communicate through a shared `traefik-network` bridge network, with Traefik acting as the edge ingress controller.\n\n```mermaid\ngraph TB\n    subgraph \"Edge Layer\"\n        traefik[\"traefik\u003cbr/\u003eContainer: traefik\u003cbr/\u003eImage: traefik:v3.0\"]\n    end\n    \n    subgraph \"Application Services\"\n        homepage[\"homepage\u003cbr/\u003eContainer: homepage\u003cbr/\u003eImage: ghcr.io/gethomepage/homepage:latest\"]\n        mlflow[\"mlflow\u003cbr/\u003eContainer: mlflow\u003cbr/\u003eImage: ghcr.io/.../mlflow:latest\"]\n    end\n    \n    subgraph \"Storage Services\"\n        minio[\"minio\u003cbr/\u003eContainer: minio\u003cbr/\u003eImage: minio/minio:RELEASE.2025-04-22T22-12-26Z\"]\n        postgres[\"postgres\u003cbr/\u003eContainer: postgres\u003cbr/\u003eImage: postgis/postgis:16-3.4-alpine\"]\n    end\n    \n    subgraph \"Remote Access Services\"\n        hbbs[\"hbbs\u003cbr/\u003eContainer: hbbs\u003cbr/\u003eImage: rustdesk/rustdesk-server:latest\"]\n        hbbr[\"hbbr\u003cbr/\u003eContainer: hbbr\u003cbr/\u003eImage: rustdesk/rustdesk-server:latest\"]\n    end\n    \n    traefik --\u003e|\"routes to\"| homepage\n    traefik --\u003e|\"routes to\"| mlflow\n    traefik --\u003e|\"routes to\"| minio\n    traefik --\u003e|\"routes to\"| postgres\n    traefik --\u003e|\"routes to\"| hbbs\n    \n    mlflow --\u003e|\"backend-store-uri\"| postgres\n    mlflow --\u003e|\"default-artifact-root\"| minio\n    \n    homepage -.-\u003e|\"monitors via\u003cbr/\u003eDocker socket\"| mlflow\n    homepage -.-\u003e|\"monitors via\u003cbr/\u003eDocker socket\"| minio\n    homepage -.-\u003e|\"monitors via\u003cbr/\u003eDocker socket\"| postgres\n    \n    hbbs --\u003e|\"depends_on\"| hbbr\n```\n\n**Service Container Mapping**\n\n| Service | Container Name | Image | Primary Port(s) |\n|---------|----------------|-------|-----------------|\n| Traefik | `traefik` | `traefik:v3.0` | 80, 443, 8080 |\n| Homepage | `homepage` | `ghcr.io/gethomepage/homepage:latest` | 3000 |\n| MLflow | `mlflow` | `${MLFLOW_IMAGE}` | 5000 |\n| MinIO | `minio` | `minio/minio:RELEASE.2025-04-22T22-12-26Z` | 9000, 9001 |\n| PostgreSQL | `postgres` | `postgis/postgis:16-3.4-alpine` | 5432 |\n| RustDesk (hbbs) | `hbbs` | `rustdesk/rustdesk-server:latest` | 21115, 21116, 21118 |\n| RustDesk (hbbr) | `hbbr` | `rustdesk/rustdesk-server:latest` | 21117, 21119 |\n\nSources: [infra/docker-compose.yml:1-184]()\n\n## Service Configuration\n\nEach service is configured through Docker Compose service definitions with three primary configuration mechanisms: environment variables, command-line arguments, and Traefik labels.\n\n### MLflow Service Configuration\n\nThe `mlflow` service [infra/docker-compose.yml:62-84]() is configured to use PostgreSQL as its backend store and MinIO as its artifact store:\n\n```mermaid\ngraph LR\n    mlflow_entrypoint[\"mlflow server\u003cbr/\u003e--backend-store-uri\u003cbr/\u003e--default-artifact-root\u003cbr/\u003e--artifacts-destination\"]\n    \n    env_vars[\"Environment Variables\u003cbr/\u003eMLFLOW_S3_ENDPOINT_URL\u003cbr/\u003eAWS_ACCESS_KEY_ID\u003cbr/\u003eAWS_SECRET_ACCESS_KEY\"]\n    \n    postgres_conn[\"postgresql+psycopg2://\u003cbr/\u003e${POSTGRES_USER}:${POSTGRES_PASSWORD}\u003cbr/\u003e@postgres/${POSTGRES_DB}\"]\n    \n    minio_conn[\"s3://${MINIO_BUCKET_NAME}/\"]\n    \n    env_vars --\u003e|\"configures S3 access\"| mlflow_entrypoint\n    postgres_conn --\u003e|\"backend-store-uri\"| mlflow_entrypoint\n    minio_conn --\u003e|\"artifact storage\"| mlflow_entrypoint\n```\n\nThe entrypoint command [infra/docker-compose.yml:71]() constructs the full MLflow server invocation:\n\n```\nmlflow server \n  --backend-store-uri postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB} \n  --default-artifact-root s3://${MINIO_BUCKET_NAME}/ \n  --artifacts-destination s3://${MINIO_BUCKET_NAME}/ \n  -h 0.0.0.0\n```\n\nSources: [infra/docker-compose.yml:62-84]()\n\n### Traefik Routing Configuration\n\nTraefik routing is configured through Docker labels attached to each service. The routing follows a consistent pattern of subdomain-based host matching:\n\n```mermaid\ngraph TB\n    entrypoint_web[\"entrypoint: web\u003cbr/\u003e:80\"]\n    entrypoint_websecure[\"entrypoint: websecure\u003cbr/\u003e:443\"]\n    entrypoint_postgres[\"entrypoint: postgres\u003cbr/\u003e:5432\"]\n    \n    redirect[\"HTTP  HTTPS\u003cbr/\u003eRedirect Middleware\"]\n    \n    letsencrypt[\"certificatesresolvers.letsencrypt\u003cbr/\u003eACME HTTP Challenge\"]\n    \n    router_homepage[\"traefik.http.routers.homepage\u003cbr/\u003eHost(`${DOMAIN}`) || Host(`www.${DOMAIN}`)\"]\n    router_mlflow[\"traefik.http.routers.mlflow\u003cbr/\u003eHost(`mlflow.${DOMAIN}`)\"]\n    router_minio_api[\"traefik.http.routers.minio-api\u003cbr/\u003eHost(`minio-api.${DOMAIN}`)\"]\n    router_minio_console[\"traefik.http.routers.minio-console\u003cbr/\u003eHost(`minio.${DOMAIN}`)\"]\n    router_traefik[\"traefik.http.routers.traefik\u003cbr/\u003eHost(`traefik.${DOMAIN}`)\"]\n    \n    tcp_router_postgres[\"traefik.tcp.routers.postgres\u003cbr/\u003eHostSNI(`postgres.${DOMAIN}`)\"]\n    \n    entrypoint_web --\u003e|\"redirections.entrypoint\"| redirect\n    redirect --\u003e entrypoint_websecure\n    \n    entrypoint_websecure --\u003e|\"tls.certresolver\"| letsencrypt\n    \n    entrypoint_websecure --\u003e router_homepage\n    entrypoint_websecure --\u003e router_mlflow\n    entrypoint_websecure --\u003e router_minio_api\n    entrypoint_websecure --\u003e router_minio_console\n    entrypoint_websecure --\u003e router_traefik\n    \n    entrypoint_postgres --\u003e tcp_router_postgres\n    \n    router_homepage --\u003e|\"service\"| svc_homepage[\"homepage:3000\"]\n    router_mlflow --\u003e|\"service\"| svc_mlflow[\"mlflow:5000\"]\n    router_minio_api --\u003e|\"service\"| svc_minio_api[\"minio:9000\"]\n    router_minio_console --\u003e|\"service\"| svc_minio_console[\"minio:9001\"]\n```\n\nThe Traefik service itself is configured with command-line flags [infra/docker-compose.yml:13-28]() that define:\n\n- API dashboard accessibility [infra/docker-compose.yml:14-15]()\n- Docker provider for service discovery [infra/docker-compose.yml:16-17]()\n- Three entry points: `web` (80), `websecure` (443), `postgres` (5432) [infra/docker-compose.yml:18-20]()\n- Automatic HTTP to HTTPS redirect [infra/docker-compose.yml:21-22]()\n- Let's Encrypt certificate resolver [infra/docker-compose.yml:23-26]()\n\nSources: [infra/docker-compose.yml:2-38](), [infra/docker-compose.yml:52-60](), [infra/docker-compose.yml:75-81](), [infra/docker-compose.yml:97-110](), [infra/docker-compose.yml:126-133]()\n\n## Network Architecture\n\nThe stack uses a single Docker bridge network named `traefik-network` [infra/docker-compose.yml:177-180]() for inter-service communication. All seven services connect to this network, enabling DNS-based service discovery where each service is reachable by its container name.\n\n```mermaid\ngraph TB\n    subgraph \"traefik-network (bridge driver)\"\n        direction TB\n        \n        traefik_container[\"traefik\u003cbr/\u003eInternal: traefik:80, traefik:443\"]\n        homepage_container[\"homepage\u003cbr/\u003eInternal: homepage:3000\"]\n        mlflow_container[\"mlflow\u003cbr/\u003eInternal: mlflow:5000\"]\n        minio_container[\"minio\u003cbr/\u003eInternal: minio:9000, minio:9001\"]\n        postgres_container[\"postgres\u003cbr/\u003eInternal: postgres:5432\"]\n        hbbs_container[\"hbbs\u003cbr/\u003eInternal: hbbs:21115-21118\"]\n        hbbr_container[\"hbbr\u003cbr/\u003eInternal: hbbr:21117, hbbr:21119\"]\n    end\n    \n    external[\"External Network\u003cbr/\u003eInternet\"]\n    \n    external --\u003e|\"80, 443, 8080\"| traefik_container\n    external --\u003e|\"21115-21119\"| hbbs_container\n    external --\u003e|\"21117, 21119\"| hbbr_container\n    \n    traefik_container -.-\u003e|\"reverse proxy\"| homepage_container\n    traefik_container -.-\u003e|\"reverse proxy\"| mlflow_container\n    traefik_container -.-\u003e|\"reverse proxy\"| minio_container\n    traefik_container -.-\u003e|\"reverse proxy\"| postgres_container\n    traefik_container -.-\u003e|\"reverse proxy\"| hbbs_container\n    \n    mlflow_container --\u003e|\"psycopg2 connection\"| postgres_container\n    mlflow_container --\u003e|\"S3 API calls\"| minio_container\n    \n    homepage_container -.-\u003e|\"Docker socket monitoring\"| traefik_container\n```\n\n**Port Mappings**\n\n| Service | Published Ports | Internal Ports | Protocol |\n|---------|----------------|----------------|----------|\n| traefik | 80, 443, 8080 | 80, 443, 8080 | HTTP/HTTPS |\n| homepage | - | 3000 | HTTP (internal only) |\n| mlflow | - | 5000 | HTTP (internal only) |\n| minio | - | 9000, 9001 | HTTP (internal only) |\n| postgres | - | 5432 | PostgreSQL (internal only) |\n| hbbs | 21115-21118 (21116 UDP) | 21115-21118 | RustDesk protocol |\n| hbbr | 21117, 21119 | 21117, 21119 | RustDesk protocol |\n\nServices that don't have published ports are only accessible through Traefik's reverse proxy, providing an additional security layer.\n\nSources: [infra/docker-compose.yml:6-9](), [infra/docker-compose.yml:147-150](), [infra/docker-compose.yml:171-173](), [infra/docker-compose.yml:177-180]()\n\n## Data Persistence\n\nThe stack implements persistent storage through Docker volumes mounted to host directories. Five primary data volumes are configured:\n\n```mermaid\ngraph LR\n    subgraph \"Host Filesystem\"\n        traefik_vol[\"${TRAEFIK_DATA_DIR}\u003cbr/\u003e./volumes/traefik-data\u003cbr/\u003eacme.json (600)\"]\n        minio_vol[\"${MINIO_DATA_DIR}\u003cbr/\u003e./volumes/minio\"]\n        postgres_vol[\"${POSTGRES_DATA_DIR}\u003cbr/\u003e./volumes/postgres\"]\n        rustdesk_vol[\"${RUSTDESK_DATA_DIR}\u003cbr/\u003e./volumes/rustdesk\"]\n        homepage_vol[\"${HOMEPAGE_CONFIG}\u003cbr/\u003e./homepage-config\"]\n    end\n    \n    subgraph \"Container Mounts\"\n        traefik_mount[\"/data\"]\n        minio_mount[\"/data/minio\"]\n        postgres_mount[\"/var/lib/postgresql/data\"]\n        rustdesk_mount[\"/root\"]\n        homepage_mount[\"/app/config\"]\n    end\n    \n    traefik_vol --\u003e traefik_mount\n    minio_vol --\u003e minio_mount\n    postgres_vol --\u003e postgres_mount\n    rustdesk_vol --\u003e rustdesk_mount\n    homepage_vol --\u003e homepage_mount\n```\n\n**Volume Configuration**\n\n| Service | Environment Variable | Default Path | Container Mount | Purpose |\n|---------|---------------------|--------------|-----------------|---------|\n| Traefik | `TRAEFIK_DATA_DIR` | `./volumes/traefik-data` | `/data` | SSL certificates (acme.json) |\n| MinIO | `MINIO_DATA_DIR` | `./volumes/minio` | `/data/minio` | S3 object storage |\n| PostgreSQL | `POSTGRES_DATA_DIR` | `./volumes/postgres` | `/var/lib/postgresql/data` | Database files |\n| RustDesk | `RUSTDESK_DATA_DIR` | `./volumes/rustdesk` | `/root` | RustDesk server data |\n| Homepage | `HOMEPAGE_CONFIG` | `./homepage-config` | `/app/config` | Dashboard configuration |\n\nThe Traefik ACME certificate file requires strict permissions [infra/setup.sh:120-121]():\n\n```bash\ntouch volumes/traefik-data/acme.json\nchmod 600 volumes/traefik-data/acme.json\n```\n\nSources: [infra/docker-compose.yml:12](), [infra/docker-compose.yml:50](), [infra/docker-compose.yml:95](), [infra/docker-compose.yml:125](), [infra/docker-compose.yml:146](), [infra/docker-compose.yml:170](), [infra/setup.sh:116-122]()\n\n## Automated Setup Process\n\nThe setup process is automated through [infra/setup.sh:1-254](), which performs initialization, credential generation, and service deployment. The script implements a multi-stage setup workflow:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant setup_sh as \"setup.sh\"\n    participant docker as \"Docker\"\n    participant env_file as \".env File\"\n    participant systemd\n    \n    User-\u003e\u003esetup_sh: \"./setup.sh\"\n    \n    setup_sh-\u003e\u003esetup_sh: \"Validate prerequisites\u003cbr/\u003edocker, docker compose, openssl\"\n    \n    alt .env does not exist\n        setup_sh-\u003e\u003eenv_file: \"cp .env.template .env\"\n        setup_sh-\u003e\u003esetup_sh: \"generate_password(20)\u003cbr/\u003ePOSTGRES_PASSWORD\"\n        setup_sh-\u003e\u003esetup_sh: \"generate_key(20)\u003cbr/\u003eAWS_ACCESS_KEY\"\n        setup_sh-\u003e\u003esetup_sh: \"generate_key(40)\u003cbr/\u003eAWS_SECRET_KEY\"\n        setup_sh-\u003e\u003esetup_sh: \"generate_key(16)\u003cbr/\u003eRUSTDESK_KEY\"\n        setup_sh-\u003e\u003esetup_sh: \"generate_password(16)\u003cbr/\u003eTRAEFIK_PASSWORD\"\n        setup_sh-\u003e\u003edocker: \"htpasswd -nbB\u003cbr/\u003eGenerate hash\"\n        docker--\u003e\u003esetup_sh: \"TRAEFIK_HASH\"\n        setup_sh-\u003e\u003eenv_file: \"sed -i replacements\"\n        setup_sh--\u003e\u003eUser: \"Display credentials\u003cbr/\u003eEXIT: Update DOMAIN and ACME_EMAIL\"\n    end\n    \n    setup_sh-\u003e\u003eenv_file: \"source .env\"\n    setup_sh-\u003e\u003esetup_sh: \"Validate DOMAIN != example.com\"\n    \n    setup_sh-\u003e\u003esetup_sh: \"mkdir -p volumes/{...}\"\n    setup_sh-\u003e\u003esetup_sh: \"chmod 600 acme.json\"\n    \n    setup_sh-\u003e\u003edocker: \"docker compose down\"\n    setup_sh-\u003e\u003edocker: \"docker compose pull\"\n    setup_sh-\u003e\u003edocker: \"docker compose up -d\"\n    \n    setup_sh-\u003e\u003esetup_sh: \"sleep 30\"\n    setup_sh-\u003e\u003edocker: \"docker compose ps\"\n    \n    setup_sh-\u003e\u003esystemd: \"Create tech-infra.service\"\n    setup_sh-\u003e\u003esystemd: \"systemctl enable tech-infra\"\n    \n    setup_sh-\u003e\u003esetup_sh: \"Create manage.sh script\"\n    \n    setup_sh--\u003e\u003eUser: \"Display service URLs\u003cbr/\u003eDisplay credentials\"\n```\n\n### Credential Generation\n\nThe script generates secure credentials using two helper functions [infra/setup.sh:11-20]():\n\n- `generate_password()`: Creates base64-encoded random strings [infra/setup.sh:11-14]()\n- `generate_key()`: Creates hex-encoded random strings [infra/setup.sh:17-20]()\n\nGenerated credentials:\n- `POSTGRES_PASSWORD`: 20-character password [infra/setup.sh:55]()\n- `AWS_ACCESS_KEY`: 20-character hex key [infra/setup.sh:56]()\n- `AWS_SECRET_KEY`: 40-character hex key [infra/setup.sh:57]()\n- `RUSTDESK_KEY`: 16-character hex key [infra/setup.sh:58]()\n- `TRAEFIK_PASSWORD`: 16-character password [infra/setup.sh:59]()\n- `TRAEFIK_HASH`: bcrypt hash generated via `htpasswd` [infra/setup.sh:62]()\n\nThe Traefik password hash generation doubles dollar signs for Docker Compose compatibility [infra/setup.sh:62]():\n\n```bash\nTRAEFIK_HASH=$(docker run --rm httpd:2.4-alpine htpasswd -nbB admin \"$TRAEFIK_PASSWORD\" 2\u003e/dev/null | cut -d \":\" -f 2 | sed 's/\\$/\\$\\$/g')\n```\n\nSources: [infra/setup.sh:1-254]()\n\n## Service Management\n\nThe setup script generates a management utility `manage.sh` [infra/setup.sh:164-208]() that provides operational commands:\n\n**Management Commands**\n\n| Command | Description | Example |\n|---------|-------------|---------|\n| `start` | Start all services | `./manage.sh start` |\n| `stop` | Stop all services | `./manage.sh stop` |\n| `restart` | Restart service(s) | `./manage.sh restart mlflow` |\n| `logs` | View service logs | `./manage.sh logs postgres` |\n| `status` | Check service status | `./manage.sh status` |\n| `update` | Pull and restart with latest images | `./manage.sh update` |\n| `backup` | Create full backup | `./manage.sh backup` |\n\nThe `backup` command [infra/setup.sh:193-199]() creates timestamped backups including:\n- All volume data copied to `./backups/YYYYMMDD_HHMMSS/volumes/`\n- PostgreSQL database dump via `pg_dump` saved to `postgres_dump.sql`\n\n### Systemd Integration\n\nThe setup script creates a systemd service unit `tech-infra.service` [infra/setup.sh:141-159]() for automatic startup:\n\n```\n[Unit]\nDescription=Tech Infrastructure Services\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nUser=$USER\nGroup=$USER\nWorkingDirectory=$(pwd)\nExecStart=/usr/bin/docker compose up -d\nExecStop=/usr/bin/docker compose down\nTimeoutStartSec=0\n\n[Install]\nWantedBy=multi-user.target\n```\n\nThis enables system-level service management:\n\n```bash\nsudo systemctl start tech-infra\nsudo systemctl stop tech-infra\nsudo systemctl status tech-infra\n```\n\nSources: [infra/setup.sh:140-162](), [infra/setup.sh:164-208]()\n\n## Security Configuration\n\nThe infrastructure implements multiple security layers:\n\n### Authentication Mechanisms\n\n```mermaid\ngraph TB\n    public[\"Public Access\u003cbr/\u003ePort 80, 443\"]\n    \n    traefik_auth[\"Traefik Dashboard\u003cbr/\u003eBasic Auth Middleware\"]\n    \n    services[\"Protected Services\"]\n    \n    postgres_tls[\"PostgreSQL\u003cbr/\u003eTLS Required\"]\n    minio_creds[\"MinIO\u003cbr/\u003eAccess Key / Secret Key\"]\n    mlflow_s3[\"MLflow\u003cbr/\u003eS3 Credentials\"]\n    \n    public --\u003e|\"HTTPS with\u003cbr/\u003eLet's Encrypt\"| traefik_auth\n    \n    traefik_auth --\u003e|\"TRAEFIK_AUTH_USER\u003cbr/\u003eTRAEFIK_AUTH_PASSWORD_HASH\"| services\n    \n    services --\u003e postgres_tls\n    services --\u003e minio_creds\n    services --\u003e mlflow_s3\n    \n    postgres_tls -.-\u003e|\"POSTGRES_USER\u003cbr/\u003ePOSTGRES_PASSWORD\"| auth_vars[\"Environment Variables\"]\n    minio_creds -.-\u003e|\"MINIO_ROOT_USER\u003cbr/\u003eMINIO_ROOT_PASSWORD\"| auth_vars\n    mlflow_s3 -.-\u003e|\"AWS_ACCESS_KEY_ID\u003cbr/\u003eAWS_SECRET_ACCESS_KEY\"| auth_vars\n```\n\n**Authentication Methods by Service**\n\n| Service | Method | Configuration |\n|---------|--------|---------------|\n| Traefik Dashboard | HTTP Basic Auth | `traefik.http.middlewares.auth.basicauth.users` [infra/docker-compose.yml:36]() |\n| PostgreSQL | Password + TLS | `POSTGRES_USER`, `POSTGRES_PASSWORD` [infra/docker-compose.yml:120-122]() |\n| MinIO | Access Key/Secret | `MINIO_ROOT_USER`, `MINIO_ROOT_PASSWORD` [infra/docker-compose.yml:91-92]() |\n| MLflow | Inherits PostgreSQL + MinIO | Environment variables [infra/docker-compose.yml:66-70]() |\n\n### SSL Certificate Management\n\nTraefik automatically obtains and renews SSL certificates through Let's Encrypt ACME protocol [infra/docker-compose.yml:23-26]():\n\n- Challenge type: HTTP-01 [infra/docker-compose.yml:25-26]()\n- Storage: `/data/acme.json` (must be 600 permissions) [infra/docker-compose.yml:24]()\n- Email: `${ACME_EMAIL}` for renewal notifications [infra/docker-compose.yml:23]()\n\n### Network Isolation\n\nServices without published ports are only accessible through Traefik's reverse proxy, preventing direct external access. The Docker socket is mounted read-only [infra/docker-compose.yml:11]() to prevent container privilege escalation.\n\nSources: [infra/docker-compose.yml:23-36](), [infra/docker-compose.yml:91-92](), [infra/docker-compose.yml:120-122](), [infra/setup.sh:59-62]()\n\n## Environment Configuration\n\nThe infrastructure is configured through environment variables defined in `.env` file, templated from [infra/.env.template](). Key configuration categories:\n\n**Core Configuration Variables**\n\n| Variable | Purpose | Example | Used By |\n|----------|---------|---------|---------|\n| `DOMAIN` | Base domain for all services | `example.com` | All services |\n| `ACME_EMAIL` | Let's Encrypt notifications | `admin@example.com` | Traefik |\n| `MLFLOW_IMAGE` | MLflow Docker image | `ghcr.io/.../mlflow:latest` | MLflow service |\n\n**Credential Variables**\n\n| Variable | Purpose | Generated By |\n|----------|---------|--------------|\n| `POSTGRES_USER` | PostgreSQL username | Template default |\n| `POSTGRES_PASSWORD` | PostgreSQL password | `generate_password(20)` |\n| `POSTGRES_DB` | PostgreSQL database name | Template default |\n| `AWS_ACCESS_KEY_ID` | MinIO/S3 access key | `generate_key(20)` |\n| `AWS_SECRET_ACCESS_KEY` | MinIO/S3 secret key | `generate_key(40)` |\n| `TRAEFIK_AUTH_USER` | Traefik dashboard user | Template default |\n| `TRAEFIK_AUTH_PASSWORD` | Traefik dashboard password | `generate_password(16)` |\n| `TRAEFIK_AUTH_PASSWORD_HASH` | Bcrypt hash for auth | `htpasswd -nbB` |\n| `RUSTDESK_KEY` | RustDesk encryption key | `generate_key(16)` |\n| `MINIO_BUCKET_NAME` | S3 bucket for artifacts | Template default |\n\n**Volume Path Variables**\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `TRAEFIK_DATA_DIR` | Traefik data directory | `./volumes/traefik-data` |\n| `MINIO_DATA_DIR` | MinIO data directory | `./volumes/minio` |\n| `POSTGRES_DATA_DIR` | PostgreSQL data directory | `./volumes/postgres` |\n| `RUSTDESK_DATA_DIR` | RustDesk data directory | `./volumes/rustdesk` |\n| `HOMEPAGE_CONFIG` | Homepage config directory | `./homepage-config` |\n\n**Homepage Configuration Variables**\n\n| Variable | Purpose |\n|----------|---------|\n| `HOMEPAGE_ALLOWED_HOSTS` | Allowed domains for homepage |\n| `PUID` | User ID for file permissions |\n| `PGID` | Group ID for file permissions |\n| `TZ` | Timezone for services |\n\nThe setup script validates that `DOMAIN` and `ACME_EMAIL` are changed from their template defaults [infra/setup.sh:107-111]() before proceeding with deployment.\n\nSources: [infra/setup.sh:47-94](), [infra/setup.sh:104-111](), [infra/docker-compose.yml:23](), [infra/docker-compose.yml:44-48](), [infra/docker-compose.yml:66-70](), [infra/docker-compose.yml:90-93](), [infra/docker-compose.yml:119-123]()\n\n## Service Dependency Chain\n\nThe infrastructure services have defined startup dependencies managed through Docker Compose `depends_on` directives:\n\n```mermaid\ngraph TD\n    postgres[\"postgres\u003cbr/\u003ePostgreSQL+PostGIS\u003cbr/\u003eNo dependencies\"]\n    minio[\"minio\u003cbr/\u003eMinIO Object Storage\u003cbr/\u003eNo dependencies\"]\n    \n    mlflow[\"mlflow\u003cbr/\u003eMLflow Tracking Server\u003cbr/\u003edepends_on: minio, postgres\"]\n    \n    hbbr[\"hbbr\u003cbr/\u003eRustDesk Relay\u003cbr/\u003eNo dependencies\"]\n    hbbs[\"hbbs\u003cbr/\u003eRustDesk Server\u003cbr/\u003edepends_on: hbbr\"]\n    \n    traefik[\"traefik\u003cbr/\u003eReverse Proxy\u003cbr/\u003eNo dependencies\"]\n    homepage[\"homepage\u003cbr/\u003eDashboard\u003cbr/\u003eNo dependencies\"]\n    \n    minio --\u003e mlflow\n    postgres --\u003e mlflow\n    hbbr --\u003e hbbs\n    \n    traefik -.-\u003e|\"routes all services\"| mlflow\n    traefik -.-\u003e|\"routes all services\"| minio\n    traefik -.-\u003e|\"routes all services\"| postgres\n    traefik -.-\u003e|\"routes all services\"| homepage\n    traefik -.-\u003e|\"routes all services\"| hbbs\n```\n\nThe MLflow service explicitly depends on both storage services [infra/docker-compose.yml:72-74](), ensuring they are started before MLflow attempts to connect. The RustDesk `hbbs` server depends on the `hbbr` relay [infra/docker-compose.yml:152-153](). All services are configured with `restart: unless-stopped` [infra/docker-compose.yml:5]() to ensure automatic recovery from failures.\n\nSources: [infra/docker-compose.yml:5](), [infra/docker-compose.yml:43](), [infra/docker-compose.yml:65](), [infra/docker-compose.yml:72-74](), [infra/docker-compose.yml:89](), [infra/docker-compose.yml:118](), [infra/docker-compose.yml:141](), [infra/docker-compose.yml:152-153](), [infra/docker-compose.yml:167]()\n\n## Service URLs and Access Points\n\nAfter deployment, services are accessible through subdomain-based routing:\n\n**Public Service Endpoints**\n\n| Service | URL Pattern | Port | Purpose |\n|---------|------------|------|---------|\n| Homepage Dashboard | `https://${DOMAIN}` or `https://www.${DOMAIN}` | 443 | System monitoring and service overview |\n| MLflow Tracking | `https://mlflow.${DOMAIN}` | 443 | Experiment tracking UI and API |\n| MinIO Console | `https://minio.${DOMAIN}` | 443 | Object storage web console |\n| MinIO API | `https://minio-api.${DOMAIN}` | 443 | S3-compatible API endpoint |\n| Traefik Dashboard | `https://traefik.${DOMAIN}` | 443 | Reverse proxy monitoring (auth required) |\n| PostgreSQL | `postgres.${DOMAIN}:5432` | 5432 (TCP) | Database connections (TLS required) |\n| RustDesk Web | `https://rustdesk.${DOMAIN}` | 443 | Remote desktop web interface |\n| RustDesk Relay | `rustdesk.${DOMAIN}:21115-21119` | 21115-21119 | Direct relay connections |\n\nThe setup script displays all access URLs and credentials upon completion [infra/setup.sh:212-253]().\n\nSources: [infra/docker-compose.yml:31](), [infra/docker-compose.yml:54](), [infra/docker-compose.yml:77](), [infra/docker-compose.yml:99-105](), [infra/docker-compose.yml:128](), [infra/docker-compose.yml:156](), [infra/setup.sh:212-253]()"])</script><script>self.__next_f.push([1,"24:T402d,"])</script><script>self.__next_f.push([1,"# Service Architecture\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/Readme.md](infra/Readme.md)\n- [infra/docker-compose.yml](infra/docker-compose.yml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document details the architecture of the infrastructure stack defined in [infra/docker-compose.yml](). It covers the composition of services, their dependencies, networking configuration, volume management, and routing mechanisms. For deployment procedures and the automated setup process, see [Setup and Deployment](#4.2). For individual service configuration details, see [Configuration Management](#4.3).\n\nThe infrastructure stack provides a production-ready MLOps platform consisting of seven containerized services orchestrated through Docker Compose. The architecture implements industry-standard patterns including edge proxy routing, centralized authentication, persistent storage abstraction, and service mesh networking.\n\n## Service Inventory\n\nThe docker-compose stack defines seven services across three functional categories:\n\n| Service | Container Name | Image | Primary Port(s) | Purpose |\n|---------|---------------|-------|-----------------|---------|\n| `traefik` | `traefik` | `traefik:v3.0` | 80, 443, 8080 | Edge router and SSL termination |\n| `homepage` | `homepage` | `ghcr.io/gethomepage/homepage:latest` | 3000 | Dashboard and service monitoring |\n| `mlflow` | `mlflow` | `ghcr.io/kshitijrajsharma/opengeoaimodelshub/mlflow:latest` | 5000 | ML experiment tracking server |\n| `minio` | `minio` | `minio/minio:RELEASE.2025-04-22T22-12-26Z` | 9000, 9001 | S3-compatible object storage |\n| `postgres` | `postgres` | `postgis/postgis:16-3.4-alpine` | 5432 | PostgreSQL database with PostGIS |\n| `hbbs` | `hbbs` | `rustdesk/rustdesk-server:latest` | 21115, 21116, 21118 | RustDesk ID/Rendezvous server |\n| `hbbr` | `hbbr` | `rustdesk/rustdesk-server:latest` | 21117, 21119 | RustDesk Relay server |\n\nSources: [infra/docker-compose.yml:1-176]()\n\n## Service Dependency Architecture\n\nThe following diagram illustrates the startup order and runtime dependencies between services:\n\n```mermaid\ngraph TB\n    traefik[\"traefik\u003cbr/\u003e(Edge Router)\"]\n    homepage[\"homepage\u003cbr/\u003e(Dashboard)\"]\n    mlflow[\"mlflow\u003cbr/\u003e(ML Tracking)\"]\n    minio[\"minio\u003cbr/\u003e(Object Storage)\"]\n    postgres[\"postgres\u003cbr/\u003e(Database)\"]\n    hbbs[\"hbbs\u003cbr/\u003e(RustDesk ID Server)\"]\n    hbbr[\"hbbr\u003cbr/\u003e(RustDesk Relay)\"]\n    \n    mlflow --\u003e|depends_on| minio\n    mlflow --\u003e|depends_on| postgres\n    hbbs --\u003e|depends_on| hbbr\n    \n    traefik -.-\u003e|routes to| homepage\n    traefik -.-\u003e|routes to| mlflow\n    traefik -.-\u003e|routes to| minio\n    traefik -.-\u003e|routes to| postgres\n    traefik -.-\u003e|routes to| hbbs\n    \n    homepage -.-\u003e|monitors via\u003cbr/\u003edocker.sock| mlflow\n    homepage -.-\u003e|monitors via\u003cbr/\u003edocker.sock| minio\n    homepage -.-\u003e|monitors via\u003cbr/\u003edocker.sock| postgres\n    \n    mlflow --\u003e|stores metadata| postgres\n    mlflow --\u003e|stores artifacts| minio\n```\n\nThe `mlflow` service has hard dependencies on both `minio` and `postgres` [infra/docker-compose.yml:72-74](), ensuring the storage backend and metadata database are available before MLflow starts. The `hbbs` service depends on `hbbr` [infra/docker-compose.yml:152-153]() for relay functionality. All other services start independently with no explicit dependency constraints.\n\nSources: [infra/docker-compose.yml:72-74](), [infra/docker-compose.yml:152-153]()\n\n## Network Architecture\n\nAll services connect to a single bridge network named `traefik-network` [infra/docker-compose.yml:177-180]():\n\n```mermaid\ngraph LR\n    subgraph \"traefik-network (bridge)\"\n        traefik_c[\"traefik:80,443,8080\"]\n        homepage_c[\"homepage:3000\"]\n        mlflow_c[\"mlflow:5000\"]\n        minio_c[\"minio:9000,9001\"]\n        postgres_c[\"postgres:5432\"]\n        hbbs_c[\"hbbs:21115,21116,21118\"]\n        hbbr_c[\"hbbr:21117,21119\"]\n    end\n    \n    internet[\"Public Internet\"] --\u003e|\":80,:443\"| traefik_c\n    \n    traefik_c --\u003e|\"internal routing\"| homepage_c\n    traefik_c --\u003e|\"internal routing\"| mlflow_c\n    traefik_c --\u003e|\"internal routing\"| minio_c\n    traefik_c --\u003e|\"internal routing\"| postgres_c\n    traefik_c --\u003e|\"internal routing\"| hbbs_c\n    \n    mlflow_c --\u003e|\"http://minio:9000\"| minio_c\n    mlflow_c --\u003e|\"postgresql://postgres:5432\"| postgres_c\n    \n    hbbs_c --\u003e|\"relay coordination\"| hbbr_c\n```\n\nThe commented-out `mlflow-network` [infra/docker-compose.yml:181-183]() suggests a previous multi-network design that has been simplified to a single network topology. All inter-service communication uses container names as DNS hostnames (e.g., `http://minio:9000`).\n\n### Port Exposure Strategy\n\nOnly `traefik` and the RustDesk services expose ports to the host:\n\n- **traefik**: Exposes `:80`, `:443`, and `:8080` [infra/docker-compose.yml:6-9]() for HTTP/HTTPS traffic and dashboard access\n- **hbbs**: Exposes `:21115`, `:21116` (TCP/UDP), `:21118` [infra/docker-compose.yml:147-151]() for RustDesk client connections\n- **hbbr**: Exposes `:21117`, `:21119` [infra/docker-compose.yml:171-173]() for relay functionality\n\nAll other services remain internal to the Docker network and are accessed exclusively through Traefik routing.\n\nSources: [infra/docker-compose.yml:177-183](), [infra/docker-compose.yml:6-9](), [infra/docker-compose.yml:147-151](), [infra/docker-compose.yml:171-173]()\n\n## Volume Management\n\nThe stack persists data using five Docker volumes with configurable mount paths:\n\n| Service | Volume Type | Host Path (Default) | Container Path | Purpose |\n|---------|-------------|---------------------|----------------|---------|\n| `traefik` | bind mount | `./volumes/traefik-data` | `/data` | ACME certificates, configuration |\n| `traefik` | bind mount | `/var/run/docker.sock` | `/var/run/docker.sock` (ro) | Docker API access for dynamic routing |\n| `homepage` | bind mount | `./homepage-config` | `/app/config` | Dashboard configuration files |\n| `homepage` | bind mount | `/var/run/docker.sock` | `/var/run/docker.sock` (ro) | Docker API access for monitoring |\n| `minio` | bind mount | `./volumes/minio` | `/data/minio` | Object storage data |\n| `postgres` | bind mount | `./volumes/postgres` | `/var/lib/postgresql/data` | Database data files |\n| `hbbs`/`hbbr` | bind mount | `./volumes/rustdesk` | `/root` | RustDesk server state |\n\nAll volume paths support environment variable overrides via `${VAR_NAME:-default}` syntax [infra/docker-compose.yml:12](), [infra/docker-compose.yml:50](), [infra/docker-compose.yml:95](), [infra/docker-compose.yml:125](), [infra/docker-compose.yml:146]().\n\nThe Docker socket mounts (`/var/run/docker.sock:ro`) provide read-only access for `traefik` [infra/docker-compose.yml:11]() to discover services dynamically and for `homepage` [infra/docker-compose.yml:51]() to monitor container status.\n\nSources: [infra/docker-compose.yml:10-12](), [infra/docker-compose.yml:49-51](), [infra/docker-compose.yml:94-95](), [infra/docker-compose.yml:124-125](), [infra/docker-compose.yml:145-146](), [infra/docker-compose.yml:169-170]()\n\n## Service Configuration Details\n\n### Traefik (Edge Router)\n\nThe `traefik` service [infra/docker-compose.yml:2-38]() acts as the ingress controller with the following configuration:\n\n**Command-line Configuration** [infra/docker-compose.yml:13-28]():\n- Enables the API dashboard [infra/docker-compose.yml:14]() with secure access (non-insecure mode)\n- Configures Docker provider with explicit exposure [infra/docker-compose.yml:16-17]()\n- Defines three entrypoints: `web` (:80), `websecure` (:443), `postgres` (:5432) [infra/docker-compose.yml:18-20]()\n- Implements HTTP to HTTPS redirect [infra/docker-compose.yml:21-22]()\n- Configures Let's Encrypt resolver with HTTP-01 challenge [infra/docker-compose.yml:23-26]()\n\n**Self-Routing Labels** [infra/docker-compose.yml:29-36]():\nThe Traefik dashboard is self-hosted using its own routing labels, exposing it at `traefik.${DOMAIN}` with basic authentication middleware [infra/docker-compose.yml:36]().\n\nSources: [infra/docker-compose.yml:2-38]()\n\n### Homepage (Dashboard)\n\nThe `homepage` service [infra/docker-compose.yml:40-60]() provides monitoring capabilities:\n\n**Environment Configuration** [infra/docker-compose.yml:44-48]():\n- `HOMEPAGE_ALLOWED_HOSTS`: Controls access restrictions\n- `PUID`/`PGID`: User/group IDs for file permissions\n- `TZ`: Timezone configuration\n\n**Routing Configuration** [infra/docker-compose.yml:54-56]():\nRoutes to both apex domain and www subdomain: `Host(\\`${DOMAIN}\\`) || Host(\\`www.${DOMAIN}\\`)` [infra/docker-compose.yml:54](), making it the default landing page.\n\nSources: [infra/docker-compose.yml:40-60]()\n\n### MLflow (Tracking Server)\n\nThe `mlflow` service [infra/docker-compose.yml:62-84]() implements the two-store pattern:\n\n**Backend Store URI** [infra/docker-compose.yml:71]():\n```\npostgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}\n```\nUses PostgreSQL for experiment metadata, metrics, and parameters.\n\n**Artifact Store URI** [infra/docker-compose.yml:71]():\n```\ns3://${MINIO_BUCKET_NAME}/\n```\nUses MinIO (S3-compatible) for model artifacts, plots, and datasets.\n\n**S3 Configuration** [infra/docker-compose.yml:67-70]():\n- `MLFLOW_S3_ENDPOINT_URL`: Points to internal MinIO service at `http://minio:9000`\n- `MLFLOW_S3_IGNORE_TLS`: Disables TLS for internal communication\n- `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`: MinIO credentials\n\nThe service uses a custom image from GitHub Container Registry [infra/docker-compose.yml:63](), allowing for pre-built deployment without local image building.\n\nSources: [infra/docker-compose.yml:62-84]()\n\n### MinIO (Object Storage)\n\nThe `minio` service [infra/docker-compose.yml:86-113]() exposes two interfaces:\n\n**API Endpoint** [infra/docker-compose.yml:99-104]():\n- Routed to `minio-api.${DOMAIN}` for S3 API operations\n- Port 9000 [infra/docker-compose.yml:103]()\n- Used by MLflow for artifact storage\n\n**Console Interface** [infra/docker-compose.yml:105-110]():\n- Routed to `minio.${DOMAIN}` for web UI\n- Port 9001 [infra/docker-compose.yml:109]()\n- Accessed via `--console-address \":9001\"` flag [infra/docker-compose.yml:96]()\n\n**Authentication** [infra/docker-compose.yml:91-92]():\nUses `MINIO_ROOT_USER` and `MINIO_ROOT_PASSWORD` which correspond to AWS credentials for S3 compatibility.\n\nSources: [infra/docker-compose.yml:86-113]()\n\n### PostgreSQL (Database)\n\nThe `postgres` service [infra/docker-compose.yml:115-136]() uses the PostGIS image with multiple geospatial extensions:\n\n**Extensions** [infra/docker-compose.yml:123]():\n```\npostgis,hstore,postgis_topology,postgis_raster,pgrouting\n```\n\n**TCP Routing** [infra/docker-compose.yml:127-133]():\nUnlike HTTP services, PostgreSQL uses Traefik's TCP routing with `HostSNI` rule [infra/docker-compose.yml:128](). The service is exposed at `postgres.${DOMAIN}:5432` with TLS termination [infra/docker-compose.yml:132-133]().\n\nSources: [infra/docker-compose.yml:115-136]()\n\n### RustDesk (Remote Desktop)\n\nRustDesk consists of two cooperating services:\n\n**hbbs (ID/Rendezvous Server)** [infra/docker-compose.yml:138-162]():\n- Handles client registration and NAT traversal\n- Command: `hbbs -r rustdesk.${DOMAIN} -k ${RUSTDESK_KEY}` [infra/docker-compose.yml:144]()\n- The `-r` flag specifies the relay server address\n\n**hbbr (Relay Server)** [infra/docker-compose.yml:164-175]():\n- Handles actual data relay when direct P2P fails\n- Command: `hbbr -k ${RUSTDESK_KEY}` [infra/docker-compose.yml:168]()\n\nBoth services share the same encryption key (`${RUSTDESK_KEY}`) and data directory [infra/docker-compose.yml:146](), [infra/docker-compose.yml:170]().\n\nSources: [infra/docker-compose.yml:138-175]()\n\n## Label-Based Routing Configuration\n\nTraefik routing is entirely configured through Docker labels rather than static configuration files. This enables dynamic service discovery and zero-downtime updates.\n\n### HTTP Routing Pattern\n\nAll HTTP services follow this label pattern:\n\n```yaml\nlabels:\n  - \"traefik.enable=true\"\n  - \"traefik.http.routers.\u003cservice\u003e.rule=Host(`\u003csubdomain\u003e.${DOMAIN}`)\"\n  - \"traefik.http.routers.\u003cservice\u003e.entrypoints=websecure\"\n  - \"traefik.http.routers.\u003cservice\u003e.tls.certresolver=letsencrypt\"\n  - \"traefik.http.services.\u003cservice\u003e.loadbalancer.server.port=\u003cport\u003e\"\n  - \"traefik.http.services.\u003cservice\u003e.loadbalancer.server.url=http://\u003ccontainer\u003e:\u003cport\u003e\"\n```\n\n**Service Resolution Mapping**:\n\n| Router Name | Host Rule | Target Service | Target Port |\n|-------------|-----------|----------------|-------------|\n| `homepage` | `${DOMAIN}` \\|\\| `www.${DOMAIN}` | `http://homepage:3000` | 3000 |\n| `mlflow` | `mlflow.${DOMAIN}` | `http://mlflow:5000` | 5000 |\n| `minio-api` | `minio-api.${DOMAIN}` | `http://minio:9000` | 9000 |\n| `minio-console` | `minio.${DOMAIN}` | `http://minio:9001` | 9001 |\n| `rustdesk` | `rustdesk.${DOMAIN}` | `http://hbbs:21118` | 21118 |\n\n### TCP Routing Pattern\n\nPostgreSQL uses TCP routing [infra/docker-compose.yml:127-133]():\n\n```yaml\nlabels:\n  - \"traefik.tcp.routers.postgres.rule=HostSNI(`postgres.${DOMAIN}`)\"\n  - \"traefik.tcp.routers.postgres.entrypoints=postgres\"\n  - \"traefik.tcp.routers.postgres.service=postgres\"\n  - \"traefik.tcp.services.postgres.loadbalancer.server.port=5432\"\n  - \"traefik.tcp.routers.postgres.tls=true\"\n  - \"traefik.tcp.routers.postgres.tls.certresolver=letsencrypt\"\n```\n\nThe `HostSNI` rule performs routing based on the Server Name Indication (SNI) field in the TLS handshake.\n\n### Middleware Configuration\n\nOnly the Traefik dashboard uses middleware for authentication [infra/docker-compose.yml:35-36]():\n\n```yaml\n- \"traefik.http.routers.traefik.middlewares=auth\"\n- \"traefik.http.middlewares.auth.basicauth.users=${TRAEFIK_AUTH_USER}:${TRAEFIK_AUTH_PASSWORD_HASH}\"\n```\n\nSources: [infra/docker-compose.yml:52-58](), [infra/docker-compose.yml:75-81](), [infra/docker-compose.yml:97-110](), [infra/docker-compose.yml:126-133](), [infra/docker-compose.yml:154-160]()\n\n## Container Restart Policy\n\nAll services use `restart: unless-stopped` [infra/docker-compose.yml:5](), [infra/docker-compose.yml:43](), [infra/docker-compose.yml:65](), [infra/docker-compose.yml:89](), [infra/docker-compose.yml:118](), [infra/docker-compose.yml:141](), [infra/docker-compose.yml:167](), ensuring:\n- Automatic restart on failure\n- Automatic restart on system reboot\n- No restart if manually stopped via `docker-compose stop`\n\nThis policy integrates with systemd service management (see [Setup and Deployment](#4.2)) for production-grade reliability.\n\nSources: [infra/docker-compose.yml:5](), [infra/docker-compose.yml:43](), [infra/docker-compose.yml:65](), [infra/docker-compose.yml:89](), [infra/docker-compose.yml:118](), [infra/docker-compose.yml:141](), [infra/docker-compose.yml:167]()\n\n## Service Communication Flow\n\nThe following diagram illustrates how services communicate at runtime:\n\n```mermaid\nsequenceDiagram\n    participant Client as \"External Client\"\n    participant Traefik as \"traefik:443\"\n    participant MLflow as \"mlflow:5000\"\n    participant MinIO as \"minio:9000\"\n    participant Postgres as \"postgres:5432\"\n    \n    Client-\u003e\u003eTraefik: \"HTTPS mlflow.${DOMAIN}/api/2.0/mlflow/runs/create\"\n    Traefik-\u003e\u003eTraefik: \"Match Host() rule, terminate TLS\"\n    Traefik-\u003e\u003eMLflow: \"HTTP mlflow:5000/api/2.0/mlflow/runs/create\"\n    \n    MLflow-\u003e\u003ePostgres: \"INSERT INTO runs (name, experiment_id, ...)\"\n    Postgres--\u003e\u003eMLflow: \"run_id = abc123\"\n    \n    MLflow-\u003e\u003eMLflow: \"Log metrics, parameters to Postgres\"\n    MLflow-\u003e\u003ePostgres: \"INSERT INTO metrics VALUES (...)\"\n    \n    Client-\u003e\u003eTraefik: \"POST /api/2.0/mlflow/artifacts (binary data)\"\n    Traefik-\u003e\u003eMLflow: \"Forward to mlflow:5000\"\n    \n    MLflow-\u003e\u003eMinIO: \"PUT http://minio:9000/${MINIO_BUCKET_NAME}/abc123/model.pth\"\n    Note over MLflow,MinIO: \"Uses MLFLOW_S3_ENDPOINT_URL env var\"\n    MinIO--\u003e\u003eMLflow: \"200 OK, ETag: xyz\"\n    \n    MLflow-\u003e\u003ePostgres: \"UPDATE runs SET artifact_uri = 's3://...'\"\n    MLflow--\u003e\u003eTraefik: \"200 OK\"\n    Traefik--\u003e\u003eClient: \"HTTPS 200 OK\"\n```\n\nThis flow demonstrates:\n1. Traefik performing TLS termination and routing based on `Host()` labels\n2. MLflow coordinating between PostgreSQL (metadata) and MinIO (artifacts)\n3. Internal service communication using container names as DNS hostnames\n4. Environment variable configuration (`MLFLOW_S3_ENDPOINT_URL`) directing artifact storage\n\nSources: [infra/docker-compose.yml:67-71](), [infra/docker-compose.yml:77-81]()"])</script><script>self.__next_f.push([1,"25:T3df6,"])</script><script>self.__next_f.push([1,"# Setup and Deployment\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/Readme.md](infra/Readme.md)\n- [infra/setup.sh](infra/setup.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page documents the automated setup and deployment process for the infrastructure stack using the `setup.sh` script, systemd service integration, and the `manage.sh` management utility. It covers initial deployment, service lifecycle management, and operational maintenance tasks.\n\nFor details about the individual services being deployed, see [Service Architecture](#4.1). For configuration file management and environment variables, see [Configuration Management](#4.3). For a complete step-by-step deployment guide including DNS setup, see [Infrastructure Deployment](#6.1).\n\n---\n\n## Setup Script Overview\n\nThe `setup.sh` script provides a fully automated deployment process that handles prerequisite validation, credential generation, service initialization, and systemd integration in a single execution.\n\n### Script Execution Flow\n\n```mermaid\nflowchart TD\n    START[\"./setup.sh\"]\n    \n    VALIDATE_DOCKER[\"Validate Docker Installation\u003cbr/\u003eLines 25-43\"]\n    CHECK_ENV[\"Check .env File Exists\u003cbr/\u003eLines 47-95\"]\n    \n    CREATE_ENV[\"Create from .env.template\u003cbr/\u003eLine 50\"]\n    GEN_CREDS[\"Generate Secure Credentials\u003cbr/\u003eLines 55-76\"]\n    EXIT_CONFIG[\"Exit: User Must Update DOMAIN\u003cbr/\u003eLines 88-90\"]\n    \n    LOAD_ENV[\"Load .env Variables\u003cbr/\u003eLine 104\"]\n    VALIDATE_DOMAIN[\"Validate DOMAIN and ACME_EMAIL\u003cbr/\u003eLines 107-111\"]\n    \n    CREATE_DIRS[\"Create Directory Structure\u003cbr/\u003eLines 115-122\"]\n    PULL_IMAGES[\"Pull Docker Images\u003cbr/\u003eLines 127-129\"]\n    START_SERVICES[\"docker compose up -d\u003cbr/\u003eLine 132\"]\n    \n    CREATE_SYSTEMD[\"Create Systemd Service\u003cbr/\u003eLines 140-162\"]\n    CREATE_MANAGE[\"Generate manage.sh Script\u003cbr/\u003eLines 164-210\"]\n    \n    DISPLAY_INFO[\"Display Service URLs and Credentials\u003cbr/\u003eLines 212-253\"]\n    \n    START --\u003e VALIDATE_DOCKER\n    VALIDATE_DOCKER --\u003e|\"Docker/Compose OK\"| CHECK_ENV\n    VALIDATE_DOCKER --\u003e|\"Missing\"| FAIL_DOCKER[\"Exit with Error\"]\n    \n    CHECK_ENV --\u003e|\"Exists\"| LOAD_ENV\n    CHECK_ENV --\u003e|\"Missing\"| CREATE_ENV\n    \n    CREATE_ENV --\u003e GEN_CREDS\n    GEN_CREDS --\u003e EXIT_CONFIG\n    \n    LOAD_ENV --\u003e VALIDATE_DOMAIN\n    VALIDATE_DOMAIN --\u003e|\"Valid\"| CREATE_DIRS\n    VALIDATE_DOMAIN --\u003e|\"Not Configured\"| FAIL_CONFIG[\"Exit with Error\"]\n    \n    CREATE_DIRS --\u003e PULL_IMAGES\n    PULL_IMAGES --\u003e START_SERVICES\n    START_SERVICES --\u003e CREATE_SYSTEMD\n    CREATE_SYSTEMD --\u003e CREATE_MANAGE\n    CREATE_MANAGE --\u003e DISPLAY_INFO\n```\n\n**Sources:** [infra/setup.sh:1-254]()\n\n### Prerequisite Validation\n\nThe script validates that all required tools are installed and running before proceeding with deployment:\n\n| Tool | Check Method | Location |\n|------|-------------|----------|\n| `docker` | `command -v docker` | [infra/setup.sh:25-28]() |\n| Docker daemon | `docker info` | [infra/setup.sh:30-33]() |\n| `docker compose` | `command -v docker compose` | [infra/setup.sh:35-38]() |\n| `openssl` | `command -v openssl` | [infra/setup.sh:40-43]() |\n\n**Sources:** [infra/setup.sh:25-45]()\n\n---\n\n## Credential Generation Process\n\nWhen creating a new `.env` file from the template, the script automatically generates cryptographically secure credentials for all services.\n\n### Credential Generation Functions\n\nThe script provides two utility functions for generating secure random values:\n\n```mermaid\ngraph LR\n    subgraph \"Helper Functions\"\n        GEN_PASS[\"generate_password()\u003cbr/\u003eLines 11-14\u003cbr/\u003eUses: openssl rand -base64\"]\n        GEN_KEY[\"generate_key()\u003cbr/\u003eLines 17-20\u003cbr/\u003eUses: openssl rand -hex\"]\n    end\n    \n    subgraph \"Generated Credentials\"\n        POSTGRES[\"POSTGRES_PASSWORD\u003cbr/\u003e20 characters\u003cbr/\u003eLine 55\"]\n        AWS_ACCESS[\"AWS_ACCESS_KEY\u003cbr/\u003e20 hex characters\u003cbr/\u003eLine 56\"]\n        AWS_SECRET[\"AWS_SECRET_KEY\u003cbr/\u003e40 hex characters\u003cbr/\u003eLine 57\"]\n        RUSTDESK[\"RUSTDESK_KEY\u003cbr/\u003e16 hex characters\u003cbr/\u003eLine 58\"]\n        TRAEFIK[\"TRAEFIK_PASSWORD\u003cbr/\u003e16 characters\u003cbr/\u003eLine 59\"]\n    end\n    \n    subgraph \"Special Processing\"\n        TRAEFIK_HASH[\"TRAEFIK_HASH\u003cbr/\u003ehtpasswd -nbB hash\u003cbr/\u003eLines 62-63\"]\n    end\n    \n    GEN_PASS --\u003e POSTGRES\n    GEN_KEY --\u003e AWS_ACCESS\n    GEN_KEY --\u003e AWS_SECRET\n    GEN_KEY --\u003e RUSTDESK\n    GEN_PASS --\u003e TRAEFIK\n    TRAEFIK --\u003e TRAEFIK_HASH\n```\n\n### Credential Substitution\n\nGenerated credentials are substituted into the `.env` file using `sed` replacement operations:\n\n| Placeholder | Replacement Variable | Line |\n|------------|---------------------|------|\n| `replace-with-postgres-user` | `postgres` | [infra/setup.sh:65]() |\n| `replace-with-postgres-password` | `$POSTGRES_PASSWORD` | [infra/setup.sh:66]() |\n| `replace-with-aws-access-key` | `$AWS_ACCESS_KEY` | [infra/setup.sh:67]() |\n| `replace-with-aws-secret-key` | `$AWS_SECRET_KEY` | [infra/setup.sh:68]() |\n| `replace-with-traefik-password` | `$TRAEFIK_PASSWORD` | [infra/setup.sh:69]() |\n| `replace-with-traefik-hash` | `$TRAEFIK_HASH` | [infra/setup.sh:70]() |\n| `replace-with-rustdesk-key` | `$RUSTDESK_KEY` | [infra/setup.sh:71]() |\n\n**Note:** The Traefik password hash is generated using the `httpd:2.4-alpine` Docker image's `htpasswd` utility with bcrypt algorithm (`-nbB`). The `$` characters in the hash are doubled (`$$`) to escape them for docker-compose substitution.\n\n**Sources:** [infra/setup.sh:11-76]()\n\n---\n\n## Directory Structure Creation\n\nThe script creates a standardized directory structure for persistent data storage and operational logs.\n\n### Created Directories\n\n```mermaid\ngraph TB\n    ROOT[\"infra/ (Working Directory)\"]\n    \n    VOLUMES[\"volumes/\"]\n    LOGS[\"logs/\"]\n    \n    TRAEFIK_DATA[\"traefik-data/\u003cbr/\u003eSSL certificates (acme.json)\"]\n    MINIO[\"minio/\u003cbr/\u003eObject storage data\"]\n    POSTGRES[\"postgres/\u003cbr/\u003eDatabase files\"]\n    RUSTDESK[\"rustdesk/\u003cbr/\u003eRustDesk server data\"]\n    \n    ROOT --\u003e VOLUMES\n    ROOT --\u003e LOGS\n    \n    VOLUMES --\u003e TRAEFIK_DATA\n    VOLUMES --\u003e MINIO\n    VOLUMES --\u003e POSTGRES\n    VOLUMES --\u003e RUSTDESK\n    \n    ACME[\"acme.json\u003cbr/\u003ePermissions: 600\u003cbr/\u003eOwner: $USER\"]\n    \n    TRAEFIK_DATA --\u003e ACME\n```\n\n### Directory Creation and Permissions\n\n1. **Directory creation** [infra/setup.sh:115-117]():\n   - `volumes/{traefik-data,minio,postgres,rustdesk}` - Service data volumes\n   - `logs/` - Application logs directory\n\n2. **Special file permissions** [infra/setup.sh:119-122]():\n   - `volumes/traefik-data/acme.json` - Created with `600` permissions (required by Traefik for SSL certificate storage)\n   - All volumes owned by `$USER:$USER` for proper access control\n\n**Sources:** [infra/setup.sh:115-122]()\n\n---\n\n## Service Initialization\n\nThe script manages the complete service lifecycle from image retrieval to container startup.\n\n### Initialization Sequence\n\n```mermaid\nsequenceDiagram\n    participant Script as setup.sh\n    participant Docker as Docker Engine\n    participant Compose as docker-compose.yml\n    participant Registry as Container Registry\n    \n    Script-\u003e\u003eDocker: docker compose down --remove-orphans\n    Note over Script,Docker: Stop any existing services\u003cbr/\u003eLine 125\n    \n    Script-\u003e\u003eRegistry: docker compose pull\n    Note over Script,Registry: Pull latest images\u003cbr/\u003eLine 129\n    Registry--\u003e\u003eDocker: MLflow image (${MLFLOW_IMAGE})\n    Registry--\u003e\u003eDocker: Traefik, MinIO, PostgreSQL, etc.\n    \n    Script-\u003e\u003eCompose: docker compose up -d\n    Note over Script,Compose: Start all services\u003cbr/\u003eLine 132\n    \n    Compose-\u003e\u003eDocker: Create traefik container\n    Compose-\u003e\u003eDocker: Create mlflow container\n    Compose-\u003e\u003eDocker: Create minio container\n    Compose-\u003e\u003eDocker: Create postgres container\n    Compose-\u003e\u003eDocker: Create rustdesk containers\n    Compose-\u003e\u003eDocker: Create homepage container\n    \n    Script-\u003e\u003eScript: sleep 30\n    Note over Script: Wait for initialization\u003cbr/\u003eLine 135\n    \n    Script-\u003e\u003eDocker: docker compose ps\n    Docker--\u003e\u003eScript: Service status\n    Note over Script: Display status\u003cbr/\u003eLine 138\n```\n\n### MLflow Image Configuration\n\nThe script uses the `MLFLOW_IMAGE` environment variable to specify which MLflow image to pull. This supports custom-built MLflow images with additional dependencies:\n\n```bash\n# From .env file\nMLFLOW_IMAGE=ghcr.io/kshitijrajsharma/mlflow:latest\n```\n\nThe image is displayed during setup [infra/setup.sh:128]() and pulled from the registry [infra/setup.sh:129]().\n\n**Sources:** [infra/setup.sh:124-138]()\n\n---\n\n## Systemd Integration\n\nThe script creates a systemd service unit for automatic startup and system integration.\n\n### Service Unit Definition\n\nThe systemd service is generated dynamically at [infra/setup.sh:141-159]():\n\n```ini\n[Unit]\nDescription=Tech Infrastructure Services\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nUser=$USER\nGroup=$USER\nWorkingDirectory=$(pwd)\nExecStart=/usr/bin/docker compose up -d\nExecStop=/usr/bin/docker compose down\nTimeoutStartSec=0\n\n[Install]\nWantedBy=multi-user.target\n```\n\n### Service Unit Characteristics\n\n| Field | Value | Purpose |\n|-------|-------|---------|\n| `Type` | `oneshot` | Service runs to completion then remains active |\n| `RemainAfterExit` | `yes` | Service stays active after ExecStart completes |\n| `Requires` | `docker.service` | Hard dependency on Docker daemon |\n| `After` | `docker.service` | Start after Docker is ready |\n| `WantedBy` | `multi-user.target` | Enable on system boot |\n| `TimeoutStartSec` | `0` | No timeout for service startup |\n\n### Systemd Installation\n\nThe script performs the following systemd operations [infra/setup.sh:161-162]():\n\n```bash\nsudo systemctl daemon-reload       # Reload systemd configuration\nsudo systemctl enable tech-infra.service  # Enable auto-start on boot\n```\n\n### System Management Commands\n\nOnce installed, the systemd service can be controlled with:\n\n```bash\nsudo systemctl start tech-infra    # Start all services\nsudo systemctl stop tech-infra     # Stop all services\nsudo systemctl status tech-infra   # Check service status\nsudo systemctl restart tech-infra  # Restart all services\n```\n\n**Sources:** [infra/setup.sh:140-162](), [infra/Readme.md:64-68]()\n\n---\n\n## Management Script Generation\n\nThe setup script generates a `manage.sh` utility script that provides convenient commands for daily operations.\n\n### Management Script Commands\n\n```mermaid\ngraph TB\n    MANAGE[\"./manage.sh\"]\n    \n    START[\"start\u003cbr/\u003edocker compose up -d\u003cbr/\u003eLines 168-170\"]\n    STOP[\"stop\u003cbr/\u003edocker compose down\u003cbr/\u003eLines 171-173\"]\n    RESTART[\"restart [service]\u003cbr/\u003edocker compose restart\u003cbr/\u003eLines 174-176\"]\n    LOGS[\"logs [service]\u003cbr/\u003edocker compose logs -f\u003cbr/\u003eLines 177-183\"]\n    STATUS[\"status\u003cbr/\u003edocker compose ps\u003cbr/\u003eLines 184-186\"]\n    UPDATE[\"update\u003cbr/\u003ePull images + restart\u003cbr/\u003eLines 187-192\"]\n    BACKUP[\"backup\u003cbr/\u003eBackup volumes + DB dump\u003cbr/\u003eLines 193-199\"]\n    \n    MANAGE --\u003e START\n    MANAGE --\u003e STOP\n    MANAGE --\u003e RESTART\n    MANAGE --\u003e LOGS\n    MANAGE --\u003e STATUS\n    MANAGE --\u003e UPDATE\n    MANAGE --\u003e BACKUP\n```\n\n### Command Reference\n\n| Command | Function | Implementation |\n|---------|----------|----------------|\n| `./manage.sh start` | Start all services | `docker compose up -d` |\n| `./manage.sh stop` | Stop all services | `docker compose down` |\n| `./manage.sh restart [service]` | Restart one or all services | `docker compose restart ${service}` |\n| `./manage.sh logs [service]` | View logs (follow mode if service specified) | `docker compose logs [-f] [service]` |\n| `./manage.sh status` | Display service status | `docker compose ps` |\n| `./manage.sh update` | Pull latest images and restart | `docker compose pull` + `docker compose up -d` |\n| `./manage.sh backup` | Create timestamped backup | Copy volumes + PostgreSQL dump |\n\n### Backup Operation Details\n\nThe `backup` command [infra/setup.sh:193-199]() creates a comprehensive backup:\n\n1. **Backup directory** created at `./backups/YYYYMMDD_HHMMSS/`\n2. **Volume data** copied with `cp -r volumes/ \"$BACKUP_DIR/\"`\n3. **PostgreSQL database** dumped with:\n   ```bash\n   docker compose exec -T postgres pg_dump -U ${POSTGRES_USER} ${POSTGRES_DB} \\\n     \u003e \"$BACKUP_DIR/postgres_dump.sql\"\n   ```\n\n### Usage Examples\n\n```bash\n# View MLflow logs in real-time\n./manage.sh logs mlflow\n\n# Restart only the PostgreSQL service\n./manage.sh restart postgres\n\n# Pull latest images and restart all services\n./manage.sh update\n\n# Create a full backup\n./manage.sh backup\n```\n\n**Sources:** [infra/setup.sh:164-210](), [infra/Readme.md:54-60]()\n\n---\n\n## Setup Completion and Output\n\nUpon successful completion, the script displays comprehensive information about the deployed infrastructure.\n\n### Service URL Display\n\nThe script outputs all service URLs using the configured `DOMAIN` variable [infra/setup.sh:212-221]():\n\n```\nHomepage Dashboard: https://${DOMAIN}\nMLflow Tracking: https://mlflow.${DOMAIN}\nMinIO Console: https://minio.${DOMAIN}\nMinIO API: https://minio-api.${DOMAIN}\nRustDesk Server: https://rustdesk.${DOMAIN}\nTraefik Dashboard: https://traefik.${DOMAIN}\nPostgreSQL Database: postgres.${DOMAIN}:5432\n```\n\n### Credential Summary\n\nAll generated credentials are displayed for immediate reference [infra/setup.sh:229-251]():\n\n1. **Traefik Dashboard**\n   - URL, username (`admin`), and password\n\n2. **PostgreSQL Database**\n   - Host, port, database name, username, and password\n\n3. **MinIO/S3 Storage**\n   - Console URL, API URL, access key, and secret key\n\n4. **MLflow Tracking**\n   - URL and integration notes\n\n### Security Notice\n\nThe script emphasizes credential security [infra/setup.sh:78-90](), [infra/setup.sh:252-253]():\n- All credentials displayed at first setup (before domain configuration)\n- Reminder that credentials are stored in `.env` file\n- Warning to keep `.env` file secure and backed up\n\n**Sources:** [infra/setup.sh:212-253]()\n\n---\n\n## Deployment Workflow Summary\n\nThe complete deployment process follows this sequence:\n\n```mermaid\nflowchart LR\n    A[\"Clone Repository\"] --\u003e B[\"Execute setup.sh\"]\n    B --\u003e C{\"First Run?\"}\n    \n    C --\u003e|\"Yes\"| D[\"Generate .env\u003cbr/\u003ewith credentials\"]\n    D --\u003e E[\"Exit: Configure\u003cbr/\u003eDOMAIN and EMAIL\"]\n    E --\u003e F[\"User edits .env\"]\n    F --\u003e G[\"Execute setup.sh again\"]\n    \n    C --\u003e|\"No\"| H[\"Validate configuration\"]\n    \n    G --\u003e H\n    H --\u003e I[\"Create directories\u003cbr/\u003eSet permissions\"]\n    I --\u003e J[\"Pull Docker images\"]\n    J --\u003e K[\"Start services\"]\n    K --\u003e L[\"Create systemd service\"]\n    L --\u003e M[\"Generate manage.sh\"]\n    M --\u003e N[\"Display credentials\u003cbr/\u003eand URLs\"]\n    \n    N --\u003e O[\"Services Running\"]\n```\n\n### Two-Stage Setup Process\n\n1. **First execution** [infra/setup.sh:47-95]():\n   - Generates `.env` from template\n   - Creates secure credentials\n   - Exits with instructions to configure `DOMAIN` and `ACME_EMAIL`\n\n2. **Second execution** [infra/setup.sh:104-253]():\n   - Loads validated configuration\n   - Deploys complete infrastructure\n   - Creates management tools\n   - Displays access information\n\nThis two-stage approach ensures users explicitly configure their domain settings before services are deployed with SSL certificates.\n\n**Sources:** [infra/setup.sh:1-254](), [infra/Readme.md:14-30]()\n\n---\n\n## Required DNS Configuration\n\nBefore running the setup script, the following DNS A records must point to the server's IP address:\n\n| Subdomain | Service | Required |\n|-----------|---------|----------|\n| `yourdomain.com` | Homepage Dashboard | Yes |\n| `mlflow.yourdomain.com` | MLflow Tracking Server | Yes |\n| `minio.yourdomain.com` | MinIO Console | Yes |\n| `minio-api.yourdomain.com` | MinIO S3 API | Yes |\n| `postgres.yourdomain.com` | PostgreSQL Database | Yes |\n| `rustdesk.yourdomain.com` | RustDesk Server | Yes |\n| `traefik.yourdomain.com` | Traefik Dashboard | Yes |\n\nThese DNS records are required for Traefik to correctly route requests and for Let's Encrypt to issue SSL certificates.\n\n**Sources:** [infra/Readme.md:32-42]()"])</script><script>self.__next_f.push([1,"26:T4459,"])</script><script>self.__next_f.push([1,"# Configuration Management\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/.env.template](infra/.env.template)\n- [infra/setup.sh](infra/setup.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document covers the configuration management system for the infrastructure stack, including environment variable templates, automated credential generation, security settings, and validation processes. The configuration system is centered around the `.env` template file and the `setup.sh` script, which together provide a secure, reproducible method for deploying the infrastructure services.\n\nFor information about the broader infrastructure architecture and service deployment, see [Service Architecture](#4.1). For details about the deployment process that uses these configurations, see [Setup and Deployment](#4.2).\n\n**Sources:** [infra/.env.template:1-37](), [infra/setup.sh:1-254]()\n\n---\n\n## Configuration System Architecture\n\nThe configuration management system uses a template-based approach where placeholders are replaced with generated credentials during the initial setup.\n\n```mermaid\nflowchart TB\n    subgraph \"Configuration Sources\"\n        TEMPLATE[\".env.template\u003cbr/\u003ePlaceholder values\"]\n        SETUP[\"setup.sh\u003cbr/\u003eCredential generator\"]\n    end\n    \n    subgraph \"Generation Process\"\n        GEN_PASS[\"generate_password()\u003cbr/\u003eOpenSSL base64\"]\n        GEN_KEY[\"generate_key()\u003cbr/\u003eOpenSSL hex\"]\n        GEN_HASH[\"htpasswd\u003cbr/\u003eBCrypt hash\"]\n    end\n    \n    subgraph \"Generated Configuration\"\n        ENV[\".env\u003cbr/\u003eRuntime configuration\"]\n        HOMEPAGE_ENV[\"homepage-config/.env\u003cbr/\u003eHomepage configuration\"]\n    end\n    \n    subgraph \"Configuration Consumers\"\n        COMPOSE[\"docker-compose.yml\u003cbr/\u003eService definitions\"]\n        SERVICES[\"Docker Services\u003cbr/\u003etraefik, mlflow, minio, etc.\"]\n    end\n    \n    TEMPLATE --\u003e|\"./setup.sh\"| SETUP\n    SETUP --\u003e GEN_PASS\n    SETUP --\u003e GEN_KEY\n    SETUP --\u003e GEN_HASH\n    \n    GEN_PASS --\u003e|\"POSTGRES_PASSWORD\u003cbr/\u003eTRAEFIK_PASSWORD\"| ENV\n    GEN_KEY --\u003e|\"AWS_ACCESS_KEY_ID\u003cbr/\u003eAWS_SECRET_ACCESS_KEY\u003cbr/\u003eRUSTDESK_KEY\"| ENV\n    GEN_HASH --\u003e|\"TRAEFIK_AUTH_PASSWORD_HASH\"| ENV\n    \n    TEMPLATE --\u003e|\"cp .env.template\"| ENV\n    TEMPLATE --\u003e|\"cp .env.template\"| HOMEPAGE_ENV\n    \n    ENV --\u003e COMPOSE\n    COMPOSE --\u003e SERVICES\n    \n    ENV -.-\u003e|\"validated by setup.sh\"| SETUP\n```\n\n**Diagram: Configuration Generation Flow**\n\nThe system generates secure credentials automatically and validates that critical configuration values are set before allowing deployment.\n\n**Sources:** [infra/setup.sh:47-95](), [infra/setup.sh:11-20]()\n\n---\n\n## Environment Variable Template Structure\n\nThe `.env.template` file is organized into logical sections, each containing related configuration variables.\n\n### Configuration Categories\n\n| Category | Variables | Purpose |\n|----------|-----------|---------|\n| Domain Configuration | `DOMAIN`, `ACME_EMAIL` | DNS domain for services and Let's Encrypt contact |\n| Traefik Configuration | `TRAEFIK_DATA_DIR`, `TRAEFIK_AUTH_USER`, `TRAEFIK_AUTH_PASSWORD`, `TRAEFIK_AUTH_PASSWORD_HASH` | Reverse proxy data storage and authentication |\n| Homepage Configuration | `HOMEPAGE_CONFIG`, `HOMEPAGE_ALLOWED_HOSTS` | Dashboard configuration path and allowed domains |\n| MLflow Configuration | `MLFLOW_IMAGE` | Custom MLflow Docker image from GHCR |\n| MinIO Configuration | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `MINIO_BUCKET_NAME`, `MINIO_DATA_DIR` | S3-compatible object storage credentials and settings |\n| PostgreSQL Configuration | `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`, `POSTGRES_DATA_DIR` | Database credentials and storage location |\n| RustDesk Configuration | `RUSTDESK_DATA_DIR`, `RUSTDESK_KEY` | Remote desktop server data and encryption key |\n| System Configuration | `PUID`, `PGID`, `TZ` | Unix user/group IDs and timezone |\n\n**Sources:** [infra/.env.template:1-37]()\n\n### Template Placeholder Pattern\n\nThe template uses specific placeholder strings that are replaced during setup:\n\n```mermaid\ngraph LR\n    subgraph \"Placeholder Patterns\"\n        P1[\"replace-with-postgres-user\"]\n        P2[\"replace-with-postgres-password\"]\n        P3[\"replace-with-aws-access-key\"]\n        P4[\"replace-with-aws-secret-key\"]\n        P5[\"replace-with-traefik-password\"]\n        P6[\"replace-with-traefik-hash\"]\n        P7[\"replace-with-rustdesk-key\"]\n    end\n    \n    subgraph \"sed Replacements\"\n        R1[\"sed -i 's|placeholder|value|g'\"]\n    end\n    \n    subgraph \"Generated Values\"\n        V1[\"postgres\"]\n        V2[\"$POSTGRES_PASSWORD\"]\n        V3[\"$AWS_ACCESS_KEY\"]\n        V4[\"$AWS_SECRET_KEY\"]\n        V5[\"$TRAEFIK_PASSWORD\"]\n        V6[\"$TRAEFIK_HASH\"]\n        V7[\"$RUSTDESK_KEY\"]\n    end\n    \n    P1 --\u003e R1\n    P2 --\u003e R1\n    P3 --\u003e R1\n    P4 --\u003e R1\n    P5 --\u003e R1\n    P6 --\u003e R1\n    P7 --\u003e R1\n    \n    R1 --\u003e V1\n    R1 --\u003e V2\n    R1 --\u003e V3\n    R1 --\u003e V4\n    R1 --\u003e V5\n    R1 --\u003e V6\n    R1 --\u003e V7\n```\n\n**Diagram: Placeholder Replacement Mechanism**\n\n**Sources:** [infra/setup.sh:64-71](), [infra/.env.template:8-9](), [infra/.env.template:19-20](), [infra/.env.template:25-26](), [infra/.env.template:32]()\n\n---\n\n## Credential Generation\n\nThe `setup.sh` script implements two primary functions for generating secure credentials using OpenSSL.\n\n### Password Generation\n\nThe `generate_password()` function creates URL-safe base64 passwords:\n\n| Parameter | Default | Purpose |\n|-----------|---------|---------|\n| `length` | 16 | Number of characters in final password |\n\n**Implementation:** Uses `openssl rand -base64` with character filtering to remove problematic characters (`=+/`).\n\n**Used for:**\n- `POSTGRES_PASSWORD` (20 characters)\n- `TRAEFIK_PASSWORD` (16 characters)\n\n**Sources:** [infra/setup.sh:11-14](), [infra/setup.sh:55](), [infra/setup.sh:59]()\n\n### Key Generation\n\nThe `generate_key()` function creates hexadecimal keys:\n\n| Parameter | Default | Purpose |\n|-----------|---------|---------|\n| `length` | 32 | Number of hex characters (bytes) |\n\n**Implementation:** Uses `openssl rand -hex` to generate cryptographically secure random hex strings.\n\n**Used for:**\n- `AWS_ACCESS_KEY_ID` (20 bytes = 40 hex characters)\n- `AWS_SECRET_ACCESS_KEY` (40 bytes = 80 hex characters)\n- `RUSTDESK_KEY` (16 bytes = 32 hex characters)\n\n**Sources:** [infra/setup.sh:17-20](), [infra/setup.sh:56-58]()\n\n### Hash Generation\n\nThe Traefik basic authentication hash is generated using Apache's `htpasswd` utility with BCrypt:\n\n```\nTRAEFIK_HASH=$(docker run --rm httpd:2.4-alpine htpasswd -nbB admin \"$TRAEFIK_PASSWORD\" 2\u003e/dev/null | cut -d \":\" -f 2 | sed 's/\\$/\\$\\$/g')\n```\n\n**Special handling:** Dollar signs are doubled (`$`  `$$`) to escape them for docker-compose variable interpolation.\n\n**Sources:** [infra/setup.sh:62]()\n\n---\n\n## Configuration Workflow\n\nThe complete configuration workflow follows this sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant setup_sh as \"setup.sh\"\n    participant env_template as \".env.template\"\n    participant env as \".env\"\n    participant openssl as \"OpenSSL\"\n    participant docker as \"Docker\"\n    \n    User-\u003e\u003esetup_sh: \"./setup.sh\"\n    setup_sh-\u003e\u003esetup_sh: \"Check prerequisites\"\n    \n    alt .env does not exist\n        setup_sh-\u003e\u003eenv_template: \"Copy to .env\"\n        env_template--\u003e\u003eenv: \".env created\"\n        \n        setup_sh-\u003e\u003eopenssl: \"generate_password(20)\"\n        openssl--\u003e\u003esetup_sh: \"POSTGRES_PASSWORD\"\n        \n        setup_sh-\u003e\u003eopenssl: \"generate_key(20)\"\n        openssl--\u003e\u003esetup_sh: \"AWS_ACCESS_KEY\"\n        \n        setup_sh-\u003e\u003eopenssl: \"generate_key(40)\"\n        openssl--\u003e\u003esetup_sh: \"AWS_SECRET_KEY\"\n        \n        setup_sh-\u003e\u003eopenssl: \"generate_key(16)\"\n        openssl--\u003e\u003esetup_sh: \"RUSTDESK_KEY\"\n        \n        setup_sh-\u003e\u003eopenssl: \"generate_password(16)\"\n        openssl--\u003e\u003esetup_sh: \"TRAEFIK_PASSWORD\"\n        \n        setup_sh-\u003e\u003edocker: \"htpasswd -nbB\"\n        docker--\u003e\u003esetup_sh: \"TRAEFIK_HASH\"\n        \n        setup_sh-\u003e\u003eenv: \"Replace placeholders\"\n        setup_sh-\u003e\u003eUser: \"Display credentials\"\n        setup_sh-\u003e\u003esetup_sh: \"Exit (user must update DOMAIN)\"\n    else .env exists\n        setup_sh-\u003e\u003eenv: \"source .env\"\n        setup_sh-\u003e\u003esetup_sh: \"Validate DOMAIN != example.com\"\n        setup_sh-\u003e\u003esetup_sh: \"Validate ACME_EMAIL != admin@example.com\"\n        \n        alt validation fails\n            setup_sh-\u003e\u003eUser: \"Error: Update DOMAIN/ACME_EMAIL\"\n            setup_sh-\u003e\u003esetup_sh: \"exit 1\"\n        else validation passes\n            setup_sh-\u003e\u003eUser: \"Configuration validated\"\n            setup_sh-\u003e\u003edocker: \"docker compose up -d\"\n        end\n    end\n```\n\n**Diagram: Configuration Setup Sequence**\n\n**Sources:** [infra/setup.sh:47-95](), [infra/setup.sh:104-113]()\n\n---\n\n## Configuration Validation\n\nThe setup script performs multiple validation checks before deployment.\n\n### Prerequisite Validation\n\n| Check | Command | Error Message |\n|-------|---------|---------------|\n| Docker installed | `command -v docker` | \"Docker is not installed\" |\n| Docker running | `docker info` | \"Docker is not running\" |\n| Docker Compose installed | `command -v docker compose` | \"Docker Compose is not installed\" |\n| OpenSSL installed | `command -v openssl` | \"OpenSSL is required for generating secure credentials\" |\n\n**Sources:** [infra/setup.sh:25-43]()\n\n### Configuration Value Validation\n\nAfter generating credentials, the script validates that critical configuration values have been updated from their defaults:\n\n```mermaid\ngraph TD\n    ENV[\"Load .env variables\"]\n    CHECK_DOMAIN{\"DOMAIN ==\u003cbr/\u003eexample.com?\"}\n    CHECK_EMAIL{\"ACME_EMAIL ==\u003cbr/\u003eadmin@example.com?\"}\n    ERROR[\"Display error:\u003cbr/\u003eUpdate DOMAIN and ACME_EMAIL\u003cbr/\u003eexit 1\"]\n    CONTINUE[\"Continue deployment\"]\n    \n    ENV --\u003e CHECK_DOMAIN\n    CHECK_DOMAIN --\u003e|\"Yes\"| ERROR\n    CHECK_DOMAIN --\u003e|\"No\"| CHECK_EMAIL\n    CHECK_EMAIL --\u003e|\"Yes\"| ERROR\n    CHECK_EMAIL --\u003e|\"No\"| CONTINUE\n```\n\n**Diagram: Configuration Validation Logic**\n\n**Sources:** [infra/setup.sh:104-111]()\n\n---\n\n## Directory and Permission Management\n\nThe setup script creates necessary directories and sets appropriate permissions for security.\n\n### Directory Structure\n\n| Path | Purpose | Owner |\n|------|---------|-------|\n| `volumes/traefik-data/` | Traefik configuration and SSL certificates | Current user |\n| `volumes/minio/` | MinIO object storage data | Current user |\n| `volumes/postgres/` | PostgreSQL database files | Current user |\n| `volumes/rustdesk/` | RustDesk server data | Current user |\n| `logs/` | Application log files | Current user |\n\n**Sources:** [infra/setup.sh:115-117](), [infra/.env.template:6](), [infra/.env.template:22](), [infra/.env.template:28](), [infra/.env.template:31]()\n\n### Critical Permission Settings\n\nThe ACME certificate file requires specific permissions to prevent security warnings:\n\n```\ntouch volumes/traefik-data/acme.json\nchmod 600 volumes/traefik-data/acme.json\n```\n\n**Purpose:** Let's Encrypt requires that the certificate storage file is readable/writable only by the owner (mode 600).\n\n**Sources:** [infra/setup.sh:119-122]()\n\n---\n\n## Service-Specific Configuration\n\n### Domain-Based Routing Configuration\n\nAll services use subdomain-based routing patterns:\n\n| Service | Subdomain Pattern | Variable Used |\n|---------|-------------------|---------------|\n| Homepage | `${DOMAIN}` | `DOMAIN` |\n| MLflow | `mlflow.${DOMAIN}` | `DOMAIN` |\n| MinIO Console | `minio.${DOMAIN}` | `DOMAIN` |\n| MinIO API | `minio-api.${DOMAIN}` | `DOMAIN` |\n| Traefik Dashboard | `traefik.${DOMAIN}` | `DOMAIN` |\n| PostgreSQL | `postgres.${DOMAIN}:5432` | `DOMAIN` |\n| RustDesk | `rustdesk.${DOMAIN}` | `DOMAIN` |\n\n**Sources:** [infra/setup.sh:213-220]()\n\n### MLflow Backend Configuration\n\nMLflow uses environment variables to configure its backend store and artifact store:\n\n| Variable | Purpose | Default Value |\n|----------|---------|---------------|\n| `MLFLOW_IMAGE` | Custom MLflow Docker image | `ghcr.io/kshitijrajsharma/opengeoaimodelshub/mlflow:latest` |\n| `POSTGRES_USER` | Database username | Generated (postgres) |\n| `POSTGRES_PASSWORD` | Database password | Generated |\n| `POSTGRES_DB` | Database name | `mlflow` |\n| `AWS_ACCESS_KEY_ID` | MinIO access key | Generated |\n| `AWS_SECRET_ACCESS_KEY` | MinIO secret key | Generated |\n| `MINIO_BUCKET_NAME` | Artifact storage bucket | `mlflow` |\n\n**Sources:** [infra/.env.template:15-28]()\n\n### Security Configuration\n\n#### Traefik Authentication\n\nTraefik dashboard uses HTTP Basic Authentication with BCrypt-hashed passwords:\n\n| Variable | Purpose |\n|----------|---------|\n| `TRAEFIK_AUTH_USER` | Username for dashboard access (default: admin) |\n| `TRAEFIK_AUTH_PASSWORD` | Plain-text password (stored for reference) |\n| `TRAEFIK_AUTH_PASSWORD_HASH` | BCrypt hash with doubled dollar signs |\n\n**Sources:** [infra/.env.template:7-9]()\n\n#### Homepage Access Control\n\nHomepage dashboard restricts access to specific domains:\n\n| Variable | Purpose | Example |\n|----------|---------|---------|\n| `HOMEPAGE_ALLOWED_HOSTS` | Comma-separated list of allowed domains | `example.com,www.example.com` |\n\n**Sources:** [infra/.env.template:13]()\n\n---\n\n## Credential Display and Storage\n\nAfter generating credentials, the setup script displays them in an organized format:\n\n```mermaid\ngraph TB\n    subgraph \"Credential Output Format\"\n        DISPLAY[\"setup.sh credential display\"]\n        \n        TRAEFIK_CREDS[\"Traefik Dashboard Credentials\u003cbr/\u003eUsername: admin\u003cbr/\u003ePassword: [generated]\"]\n        POSTGRES_CREDS[\"PostgreSQL Database Credentials\u003cbr/\u003eUsername: postgres\u003cbr/\u003ePassword: [generated]\"]\n        MINIO_CREDS[\"MinIO/S3 Credentials\u003cbr/\u003eAccess Key: [generated]\u003cbr/\u003eSecret Key: [generated]\"]\n        RUSTDESK_CREDS[\"RustDesk Key\u003cbr/\u003e[generated]\"]\n    end\n    \n    subgraph \"Storage Location\"\n        ENV_FILE[\".env file\u003cbr/\u003ePlain-text storage\"]\n    end\n    \n    subgraph \"Security Notes\"\n        WARNING[\"IMPORTANT:\u003cbr/\u003eKeep .env file secure\u003cbr/\u003eand backed up\"]\n    end\n    \n    DISPLAY --\u003e TRAEFIK_CREDS\n    DISPLAY --\u003e POSTGRES_CREDS\n    DISPLAY --\u003e MINIO_CREDS\n    DISPLAY --\u003e RUSTDESK_CREDS\n    \n    TRAEFIK_CREDS -.-\u003e ENV_FILE\n    POSTGRES_CREDS -.-\u003e ENV_FILE\n    MINIO_CREDS -.-\u003e ENV_FILE\n    RUSTDESK_CREDS -.-\u003e ENV_FILE\n    \n    ENV_FILE --\u003e WARNING\n```\n\n**Diagram: Credential Storage and Display**\n\nThe script displays credentials twice: once during initial generation (with a warning to update `DOMAIN`) and again after successful deployment.\n\n**Sources:** [infra/setup.sh:78-88](), [infra/setup.sh:229-253]()\n\n---\n\n## Configuration Update Workflow\n\nWhen updating the infrastructure, the configuration system follows this pattern:\n\n| Scenario | Action | Result |\n|----------|--------|--------|\n| First run | Copy `.env.template`  `.env`, generate credentials | `.env` created with secure credentials |\n| `.env` exists with defaults | Validation fails | Script exits with error message |\n| `.env` exists with valid values | Source variables, validate | Deployment proceeds |\n| Update configuration | Manually edit `.env` | Run `./manage.sh restart` to apply |\n\n**Sources:** [infra/setup.sh:47-113]()\n\n---\n\n## Integration with Docker Compose\n\nThe `.env` file is automatically sourced by Docker Compose through environment variable substitution:\n\n```mermaid\ngraph LR\n    subgraph \"Configuration Files\"\n        ENV[\".env\u003cbr/\u003eKey-value pairs\"]\n    end\n    \n    subgraph \"Docker Compose\"\n        COMPOSE[\"docker-compose.yml\u003cbr/\u003eService definitions\"]\n        VARS[\"${VARIABLE} substitution\"]\n    end\n    \n    subgraph \"Services\"\n        TRAEFIK[\"traefik service\u003cbr/\u003eTRAEFIK_AUTH_USER\u003cbr/\u003eTRAEFIK_AUTH_PASSWORD_HASH\"]\n        MLFLOW[\"mlflow service\u003cbr/\u003ePOSTGRES_*\u003cbr/\u003eAWS_*\"]\n        MINIO[\"minio service\u003cbr/\u003eAWS_ACCESS_KEY_ID\u003cbr/\u003eAWS_SECRET_ACCESS_KEY\"]\n        POSTGRES[\"postgres service\u003cbr/\u003ePOSTGRES_USER\u003cbr/\u003ePOSTGRES_PASSWORD\u003cbr/\u003ePOSTGRES_DB\"]\n    end\n    \n    ENV --\u003e COMPOSE\n    COMPOSE --\u003e VARS\n    VARS --\u003e TRAEFIK\n    VARS --\u003e MLFLOW\n    VARS --\u003e MINIO\n    VARS --\u003e POSTGRES\n```\n\n**Diagram: Environment Variable Flow to Services**\n\n**Sources:** [infra/.env.template:1-37]()\n\n---\n\n## System Configuration Variables\n\nThe system-level configuration variables affect file ownership and timestamps:\n\n| Variable | Purpose | Default | Usage |\n|----------|---------|---------|-------|\n| `PUID` | Process User ID | 1000 | File ownership for volume-mounted data |\n| `PGID` | Process Group ID | 1000 | File ownership for volume-mounted data |\n| `TZ` | Timezone | UTC | Container timezone settings |\n\nThese are particularly important for ensuring that files created by containerized services have the correct ownership on the host filesystem.\n\n**Sources:** [infra/.env.template:35-37]()\n\n---\n\n## Best Practices\n\n### Security Recommendations\n\n1. **Never commit `.env` file to version control** - Contains plain-text credentials\n2. **Backup `.env` file securely** - Required for disaster recovery\n3. **Rotate credentials periodically** - Generate new credentials and update `.env`\n4. **Use strong domain names** - Avoid predictable patterns\n5. **Keep `ACME_EMAIL` valid** - Required for Let's Encrypt notifications\n\n### Configuration Management\n\n1. **Document custom changes** - Add comments to `.env` for non-standard configurations\n2. **Test in staging first** - Validate configuration changes before production\n3. **Use version control for `.env.template`** - Track template changes\n4. **Maintain separate environments** - Use different `.env` files for dev/staging/prod\n\n**Sources:** [infra/setup.sh:88](), [infra/setup.sh:253]()"])</script><script>self.__next_f.push([1,"27:T5871,"])</script><script>self.__next_f.push([1,"# Traefik Reverse Proxy\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/.env.template](infra/.env.template)\n- [infra/docker-compose.yml](infra/docker-compose.yml)\n\n\u003c/details\u003e\n\n\n\nThis document covers Traefik's configuration and operation as the edge router in the OpenGeoAIModelHub infrastructure stack. Traefik serves as the single entry point for all HTTP/HTTPS traffic, handles automatic SSL certificate provisioning via Let's Encrypt, and routes requests to appropriate backend services using Docker label-based service discovery.\n\nFor information about the overall infrastructure architecture, see [Service Architecture](#4.1). For details about individual backend services that Traefik routes to, see [MLflow Tracking Server](#4.5), [Storage Services](#4.6), and [Additional Services](#4.7).\n\n**Sources:** [infra/docker-compose.yml:1-184]()\n\n---\n\n## Purpose and Capabilities\n\nTraefik functions as a cloud-native edge router that automatically discovers services running in Docker containers and configures routing rules dynamically. The implementation in this infrastructure provides:\n\n- **Automatic Service Discovery**: Reads Docker labels to configure routes without manual intervention\n- **SSL Termination**: Automatically provisions and renews Let's Encrypt certificates for all services\n- **HTTP to HTTPS Redirection**: Forces secure connections for all web traffic\n- **Multi-Protocol Support**: Handles both HTTP/HTTPS (ports 80/443) and TCP traffic (port 5432 for PostgreSQL)\n- **Dashboard Interface**: Web UI for monitoring routing configuration and service health\n- **Access Control**: Basic authentication middleware for protecting sensitive services\n\n**Sources:** [infra/docker-compose.yml:2-38]()\n\n---\n\n## Service Configuration\n\nThe Traefik service is defined in the Docker Compose stack with specific port bindings, volume mounts, and command-line configuration.\n\n### Container Definition\n\n```yaml\ntraefik:\n  image: traefik:v3.0\n  container_name: traefik\n  restart: unless-stopped\n  ports:\n    - \"80:80\"      # HTTP entry point\n    - \"443:443\"    # HTTPS entry point\n    - \"8080:8080\"  # Dashboard (optional)\n```\n\n| Port | Protocol | Purpose |\n|------|----------|---------|\n| 80 | HTTP | Web entry point, redirects to HTTPS |\n| 443 | HTTPS | Secure web traffic, SSL termination |\n| 8080 | HTTP | Dashboard access (can be disabled) |\n\nThe service uses the `traefik:v3.0` image and runs with `unless-stopped` restart policy to ensure availability across system reboots.\n\n**Sources:** [infra/docker-compose.yml:2-9]()\n\n### Volume Mounts\n\nTwo critical volumes are mounted:\n\n```yaml\nvolumes:\n  - /var/run/docker.sock:/var/run/docker.sock:ro\n  - ${TRAEFIK_DATA_DIR:-./volumes/traefik-data}:/data\n```\n\n| Volume | Mode | Purpose |\n|--------|------|---------|\n| `/var/run/docker.sock` | read-only | Docker API access for service discovery |\n| `${TRAEFIK_DATA_DIR}:/data` | read-write | Persistent storage for ACME certificates and configuration |\n\nThe Docker socket mount enables Traefik to monitor container events and automatically update routing configuration. The `/data` volume persists Let's Encrypt certificates in `/data/acme.json`.\n\n**Sources:** [infra/docker-compose.yml:10-12]()\n\n---\n\n## Command-Line Configuration\n\nTraefik is configured entirely via command-line flags, enabling zero-config startup once environment variables are set.\n\n### Provider Configuration\n\n```\n--providers.docker=true\n--providers.docker.exposedbydefault=false\n```\n\nThe Docker provider enables automatic service discovery. Setting `exposedbydefault=false` requires explicit opt-in via `traefik.enable=true` labels, preventing accidental exposure of services.\n\n**Mermaid: Docker Provider Service Discovery Flow**\n\n```mermaid\ngraph TD\n    Docker[\"Docker Engine\u003cbr/\u003e/var/run/docker.sock\"]\n    Traefik[\"Traefik Container\u003cbr/\u003eproviders.docker=true\"]\n    Labels[\"Container Labels\u003cbr/\u003etraefik.enable=true\u003cbr/\u003etraefik.http.routers.*\"]\n    Routes[\"Active Routes\u003cbr/\u003eRuntime Configuration\"]\n    \n    Docker --\u003e|\"Container events\u003cbr/\u003e(start/stop/update)\"| Traefik\n    Labels --\u003e|\"Read from containers\"| Traefik\n    Traefik --\u003e|\"Build routing table\"| Routes\n```\n\n**Sources:** [infra/docker-compose.yml:16-17]()\n\n### Entry Points\n\n```\n--entrypoints.web.address=:80\n--entrypoints.websecure.address=:443\n--entrypoints.postgres.address=:5432\n```\n\nThree entry points are defined:\n\n| Entry Point | Address | Protocol | Usage |\n|-------------|---------|----------|-------|\n| `web` | `:80` | HTTP | Initial requests, redirects to HTTPS |\n| `websecure` | `:443` | HTTPS | Secure HTTP traffic for all web services |\n| `postgres` | `:5432` | TCP | Direct PostgreSQL database connections |\n\nThe `postgres` entry point enables TCP-level routing for database traffic, distinct from HTTP routing.\n\n**Sources:** [infra/docker-compose.yml:18-20]()\n\n### Automatic HTTPS Redirection\n\n```\n--entrypoints.web.http.redirections.entrypoint.to=websecure\n--entrypoints.web.http.redirections.entrypoint.scheme=https\n```\n\nAll traffic arriving on the `web` entry point (port 80) is automatically redirected to `websecure` (port 443) with the HTTPS scheme, enforcing encryption for all services.\n\n**Sources:** [infra/docker-compose.yml:21-22]()\n\n### Let's Encrypt Integration\n\n```\n--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}\n--certificatesresolvers.letsencrypt.acme.storage=/data/acme.json\n--certificatesresolvers.letsencrypt.acme.httpchallenge=true\n--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web\n```\n\nThe `letsencrypt` certificate resolver automatically provisions SSL certificates:\n\n- **Email**: `${ACME_EMAIL}` receives certificate expiration notifications\n- **Storage**: `/data/acme.json` persists certificates across restarts\n- **Challenge**: HTTP-01 challenge on the `web` entry point validates domain ownership\n\nCertificate renewal occurs automatically approximately 30 days before expiration.\n\n**Sources:** [infra/docker-compose.yml:23-26](), [infra/.env.template:3]()\n\n### Logging Configuration\n\n```\n--log.level=INFO\n--accesslog=true\n```\n\n- **Log Level**: `INFO` provides operational visibility without excessive verbosity\n- **Access Logs**: Enabled for request auditing and debugging\n\n**Sources:** [infra/docker-compose.yml:27-28]()\n\n---\n\n## Dashboard Access\n\nTraefik exposes its own dashboard via self-referential labels.\n\n### Dashboard Configuration\n\n```yaml\nlabels:\n  - \"traefik.enable=true\"\n  - \"traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN}`)\"\n  - \"traefik.http.routers.traefik.entrypoints=websecure\"\n  - \"traefik.http.routers.traefik.tls.certresolver=letsencrypt\"\n  - \"traefik.http.routers.traefik.service=api@internal\"\n  - \"traefik.http.routers.traefik.middlewares=auth\"\n  - \"traefik.http.middlewares.auth.basicauth.users=${TRAEFIK_AUTH_USER}:${TRAEFIK_AUTH_PASSWORD_HASH}\"\n```\n\n| Label Key | Value | Purpose |\n|-----------|-------|---------|\n| `traefik.enable` | `true` | Enables routing for Traefik itself |\n| `traefik.http.routers.traefik.rule` | `Host(\\`traefik.${DOMAIN}\\`)` | Routes requests for subdomain |\n| `traefik.http.routers.traefik.service` | `api@internal` | Special internal service for dashboard |\n| `traefik.http.routers.traefik.middlewares` | `auth` | Applies basic authentication |\n| `traefik.http.middlewares.auth.basicauth.users` | `${TRAEFIK_AUTH_USER}:${TRAEFIK_AUTH_PASSWORD_HASH}` | Credentials |\n\nThe dashboard is accessible at `https://traefik.${DOMAIN}` and protected by basic authentication. The `api@internal` service is a built-in Traefik service that doesn't require external container configuration.\n\n**Sources:** [infra/docker-compose.yml:29-36](), [infra/.env.template:7-9]()\n\n### API Dashboard Flag\n\n```\n--api.dashboard=true\n--api.insecure=false\n```\n\nThe dashboard is enabled but not exposed insecurely on port 8080. Access is only via the authenticated HTTPS route.\n\n**Sources:** [infra/docker-compose.yml:14-15]()\n\n---\n\n## Routing Patterns\n\nTraefik routes traffic based on Docker labels attached to each service container. This section documents the routing patterns used across the infrastructure.\n\n### HTTP Router Configuration\n\n**Mermaid: HTTP Routing Architecture**\n\n```mermaid\ngraph LR\n    Client[\"External Client\u003cbr/\u003eHTTPS Request\"]\n    Traefik[\"Traefik\u003cbr/\u003e:443\"]\n    \n    Homepage[\"homepage container\u003cbr/\u003e:3000\"]\n    MLflow[\"mlflow container\u003cbr/\u003e:5000\"]\n    MinioAPI[\"minio container\u003cbr/\u003e:9000\"]\n    MinioConsole[\"minio container\u003cbr/\u003e:9001\"]\n    Rustdesk[\"hbbs container\u003cbr/\u003e:21118\"]\n    Dashboard[\"Traefik API\u003cbr/\u003eapi@internal\"]\n    \n    Client --\u003e|\"Host: example.com\"| Traefik\n    Client --\u003e|\"Host: mlflow.example.com\"| Traefik\n    Client --\u003e|\"Host: minio-api.example.com\"| Traefik\n    Client --\u003e|\"Host: minio.example.com\"| Traefik\n    Client --\u003e|\"Host: rustdesk.example.com\"| Traefik\n    Client --\u003e|\"Host: traefik.example.com\"| Traefik\n    \n    Traefik --\u003e|\"Route: homepage\"| Homepage\n    Traefik --\u003e|\"Route: mlflow\"| MLflow\n    Traefik --\u003e|\"Route: minio-api\"| MinioAPI\n    Traefik --\u003e|\"Route: minio-console\"| MinioConsole\n    Traefik --\u003e|\"Route: rustdesk\"| Rustdesk\n    Traefik --\u003e|\"Route: traefik\"| Dashboard\n```\n\n**Sources:** [infra/docker-compose.yml:40-176]()\n\n### Homepage Service Routing\n\n```yaml\nhomepage:\n  labels:\n    - \"traefik.enable=true\"\n    - \"traefik.http.routers.homepage.rule=Host(`${DOMAIN}`) || Host(`www.${DOMAIN}`)\"\n    - \"traefik.http.routers.homepage.entrypoints=websecure\"\n    - \"traefik.http.routers.homepage.tls.certresolver=letsencrypt\"\n    - \"traefik.http.services.homepage.loadbalancer.server.port=3000\"\n```\n\nThe homepage service demonstrates a multi-host routing rule, accepting both apex domain and www subdomain. The router named `homepage` forwards to the service also named `homepage`, which load balances to port 3000.\n\n**Sources:** [infra/docker-compose.yml:52-58]()\n\n### MLflow Service Routing\n\n```yaml\nmlflow:\n  labels:\n    - \"traefik.enable=true\"\n    - \"traefik.http.routers.mlflow.rule=Host(`mlflow.${DOMAIN}`)\"\n    - \"traefik.http.routers.mlflow.entrypoints=websecure\"\n    - \"traefik.http.routers.mlflow.tls.certresolver=letsencrypt\"\n    - \"traefik.http.services.mlflow.loadbalancer.server.port=5000\"\n```\n\nStandard single-subdomain routing pattern. The `mlflow` router uses the `letsencrypt` certificate resolver to automatically provision SSL certificates for the subdomain.\n\n**Sources:** [infra/docker-compose.yml:76-81]()\n\n### MinIO Dual-Service Routing\n\n```yaml\nminio:\n  labels:\n    # API endpoint\n    - \"traefik.http.routers.minio-api.rule=Host(`minio-api.${DOMAIN}`)\"\n    - \"traefik.http.routers.minio-api.service=minio-api\"\n    - \"traefik.http.services.minio-api.loadbalancer.server.port=9000\"\n    \n    # Console endpoint\n    - \"traefik.http.routers.minio-console.rule=Host(`minio.${DOMAIN}`)\"\n    - \"traefik.http.routers.minio-console.service=minio-console\"\n    - \"traefik.http.services.minio-console.loadbalancer.server.port=9001\"\n```\n\nMinIO exposes two distinct HTTP services on different ports, requiring two separate routers. The `minio-api` router forwards to port 9000 (S3 API), while `minio-console` router forwards to port 9001 (web console).\n\n**Sources:** [infra/docker-compose.yml:98-110]()\n\n### PostgreSQL TCP Routing\n\n```yaml\npostgres:\n  labels:\n    - \"traefik.enable=true\"\n    - \"traefik.tcp.routers.postgres.rule=HostSNI(`postgres.${DOMAIN}`)\"\n    - \"traefik.tcp.routers.postgres.entrypoints=postgres\"\n    - \"traefik.tcp.routers.postgres.service=postgres\"\n    - \"traefik.tcp.services.postgres.loadbalancer.server.port=5432\"\n    - \"traefik.tcp.routers.postgres.tls=true\"\n    - \"traefik.tcp.routers.postgres.tls.certresolver=letsencrypt\"\n```\n\nPostgreSQL uses TCP routing instead of HTTP routing. Key differences:\n\n- Uses `traefik.tcp.routers.*` instead of `traefik.http.routers.*`\n- Routing rule uses `HostSNI` (Server Name Indication) for TLS-based routing\n- Entry point is `postgres` (port 5432) instead of `websecure`\n- TLS is explicitly enabled at the TCP level\n\n**Sources:** [infra/docker-compose.yml:127-133]()\n\n---\n\n## Label Reference\n\nAll services route through Traefik by specifying Docker labels. This section documents the label patterns.\n\n### Common Label Patterns\n\n**Mermaid: Label Configuration Flow**\n\n```mermaid\ngraph TD\n    Container[\"Service Container\u003cbr/\u003e(e.g., mlflow)\"]\n    EnableLabel[\"traefik.enable=true\"]\n    RouterLabels[\"Router Labels\u003cbr/\u003etraefik.http.routers.*\"]\n    ServiceLabels[\"Service Labels\u003cbr/\u003etraefik.http.services.*\"]\n    TraefikConfig[\"Traefik Runtime\u003cbr/\u003eConfiguration\"]\n    \n    Container --\u003e EnableLabel\n    Container --\u003e RouterLabels\n    Container --\u003e ServiceLabels\n    \n    EnableLabel --\u003e TraefikConfig\n    RouterLabels --\u003e TraefikConfig\n    ServiceLabels --\u003e TraefikConfig\n    \n    RouterLabels --\u003e RuleLabel[\"rule=Host(...)\"]\n    RouterLabels --\u003e EntrypointLabel[\"entrypoints=websecure\"]\n    RouterLabels --\u003e TLSLabel[\"tls.certresolver=letsencrypt\"]\n    RouterLabels --\u003e ServiceRef[\"service=\u003cname\u003e\"]\n    \n    ServiceLabels --\u003e PortLabel[\"loadbalancer.server.port=\u003cport\u003e\"]\n```\n\n**Sources:** [infra/docker-compose.yml:52-160]()\n\n### Label Schema\n\n#### Router Labels\n\n| Label Pattern | Purpose | Example |\n|---------------|---------|---------|\n| `traefik.enable` | Opt-in to Traefik routing | `true` |\n| `traefik.http.routers.\u003cname\u003e.rule` | Host-based routing rule | `Host(\\`mlflow.example.com\\`)` |\n| `traefik.http.routers.\u003cname\u003e.entrypoints` | Entry point selection | `websecure` |\n| `traefik.http.routers.\u003cname\u003e.tls.certresolver` | Certificate resolver | `letsencrypt` |\n| `traefik.http.routers.\u003cname\u003e.service` | Target service reference | `mlflow` |\n| `traefik.http.routers.\u003cname\u003e.middlewares` | Middleware chain | `auth` |\n\n#### Service Labels\n\n| Label Pattern | Purpose | Example |\n|---------------|---------|---------|\n| `traefik.http.services.\u003cname\u003e.loadbalancer.server.port` | Backend container port | `5000` |\n| `traefik.http.services.\u003cname\u003e.loadbalancer.server.url` | Complete backend URL | `http://mlflow:5000` |\n\n#### TCP-Specific Labels\n\n| Label Pattern | Purpose | Example |\n|---------------|---------|---------|\n| `traefik.tcp.routers.\u003cname\u003e.rule` | TCP routing rule | `HostSNI(\\`postgres.example.com\\`)` |\n| `traefik.tcp.routers.\u003cname\u003e.tls` | Enable TLS | `true` |\n| `traefik.tcp.services.\u003cname\u003e.loadbalancer.server.port` | TCP backend port | `5432` |\n\n**Sources:** [infra/docker-compose.yml:29-160]()\n\n---\n\n## Middleware Configuration\n\nTraefik supports middleware for request/response transformation. The infrastructure uses basic authentication middleware.\n\n### Basic Auth Middleware\n\n```yaml\nlabels:\n  - \"traefik.http.middlewares.auth.basicauth.users=${TRAEFIK_AUTH_USER}:${TRAEFIK_AUTH_PASSWORD_HASH}\"\n  - \"traefik.http.routers.traefik.middlewares=auth\"\n```\n\nThe `auth` middleware is defined globally on the Traefik service itself and can be referenced by any router. The credentials are stored as `username:hashed-password`, where the hash is bcrypt format.\n\nTo generate a password hash:\n\n```bash\nhtpasswd -nb admin password\n# Or using Python:\npython3 -c \"import bcrypt; print(bcrypt.hashpw(b'password', bcrypt.gensalt()).decode())\"\n```\n\nThe hash must have dollar signs (`$`) escaped as `$$` in Docker Compose files.\n\n**Sources:** [infra/docker-compose.yml:35-36](), [infra/.env.template:7-9]()\n\n---\n\n## Network Configuration\n\nAll services route through Traefik via a shared Docker network.\n\n### Network Definition\n\n```yaml\nnetworks:\n  traefik-network:\n    name: traefik-network\n    driver: bridge\n```\n\nThe `traefik-network` uses the bridge driver, creating an isolated network segment. All services that need external access must attach to this network.\n\n### Service Network Attachment\n\n```yaml\nservices:\n  traefik:\n    networks:\n      - traefik-network\n  \n  mlflow:\n    networks:\n      - traefik-network\n```\n\nEach service explicitly declares network membership. Traefik can only route to services on networks it has access to.\n\n**Sources:** [infra/docker-compose.yml:37-38](), [infra/docker-compose.yml:177-180]()\n\n---\n\n## Environment Variables\n\nTraefik configuration depends on several environment variables defined in `.env`.\n\n### Required Variables\n\n| Variable | Purpose | Example |\n|----------|---------|---------|\n| `DOMAIN` | Base domain for all services | `example.com` |\n| `ACME_EMAIL` | Let's Encrypt notification email | `admin@example.com` |\n| `TRAEFIK_DATA_DIR` | Volume path for certificate storage | `./volumes/traefik-data` |\n| `TRAEFIK_AUTH_USER` | Dashboard username | `admin` |\n| `TRAEFIK_AUTH_PASSWORD_HASH` | Bcrypt hash of dashboard password | `$2y$05$...` |\n\n### Usage in Configuration\n\n```yaml\ncommand:\n  - --certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}\nlabels:\n  - \"traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN}`)\"\n  - \"traefik.http.middlewares.auth.basicauth.users=${TRAEFIK_AUTH_USER}:${TRAEFIK_AUTH_PASSWORD_HASH}\"\n```\n\nEnvironment variable substitution occurs at Docker Compose startup, generating the final configuration.\n\n**Sources:** [infra/.env.template:1-9](), [infra/docker-compose.yml:23](), [infra/docker-compose.yml:31](), [infra/docker-compose.yml:36]()\n\n---\n\n## Certificate Management\n\nLet's Encrypt certificates are automatically provisioned, renewed, and stored.\n\n### Certificate Storage\n\nCertificates are stored in `/data/acme.json` inside the container, backed by the `${TRAEFIK_DATA_DIR}` volume on the host. The file has the following structure:\n\n```json\n{\n  \"letsencrypt\": {\n    \"Account\": {...},\n    \"Certificates\": [\n      {\n        \"domain\": {\"main\": \"example.com\"},\n        \"certificate\": \"...\",\n        \"key\": \"...\"\n      }\n    ]\n  }\n}\n```\n\n**Important**: The `acme.json` file must have 600 permissions (`-rw-------`) to prevent security warnings.\n\n### Certificate Lifecycle\n\n**Mermaid: Certificate Provisioning Flow**\n\n```mermaid\nsequenceDiagram\n    participant Client as External Client\n    participant Traefik as Traefik Container\n    participant LE as Let's Encrypt CA\n    participant Service as Backend Service\n    \n    Client-\u003e\u003eTraefik: HTTPS request (first time)\n    Traefik-\u003e\u003eTraefik: Check /data/acme.json\n    Traefik-\u003e\u003eTraefik: No certificate found\n    Traefik-\u003e\u003eLE: Request certificate for domain\n    LE-\u003e\u003eTraefik: HTTP-01 challenge\n    Traefik-\u003e\u003eLE: Respond on :80/.well-known/acme-challenge/\n    LE-\u003e\u003eLE: Validate domain ownership\n    LE-\u003e\u003eTraefik: Issue certificate + private key\n    Traefik-\u003e\u003eTraefik: Store in /data/acme.json\n    Traefik-\u003e\u003eClient: Complete TLS handshake\n    Traefik-\u003e\u003eService: Forward request to backend\n    Service-\u003e\u003eTraefik: Response\n    Traefik-\u003e\u003eClient: Encrypted response\n    \n    Note over Traefik,LE: Renewal (30 days before expiry)\n    Traefik-\u003e\u003eLE: Request renewal\n    LE-\u003e\u003eTraefik: New certificate\n    Traefik-\u003e\u003eTraefik: Update /data/acme.json\n```\n\n**Sources:** [infra/docker-compose.yml:23-26]()\n\n### Multi-Domain Certificates\n\nEach service with a unique subdomain gets its own certificate:\n\n- `example.com` (homepage)\n- `mlflow.example.com`\n- `minio-api.example.com`\n- `minio.example.com`\n- `postgres.example.com`\n- `rustdesk.example.com`\n- `traefik.example.com`\n\nLet's Encrypt has rate limits of 50 certificates per registered domain per week, which is sufficient for typical deployments.\n\n**Sources:** [infra/docker-compose.yml:54](), [infra/docker-compose.yml:77](), [infra/docker-compose.yml:99](), [infra/docker-compose.yml:105](), [infra/docker-compose.yml:128](), [infra/docker-compose.yml:156](), [infra/docker-compose.yml:31]()\n\n---\n\n## Complete Routing Table\n\nThis table summarizes all routes configured in the infrastructure stack.\n\n| Subdomain | Service | Container Port | Router Name | Protocol | Middleware |\n|-----------|---------|----------------|-------------|----------|------------|\n| `${DOMAIN}` | homepage | 3000 | `homepage` | HTTPS | - |\n| `www.${DOMAIN}` | homepage | 3000 | `homepage` | HTTPS | - |\n| `mlflow.${DOMAIN}` | mlflow | 5000 | `mlflow` | HTTPS | - |\n| `minio-api.${DOMAIN}` | minio | 9000 | `minio-api` | HTTPS | - |\n| `minio.${DOMAIN}` | minio | 9001 | `minio-console` | HTTPS | - |\n| `postgres.${DOMAIN}` | postgres | 5432 | `postgres` | TLS/TCP | - |\n| `rustdesk.${DOMAIN}` | hbbs | 21118 | `rustdesk` | HTTPS | - |\n| `traefik.${DOMAIN}` | traefik | `api@internal` | `traefik` | HTTPS | `auth` |\n\n**Sources:** [infra/docker-compose.yml:1-176]()\n\n---\n\n## Operational Considerations\n\n### Monitoring Traefik\n\nAccess the dashboard at `https://traefik.${DOMAIN}` to view:\n\n- Active routers and their matching rules\n- Configured services and health status\n- Middleware chains\n- Certificate status\n- Recent access logs\n\n### Troubleshooting Common Issues\n\n#### Certificate Provisioning Failures\n\nIf certificates fail to provision:\n\n1. Verify DNS records point to the server\n2. Ensure ports 80 and 443 are open in firewall\n3. Check `acme.json` file permissions (must be 600)\n4. Review Traefik logs: `docker logs traefik`\n5. Verify `ACME_EMAIL` is set correctly\n\n#### Service Not Routing\n\nIf a service is not accessible:\n\n1. Verify `traefik.enable=true` label is present\n2. Check router labels for syntax errors\n3. Confirm service is on `traefik-network`\n4. Review Traefik dashboard for router status\n5. Check service logs for binding errors\n\n#### SSL Mixed Content Warnings\n\nIf browsers show mixed content warnings:\n\n1. Ensure all `entrypoints` use `websecure`\n2. Verify HTTP to HTTPS redirection is working\n3. Check internal service URLs use `http://` (not `https://`)\n4. Traefik handles SSL termination; backend services use plain HTTP\n\n**Sources:** [infra/docker-compose.yml:1-184]()\n\n---\n\n## Security Considerations\n\n### Docker Socket Access\n\nTraefik mounts `/var/run/docker.sock` with read-only access. This grants Traefik permission to query container metadata but not modify containers. This is necessary for service discovery but represents a privileged operation.\n\n### Basic Auth Credentials\n\nThe Traefik dashboard uses basic authentication. Best practices:\n\n- Use strong passwords (minimum 16 characters)\n- Rotate credentials periodically\n- Never commit plaintext passwords to version control\n- Use bcrypt hashing for password storage\n\n### Certificate Storage\n\nThe `acme.json` file contains private keys for all certificates. Protect this file:\n\n- Ensure 600 permissions (`chmod 600 acme.json`)\n- Include in backup procedures\n- Exclude from version control (add to `.gitignore`)\n\n### Network Isolation\n\nThe `traefik-network` provides a security boundary. Services not on this network cannot be reached via Traefik, providing defense-in-depth.\n\n**Sources:** [infra/docker-compose.yml:11](), [infra/docker-compose.yml:12](), [infra/docker-compose.yml:36](), [infra/docker-compose.yml:177-180]()"])</script><script>self.__next_f.push([1,"28:T4ab1,"])</script><script>self.__next_f.push([1,"# MLflow Tracking Server\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.dockerignore](.dockerignore)\n- [.github/workflows/docker-publish.yml](.github/workflows/docker-publish.yml)\n- [.github/workflows/mlflow-image-publish.yml](.github/workflows/mlflow-image-publish.yml)\n- [dockerfile.mlflow](dockerfile.mlflow)\n- [infra/docker-compose.yml](infra/docker-compose.yml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document details the MLflow Tracking Server service within the OpenGeoAIModelHub infrastructure stack. It covers the service configuration, custom Docker image, integration with backend stores (PostgreSQL and MinIO), and the automated image publishing pipeline.\n\nFor information about other infrastructure services, see [Service Architecture](#4.1). For details about storage services configuration, see [Storage Services (MinIO and PostgreSQL)](#4.6). For guidance on using MLflow with the example model training pipeline, see [Training Pipeline](#3.2).\n\n## Service Overview\n\nMLflow is the central experiment tracking and model registry service in the infrastructure stack. It implements the standard two-store architecture pattern: PostgreSQL stores metadata (experiments, runs, metrics, parameters), while MinIO stores large artifacts (models, datasets, plots). The service is exposed via Traefik at `mlflow.${DOMAIN}` with automatic SSL certificate provisioning.\n\n```mermaid\ngraph TB\n    subgraph \"MLflow Service Container\"\n        MLFLOW_SERVER[\"mlflow server\u003cbr/\u003ePort 5000\"]\n        MLFLOW_BINARY[\"/usr/local/bin/mlflow\"]\n    end\n    \n    subgraph \"Backend Store\"\n        POSTGRES[\"PostgreSQL Database\u003cbr/\u003epostgres:5432\u003cbr/\u003ePOSTGRES_DB\"]\n        PSYCOPG2[\"psycopg2-binary\u003cbr/\u003ePython Driver\"]\n    end\n    \n    subgraph \"Artifact Store\"\n        MINIO[\"MinIO S3 Storage\u003cbr/\u003eminio:9000\u003cbr/\u003eMINIO_BUCKET_NAME\"]\n        BOTO3[\"boto3\u003cbr/\u003eS3 Client\"]\n    end\n    \n    subgraph \"Access Layer\"\n        TRAEFIK[\"Traefik Reverse Proxy\"]\n        USERS[\"Users/Training Pipeline\"]\n    end\n    \n    USERS --\u003e|\"HTTPS mlflow.${DOMAIN}\"| TRAEFIK\n    TRAEFIK --\u003e|\"HTTP :5000\"| MLFLOW_SERVER\n    \n    MLFLOW_SERVER --\u003e|\"backend-store-uri\"| PSYCOPG2\n    PSYCOPG2 --\u003e|\"postgresql+psycopg2://\"| POSTGRES\n    \n    MLFLOW_SERVER --\u003e|\"default-artifact-root\"| BOTO3\n    BOTO3 --\u003e|\"s3://\"| MINIO\n    \n    MLFLOW_BINARY -.-\u003e|\"uses\"| MLFLOW_SERVER\n    \n    POSTGRES -.-\u003e|\"stores\"| METADATA[\"Experiments\u003cbr/\u003eRuns\u003cbr/\u003eMetrics\u003cbr/\u003eParameters\"]\n    MINIO -.-\u003e|\"stores\"| ARTIFACTS[\"Models\u003cbr/\u003ePlots\u003cbr/\u003eDatasets\u003cbr/\u003eDLPK files\"]\n```\n\n**Diagram: MLflow Two-Store Architecture**\n\nThe service depends on both `postgres` and `minio` containers, ensuring they start before MLflow initializes.\n\nSources: [infra/docker-compose.yml:62-84]()\n\n## Service Configuration\n\nThe MLflow service is defined in the Docker Compose stack with specific environment variables, entrypoint configuration, and service dependencies.\n\n### Container Definition\n\n```yaml\nmlflow:\n  image: ${MLFLOW_IMAGE:-ghcr.io/kshitijrajsharma/opengeoaimodelshub/mlflow:latest}\n  container_name: mlflow\n  restart: unless-stopped\n```\n\nThe service uses a custom-built Docker image, defaulting to `ghcr.io/kshitijrajsharma/opengeoaimodelshub/mlflow:latest` if the `MLFLOW_IMAGE` environment variable is not set. The container automatically restarts unless explicitly stopped.\n\n| Configuration | Value | Purpose |\n|---------------|-------|---------|\n| `image` | `${MLFLOW_IMAGE}` | Custom MLflow image with boto3 and psycopg2 |\n| `container_name` | `mlflow` | Fixed container name for DNS resolution |\n| `restart` | `unless-stopped` | Automatic restart policy |\n| `depends_on` | `minio`, `postgres` | Start order dependencies |\n\nSources: [infra/docker-compose.yml:62-65](), [infra/docker-compose.yml:72-74]()\n\n### Environment Variables\n\nThe service requires five environment variables for S3 and database connectivity:\n\n| Variable | Purpose | Example |\n|----------|---------|---------|\n| `MLFLOW_S3_ENDPOINT_URL` | MinIO endpoint for artifact storage | `http://minio:9000` |\n| `MLFLOW_S3_IGNORE_TLS` | Disable TLS for internal S3 communication | `true` |\n| `AWS_ACCESS_KEY_ID` | MinIO access key (root user) | From `.env` file |\n| `AWS_SECRET_ACCESS_KEY` | MinIO secret key (root password) | From `.env` file |\n\n```mermaid\ngraph LR\n    ENV_FILE[\".env file\"]\n    \n    subgraph \"Environment Variables\"\n        AWS_KEY[\"AWS_ACCESS_KEY_ID\"]\n        AWS_SECRET[\"AWS_SECRET_ACCESS_KEY\"]\n        PG_USER[\"POSTGRES_USER\"]\n        PG_PASS[\"POSTGRES_PASSWORD\"]\n        PG_DB[\"POSTGRES_DB\"]\n        BUCKET[\"MINIO_BUCKET_NAME\"]\n    end\n    \n    subgraph \"MLflow Container\"\n        MLFLOW_ENV[\"Environment:\u003cbr/\u003eMLFLOW_S3_ENDPOINT_URL\u003cbr/\u003eMLFLOW_S3_IGNORE_TLS\u003cbr/\u003eAWS_ACCESS_KEY_ID\u003cbr/\u003eAWS_SECRET_ACCESS_KEY\"]\n        MLFLOW_CMD[\"Entrypoint:\u003cbr/\u003e--backend-store-uri\u003cbr/\u003e--default-artifact-root\u003cbr/\u003e--artifacts-destination\"]\n    end\n    \n    ENV_FILE --\u003e|\"provides\"| AWS_KEY\n    ENV_FILE --\u003e|\"provides\"| AWS_SECRET\n    ENV_FILE --\u003e|\"provides\"| PG_USER\n    ENV_FILE --\u003e|\"provides\"| PG_PASS\n    ENV_FILE --\u003e|\"provides\"| PG_DB\n    ENV_FILE --\u003e|\"provides\"| BUCKET\n    \n    AWS_KEY --\u003e|\"injected\"| MLFLOW_ENV\n    AWS_SECRET --\u003e|\"injected\"| MLFLOW_ENV\n    PG_USER --\u003e|\"interpolated\"| MLFLOW_CMD\n    PG_PASS --\u003e|\"interpolated\"| MLFLOW_CMD\n    PG_DB --\u003e|\"interpolated\"| MLFLOW_CMD\n    BUCKET --\u003e|\"interpolated\"| MLFLOW_CMD\n```\n\n**Diagram: Environment Variable Flow**\n\nThe `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` serve dual purposes: they authenticate to MinIO (where they map to `MINIO_ROOT_USER` and `MINIO_ROOT_PASSWORD`) and provide S3 client credentials to MLflow.\n\nSources: [infra/docker-compose.yml:66-71]()\n\n### Entrypoint Command\n\nThe service overrides the default MLflow image entrypoint with a complete server command:\n\n```bash\nmlflow server \\\n  --backend-store-uri postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB} \\\n  --default-artifact-root s3://${MINIO_BUCKET_NAME}/ \\\n  --artifacts-destination s3://${MINIO_BUCKET_NAME}/ \\\n  -h 0.0.0.0\n```\n\n| Parameter | Purpose | Value |\n|-----------|---------|-------|\n| `--backend-store-uri` | Database connection string for metadata | `postgresql+psycopg2://user:pass@postgres/db` |\n| `--default-artifact-root` | Default S3 path for new experiments | `s3://bucket_name/` |\n| `--artifacts-destination` | Artifact storage location | `s3://bucket_name/` |\n| `-h` | Bind address | `0.0.0.0` (all interfaces) |\n\nThe `backend-store-uri` uses the `postgresql+psycopg2` dialect, requiring the `psycopg2-binary` package installed in the custom Docker image. The hostname `postgres` resolves via Docker's internal DNS to the `postgres` container on the `traefik-network`.\n\nSources: [infra/docker-compose.yml:71]()\n\n## Custom Docker Image\n\nThe MLflow service uses a custom Docker image built on top of the official MLflow image, adding PostgreSQL and S3 support dependencies.\n\n### Dockerfile Structure\n\n```mermaid\ngraph TB\n    BASE[\"Base Image\u003cbr/\u003eghcr.io/mlflow/mlflow:v3.1.1\"]\n    \n    subgraph \"Build Stage: base\"\n        APT[\"apt-get update\u003cbr/\u003eInstall system dependencies\"]\n        REQ[\"Create requirements.txt:\u003cbr/\u003eboto3\u003cbr/\u003epsycopg2-binary\"]\n        PIP[\"pip install -r requirements.txt\"]\n    end\n    \n    subgraph \"Final Stage\"\n        COPY[\"Copy site-packages\u003cbr/\u003eCopy /usr/local/bin\"]\n        WORKDIR[\"Set WORKDIR /app\"]\n        EXPOSE[\"EXPOSE 5000\"]\n    end\n    \n    BASE --\u003e|\"FROM\"| APT\n    APT --\u003e REQ\n    REQ --\u003e PIP\n    PIP --\u003e COPY\n    COPY --\u003e WORKDIR\n    WORKDIR --\u003e EXPOSE\n```\n\n**Diagram: Dockerfile Multi-Stage Build**\n\nThe Dockerfile uses build arguments for version flexibility:\n\n| Build Argument | Default | Purpose |\n|----------------|---------|---------|\n| `MLFLOW_VERSION` | `v3.1.1` | Base MLflow image version |\n| `BOTO3_VERSION` | `\"\"` (latest) | AWS SDK for S3 support |\n| `PSYCOPG2_VERSION` | `\"\"` (latest) | PostgreSQL driver |\n\nThe multi-stage build pattern (`base`  `final`) optimizes the final image size by copying only necessary Python packages and binaries.\n\nSources: [dockerfile.mlflow:1-30]()\n\n### Python Dependencies\n\nThe custom image adds two critical dependencies not included in the base MLflow image:\n\n1. **boto3**: AWS SDK for Python, enabling S3-compatible storage (MinIO) integration\n2. **psycopg2-binary**: PostgreSQL database adapter for Python, required for the `postgresql+psycopg2://` backend-store-uri\n\n```dockerfile\nRUN echo \"boto3${BOTO3_VERSION:+==}${BOTO3_VERSION}\" \u003e /tmp/requirements.txt \u0026\u0026 \\\n    echo \"psycopg2-binary${PSYCOPG2_VERSION:+==}${PSYCOPG2_VERSION}\" \u003e\u003e /tmp/requirements.txt\n\nRUN pip install --no-cache-dir -r /tmp/requirements.txt \u0026\u0026 \\\n    rm /tmp/requirements.txt\n```\n\nThe build process conditionally pins versions if build arguments are provided, otherwise installs latest versions.\n\nSources: [dockerfile.mlflow:16-20]()\n\n## Backend Store Integration\n\nMLflow uses PostgreSQL as the backend store for all experiment metadata. The connection is established via SQLAlchemy's PostgreSQL dialect with psycopg2 as the driver.\n\n### Database Connection\n\nThe connection string format is:\n```\npostgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}\n```\n\n| Component | Purpose |\n|-----------|---------|\n| `postgresql+psycopg2` | SQLAlchemy dialect + driver specification |\n| `${POSTGRES_USER}` | Database username (from environment) |\n| `${POSTGRES_PASSWORD}` | Database password (from environment) |\n| `@postgres` | Hostname (Docker service name) |\n| `/${POSTGRES_DB}` | Database name (from environment) |\n\nThe hostname `postgres` is resolved by Docker's embedded DNS server to the IP address of the `postgres` container on the `traefik-network`.\n\n### Stored Metadata\n\nThe backend store persists the following entities:\n\n| Entity Type | Content | Example |\n|-------------|---------|---------|\n| Experiments | Project-level organization | \"refugee-camp-detection\" |\n| Runs | Individual training executions | Run UUID, timestamps |\n| Metrics | Time-series measurements | Loss, accuracy, F1-score |\n| Parameters | Hyperparameters | Learning rate, batch size |\n| Tags | Metadata annotations | \"production\", \"baseline\" |\n\nSources: [infra/docker-compose.yml:71]()\n\n## Artifact Store Integration\n\nMLflow stores large binary artifacts in MinIO using the S3 protocol. The artifact storage is configured with two related parameters.\n\n### S3 Configuration\n\n```bash\n--default-artifact-root s3://${MINIO_BUCKET_NAME}/\n--artifacts-destination s3://${MINIO_BUCKET_NAME}/\n```\n\nBoth parameters point to the same S3 bucket but serve different purposes:\n- `default-artifact-root`: Default location for new experiments\n- `artifacts-destination`: Explicit override for artifact storage\n\nThe S3 endpoint is configured via environment variables rather than command-line arguments:\n\n```yaml\nenvironment:\n  MLFLOW_S3_ENDPOINT_URL: \"http://minio:9000\"\n  MLFLOW_S3_IGNORE_TLS: \"true\"\n```\n\n| Variable | Value | Purpose |\n|----------|-------|---------|\n| `MLFLOW_S3_ENDPOINT_URL` | `http://minio:9000` | MinIO API endpoint |\n| `MLFLOW_S3_IGNORE_TLS` | `true` | Disable TLS verification for internal traffic |\n\nThe `boto3` library reads these environment variables and configures the S3 client to connect to MinIO instead of AWS S3.\n\n### Stored Artifacts\n\nThe artifact store persists the following file types:\n\n| Artifact Type | File Extension | Example Use Case |\n|---------------|----------------|------------------|\n| PyTorch models | `.pth`, `.pt` | Model checkpoints |\n| ONNX models | `.onnx` | Cross-platform inference |\n| ESRI packages | `.dlpk` | ArcGIS deployment |\n| STAC metadata | `.json` | Model metadata |\n| Plots | `.png`, `.jpg` | Confusion matrices, visualizations |\n| Datasets | `.csv`, `.parquet` | Training/validation data |\n\nSources: [infra/docker-compose.yml:67-71]()\n\n## Network and Routing Configuration\n\nThe MLflow service is exposed through Traefik using Docker labels for automatic service discovery and routing configuration.\n\n### Traefik Labels\n\n```yaml\nlabels:\n  - \"traefik.enable=true\"\n  - \"traefik.http.routers.mlflow.rule=Host(`mlflow.${DOMAIN}`)\"\n  - \"traefik.http.routers.mlflow.entrypoints=websecure\"\n  - \"traefik.http.routers.mlflow.tls.certresolver=letsencrypt\"\n  - \"traefik.http.services.mlflow.loadbalancer.server.port=5000\"\n  - \"traefik.http.services.mlflow.loadbalancer.server.url=http://mlflow:5000\"\n```\n\n```mermaid\ngraph LR\n    INTERNET[\"Internet\"]\n    \n    subgraph \"Traefik Edge Router\"\n        HTTP[\"HTTP :80\"]\n        HTTPS[\"HTTPS :443\"]\n        ROUTER[\"HTTP Router:\u003cbr/\u003emlflow\"]\n        MIDDLEWARE[\"TLS Termination\u003cbr/\u003eLet's Encrypt\"]\n        LB[\"Load Balancer\"]\n    end\n    \n    subgraph \"MLflow Container\"\n        MLFLOW[\"MLflow Server\u003cbr/\u003e:5000\"]\n    end\n    \n    INTERNET --\u003e|\"HTTP\"| HTTP\n    INTERNET --\u003e|\"HTTPS\"| HTTPS\n    \n    HTTP --\u003e|\"redirect to HTTPS\"| HTTPS\n    HTTPS --\u003e|\"Host: mlflow.${DOMAIN}\"| ROUTER\n    ROUTER --\u003e MIDDLEWARE\n    MIDDLEWARE --\u003e LB\n    LB --\u003e|\"http://mlflow:5000\"| MLFLOW\n```\n\n**Diagram: Traefik Routing to MLflow Service**\n\n| Label | Purpose |\n|-------|---------|\n| `traefik.enable=true` | Enable Traefik for this service |\n| `traefik.http.routers.mlflow.rule=Host(...)` | Route based on hostname |\n| `traefik.http.routers.mlflow.entrypoints=websecure` | Use HTTPS entrypoint (:443) |\n| `traefik.http.routers.mlflow.tls.certresolver=letsencrypt` | Automatic SSL certificates |\n| `traefik.http.services.mlflow.loadbalancer.server.port=5000` | Backend container port |\n\n### Network Configuration\n\nThe service is connected to the `traefik-network` bridge network, enabling communication with other services:\n\n```yaml\nnetworks:\n  - traefik-network\n```\n\nThis network provides:\n1. **DNS resolution**: Service names (e.g., `postgres`, `minio`) resolve to container IPs\n2. **Isolation**: Services on this network can communicate; external containers cannot\n3. **Traefik integration**: Enables Traefik to proxy requests to the service\n\nSources: [infra/docker-compose.yml:75-83]()\n\n## Image Publishing Pipeline\n\nThe custom MLflow Docker image is automatically built and published via GitHub Actions when the Dockerfile or workflow is modified.\n\n### Workflow Triggers\n\nThe workflow [.github/workflows/mlflow-image-publish.yml]() executes on:\n\n```yaml\non:\n  push:\n    branches: [main, master]\n    paths:\n      - \"dockerfile.mlflow\"\n      - \".github/workflows/build-mlflow.yml\"\n  pull_request:\n    branches: [main, master]\n    paths:\n      - \"dockerfile.mlflow\"\n  workflow_dispatch:\n```\n\n| Trigger | Condition | Action |\n|---------|-----------|--------|\n| `push` | Dockerfile or workflow changes on main/master | Build and push |\n| `pull_request` | Dockerfile changes in PR | Build only (no push) |\n| `workflow_dispatch` | Manual trigger | Build and push |\n\n### Build Process\n\n```mermaid\ngraph TB\n    TRIGGER[\"GitHub Push Event\"]\n    \n    subgraph \"GitHub Actions Workflow\"\n        CHECKOUT[\"Checkout repository\u003cbr/\u003eactions/checkout@v4\"]\n        BUILDX[\"Setup Docker Buildx\u003cbr/\u003edocker/setup-buildx-action@v3\"]\n        LOGIN[\"Login to GHCR\u003cbr/\u003edocker/login-action@v3\"]\n        META[\"Extract metadata\u003cbr/\u003edocker/metadata-action@v5\"]\n        BUILD[\"Build and push\u003cbr/\u003edocker/build-push-action@v5\"]\n    end\n    \n    subgraph \"Build Configuration\"\n        CONTEXT[\"Context: .\"]\n        DOCKERFILE[\"File: dockerfile.mlflow\"]\n        PLATFORMS[\"Platforms:\u003cbr/\u003elinux/amd64\u003cbr/\u003elinux/arm64\"]\n        CACHE[\"Cache: GitHub Actions\"]\n    end\n    \n    subgraph \"Output\"\n        GHCR[\"GitHub Container Registry\u003cbr/\u003eghcr.io/.../mlflow:latest\"]\n        TAGS[\"Tags:\u003cbr/\u003e:latest\u003cbr/\u003e:master\u003cbr/\u003e:master-{sha}\"]\n    end\n    \n    TRIGGER --\u003e CHECKOUT\n    CHECKOUT --\u003e BUILDX\n    BUILDX --\u003e LOGIN\n    LOGIN --\u003e META\n    META --\u003e BUILD\n    \n    CONTEXT -.-\u003e|\"input\"| BUILD\n    DOCKERFILE -.-\u003e|\"input\"| BUILD\n    PLATFORMS -.-\u003e|\"input\"| BUILD\n    CACHE -.-\u003e|\"input\"| BUILD\n    \n    BUILD --\u003e|\"push\"| GHCR\n    BUILD --\u003e|\"generates\"| TAGS\n```\n\n**Diagram: CI/CD Pipeline for MLflow Image**\n\n### Multi-Platform Build\n\nThe workflow builds for both AMD64 and ARM64 architectures:\n\n```yaml\nplatforms: linux/amd64,linux/arm64\n```\n\nThis enables deployment on:\n- **x86_64 servers**: Traditional cloud VMs, data center hardware\n- **ARM64 servers**: AWS Graviton, Apple Silicon, Raspberry Pi clusters\n\nThe build cache is stored in GitHub Actions cache to speed up subsequent builds:\n\n```yaml\ncache-from: type=gha\ncache-to: type=gha,mode=max\n```\n\n### Tag Strategy\n\nThe workflow generates multiple tags for version management:\n\n| Tag Format | Example | Purpose |\n|------------|---------|---------|\n| `type=ref,event=branch` | `master` | Branch-based tag |\n| `type=sha,prefix={{branch}}-` | `master-a1b2c3d` | Commit-specific tag |\n| `type=raw,value=latest` | `latest` | Latest stable version |\n\nSources: [.github/workflows/mlflow-image-publish.yml:1-81]()\n\n## Service Dependencies\n\nThe MLflow service declares explicit dependencies on storage services:\n\n```yaml\ndepends_on:\n  - minio\n  - postgres\n```\n\nThis ensures:\n1. PostgreSQL starts before MLflow (backend store availability)\n2. MinIO starts before MLflow (artifact store availability)\n3. Docker Compose orders the startup sequence correctly\n\nHowever, `depends_on` only waits for containers to start, not for services to be ready. MLflow implements retry logic to handle cases where PostgreSQL or MinIO take additional time to initialize.\n\n```mermaid\nsequenceDiagram\n    participant DC as docker-compose\n    participant PG as postgres\n    participant MC as minio\n    participant MF as mlflow\n    \n    DC-\u003e\u003ePG: Start postgres container\n    DC-\u003e\u003eMC: Start minio container\n    PG-\u003e\u003ePG: Initialize database\n    MC-\u003e\u003eMC: Initialize S3 storage\n    \n    DC-\u003e\u003eMF: Start mlflow container\n    MF-\u003e\u003ePG: Connect to PostgreSQL\n    alt Database not ready\n        PG--\u003e\u003eMF: Connection refused\n        MF-\u003e\u003eMF: Retry connection\n    end\n    PG--\u003e\u003eMF: Connection established\n    \n    MF-\u003e\u003eMC: Connect to MinIO S3\n    MC--\u003e\u003eMF: S3 client initialized\n    \n    MF-\u003e\u003eMF: Start MLflow server on :5000\n```\n\n**Diagram: Service Startup Sequence**\n\nSources: [infra/docker-compose.yml:72-74]()\n\n## Access and Authentication\n\nThe MLflow UI and API are accessible at `https://mlflow.${DOMAIN}` after Traefik provisions an SSL certificate. The service does not implement authentication by default; access control relies on network-level security (firewall rules, VPN) or can be added via Traefik middleware.\n\n### API Endpoints\n\n| Endpoint | Purpose |\n|----------|---------|\n| `https://mlflow.${DOMAIN}/` | MLflow UI dashboard |\n| `https://mlflow.${DOMAIN}/api/2.0/mlflow/experiments/list` | List experiments |\n| `https://mlflow.${DOMAIN}/api/2.0/mlflow/runs/search` | Search runs |\n| `https://mlflow.${DOMAIN}/#/models` | Model registry UI |\n\n### Client Configuration\n\nTraining scripts connect to MLflow using the tracking URI:\n\n```python\nimport mlflow\n\nmlflow.set_tracking_uri(\"https://mlflow.yourdomain.com\")\n```\n\nThe client automatically handles SSL verification and communicates with both the backend store (via MLflow API) and artifact store (direct S3 connection to MinIO).\n\nSources: [infra/docker-compose.yml:75-81]()"])</script><script>self.__next_f.push([1,"29:T3dd0,"])</script><script>self.__next_f.push([1,"# Storage Services (MinIO and PostgreSQL)\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/Readme.md](infra/Readme.md)\n- [infra/docker-compose.yml](infra/docker-compose.yml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document details the two storage services that form the data layer of the OpenGeoAIModelHub infrastructure stack: MinIO for S3-compatible object storage and PostgreSQL with PostGIS extensions for relational data storage. These services provide persistent storage for MLflow experiments, model artifacts, and metadata. \n\nFor information about how MLflow integrates with these services, see [MLflow Tracking Server](#4.5). For overall infrastructure architecture, see [Service Architecture](#4.1).\n\n## Storage Architecture Overview\n\nThe infrastructure implements a standard two-store pattern where PostgreSQL handles structured metadata and MinIO handles large binary artifacts. Both services persist data to Docker volumes and are accessible through Traefik reverse proxy with automatic SSL termination.\n\n```mermaid\ngraph TB\n    subgraph \"Storage Layer\"\n        minio[\"minio container\"]\n        postgres[\"postgres container\"]\n    end\n    \n    subgraph \"Docker Volumes\"\n        minio_vol[\"./volumes/minio\u003cbr/\u003e/data/minio\"]\n        postgres_vol[\"./volumes/postgres\u003cbr/\u003e/var/lib/postgresql/data\"]\n    end\n    \n    subgraph \"Network Access via Traefik\"\n        minio_api[\"minio-api.DOMAIN:443\u003cbr/\u003ePort 9000\"]\n        minio_console[\"minio.DOMAIN:443\u003cbr/\u003ePort 9001\"]\n        postgres_tcp[\"postgres.DOMAIN:5432\u003cbr/\u003eTCP with TLS\"]\n    end\n    \n    subgraph \"Client Services\"\n        mlflow[\"mlflow container\"]\n        external[\"External Clients\u003cbr/\u003epsql, DBeaver, s3cmd\"]\n    end\n    \n    mlflow --\u003e|\"backend-store-uri\u003cbr/\u003epostgresql+psycopg2://\"| postgres\n    mlflow --\u003e|\"default-artifact-root\u003cbr/\u003es3://bucket/\"| minio\n    \n    external --\u003e|\"S3 API\"| minio_api\n    external --\u003e|\"Web Console\"| minio_console\n    external --\u003e|\"PostgreSQL Protocol\"| postgres_tcp\n    \n    minio_api --\u003e|\"routes to\"| minio\n    minio_console --\u003e|\"routes to\"| minio\n    postgres_tcp --\u003e|\"routes to\"| postgres\n    \n    minio -.-\u003e|\"persists to\"| minio_vol\n    postgres -.-\u003e|\"persists to\"| postgres_vol\n```\n\n**Sources:** [infra/docker-compose.yml:62-136](), [infra/Readme.md:5-12]()\n\n## MinIO Object Storage Service\n\nMinIO provides S3-compatible object storage for MLflow artifacts including model files, plots, datasets, and ONNX/DLPK packages. The service exposes two interfaces: an S3-compatible API endpoint and a web-based management console.\n\n### Container Configuration\n\nThe `minio` service is defined in [infra/docker-compose.yml:86-113]() with the following configuration:\n\n| Configuration | Value | Description |\n|--------------|-------|-------------|\n| **Image** | `minio/minio:RELEASE.2025-04-22T22-12-26Z` | Specific MinIO release version |\n| **Container Name** | `minio` | Docker container identifier |\n| **Restart Policy** | `unless-stopped` | Automatic restart on failure |\n| **Command** | `minio server /data/minio --console-address \":9001\"` | Server startup with console on port 9001 |\n| **Volume Mount** | `./volumes/minio:/data/minio` | Persistent storage location |\n\n### Authentication and Access\n\nMinIO uses root credentials configured via environment variables:\n\n| Environment Variable | Purpose | Configuration Source |\n|---------------------|---------|---------------------|\n| `MINIO_ROOT_USER` | Root access username | `${AWS_ACCESS_KEY_ID}` from `.env` |\n| `MINIO_ROOT_PASSWORD` | Root access password | `${AWS_SECRET_ACCESS_KEY}` from `.env` |\n| `MINIO_IDENTITY_TYPE` | Authentication mode | `internal` (local user database) |\n\n**Sources:** [infra/docker-compose.yml:90-93]()\n\n### Endpoint Access via Traefik\n\nMinIO exposes two distinct endpoints through Traefik routing:\n\n```mermaid\ngraph LR\n    subgraph \"External Access\"\n        api_client[\"S3 API Clients\u003cbr/\u003eboto3, s3cmd, aws-cli\"]\n        web_browser[\"Web Browsers\"]\n    end\n    \n    subgraph \"Traefik Routing\"\n        api_router[\"minio-api router\u003cbr/\u003eHost: minio-api.DOMAIN\"]\n        console_router[\"minio-console router\u003cbr/\u003eHost: minio.DOMAIN\"]\n    end\n    \n    subgraph \"MinIO Container\"\n        api_port[\"API Server\u003cbr/\u003ePort 9000\"]\n        console_port[\"Console UI\u003cbr/\u003ePort 9001\"]\n    end\n    \n    api_client --\u003e|\"HTTPS\"| api_router\n    web_browser --\u003e|\"HTTPS\"| console_router\n    \n    api_router --\u003e|\"minio-api service\"| api_port\n    console_router --\u003e|\"minio-console service\"| console_port\n```\n\n**Routing Configuration:**\n- **API Endpoint:** `minio-api.${DOMAIN}`  `http://minio:9000` [infra/docker-compose.yml:99-104]()\n- **Console Endpoint:** `minio.${DOMAIN}`  `http://minio:9001` [infra/docker-compose.yml:105-110]()\n- **TLS Certificate:** Let's Encrypt via `letsencrypt` resolver [infra/docker-compose.yml:101,107]()\n\n**Sources:** [infra/docker-compose.yml:97-111](), [infra/Readme.md:37-38]()\n\n### Integration with MLflow\n\nMLflow connects to MinIO using S3-compatible API credentials. The connection is established through environment variables in the `mlflow` service:\n\n```\nMLFLOW_S3_ENDPOINT_URL: \"http://minio:9000\"\nMLFLOW_S3_IGNORE_TLS: \"true\"\nAWS_ACCESS_KEY_ID: \"${AWS_ACCESS_KEY_ID}\"\nAWS_SECRET_ACCESS_KEY: \"${AWS_SECRET_ACCESS_KEY}\"\n```\n\nThe MLflow server is configured to use MinIO as its artifact store:\n- **Artifact Root:** `s3://${MINIO_BUCKET_NAME}/` [infra/docker-compose.yml:71]()\n- **Artifacts Destination:** `s3://${MINIO_BUCKET_NAME}/` [infra/docker-compose.yml:71]()\n- **Internal Access:** Direct container-to-container communication via `http://minio:9000` [infra/docker-compose.yml:67]()\n\n**Sources:** [infra/docker-compose.yml:66-71]()\n\n### Data Persistence\n\nMinIO stores all object data in a Docker volume mounted at `/data/minio` inside the container. The host directory defaults to `./volumes/minio` but can be customized via the `MINIO_DATA_DIR` environment variable [infra/docker-compose.yml:95]().\n\n**Volume Structure:**\n```\nvolumes/\n minio/\n     [bucket-name]/\n         [run-id]/\n            artifacts/\n            models/\n            outputs/\n         .minio.sys/\n```\n\n## PostgreSQL Database Service\n\nPostgreSQL serves as the backend store for MLflow, storing all experiment metadata, run parameters, metrics, and tags. The service uses the PostGIS-enabled image to support geospatial data operations.\n\n### Container Configuration\n\nThe `postgres` service is defined in [infra/docker-compose.yml:115-136]() with the following configuration:\n\n| Configuration | Value | Description |\n|--------------|-------|-------------|\n| **Image** | `postgis/postgis:16-3.4-alpine` | PostgreSQL 16 with PostGIS 3.4 |\n| **Container Name** | `postgres` | Docker container identifier |\n| **Restart Policy** | `unless-stopped` | Automatic restart on failure |\n| **Volume Mount** | `./volumes/postgres:/var/lib/postgresql/data` | Persistent storage location |\n\n**Sources:** [infra/docker-compose.yml:116-125]()\n\n### Database Configuration\n\nPostgreSQL is configured with multiple extensions to support geospatial and MLflow operations:\n\n| Environment Variable | Value | Purpose |\n|---------------------|-------|---------|\n| `POSTGRES_USER` | From `${POSTGRES_USER}` | Database superuser username |\n| `POSTGRES_PASSWORD` | From `${POSTGRES_PASSWORD}` | Database superuser password |\n| `POSTGRES_DB` | From `${POSTGRES_DB}` | Initial database name (typically `mlflow`) |\n| `POSTGRES_MULTIPLE_EXTENSIONS` | `postgis,hstore,postgis_topology,postgis_raster,pgrouting` | Enabled PostGIS extensions |\n\n**Sources:** [infra/docker-compose.yml:119-123]()\n\n### Network Access Configuration\n\nPostgreSQL is exposed through Traefik using TCP routing with TLS termination:\n\n```mermaid\ngraph TB\n    subgraph \"External Clients\"\n        psql[\"psql CLI\"]\n        dbeaver[\"DBeaver GUI\"]\n        mlflow_remote[\"MLflow Remote Clients\"]\n    end\n    \n    subgraph \"Traefik TCP Router\"\n        postgres_router[\"postgres router\u003cbr/\u003eHostSNI: postgres.DOMAIN\u003cbr/\u003eEntrypoint: postgres (5432)\"]\n        tls_termination[\"TLS Termination\u003cbr/\u003ecertresolver: letsencrypt\"]\n    end\n    \n    subgraph \"PostgreSQL Container\"\n        postgres_server[\"PostgreSQL Server\u003cbr/\u003ePort 5432\u003cbr/\u003eDatabase: mlflow\"]\n        extensions[\"Extensions:\u003cbr/\u003epostgis, hstore,\u003cbr/\u003epostgis_topology,\u003cbr/\u003epostgis_raster,\u003cbr/\u003epgrouting\"]\n    end\n    \n    psql --\u003e|\"PostgreSQL Protocol\u003cbr/\u003eTLS Required\"| postgres_router\n    dbeaver --\u003e|\"PostgreSQL Protocol\u003cbr/\u003eTLS Required\"| postgres_router\n    mlflow_remote --\u003e|\"PostgreSQL Protocol\u003cbr/\u003eTLS Required\"| postgres_router\n    \n    postgres_router --\u003e|\"Decrypts\"| tls_termination\n    tls_termination --\u003e|\"postgres service\"| postgres_server\n    postgres_server -.-\u003e|\"provides\"| extensions\n```\n\n**Traefik TCP Routing Configuration:**\n- **Rule:** `HostSNI(\\`postgres.${DOMAIN}\\`)` [infra/docker-compose.yml:128]()\n- **Entrypoint:** `postgres` (port 5432) [infra/docker-compose.yml:129]()\n- **Service Target:** Port 5432 on `postgres` container [infra/docker-compose.yml:131]()\n- **TLS:** Enabled with Let's Encrypt certificates [infra/docker-compose.yml:132-133]()\n\n**Sources:** [infra/docker-compose.yml:126-133]()\n\n### Connection Details for External Clients\n\nThe database can be accessed externally using standard PostgreSQL clients with the following parameters:\n\n| Parameter | Value | Notes |\n|-----------|-------|-------|\n| **Host** | `postgres.yourdomain.com` | DNS A record must point to server IP |\n| **Port** | `5432` | Standard PostgreSQL port |\n| **Database** | Value of `${POSTGRES_DB}` | Typically `mlflow` |\n| **Username** | Value of `${POSTGRES_USER}` | From `.env` file |\n| **Password** | Value of `${POSTGRES_PASSWORD}` | From `.env` file |\n| **SSL Mode** | `require` | TLS is mandatory |\n\n**Example connection using psql:**\n```bash\npsql \"postgresql://username:password@postgres.yourdomain.com:5432/mlflow?sslmode=require\"\n```\n\n**Example DBeaver configuration:**\n- Connection Type: PostgreSQL\n- Host: `postgres.yourdomain.com`\n- Port: `5432`\n- Database: `mlflow`\n- Authentication: Database Native\n- SSL: Use SSL (Require)\n\n**Sources:** [infra/Readme.md:43-50]()\n\n### Integration with MLflow Backend Store\n\nMLflow uses PostgreSQL as its backend store for all metadata via SQLAlchemy connection URI:\n\n```\npostgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}\n```\n\nThis URI is passed to the MLflow server via the `--backend-store-uri` parameter [infra/docker-compose.yml:71]().\n\n**MLflow Metadata Stored in PostgreSQL:**\n- Experiments and experiment metadata\n- Runs and run parameters\n- Metrics time series data\n- Tags and notes\n- Model registry metadata\n- Model versions and stage transitions\n\n**Sources:** [infra/docker-compose.yml:71]()\n\n### Data Persistence\n\nPostgreSQL stores all database files in a Docker volume mounted at `/var/lib/postgresql/data` inside the container. The host directory defaults to `./volumes/postgres` but can be customized via the `PUID_DATA_DIR` environment variable [infra/docker-compose.yml:125]().\n\n**Volume Structure:**\n```\nvolumes/\n postgres/\n     base/           # Database files\n     global/         # Cluster-wide tables\n     pg_wal/         # Write-ahead logs\n     pg_stat/        # Statistics files\n     postgresql.conf # Configuration (if overridden)\n```\n\n## Storage Service Integration Flow\n\nThe following diagram shows how a typical MLflow training run interacts with both storage services:\n\n```mermaid\nsequenceDiagram\n    participant Train as \"Training Script\"\n    participant MLflow as \"mlflow container\u003cbr/\u003ePort 5000\"\n    participant PG as \"postgres container\u003cbr/\u003ePort 5432\"\n    participant MinIO as \"minio container\u003cbr/\u003ePort 9000\"\n    participant PGVol as \"postgres volume\u003cbr/\u003e./volumes/postgres\"\n    participant MinIOVol as \"minio volume\u003cbr/\u003e./volumes/minio\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.start_run()\"\n    MLflow-\u003e\u003ePG: \"INSERT INTO runs (...)\"\n    PG-\u003e\u003ePGVol: \"Write to disk\"\n    PG--\u003e\u003eMLflow: \"Run ID\"\n    MLflow--\u003e\u003eTrain: \"Run context\"\n    \n    loop \"Training Iterations\"\n        Train-\u003e\u003eMLflow: \"mlflow.log_metric(key, value, step)\"\n        MLflow-\u003e\u003ePG: \"INSERT INTO metrics (...)\"\n        PG-\u003e\u003ePGVol: \"Write to disk\"\n    end\n    \n    Train-\u003e\u003eMLflow: \"mlflow.log_param(key, value)\"\n    MLflow-\u003e\u003ePG: \"INSERT INTO params (...)\"\n    PG-\u003e\u003ePGVol: \"Write to disk\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.log_artifact(file)\"\n    MLflow-\u003e\u003eMinIO: \"S3 PUT s3://bucket/run-id/artifacts/file\"\n    MinIO-\u003e\u003eMinIOVol: \"Write to disk\"\n    MinIO--\u003e\u003eMLflow: \"Success\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.pytorch.log_model(model)\"\n    MLflow-\u003e\u003eMinIO: \"S3 PUT s3://bucket/run-id/artifacts/model/*\"\n    MinIO-\u003e\u003eMinIOVol: \"Write model files\"\n    MLflow-\u003e\u003ePG: \"UPDATE runs SET artifact_uri=...\"\n    PG-\u003e\u003ePGVol: \"Write to disk\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.end_run()\"\n    MLflow-\u003e\u003ePG: \"UPDATE runs SET status='FINISHED'\"\n    PG-\u003e\u003ePGVol: \"Write to disk\"\n```\n\n**Sources:** [infra/docker-compose.yml:62-136]()\n\n## Network Configuration\n\nBoth storage services are attached to the `traefik-network` Docker network, enabling:\n- Direct container-to-container communication (e.g., `mlflow`  `postgres`, `mlflow`  `minio`)\n- External access through Traefik reverse proxy\n- DNS-based service discovery within the Docker network\n\n```mermaid\ngraph TB\n    subgraph \"traefik-network\"\n        traefik[\"traefik\u003cbr/\u003eReverse Proxy\"]\n        mlflow[\"mlflow\u003cbr/\u003eTracking Server\"]\n        minio[\"minio\u003cbr/\u003eObject Storage\"]\n        postgres[\"postgres\u003cbr/\u003eDatabase\"]\n    end\n    \n    mlflow --\u003e|\"Internal DNS\u003cbr/\u003epostgres:5432\"| postgres\n    mlflow --\u003e|\"Internal DNS\u003cbr/\u003eminio:9000\"| minio\n    \n    traefik --\u003e|\"Proxies to\"| mlflow\n    traefik --\u003e|\"Proxies to\"| minio\n    traefik --\u003e|\"Proxies to\"| postgres\n    \n    external[\"External Clients\"] --\u003e|\"HTTPS/TCP+TLS\"| traefik\n```\n\n**Network Driver:** `bridge` [infra/docker-compose.yml:178-180]()\n\n**Sources:** [infra/docker-compose.yml:82-83,111-113,134-136,177-180]()\n\n## Data Backup Considerations\n\nBoth storage services persist data to local Docker volumes, which should be included in backup strategies:\n\n| Service | Volume Path | Critical Data | Backup Method |\n|---------|-------------|---------------|---------------|\n| **MinIO** | `./volumes/minio` | Model artifacts, plots, datasets, ONNX/DLPK files | `tar` archive or `rsync` of entire directory; MinIO CLI `mc mirror` |\n| **PostgreSQL** | `./volumes/postgres` | Experiment metadata, metrics, parameters, model registry | `pg_dump` via PostgreSQL client; `tar` archive of volume directory |\n\n**Example PostgreSQL backup command:**\n```bash\npg_dump \"postgresql://user:pass@postgres.yourdomain.com:5432/mlflow?sslmode=require\" \u003e mlflow_backup.sql\n```\n\n**Example MinIO backup using mc CLI:**\n```bash\nmc alias set myminio https://minio-api.yourdomain.com access_key secret_key\nmc mirror myminio/bucket-name ./backup/minio-bucket\n```\n\nThe infrastructure provides a management script for backup operations at [infra/manage.sh]() which can be invoked with `./manage.sh backup` [infra/Readme.md:59]().\n\n**Sources:** [infra/Readme.md:54-60]()\n\n## Volume Management\n\nBoth services use environment variables to customize volume mount locations:\n\n| Environment Variable | Default Value | Purpose | Configuration Line |\n|---------------------|---------------|---------|-------------------|\n| `MINIO_DATA_DIR` | `./volumes/minio` | MinIO data directory | [infra/docker-compose.yml:95]() |\n| `POSTGRES_DATA_DIR` | `./volumes/postgres` | PostgreSQL data directory | [infra/docker-compose.yml:125]() |\n\nThese variables allow for flexible storage configuration, such as mounting external block storage or NFS volumes for production deployments.\n\n**Sources:** [infra/docker-compose.yml:95,125]()"])</script><script>self.__next_f.push([1,"2a:T3a19,"])</script><script>self.__next_f.push([1,"# Additional Services\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/docker-compose.yml](infra/docker-compose.yml)\n- [infra/homepage-config/services.yaml](infra/homepage-config/services.yaml)\n\n\u003c/details\u003e\n\n\n\nThis page documents the auxiliary services in the infrastructure stack that provide operational visibility and remote access capabilities. The **Homepage dashboard** serves as a centralized monitoring interface for all services, while the **RustDesk server** enables secure remote desktop access to development environments.\n\nFor information about the core infrastructure services (Traefik, MLflow, MinIO, PostgreSQL), see [Service Architecture](#4.1), [Traefik Reverse Proxy](#4.4), [MLflow Tracking Server](#4.5), and [Storage Services](#4.6).\n\n## Service Overview\n\nThe infrastructure stack includes two additional services beyond the core ML operations components:\n\n| Service | Container(s) | Purpose | Subdomain |\n|---------|-------------|---------|-----------|\n| Homepage | `homepage` | Service monitoring dashboard | Root domain (`${DOMAIN}`) |\n| RustDesk | `hbbs`, `hbbr` | Remote desktop server | `rustdesk.${DOMAIN}` |\n\nSources: [infra/docker-compose.yml:40-60](), [infra/docker-compose.yml:138-175]()\n\n## Homepage Dashboard Service\n\n### Architecture and Role\n\nHomepage is a web-based dashboard that provides centralized monitoring and quick access to all infrastructure services. The service monitors Docker containers through the Docker socket, displaying real-time status, links, and descriptions for each service in the stack.\n\n```mermaid\ngraph TB\n    subgraph \"Homepage Container\"\n        HOMEPAGE_APP[\"Homepage Application\u003cbr/\u003ePort 3000\"]\n        CONFIG[\"Configuration Files\u003cbr/\u003e/app/config\"]\n    end\n    \n    subgraph \"Monitoring\"\n        DOCKER_SOCK[\"Docker Socket\u003cbr/\u003e/var/run/docker.sock\"]\n        CONTAINERS[\"Container Metadata\u003cbr/\u003etraefik, mlflow, minio, postgres, hbbs, hbbr\"]\n    end\n    \n    subgraph \"Configuration Sources\"\n        SERVICES_YAML[\"services.yaml\u003cbr/\u003eService Definitions\"]\n        WIDGETS_YAML[\"widgets.yaml\"]\n        SETTINGS_YAML[\"settings.yaml\"]\n    end\n    \n    subgraph \"Traefik Integration\"\n        TRAEFIK_ROUTE[\"Traefik Router\u003cbr/\u003ehomepage.websecure\"]\n        DOMAIN_RULE[\"Host Rule\u003cbr/\u003e${DOMAIN} || www.${DOMAIN}\"]\n    end\n    \n    SERVICES_YAML --\u003e CONFIG\n    WIDGETS_YAML --\u003e CONFIG\n    SETTINGS_YAML --\u003e CONFIG\n    \n    CONFIG --\u003e HOMEPAGE_APP\n    DOCKER_SOCK --\u003e HOMEPAGE_APP\n    HOMEPAGE_APP --\u003e CONTAINERS\n    \n    TRAEFIK_ROUTE --\u003e DOMAIN_RULE\n    DOMAIN_RULE --\u003e HOMEPAGE_APP\n    \n    HOMEPAGE_APP --\u003e |\"Displays\"| SERVICE_LINKS[\"MLflow\u003cbr/\u003eMinIO\u003cbr/\u003ePostgreSQL\u003cbr/\u003eRustDesk\u003cbr/\u003eTraefik\"]\n```\n\n**Homepage Dashboard Architecture**\n\nThe `homepage` container runs the dashboard application and mounts two critical volumes: the configuration directory and the Docker socket for container monitoring.\n\nSources: [infra/docker-compose.yml:40-60]()\n\n### Container Configuration\n\nThe Homepage service is defined in the Docker Compose stack with the following configuration:\n\n```yaml\nhomepage:\n  image: ghcr.io/gethomepage/homepage:latest\n  container_name: homepage\n  restart: unless-stopped\n  environment:\n    HOMEPAGE_ALLOWED_HOSTS: ${HOMEPAGE_ALLOWED_HOSTS}\n    PUID: ${PUID}\n    PGID: ${PGID}\n    TZ: ${TZ}\n  volumes:\n    - ${HOMEPAGE_CONFIG:-./homepage-config}:/app/config\n    - /var/run/docker.sock:/var/run/docker.sock:ro\n```\n\n**Environment Variables:**\n\n| Variable | Purpose | Source |\n|----------|---------|--------|\n| `HOMEPAGE_ALLOWED_HOSTS` | Permitted hostnames for accessing the dashboard | `.env` file |\n| `PUID` | User ID for file permissions | `.env` file |\n| `PGID` | Group ID for file permissions | `.env` file |\n| `TZ` | Timezone for timestamp display | `.env` file |\n\nSources: [infra/docker-compose.yml:44-48]()\n\n### Traefik Routing Configuration\n\nHomepage is exposed on the root domain using Traefik labels:\n\n| Label | Value | Purpose |\n|-------|-------|---------|\n| `traefik.enable` | `true` | Enable Traefik routing |\n| `traefik.http.routers.homepage.rule` | `Host(\\`${DOMAIN}\\`) \\|\\| Host(\\`www.${DOMAIN}\\`)` | Route root and www subdomain |\n| `traefik.http.routers.homepage.entrypoints` | `websecure` | Use HTTPS entrypoint (port 443) |\n| `traefik.http.routers.homepage.tls.certresolver` | `letsencrypt` | Automatic SSL certificates |\n| `traefik.http.services.homepage.loadbalancer.server.port` | `3000` | Internal container port |\n\nThe dashboard is accessible at both `https://${DOMAIN}` and `https://www.${DOMAIN}`.\n\nSources: [infra/docker-compose.yml:52-58]()\n\n### Service Configuration\n\nThe Homepage dashboard reads service definitions from the configuration directory. The `services.yaml` file defines the services displayed on the dashboard:\n\n```mermaid\ngraph LR\n    subgraph \"Service Categories\"\n        ML[\"Machine Learning\u003cbr/\u003e- MLflow\"]\n        STORAGE[\"Storage\u003cbr/\u003e- MinIO Console\u003cbr/\u003e- MinIO API\"]\n        DATABASE[\"Database\u003cbr/\u003e- PostgreSQL + PostGIS\"]\n        REMOTE[\"Remote Access\u003cbr/\u003e- RustDesk\"]\n        SYSTEM[\"System\u003cbr/\u003e- Traefik\"]\n    end\n    \n    subgraph \"Service Attributes\"\n        HREF[\"href: URL\"]\n        DESC[\"description: Text\"]\n        ICON[\"icon: Image/URL\"]\n    end\n    \n    ML --\u003e HREF\n    ML --\u003e DESC\n    ML --\u003e ICON\n    \n    STORAGE --\u003e HREF\n    DATABASE --\u003e HREF\n    REMOTE --\u003e HREF\n    SYSTEM --\u003e HREF\n```\n\n**Service Categories and Definitions**\n\nEach service entry in `services.yaml` includes three attributes: `href` (URL), `description` (explanatory text), and `icon` (visual identifier).\n\nSources: [infra/homepage-config/services.yaml:1-35]()\n\n### Docker Socket Monitoring\n\nThe Homepage service monitors Docker containers by mounting the Docker socket in read-only mode:\n\n```\n/var/run/docker.sock:/var/run/docker.sock:ro\n```\n\nThis allows Homepage to:\n- Query container status (running, stopped, restarting)\n- Display container metadata (names, images, ports)\n- Show resource usage (CPU, memory, network)\n- Detect container lifecycle events\n\nThe read-only mount ensures Homepage cannot modify container state, maintaining security boundaries.\n\nSources: [infra/docker-compose.yml:51]()\n\n## RustDesk Remote Desktop Server\n\n### Architecture and Components\n\nRustDesk is an open-source remote desktop solution consisting of two server components: **hbbs** (hub server) and **hbbr** (relay server). Together, these services enable secure remote desktop connections to development machines.\n\n```mermaid\ngraph TB\n    subgraph \"RustDesk Client\"\n        CLIENT[\"RustDesk Desktop Client\"]\n    end\n    \n    subgraph \"RustDesk Server Components\"\n        HBBS[\"hbbs Container\u003cbr/\u003eHub/Registration Server\u003cbr/\u003erustdesk-server:latest\"]\n        HBBR[\"hbbr Container\u003cbr/\u003eRelay Server\u003cbr/\u003erustdesk-server:latest\"]\n    end\n    \n    subgraph \"Port Mappings\"\n        P21115[\"21115: TCP\u003cbr/\u003eNAT Type Test\"]\n        P21116[\"21116: TCP/UDP\u003cbr/\u003eID Registration\"]\n        P21117[\"21117: TCP\u003cbr/\u003eRelay\"]\n        P21118[\"21118: TCP\u003cbr/\u003eWeb Client\"]\n        P21119[\"21119: TCP\u003cbr/\u003eRelay WebSocket\"]\n    end\n    \n    subgraph \"Configuration\"\n        KEY[\"RUSTDESK_KEY\u003cbr/\u003eEncryption Key\"]\n        DOMAIN_CONFIG[\"rustdesk.${DOMAIN}\u003cbr/\u003eServer Address\"]\n        VOLUMES[\"volumes/rustdesk\u003cbr/\u003ePersistent Data\"]\n    end\n    \n    CLIENT --\u003e|\"Connect\"| HBBS\n    HBBS --\u003e|\"Relay via\"| HBBR\n    \n    HBBS --\u003e P21115\n    HBBS --\u003e P21116\n    HBBS --\u003e P21118\n    HBBR --\u003e P21117\n    HBBR --\u003e P21119\n    \n    KEY --\u003e HBBS\n    KEY --\u003e HBBR\n    DOMAIN_CONFIG --\u003e HBBS\n    \n    HBBS --\u003e VOLUMES\n    HBBR --\u003e VOLUMES\n```\n\n**RustDesk Server Architecture**\n\nThe `hbbs` container handles client registration and coordination, while `hbbr` provides relay functionality for NAT traversal. Both containers share the same encryption key and data volume.\n\nSources: [infra/docker-compose.yml:138-175]()\n\n### HBBS (Hub Server) Configuration\n\nThe `hbbs` container is the primary registration and coordination server:\n\n**Container Definition:**\n\n| Property | Value | Purpose |\n|----------|-------|---------|\n| Image | `rustdesk/rustdesk-server:latest` | Official RustDesk server image |\n| Container Name | `hbbs` | Hub/registration server identifier |\n| Restart Policy | `unless-stopped` | Automatic restart on failure |\n| Environment | `ALWAYS_USE_RELAY=Y` | Force all connections through relay |\n| Command | `hbbs -r rustdesk.${DOMAIN} -k ${RUSTDESK_KEY}` | Start hub server with domain and key |\n\n**Port Mappings:**\n\n| Host Port | Container Port | Protocol | Purpose |\n|-----------|----------------|----------|---------|\n| 21115 | 21115 | TCP | NAT type test server |\n| 21116 | 21116 | TCP | ID registration server |\n| 21116 | 21116 | UDP | ID registration server (UDP) |\n| 21118 | 21118 | TCP | Web client HTTP server |\n\nThe `hbbs` container depends on `hbbr` to ensure the relay server is available before accepting connections.\n\nSources: [infra/docker-compose.yml:138-162]()\n\n### HBBR (Relay Server) Configuration\n\nThe `hbbr` container provides NAT traversal and relay services:\n\n**Container Definition:**\n\n| Property | Value | Purpose |\n|----------|-------|---------|\n| Image | `rustdesk/rustdesk-server:latest` | Official RustDesk server image |\n| Container Name | `hbbr` | Relay server identifier |\n| Restart Policy | `unless-stopped` | Automatic restart on failure |\n| Command | `hbbr -k ${RUSTDESK_KEY}` | Start relay server with encryption key |\n\n**Port Mappings:**\n\n| Host Port | Container Port | Protocol | Purpose |\n|-----------|----------------|----------|---------|\n| 21117 | 21117 | TCP | Relay server |\n| 21119 | 21119 | TCP | Relay server WebSocket |\n\nSources: [infra/docker-compose.yml:164-175]()\n\n### RustDesk Configuration Parameters\n\nBoth RustDesk containers share common configuration:\n\n**Environment Variables:**\n\n| Variable | Usage | Description |\n|----------|-------|-------------|\n| `RUSTDESK_KEY` | Command line `-k` flag | Encryption key for secure communication |\n| `DOMAIN` | Command line `-r rustdesk.${DOMAIN}` | Server domain for client configuration |\n\n**Volume Mounts:**\n\n```\n${RUSTDESK_DATA_DIR:-./volumes/rustdesk}:/root\n```\n\nThe data directory stores:\n- Server identification keys\n- Client registration data\n- Connection logs\n- Configuration persistence\n\nSources: [infra/docker-compose.yml:144](), [infra/docker-compose.yml:168]()\n\n### Traefik Integration\n\nThe `hbbs` container exposes its web client interface through Traefik:\n\n**Traefik Labels:**\n\n| Label | Value | Purpose |\n|-------|-------|---------|\n| `traefik.enable` | `true` | Enable Traefik routing |\n| `traefik.http.routers.rustdesk.rule` | `Host(\\`rustdesk.${DOMAIN}\\`)` | Route rustdesk subdomain |\n| `traefik.http.routers.rustdesk.entrypoints` | `websecure` | Use HTTPS (port 443) |\n| `traefik.http.routers.rustdesk.tls.certresolver` | `letsencrypt` | Automatic SSL certificates |\n| `traefik.http.services.rustdesk.loadbalancer.server.port` | `21118` | Route to web client port |\n\nThe web interface is accessible at `https://rustdesk.${DOMAIN}`, while the native RustDesk desktop client connects directly to the exposed ports (21115, 21116, 21117, 21119).\n\nSources: [infra/docker-compose.yml:155-160]()\n\n## Service Integration\n\nThe following diagram illustrates how Homepage and RustDesk integrate with the core infrastructure:\n\n```mermaid\ngraph TB\n    subgraph \"Public Access\"\n        USERS[\"Users/Clients\"]\n        RDCLIENT[\"RustDesk Desktop Client\"]\n    end\n    \n    subgraph \"Traefik Reverse Proxy\"\n        TRAEFIK[\"traefik container\u003cbr/\u003e:80, :443\"]\n        TRAEFIK_ROUTES[\"HTTP Routers:\u003cbr/\u003ehomepage.websecure\u003cbr/\u003erustdesk.websecure\"]\n    end\n    \n    subgraph \"Additional Services\"\n        HOMEPAGE[\"homepage container\u003cbr/\u003ePort 3000\u003cbr/\u003eDashboard UI\"]\n        HBBS[\"hbbs container\u003cbr/\u003ePorts 21115-21118\u003cbr/\u003eHub Server\"]\n        HBBR[\"hbbr container\u003cbr/\u003ePorts 21117, 21119\u003cbr/\u003eRelay Server\"]\n    end\n    \n    subgraph \"Core Services\"\n        MLFLOW[\"mlflow container\"]\n        MINIO[\"minio container\"]\n        POSTGRES[\"postgres container\"]\n    end\n    \n    subgraph \"Docker Infrastructure\"\n        DOCKER_SOCK[\"Docker Socket\u003cbr/\u003e/var/run/docker.sock\"]\n        DOCKER_ENGINE[\"Docker Engine\"]\n    end\n    \n    subgraph \"Persistent Storage\"\n        HP_CONFIG[\"homepage-config/\u003cbr/\u003eservices.yaml\"]\n        RD_VOLUME[\"volumes/rustdesk/\u003cbr/\u003eServer Keys \u0026 Logs\"]\n    end\n    \n    USERS --\u003e|\"HTTPS\"| TRAEFIK\n    RDCLIENT --\u003e|\"Direct TCP/UDP\"| HBBS\n    RDCLIENT --\u003e|\"Direct TCP\"| HBBR\n    \n    TRAEFIK --\u003e TRAEFIK_ROUTES\n    TRAEFIK_ROUTES --\u003e|\"${DOMAIN}\"| HOMEPAGE\n    TRAEFIK_ROUTES --\u003e|\"rustdesk.${DOMAIN}\"| HBBS\n    \n    HP_CONFIG --\u003e HOMEPAGE\n    DOCKER_SOCK --\u003e HOMEPAGE\n    DOCKER_ENGINE --\u003e DOCKER_SOCK\n    \n    HOMEPAGE -.-\u003e|\"Monitors\"| MLFLOW\n    HOMEPAGE -.-\u003e|\"Monitors\"| MINIO\n    HOMEPAGE -.-\u003e|\"Monitors\"| POSTGRES\n    HOMEPAGE -.-\u003e|\"Monitors\"| TRAEFIK\n    HOMEPAGE -.-\u003e|\"Monitors\"| HBBS\n    HOMEPAGE -.-\u003e|\"Monitors\"| HBBR\n    \n    HBBS --\u003e|\"Depends on\"| HBBR\n    \n    HBBS --\u003e RD_VOLUME\n    HBBR --\u003e RD_VOLUME\n```\n\n**Infrastructure Integration Architecture**\n\nThe Homepage dashboard monitors all services through the Docker socket, while RustDesk operates independently with direct port access for remote desktop connections. Both services integrate with Traefik for HTTPS access to their web interfaces.\n\nSources: [infra/docker-compose.yml:40-175]()\n\n### Network Configuration\n\nAll services in the stack share the `traefik-network` bridge network, enabling:\n\n- **Homepage**: Access to Docker socket for monitoring, HTTP communication with Traefik\n- **RustDesk (hbbs, hbbr)**: HTTP communication with Traefik for web interface, direct port exposure for desktop client\n- **Cross-service communication**: Homepage can query metadata from all containers on the shared network\n\nThe network is defined at the bottom of the Docker Compose file:\n\n```yaml\nnetworks:\n  traefik-network:\n    name: traefik-network\n    driver: bridge\n```\n\nSources: [infra/docker-compose.yml:177-180](), [infra/docker-compose.yml:60](), [infra/docker-compose.yml:162](), [infra/docker-compose.yml:175]()\n\n### Volume Management\n\nThe additional services use the following volume mounts:\n\n| Service | Host Path | Container Path | Purpose |\n|---------|-----------|----------------|---------|\n| `homepage` | `${HOMEPAGE_CONFIG:-./homepage-config}` | `/app/config` | Dashboard configuration files |\n| `homepage` | `/var/run/docker.sock` | `/var/run/docker.sock` | Docker container monitoring (read-only) |\n| `hbbs` | `${RUSTDESK_DATA_DIR:-./volumes/rustdesk}` | `/root` | Server keys and registration data |\n| `hbbr` | `${RUSTDESK_DATA_DIR:-./volumes/rustdesk}` | `/root` | Server keys and relay data |\n\nBoth RustDesk containers share the same data directory to maintain consistent server identification and encryption keys.\n\nSources: [infra/docker-compose.yml:49-51](), [infra/docker-compose.yml:145-146](), [infra/docker-compose.yml:169-170]()"])</script><script>self.__next_f.push([1,"2b:T4029,"])</script><script>self.__next_f.push([1,"# CI/CD Pipelines\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.dockerignore](.dockerignore)\n- [.github/workflows/docker-publish.yml](.github/workflows/docker-publish.yml)\n- [.github/workflows/mlflow-image-publish.yml](.github/workflows/mlflow-image-publish.yml)\n- [dockerfile.mlflow](dockerfile.mlflow)\n- [examplemodel/Dockerfile](examplemodel/Dockerfile)\n- [examplemodel/requirements.txt](examplemodel/requirements.txt)\n\n\u003c/details\u003e\n\n\n\nThis document describes the automated CI/CD pipelines implemented via GitHub Actions for building and publishing Docker images. The repository contains two primary pipelines: one for the example model container and one for the custom MLflow server image. Both pipelines publish images to GitHub Container Registry (GHCR) for use in the infrastructure stack.\n\nFor information about deploying the published images, see [Infrastructure Deployment](#6.1). For details on the infrastructure services that consume these images, see [Service Architecture](#4.1).\n\n## Overview\n\nThe CI/CD system automates the creation of production-ready Docker images through GitHub Actions workflows. Each workflow is triggered on code changes and publishes multi-architecture images to GHCR with appropriate tagging strategies.\n\n### CI/CD Pipeline Architecture\n\n```mermaid\ngraph TB\n    subgraph \"Source Repository\"\n        EXAMPLE_DF[\"examplemodel/Dockerfile\"]\n        MLFLOW_DF[\"dockerfile.mlflow\"]\n        EXAMPLE_CODE[\"examplemodel/src/\"]\n        REQUIREMENTS[\"examplemodel/requirements.txt\"]\n    end\n    \n    subgraph \"GitHub Actions Workflows\"\n        DOCKER_WF[\"docker-publish.yml\u003cbr/\u003eExample Model Pipeline\"]\n        MLFLOW_WF[\"mlflow-image-publish.yml\u003cbr/\u003eMLflow Image Pipeline\"]\n    end\n    \n    subgraph \"Build Triggers\"\n        PUSH_MASTER[\"Push to master\u003cbr/\u003ebranch\"]\n        PUSH_MAIN[\"Push to main/master\u003cbr/\u003ebranches\"]\n        PR[\"Pull Request\u003cbr/\u003eevents\"]\n        MANUAL[\"workflow_dispatch\u003cbr/\u003emanual trigger\"]\n    end\n    \n    subgraph \"Build Process\"\n        CHECKOUT[\"actions/checkout@v4\u003cbr/\u003eClone repository\"]\n        LOGIN[\"docker/login-action@v3\u003cbr/\u003eAuthenticate to GHCR\"]\n        META[\"docker/metadata-action@v5\u003cbr/\u003eGenerate tags/labels\"]\n        BUILD[\"docker/build-push-action\u003cbr/\u003eBuild and push image\"]\n        ATTEST[\"actions/attest-build-provenance@v2\u003cbr/\u003eGenerate attestation\"]\n    end\n    \n    subgraph \"GitHub Container Registry\"\n        EXAMPLE_IMG[\"ghcr.io/repo/mlflow\u003cbr/\u003eExample Model Image\"]\n        MLFLOW_IMG[\"ghcr.io/repo/mlflow/mlflow\u003cbr/\u003eMLflow Server Image\"]\n    end\n    \n    subgraph \"Deployment Targets\"\n        COMPOSE[\"docker-compose.yml\u003cbr/\u003eInfrastructure Stack\"]\n        LOCAL[\"Local Development\u003cbr/\u003eEnvironments\"]\n    end\n    \n    PUSH_MASTER --\u003e|triggers| DOCKER_WF\n    PUSH_MAIN --\u003e|triggers| MLFLOW_WF\n    PR --\u003e|triggers| MLFLOW_WF\n    MANUAL --\u003e|triggers| MLFLOW_WF\n    \n    DOCKER_WF --\u003e|step 1| CHECKOUT\n    MLFLOW_WF --\u003e|step 1| CHECKOUT\n    \n    CHECKOUT --\u003e|step 2| LOGIN\n    LOGIN --\u003e|step 3| META\n    META --\u003e|step 4| BUILD\n    \n    EXAMPLE_DF --\u003e|build context| BUILD\n    MLFLOW_DF --\u003e|build context| BUILD\n    EXAMPLE_CODE --\u003e|included in| BUILD\n    REQUIREMENTS --\u003e|installed in| BUILD\n    \n    BUILD --\u003e|publishes| EXAMPLE_IMG\n    BUILD --\u003e|publishes| MLFLOW_IMG\n    \n    BUILD --\u003e|step 5| ATTEST\n    \n    EXAMPLE_IMG --\u003e|consumed by| COMPOSE\n    MLFLOW_IMG --\u003e|consumed by| COMPOSE\n    EXAMPLE_IMG --\u003e|pulled for| LOCAL\n    MLFLOW_IMG --\u003e|pulled for| LOCAL\n```\n\n**Sources:** [.github/workflows/docker-publish.yml:1-51](), [.github/workflows/mlflow-image-publish.yml:1-81]()\n\n### Published Images\n\n| Image | Registry Path | Purpose | Workflow |\n|-------|--------------|---------|----------|\n| Example Model | `ghcr.io/\u003crepository\u003e` | Contains trained model and inference code | `docker-publish.yml` |\n| MLflow Server | `ghcr.io/\u003crepository\u003e/mlflow` | Custom MLflow with S3 and PostgreSQL support | `mlflow-image-publish.yml` |\n\n**Sources:** [.github/workflows/docker-publish.yml:8-9](), [.github/workflows/mlflow-image-publish.yml:16-17]()\n\n## Workflow Components\n\n### Common Workflow Elements\n\nBoth workflows share common structural components:\n\n**Authentication**: Uses `docker/login-action@v3` with GitHub token-based authentication to GHCR [.github/workflows/docker-publish.yml:24-28](), [.github/workflows/mlflow-image-publish.yml:34-38]().\n\n**Metadata Extraction**: Uses `docker/metadata-action@v5` to generate Docker tags and OCI labels from Git metadata [.github/workflows/docker-publish.yml:29-33](), [.github/workflows/mlflow-image-publish.yml:40-49]().\n\n**Build and Push**: Uses `docker/build-push-action` to build images and push to GHCR [.github/workflows/docker-publish.yml:35-43](), [.github/workflows/mlflow-image-publish.yml:51-61]().\n\n### Permissions Model\n\n```mermaid\ngraph LR\n    subgraph \"GitHub Actions Runner\"\n        WORKFLOW[\"Workflow Job\"]\n    end\n    \n    subgraph \"Required Permissions\"\n        CONTENTS[\"contents: read\u003cbr/\u003eClone repository\"]\n        PACKAGES[\"packages: write\u003cbr/\u003ePush to GHCR\"]\n        ATTEST_PERM[\"attestations: write\u003cbr/\u003eGenerate provenance\"]\n        ID_TOKEN[\"id-token: write\u003cbr/\u003eOIDC authentication\"]\n    end\n    \n    subgraph \"GitHub Resources\"\n        REPO[\"Source Repository\"]\n        GHCR_REG[\"GitHub Container\u003cbr/\u003eRegistry\"]\n        PROV[\"Provenance\u003cbr/\u003eAttestations\"]\n    end\n    \n    WORKFLOW --\u003e|requires| CONTENTS\n    WORKFLOW --\u003e|requires| PACKAGES\n    WORKFLOW --\u003e|requires| ATTEST_PERM\n    WORKFLOW --\u003e|requires| ID_TOKEN\n    \n    CONTENTS --\u003e|grants access to| REPO\n    PACKAGES --\u003e|grants access to| GHCR_REG\n    ATTEST_PERM --\u003e|grants access to| PROV\n    ID_TOKEN --\u003e|used for| PROV\n```\n\n**Sources:** [.github/workflows/docker-publish.yml:14-18](), [.github/workflows/mlflow-image-publish.yml:22-24]()\n\n## Example Model Image Pipeline\n\nThe example model pipeline builds a containerized version of the refugee camp detection model with all dependencies and code.\n\n### Workflow Triggers\n\n- **Push events**: Triggers on pushes to `master` branch [.github/workflows/docker-publish.yml:4-5]()\n- **Manual**: Can be triggered via GitHub Actions UI (implied by workflow structure)\n\n### Build Process\n\n```mermaid\nflowchart TB\n    TRIGGER[\"Push to master branch\"]\n    \n    subgraph \"Build Steps\"\n        STEP1[\"Checkout repository\u003cbr/\u003eactions/checkout@v4\"]\n        STEP2[\"Login to GHCR\u003cbr/\u003edocker/login-action@v3\"]\n        STEP3[\"Extract metadata\u003cbr/\u003edocker/metadata-action@v5\"]\n        STEP4[\"Build and push image\u003cbr/\u003edocker/build-push-action@v3\"]\n        STEP5[\"Generate attestation\u003cbr/\u003eactions/attest-build-provenance@v2\"]\n    end\n    \n    subgraph \"Build Context\"\n        CTX_PATH[\"context: ./examplemodel\"]\n        DOCKERFILE[\"file: examplemodel/Dockerfile\"]\n    end\n    \n    subgraph \"Build Artifacts\"\n        TAGS[\"Tags from metadata\"]\n        LABELS[\"OCI labels\"]\n        IMAGE[\"Docker image\"]\n        DIGEST[\"Image digest\"]\n    end\n    \n    subgraph \"Output\"\n        REGISTRY[\"ghcr.io/repository\"]\n        ATTESTATION[\"Build provenance\u003cbr/\u003eattestation\"]\n    end\n    \n    TRIGGER --\u003e STEP1\n    STEP1 --\u003e STEP2\n    STEP2 --\u003e STEP3\n    STEP3 --\u003e STEP4\n    \n    CTX_PATH --\u003e STEP4\n    DOCKERFILE --\u003e STEP4\n    \n    STEP3 --\u003e|generates| TAGS\n    STEP3 --\u003e|generates| LABELS\n    \n    TAGS --\u003e STEP4\n    LABELS --\u003e STEP4\n    \n    STEP4 --\u003e|produces| IMAGE\n    STEP4 --\u003e|produces| DIGEST\n    \n    IMAGE --\u003e REGISTRY\n    DIGEST --\u003e STEP5\n    STEP5 --\u003e|creates| ATTESTATION\n    ATTESTATION --\u003e REGISTRY\n```\n\n**Sources:** [.github/workflows/docker-publish.yml:3-50]()\n\n### Dockerfile Structure\n\nThe example model Dockerfile [examplemodel/Dockerfile:1-14]() implements a lean build:\n\n1. **Base Image**: `python:3.11-slim-bookworm` - Debian-based Python runtime\n2. **System Dependencies**: Installs GDAL for geospatial operations [examplemodel/Dockerfile:3-7]()\n3. **Application Setup**: Copies `requirements.txt` and installs Python dependencies [examplemodel/Dockerfile:11-12]()\n4. **Code Deployment**: Copies entire application code [examplemodel/Dockerfile:14]()\n\n**Build Context Exclusions**: The `.dockerignore` file [.dockerignore:1-51]() excludes development artifacts, documentation, and virtual environments to minimize image size.\n\n### Image Tagging Strategy\n\nThe `docker/metadata-action` generates tags based on Git references:\n\n| Git Event | Generated Tag |\n|-----------|---------------|\n| Push to master | `ghcr.io/\u003crepo\u003e:master` |\n| Commit SHA | `ghcr.io/\u003crepo\u003e:sha-\u003cshort-sha\u003e` |\n\n**Sources:** [.github/workflows/docker-publish.yml:29-33]()\n\n## MLflow Custom Image Pipeline\n\nThe MLflow pipeline builds a custom server image with S3 (MinIO) and PostgreSQL backend support, which is consumed by the infrastructure stack.\n\n### Workflow Triggers\n\nThe workflow supports multiple trigger mechanisms [.github/workflows/mlflow-image-publish.yml:3-13]():\n\n| Trigger Type | Condition | Purpose |\n|--------------|-----------|---------|\n| Push to main/master | Changes to `dockerfile.mlflow` or workflow file | Automated production builds |\n| Pull Request | Changes to `dockerfile.mlflow` | Preview builds for testing |\n| Manual Dispatch | `workflow_dispatch` | On-demand builds |\n\n### Multi-Platform Build\n\n```mermaid\ngraph TB\n    subgraph \"Build Configuration\"\n        BUILDX[\"Docker Buildx\u003cbr/\u003esetup-buildx-action@v3\"]\n        PLATFORMS[\"Platform Targets:\u003cbr/\u003elinux/amd64\u003cbr/\u003elinux/arm64\"]\n        CACHE_FROM[\"Cache from:\u003cbr/\u003eGitHub Actions cache\"]\n        CACHE_TO[\"Cache to:\u003cbr/\u003eGitHub Actions cache\"]\n    end\n    \n    subgraph \"Build Process\"\n        BUILD_ACTION[\"docker/build-push-action@v5\"]\n    end\n    \n    subgraph \"Base Image\"\n        MLFLOW_BASE[\"ghcr.io/mlflow/mlflow:v3.1.1\"]\n    end\n    \n    subgraph \"Installed Packages\"\n        BOTO3[\"boto3\u003cbr/\u003eS3/MinIO client\"]\n        PSYCOPG2[\"psycopg2-binary\u003cbr/\u003ePostgreSQL adapter\"]\n    end\n    \n    subgraph \"Output Images\"\n        AMD64_IMG[\"linux/amd64 image\"]\n        ARM64_IMG[\"linux/arm64 image\"]\n        MANIFEST[\"Multi-arch manifest\"]\n    end\n    \n    BUILDX --\u003e|enables| PLATFORMS\n    PLATFORMS --\u003e|configured in| BUILD_ACTION\n    CACHE_FROM --\u003e|speeds up| BUILD_ACTION\n    BUILD_ACTION --\u003e|writes to| CACHE_TO\n    \n    MLFLOW_BASE --\u003e|base for| BUILD_ACTION\n    \n    BUILD_ACTION --\u003e|installs| BOTO3\n    BUILD_ACTION --\u003e|installs| PSYCOPG2\n    \n    BUILD_ACTION --\u003e|produces| AMD64_IMG\n    BUILD_ACTION --\u003e|produces| ARM64_IMG\n    BUILD_ACTION --\u003e|creates| MANIFEST\n    \n    AMD64_IMG --\u003e|referenced by| MANIFEST\n    ARM64_IMG --\u003e|referenced by| MANIFEST\n```\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:30-61]()\n\nThe multi-platform build enables deployment on both x86_64 and ARM64 architectures (e.g., Apple Silicon, AWS Graviton). The GitHub Actions cache is used to accelerate subsequent builds [.github/workflows/mlflow-image-publish.yml:60-61]().\n\n### Custom MLflow Dockerfile\n\nThe `dockerfile.mlflow` [dockerfile.mlflow:1-30]() implements a multi-stage build:\n\n**Stage 1: Base Configuration** [dockerfile.mlflow:5-20]()\n- Starts from official MLflow image `ghcr.io/mlflow/mlflow:v3.1.1`\n- Sets Python environment variables for optimization\n- Installs `boto3` for S3/MinIO compatibility\n- Installs `psycopg2-binary` for PostgreSQL connectivity\n\n**Stage 2: Final Image** [dockerfile.mlflow:22-30]()\n- Copies installed packages from base stage\n- Sets working directory to `/app`\n- Exposes port 5000 for MLflow UI and API\n\nThis custom image is required because the standard MLflow image lacks S3 and PostgreSQL backend support needed by the infrastructure stack.\n\n### Tagging Strategy\n\nThe workflow implements a sophisticated tagging strategy [.github/workflows/mlflow-image-publish.yml:45-49]():\n\n| Tag Type | Format | Example | When Applied |\n|----------|--------|---------|--------------|\n| Branch reference | `type=ref,event=branch` | `master`, `main` | On branch push |\n| PR reference | `type=ref,event=pr` | `pr-123` | On pull request |\n| Commit SHA | `type=sha,prefix={{branch}}-` | `master-abc1234` | All pushes |\n| Latest | `type=raw,value=latest` | `latest` | Only on default branch |\n\nThis strategy provides:\n- **Stable tags** (`latest`) for production deployments\n- **Versioned tags** (commit SHA) for reproducibility\n- **Preview tags** (PR numbers) for testing\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:40-49]()\n\n### Build Summary Generation\n\nThe workflow generates a GitHub Actions job summary [.github/workflows/mlflow-image-publish.yml:63-80]() that displays:\n\n1. Image registry path\n2. All generated tags\n3. Supported platforms\n4. Usage example for `docker-compose.yml`\n\nThis summary provides immediate feedback on build artifacts and their consumption path.\n\n## Image Consumption\n\n### Infrastructure Stack Integration\n\nThe published images are referenced in the infrastructure stack:\n\n```mermaid\ngraph LR\n    subgraph \"GitHub Container Registry\"\n        MLFLOW_IMG[\"ghcr.io/repo/mlflow:latest\"]\n        EXAMPLE_IMG[\"ghcr.io/repo:master\"]\n    end\n    \n    subgraph \"docker-compose.yml\"\n        MLFLOW_SVC[\"mlflow service\"]\n        SETUP_SCRIPT[\"setup.sh script\"]\n    end\n    \n    subgraph \"Deployment\"\n        PULL[\"docker compose pull\"]\n        UP[\"docker compose up -d\"]\n    end\n    \n    MLFLOW_IMG --\u003e|referenced by| MLFLOW_SVC\n    EXAMPLE_IMG --\u003e|can be used by| SETUP_SCRIPT\n    \n    MLFLOW_SVC --\u003e|pulled by| PULL\n    PULL --\u003e|then| UP\n```\n\nThe MLflow service in [docker-compose.yml:16.88]() (referenced in high-level diagrams) uses the custom MLflow image. The `setup.sh` script [setup.sh:6.88]() (referenced in high-level diagrams) pulls the latest images before starting services.\n\n**Sources:** High-level system diagrams, [.github/workflows/mlflow-image-publish.yml:76-80]()\n\n### Local Development Usage\n\nDevelopers can pull and use the published images:\n\n```bash\n# Pull the latest MLflow image\ndocker pull ghcr.io/\u003crepository\u003e/mlflow:latest\n\n# Pull a specific version by commit SHA\ndocker pull ghcr.io/\u003crepository\u003e/mlflow:master-abc1234\n\n# Pull the example model image\ndocker pull ghcr.io/\u003crepository\u003e:master\n```\n\n## Security Features\n\n### Build Provenance Attestation\n\nThe example model pipeline generates cryptographically signed build provenance [.github/workflows/docker-publish.yml:45-50]():\n\n1. **Subject**: The published Docker image\n2. **Digest**: SHA256 hash of image layers\n3. **Provenance**: Build environment, inputs, and process\n4. **Storage**: Pushed to GHCR alongside image\n\nThis attestation enables:\n- **Supply chain verification**: Confirm image was built by official workflow\n- **Reproducibility**: Trace image back to exact source code and build environment\n- **Compliance**: Meet security and audit requirements\n\n### Authentication Security\n\nBoth workflows use `GITHUB_TOKEN` [.github/workflows/docker-publish.yml:28]() [.github/workflows/mlflow-image-publish.yml:38](), which:\n- Is automatically generated by GitHub Actions\n- Has minimal required permissions\n- Expires after workflow completion\n- Is scoped to the specific repository\n\n**Sources:** [.github/workflows/docker-publish.yml:14-28](), [.github/workflows/mlflow-image-publish.yml:22-38]()\n\n## Workflow Maintenance\n\n### Updating Dependencies\n\n**Example Model Dependencies**: Modify [examplemodel/requirements.txt:1-402]() and commit. The workflow rebuilds automatically on next push to `master`.\n\n**MLflow Base Image**: Update the `MLFLOW_VERSION` build argument in [dockerfile.mlflow:1]() to use a different MLflow release. The workflow rebuilds automatically on file change.\n\n**MLflow Python Packages**: Update `BOTO3_VERSION` or `PSYCOPG2_VERSION` arguments in [dockerfile.mlflow:2-3]() to pin specific versions.\n\n### Triggering Manual Builds\n\nThe MLflow workflow supports manual triggering:\n\n1. Navigate to GitHub Actions tab\n2. Select \"Build and Push MLflow Docker Image\" workflow\n3. Click \"Run workflow\" dropdown\n4. Select branch and click \"Run workflow\" button\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:13]()\n\n### Monitoring Build Status\n\nBuild status is visible:\n- **Repository badges**: Add workflow status badge to README\n- **GitHub Actions tab**: View all workflow runs and logs\n- **GHCR packages page**: View published images and tags\n- **Job summaries**: Generated markdown summary for each MLflow build [.github/workflows/mlflow-image-publish.yml:63-80]()\n\n**Sources:** [.github/workflows/docker-publish.yml:1-51](), [.github/workflows/mlflow-image-publish.yml:1-81]()"])</script><script>self.__next_f.push([1,"2c:T4659,"])</script><script>self.__next_f.push([1,"# Docker Image Publishing Workflows\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.dockerignore](.dockerignore)\n- [.github/workflows/docker-publish.yml](.github/workflows/docker-publish.yml)\n- [dockerfile.mlflow](dockerfile.mlflow)\n- [examplemodel/Dockerfile](examplemodel/Dockerfile)\n- [examplemodel/requirements.txt](examplemodel/requirements.txt)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document describes the automated Docker image publishing workflows implemented using GitHub Actions in the OpenGeoAIModelHub repository. These workflows automatically build and publish Docker images to GitHub Container Registry (GHCR) whenever code is pushed to the master branch. \n\nThe primary workflow covered here handles publishing the example model's Docker image. For details about the MLflow custom image build process, see [MLflow Custom Image Pipeline](#5.2). For information about deploying the published images, see [Infrastructure Deployment](#6.1).\n\n**Sources:** [.github/workflows/docker-publish.yml:1-51]()\n\n---\n\n## Workflow Overview\n\nThe Docker image publishing system uses GitHub Actions to automate the build-and-push process. The workflow is triggered on every push to the `master` branch and publishes the resulting image to `ghcr.io`, making it available for deployment in production environments.\n\n```mermaid\nflowchart TB\n    PUSH[\"Push to master branch\"]\n    TRIGGER[\"GitHub Actions Trigger\"]\n    CHECKOUT[\"Checkout repository\u003cbr/\u003e(actions/checkout@v4)\"]\n    LOGIN[\"Log in to GHCR\u003cbr/\u003e(docker/login-action@v3)\"]\n    META[\"Extract metadata\u003cbr/\u003e(docker/metadata-action@v5)\"]\n    BUILD[\"Build and push image\u003cbr/\u003e(docker/build-push-action@v3)\"]\n    ATTEST[\"Generate attestation\u003cbr/\u003e(actions/attest-build-provenance@v2)\"]\n    REGISTRY[\"ghcr.io registry\u003cbr/\u003eImage published\"]\n    \n    PUSH --\u003e TRIGGER\n    TRIGGER --\u003e CHECKOUT\n    CHECKOUT --\u003e LOGIN\n    LOGIN --\u003e META\n    META --\u003e BUILD\n    BUILD --\u003e ATTEST\n    ATTEST --\u003e REGISTRY\n    \n    BUILD -.-\u003e|\"uses context\"| CONTEXT[\"./examplemodel/\"]\n    BUILD -.-\u003e|\"uses Dockerfile\"| DOCKERFILE[\"examplemodel/Dockerfile\"]\n    BUILD -.-\u003e|\"ignores files\"| DOCKERIGNORE[\".dockerignore\"]\n```\n\n**Workflow Execution Flow**\n\nThe publishing process follows these stages:\n\n1. **Trigger**: Push to master branch activates the workflow\n2. **Checkout**: Repository code is checked out to the runner\n3. **Authentication**: GitHub token authenticates to GHCR\n4. **Metadata Generation**: Tags and labels are extracted from repository metadata\n5. **Build and Push**: Docker image is built from the examplemodel context and pushed to GHCR\n6. **Attestation**: Build provenance is generated and attached to the image\n\n**Sources:** [.github/workflows/docker-publish.yml:3-51]()\n\n---\n\n## GitHub Actions Workflow Configuration\n\n### Workflow Trigger and Environment\n\nThe workflow is defined in `.github/workflows/docker-publish.yml` and uses the following configuration:\n\n| Configuration | Value | Purpose |\n|--------------|-------|---------|\n| **Workflow Name** | `Create and publish a Docker image` | Identifies the workflow in GitHub UI |\n| **Trigger** | `push` to `branches: [\"master\"]` | Activates on master branch commits |\n| **Registry** | `ghcr.io` | GitHub Container Registry endpoint |\n| **Image Name** | `${{ github.repository }}` | Uses repository name as image name |\n\nThe workflow defines two critical environment variables at the workflow level:\n\n```yaml\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n```\n\nThese variables are referenced throughout the workflow to ensure consistent registry and naming conventions.\n\n**Sources:** [.github/workflows/docker-publish.yml:1-10]()\n\n### Job Permissions\n\nThe `build-and-push-image` job requires specific permissions to interact with GitHub resources:\n\n```mermaid\ngraph LR\n    JOB[\"build-and-push-image job\"]\n    \n    JOB --\u003e|\"read\"| CONTENTS[\"contents\u003cbr/\u003e(read repository)\"]\n    JOB --\u003e|\"write\"| PACKAGES[\"packages\u003cbr/\u003e(push to GHCR)\"]\n    JOB --\u003e|\"write\"| ATTESTATIONS[\"attestations\u003cbr/\u003e(generate provenance)\"]\n    JOB --\u003e|\"write\"| IDTOKEN[\"id-token\u003cbr/\u003e(OIDC authentication)\"]\n```\n\n**Permission Mapping**\n\n| Permission | Access Level | Usage |\n|-----------|-------------|-------|\n| `contents` | `read` | Access repository files for build context |\n| `packages` | `write` | Push Docker images to GitHub Packages (GHCR) |\n| `attestations` | `write` | Attach build provenance attestations to images |\n| `id-token` | `write` | Generate OIDC tokens for secure attestation signing |\n\n**Sources:** [.github/workflows/docker-publish.yml:11-18]()\n\n---\n\n## Workflow Steps Breakdown\n\n### Step 1: Repository Checkout\n\n```mermaid\ngraph LR\n    STEP1[\"Checkout repository\"]\n    ACTION[\"actions/checkout@v4\"]\n    RUNNER[\"GitHub Runner\"]\n    REPO[\"Repository code\"]\n    \n    STEP1 --\u003e|\"uses\"| ACTION\n    ACTION --\u003e|\"clones to\"| RUNNER\n    RUNNER --\u003e|\"contains\"| REPO\n```\n\nThe first step uses `actions/checkout@v4` to clone the repository to the GitHub Actions runner. This provides access to all files needed for the Docker build, including the Dockerfile and build context.\n\n**Sources:** [.github/workflows/docker-publish.yml:21-22]()\n\n### Step 2: Container Registry Authentication\n\nThe workflow authenticates to GHCR using the `docker/login-action@v3`:\n\n```yaml\n- name: Log in to the Container registry\n  uses: docker/login-action@v3\n  with:\n    registry: ${{ env.REGISTRY }}\n    username: ${{ github.actor }}\n    password: ${{ secrets.GITHUB_TOKEN }}\n```\n\n**Authentication Parameters**\n\n| Parameter | Value | Description |\n|-----------|-------|-------------|\n| `registry` | `ghcr.io` | Target container registry |\n| `username` | `${{ github.actor }}` | GitHub user who triggered the workflow |\n| `password` | `${{ secrets.GITHUB_TOKEN }}` | Automatically provided GitHub token with package write permissions |\n\nThe `GITHUB_TOKEN` is automatically available in GitHub Actions and has the necessary permissions to push to GHCR when the job has `packages: write` permission.\n\n**Sources:** [.github/workflows/docker-publish.yml:23-28]()\n\n### Step 3: Metadata Extraction\n\nThe `docker/metadata-action@v5` generates Docker tags and OCI-compliant labels from repository metadata:\n\n```mermaid\ngraph TB\n    META_ACTION[\"docker/metadata-action@v5\"]\n    \n    INPUTS[\"Input:\u003cbr/\u003eimages: ghcr.io/repo\"]\n    \n    OUTPUTS[\"Outputs:\u003cbr/\u003etags (latest, sha, etc)\u003cbr/\u003elabels (OCI standard)\"]\n    \n    GITHUB_CONTEXT[\"GitHub Context:\u003cbr/\u003erepository name\u003cbr/\u003ecommit SHA\u003cbr/\u003eref/branch\"]\n    \n    INPUTS --\u003e META_ACTION\n    GITHUB_CONTEXT -.-\u003e|\"informs\"| META_ACTION\n    META_ACTION --\u003e OUTPUTS\n    \n    OUTPUTS --\u003e|\"passed to\"| BUILD_STEP[\"Build step\"]\n```\n\nThe action automatically generates tags based on the Git reference (branch, tag, or SHA) and creates standard OCI labels including repository URL, commit SHA, and creation timestamp.\n\n**Sources:** [.github/workflows/docker-publish.yml:29-33]()\n\n### Step 4: Build and Push Image\n\nThe core build step uses `docker/build-push-action@v3`:\n\n```mermaid\ngraph TB\n    BUILD[\"docker/build-push-action@v3\u003cbr/\u003e(Step: Build and push Docker image)\"]\n    \n    subgraph \"Build Configuration\"\n        CONTEXT[\"context: ./examplemodel\"]\n        DOCKERFILE[\"file: examplemodel/Dockerfile\"]\n        PUSH[\"push: true\"]\n        TAGS[\"tags: from metadata step\"]\n        LABELS[\"labels: from metadata step\"]\n    end\n    \n    subgraph \"Build Artifacts\"\n        BUILDCONTEXT[\"Build Context Files:\u003cbr/\u003eexamplemodel/\"]\n        REQUIREMENTS[\"requirements.txt\"]\n        SOURCE[\"Source code\"]\n        MLPROJECT[\"MLproject\"]\n    end\n    \n    subgraph \"Output\"\n        DIGEST[\"Image digest\u003cbr/\u003e(SHA256 hash)\"]\n        IMAGE[\"Published image\u003cbr/\u003eghcr.io/repo:tag\"]\n    end\n    \n    BUILD --\u003e CONTEXT\n    BUILD --\u003e DOCKERFILE\n    BUILD --\u003e PUSH\n    BUILD --\u003e TAGS\n    BUILD --\u003e LABELS\n    \n    CONTEXT -.-\u003e|\"includes\"| BUILDCONTEXT\n    BUILDCONTEXT --\u003e REQUIREMENTS\n    BUILDCONTEXT --\u003e SOURCE\n    BUILDCONTEXT --\u003e MLPROJECT\n    \n    BUILD --\u003e DIGEST\n    BUILD --\u003e IMAGE\n    \n    DOCKERFILE -.-\u003e|\"referenced from\"| DOCKERIGNORE[\".dockerignore\u003cbr/\u003e(exclusions)\"]\n```\n\n**Build Configuration Details**\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `context` | `./examplemodel` | Directory containing files to include in build context |\n| `push` | `true` | Push the built image to the registry |\n| `file` | `examplemodel/Dockerfile` | Path to Dockerfile relative to repository root |\n| `tags` | `${{ steps.meta.outputs.tags }}` | Image tags from metadata action |\n| `labels` | `${{ steps.meta.outputs.labels }}` | OCI labels from metadata action |\n\nThe build context (`./examplemodel`) includes all files in the examplemodel directory except those excluded by `.dockerignore`.\n\n**Sources:** [.github/workflows/docker-publish.yml:35-43]()\n\n### Step 5: Build Provenance Attestation\n\nThe final step generates cryptographic attestations for supply chain security:\n\n```mermaid\ngraph TB\n    ATTEST[\"actions/attest-build-provenance@v2\"]\n    \n    subgraph \"Inputs\"\n        SUBJECT_NAME[\"subject-name:\u003cbr/\u003eghcr.io/repo\"]\n        SUBJECT_DIGEST[\"subject-digest:\u003cbr/\u003efrom push step output\"]\n        PUSH_TO_REG[\"push-to-registry: true\"]\n    end\n    \n    subgraph \"Attestation Process\"\n        SLSA[\"Generate SLSA provenance\"]\n        SIGN[\"Sign with OIDC token\"]\n        ATTACH[\"Attach to image\"]\n    end\n    \n    subgraph \"Output\"\n        ATTESTATION[\"Attestation artifact\u003cbr/\u003e(in-toto format)\"]\n        REGISTRY_ATTACH[\"Stored in GHCR\u003cbr/\u003ealongside image\"]\n    end\n    \n    ATTEST --\u003e SUBJECT_NAME\n    ATTEST --\u003e SUBJECT_DIGEST\n    ATTEST --\u003e PUSH_TO_REG\n    \n    SUBJECT_NAME --\u003e SLSA\n    SUBJECT_DIGEST --\u003e SLSA\n    SLSA --\u003e SIGN\n    SIGN --\u003e ATTACH\n    ATTACH --\u003e ATTESTATION\n    ATTESTATION --\u003e REGISTRY_ATTACH\n```\n\n**Attestation Parameters**\n\n| Parameter | Value | Purpose |\n|-----------|-------|---------|\n| `subject-name` | `${{ env.REGISTRY }}/${{ env.IMAGE_NAME}}` | Identifies the attested image |\n| `subject-digest` | `${{ steps.push.outputs.digest }}` | SHA256 digest of the built image |\n| `push-to-registry` | `true` | Stores attestation in GHCR alongside the image |\n\nThe attestation provides verifiable proof of:\n- Where the image was built (GitHub Actions)\n- When it was built (timestamp)\n- What source code was used (commit SHA)\n- How it was built (workflow definition)\n\nThis enables supply chain verification using tools like `cosign` or `slsa-verifier`.\n\n**Sources:** [.github/workflows/docker-publish.yml:45-50]()\n\n---\n\n## Build Context and Docker Configuration\n\n### Example Model Build Context\n\nThe workflow builds from the `./examplemodel` directory, which contains:\n\n```mermaid\ngraph TB\n    CONTEXT[\"./examplemodel/\u003cbr/\u003e(Build Context Root)\"]\n    \n    subgraph \"Core Files\"\n        DOCKERFILE_FILE[\"Dockerfile\u003cbr/\u003e(Build instructions)\"]\n        REQUIREMENTS[\"requirements.txt\u003cbr/\u003e(Python dependencies)\"]\n        MLPROJECT_FILE[\"MLproject\u003cbr/\u003e(MLflow entry points)\"]\n    end\n    \n    subgraph \"Source Code\"\n        SRC[\"src/\u003cbr/\u003e(Python modules)\"]\n        TRAIN[\"train.py\"]\n        INFERENCE[\"inference.py\"]\n        MODEL[\"model.py\"]\n    end\n    \n    subgraph \"Excluded Files\"\n        IGNORE[\".dockerignore\u003cbr/\u003e(at repo root)\"]\n        EXCLUDED[\"Excluded:\u003cbr/\u003e*.md, .git, __pycache__,\u003cbr/\u003evenv/, .vscode/, etc.\"]\n    end\n    \n    CONTEXT --\u003e DOCKERFILE_FILE\n    CONTEXT --\u003e REQUIREMENTS\n    CONTEXT --\u003e MLPROJECT_FILE\n    CONTEXT --\u003e SRC\n    \n    SRC --\u003e TRAIN\n    SRC --\u003e INFERENCE\n    SRC --\u003e MODEL\n    \n    IGNORE -.-\u003e|\"filters\"| CONTEXT\n    IGNORE --\u003e EXCLUDED\n```\n\n**Sources:** [.github/workflows/docker-publish.yml:39](), [examplemodel/Dockerfile:1-15]()\n\n### Dockerfile Structure\n\nThe `examplemodel/Dockerfile` defines the image build process:\n\n**Base Image and System Dependencies**\n\n```dockerfile\nFROM python:3.11-slim-bookworm\n```\n\nThe image uses Python 3.11 on Debian Bookworm (slim variant for smaller image size). It installs GDAL for geospatial data processing:\n\n```dockerfile\nRUN apt-get update \u0026\u0026 apt-get install -y --no-install-recommends \\\n    gdal-bin \\\n    libgdal-dev \\\n    \u0026\u0026 apt-get clean \\\n    \u0026\u0026 rm -rf /var/lib/apt/lists/*\n```\n\n**Application Setup**\n\nThe Dockerfile sets the working directory and installs Python dependencies:\n\n```dockerfile\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY . .\n```\n\n**Layer Optimization**\n\nThe build process is optimized for layer caching:\n1. Install system dependencies (rarely changes)\n2. Copy `requirements.txt` only (triggers rebuild only when dependencies change)\n3. Install Python packages\n4. Copy application code (changes most frequently)\n\n**Sources:** [examplemodel/Dockerfile:1-15]()\n\n### Excluded Files\n\nThe `.dockerignore` file at the repository root excludes unnecessary files from the build context:\n\n| Category | Patterns | Reason for Exclusion |\n|----------|----------|---------------------|\n| Version Control | `.git`, `.gitignore` | Not needed in container |\n| Python Artifacts | `__pycache__/`, `*.pyc`, `.pytest_cache/` | Generated files, recreated during build |\n| Virtual Environments | `venv/`, `env/`, `ENV/` | Dependencies installed via requirements.txt |\n| IDE Files | `.vscode/`, `.idea/`, `*.swp` | Editor-specific configuration |\n| Documentation | `*.md`, `docs/` | Not needed at runtime |\n| Build Artifacts | `build/`, `dist/`, `*.egg-info/` | Intermediate files |\n\nExcluding these files reduces the build context size, speeding up uploads to the build environment and reducing final image size.\n\n**Sources:** [.dockerignore:1-51]()\n\n---\n\n## Image Tagging Strategy\n\nThe `docker/metadata-action@v5` generates image tags based on Git references:\n\n```mermaid\ngraph TB\n    GIT_EVENT[\"Git Push Event\"]\n    \n    subgraph \"Branch Push\"\n        MASTER_PUSH[\"Push to master\"]\n        MASTER_TAG[\"Tag: latest\"]\n        MASTER_SHA[\"Tag: sha-COMMIT\"]\n    end\n    \n    subgraph \"Tag Push\"\n        VERSION_PUSH[\"Push tag v1.0.0\"]\n        VERSION_TAG[\"Tag: 1.0.0\"]\n        VERSION_MAJOR[\"Tag: 1.0\"]\n        VERSION_MINOR[\"Tag: 1\"]\n    end\n    \n    subgraph \"Pull Request\"\n        PR_PUSH[\"Push to PR branch\"]\n        PR_TAG[\"Tag: pr-123\"]\n    end\n    \n    GIT_EVENT --\u003e|\"branch: master\"| MASTER_PUSH\n    MASTER_PUSH --\u003e MASTER_TAG\n    MASTER_PUSH --\u003e MASTER_SHA\n    \n    GIT_EVENT --\u003e|\"tag: v1.0.0\"| VERSION_PUSH\n    VERSION_PUSH --\u003e VERSION_TAG\n    VERSION_PUSH --\u003e VERSION_MAJOR\n    VERSION_PUSH --\u003e VERSION_MINOR\n    \n    GIT_EVENT --\u003e|\"PR branch\"| PR_PUSH\n    PR_PUSH --\u003e PR_TAG\n```\n\n**Default Tagging Behavior**\n\nFor pushes to `master` (the configured trigger branch), the action generates:\n- `latest`: Points to the most recent master build\n- `sha-\u003ccommit\u003e`: Immutable tag referencing specific commit\n\n**Image Location**\n\nThe published image is available at:\n```\nghcr.io/\u003cgithub-username\u003e/\u003crepo-name\u003e:\u003ctag\u003e\n```\n\nFor example, if the repository is `kshitijrajsharma/opengeoaimodelshub`, images are published to:\n```\nghcr.io/kshitijrajsharma/opengeoaimodelshub:latest\nghcr.io/kshitijrajsharma/opengeoaimodelshub:sha-abc123\n```\n\n**Sources:** [.github/workflows/docker-publish.yml:29-33]()\n\n---\n\n## Relationship to Infrastructure Stack\n\nThe published Docker image integrates with the broader infrastructure system:\n\n```mermaid\ngraph TB\n    subgraph \"GitHub Actions\"\n        WORKFLOW[\"docker-publish.yml\u003cbr/\u003eWorkflow\"]\n        BUILD_JOB[\"build-and-push-image\u003cbr/\u003eJob\"]\n    end\n    \n    subgraph \"GitHub Container Registry\"\n        GHCR[\"ghcr.io\"]\n        IMAGE_LATEST[\"opengeoaimodelshub:latest\"]\n        IMAGE_SHA[\"opengeoaimodelshub:sha-abc123\"]\n    end\n    \n    subgraph \"Deployment Environment\"\n        SETUP[\"setup.sh\u003cbr/\u003eInfrastructure Setup\"]\n        DOCKER_COMPOSE[\"docker-compose.yml\"]\n        MLFLOW_SERVICE[\"MLflow Service\"]\n    end\n    \n    subgraph \"Example Model Usage\"\n        TRAIN_PY[\"train.py\u003cbr/\u003eTraining Pipeline\"]\n        INFERENCE_PY[\"inference.py\u003cbr/\u003eInference System\"]\n    end\n    \n    WORKFLOW --\u003e BUILD_JOB\n    BUILD_JOB --\u003e|\"pushes to\"| GHCR\n    GHCR --\u003e IMAGE_LATEST\n    GHCR --\u003e IMAGE_SHA\n    \n    SETUP -.-\u003e|\"may pull\"| IMAGE_LATEST\n    DOCKER_COMPOSE -.-\u003e|\"references (for custom services)\"| IMAGE_LATEST\n    \n    IMAGE_LATEST -.-\u003e|\"provides environment for\"| TRAIN_PY\n    IMAGE_LATEST -.-\u003e|\"provides environment for\"| INFERENCE_PY\n    \n    TRAIN_PY --\u003e|\"logs experiments to\"| MLFLOW_SERVICE\n    INFERENCE_PY --\u003e|\"logs results to\"| MLFLOW_SERVICE\n```\n\n**Integration Points**\n\n1. **Infrastructure Setup**: The `setup.sh` script can pull the published image for custom service deployment\n2. **MLflow Execution**: The image provides a consistent environment for MLflow project runs\n3. **Example Model Training**: The published image includes all dependencies needed for the training pipeline\n4. **Inference Deployment**: The image can be deployed to run inference services\n\n**Sources:** [.github/workflows/docker-publish.yml:1-51]()\n\n---\n\n## Workflow Execution Summary\n\nThe complete workflow execution can be summarized as follows:\n\n| Stage | Action | Input | Output |\n|-------|--------|-------|--------|\n| **Trigger** | Push to master detected | Git push event | Workflow initiated |\n| **Checkout** | `actions/checkout@v4` | Repository URL | Code on runner |\n| **Authentication** | `docker/login-action@v3` | GITHUB_TOKEN | Authenticated Docker client |\n| **Metadata** | `docker/metadata-action@v5` | Repository metadata | Tags and labels |\n| **Build** | `docker/build-push-action@v3` | Dockerfile, build context | Published image, digest |\n| **Attestation** | `actions/attest-build-provenance@v2` | Image digest | SLSA provenance |\n\n**Workflow File Location**: [.github/workflows/docker-publish.yml:1-51]()\n\n**Related Files**:\n- Build Instructions: [examplemodel/Dockerfile:1-15]()\n- Context Exclusions: [.dockerignore:1-51]()\n- Python Dependencies: [examplemodel/requirements.txt:1-402]()\n\n**Sources:** [.github/workflows/docker-publish.yml:1-51](), [examplemodel/Dockerfile:1-15](), [.dockerignore:1-51](), [examplemodel/requirements.txt:1-402]()"])</script><script>self.__next_f.push([1,"2d:T3e5e,"])</script><script>self.__next_f.push([1,"# MLflow Custom Image Pipeline\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.dockerignore](.dockerignore)\n- [.github/workflows/docker-publish.yml](.github/workflows/docker-publish.yml)\n- [.github/workflows/mlflow-image-publish.yml](.github/workflows/mlflow-image-publish.yml)\n- [dockerfile.mlflow](dockerfile.mlflow)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document details the automated build and publishing pipeline for the custom MLflow Docker image used in the OpenGeoAIModelHub infrastructure stack. The pipeline extends the official MLflow base image with additional dependencies required for S3-compatible storage (MinIO) and PostgreSQL backend connectivity.\n\nFor information about the general Docker image publishing workflows including the example model image, see [Docker Image Publishing Workflows](#5.1). For details on how this MLflow image integrates with the infrastructure stack, see [MLflow Tracking Server](#4.5).\n\n## MLflow Custom Image Overview\n\nThe custom MLflow image serves as the foundation for the MLflow tracking server in the infrastructure stack. It extends the official `ghcr.io/mlflow/mlflow` base image by adding two critical Python dependencies:\n\n- **boto3**: Enables S3-compatible object storage integration with MinIO\n- **psycopg2-binary**: Provides PostgreSQL database connectivity for the backend store\n\nThe image is automatically built and published to GitHub Container Registry (GHCR) whenever changes are pushed to the repository, ensuring the infrastructure stack always has access to an up-to-date image.\n\n**Sources:** [dockerfile.mlflow:1-30](), [.github/workflows/mlflow-image-publish.yml:1-81]()\n\n## Dockerfile Structure\n\n### Build Arguments and Base Image\n\nThe Dockerfile uses a multi-stage build approach with configurable build arguments:\n\n```dockerfile\nARG MLFLOW_VERSION=v3.1.1\nARG BOTO3_VERSION=\"\"\nARG PSYCOPG2_VERSION=\"\"\n\nFROM ghcr.io/mlflow/mlflow:${MLFLOW_VERSION} as base\n```\n\n| Build Argument | Default Value | Purpose |\n|----------------|---------------|---------|\n| `MLFLOW_VERSION` | `v3.1.1` | Specifies the MLflow base image version |\n| `BOTO3_VERSION` | `\"\"` (empty) | Optional version pin for boto3 |\n| `PSYCOPG2_VERSION` | `\"\"` (empty) | Optional version pin for psycopg2-binary |\n\nThe empty string defaults allow for installing the latest compatible versions of dependencies unless explicitly pinned.\n\n**Sources:** [dockerfile.mlflow:1-5]()\n\n### Environment Configuration\n\nThe image configures Python environment variables for optimal container execution:\n\n| Environment Variable | Value | Purpose |\n|---------------------|-------|---------|\n| `PYTHONUNBUFFERED` | `1` | Ensures real-time log output without buffering |\n| `PYTHONDONTWRITEBYTECODE` | `1` | Prevents `.pyc` file generation |\n| `PIP_NO_CACHE_DIR` | `1` | Reduces image size by disabling pip cache |\n| `PIP_DISABLE_PIP_VERSION_CHECK` | `1` | Suppresses pip version warnings |\n\n**Sources:** [dockerfile.mlflow:7-11]()\n\n### Dependency Installation\n\nThe build process dynamically generates a `requirements.txt` file with conditional version pinning:\n\n```dockerfile\nRUN echo \"boto3${BOTO3_VERSION:+==}${BOTO3_VERSION}\" \u003e /tmp/requirements.txt \u0026\u0026 \\\n    echo \"psycopg2-binary${PSYCOPG2_VERSION:+==}${PSYCOPG2_VERSION}\" \u003e\u003e /tmp/requirements.txt\n\nRUN pip install --no-cache-dir -r /tmp/requirements.txt \u0026\u0026 \\\n    rm /tmp/requirements.txt\n```\n\nThe bash parameter expansion `${BOTO3_VERSION:+==}${BOTO3_VERSION}` ensures that if the build argument is empty, no version constraint is applied. If a version is specified, it adds `==\u003cversion\u003e` to the requirement.\n\n**Sources:** [dockerfile.mlflow:16-20]()\n\n### Final Stage Configuration\n\nThe final stage uses a multi-stage copy pattern to ensure a clean image:\n\n```dockerfile\nFROM base as final\n\nCOPY --from=base /usr/local/lib/python*/site-packages /usr/local/lib/python*/site-packages\nCOPY --from=base /usr/local/bin /usr/local/bin\n\nWORKDIR /app\nEXPOSE 5000\n```\n\nThe wildcard pattern `python*/site-packages` ensures compatibility across different Python versions in the base image.\n\n**Sources:** [dockerfile.mlflow:22-30]()\n\n## GitHub Actions Workflow Architecture\n\n### Workflow Trigger Configuration\n\nThe workflow is triggered by three mechanisms:\n\n```yaml\non:\n  push:\n    branches: [main, master]\n    paths:\n      - \"dockerfile.mlflow\"\n      - \".github/workflows/build-mlflow.yml\"\n  pull_request:\n    branches: [main, master]\n    paths:\n      - \"dockerfile.mlflow\"\n  workflow_dispatch:\n```\n\n| Trigger Type | Branches | Path Filters | Purpose |\n|-------------|----------|--------------|---------|\n| `push` | `main`, `master` | Dockerfile and workflow file | Automatic builds on commits |\n| `pull_request` | `main`, `master` | Dockerfile only | Validation builds for PRs |\n| `workflow_dispatch` | N/A | N/A | Manual triggering via UI |\n\nThe path filters optimize CI/CD resource usage by only triggering builds when relevant files change.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:3-13]()\n\n### Workflow Environment Variables\n\n```yaml\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}/mlflow\n```\n\nThe `IMAGE_NAME` dynamically constructs the full image path as `ghcr.io/\u003cowner\u003e/\u003crepo\u003e/mlflow`, ensuring consistent naming across forks.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:15-17]()\n\n### Workflow Permissions\n\nThe job requires specific GitHub token permissions:\n\n| Permission | Access Level | Purpose |\n|-----------|-------------|---------|\n| `contents` | `read` | Checkout repository code |\n| `packages` | `write` | Push images to GHCR |\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:22-24]()\n\n## Build Process Flow\n\n```mermaid\ngraph TB\n    subgraph \"GitHub Actions Runner\"\n        TRIGGER[\"Workflow Trigger\u003cbr/\u003e(push/PR/manual)\"]\n        CHECKOUT[\"Checkout Repository\u003cbr/\u003eactions/checkout@v4\"]\n        BUILDX[\"Setup Docker Buildx\u003cbr/\u003edocker/setup-buildx-action@v3\"]\n        LOGIN[\"Login to GHCR\u003cbr/\u003edocker/login-action@v3\"]\n        META[\"Extract Metadata\u003cbr/\u003edocker/metadata-action@v5\"]\n        BUILD[\"Build and Push Image\u003cbr/\u003edocker/build-push-action@v5\"]\n        SUMMARY[\"Generate Summary\u003cbr/\u003eGITHUB_STEP_SUMMARY\"]\n    end\n    \n    subgraph \"External Resources\"\n        GHCR_BASE[\"ghcr.io/mlflow/mlflow:v3.1.1\u003cbr/\u003eBase Image\"]\n        REPO[\"GitHub Repository\u003cbr/\u003edockerfile.mlflow\"]\n    end\n    \n    subgraph \"Output Artifacts\"\n        GHCR_OUT[\"ghcr.io/\u003crepo\u003e/mlflow:latest\u003cbr/\u003eghcr.io/\u003crepo\u003e/mlflow:\u003cbranch\u003e\u003cbr/\u003eghcr.io/\u003crepo\u003e/mlflow:\u003cbranch\u003e-\u003csha\u003e\"]\n        CACHE[\"GitHub Actions Cache\u003cbr/\u003eBuild layers\"]\n    end\n    \n    TRIGGER --\u003e CHECKOUT\n    CHECKOUT --\u003e BUILDX\n    BUILDX --\u003e LOGIN\n    LOGIN --\u003e META\n    META --\u003e BUILD\n    \n    REPO --\u003e CHECKOUT\n    GHCR_BASE --\u003e BUILD\n    \n    BUILD --\u003e GHCR_OUT\n    BUILD --\u003e CACHE\n    BUILD --\u003e SUMMARY\n    \n    CACHE -.-\u003e BUILD\n```\n\n**Diagram: MLflow Image Build Pipeline Flow**\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:26-80]()\n\n### Step-by-Step Breakdown\n\n#### 1. Repository Checkout\n\n```yaml\n- name: Checkout repository\n  uses: actions/checkout@v4\n```\n\nClones the repository to access [dockerfile.mlflow:1-30]().\n\n#### 2. Docker Buildx Setup\n\n```yaml\n- name: Set up Docker Buildx\n  uses: docker/setup-buildx-action@v3\n```\n\nEnables multi-platform builds and build caching through the Buildx builder.\n\n#### 3. Container Registry Authentication\n\n```yaml\n- name: Log in to Container Registry\n  uses: docker/login-action@v3\n  with:\n    registry: ${{ env.REGISTRY }}\n    username: ${{ github.actor }}\n    password: ${{ secrets.GITHUB_TOKEN }}\n```\n\nAuthenticates to GHCR using the automatically provided `GITHUB_TOKEN`, scoped to the repository.\n\n#### 4. Metadata Extraction\n\n```yaml\n- name: Extract metadata\n  id: meta\n  uses: docker/metadata-action@v5\n  with:\n    images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n    tags: |\n      type=ref,event=branch\n      type=ref,event=pr\n      type=sha,prefix={{branch}}-\n      type=raw,value=latest,enable={{is_default_branch}}\n```\n\nGenerates Docker tags based on Git context. See the Image Tagging Strategy section below for details.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:26-50]()\n\n#### 5. Build and Push\n\n```yaml\n- name: Build and push Docker image\n  uses: docker/build-push-action@v5\n  with:\n    context: .\n    file: ./dockerfile.mlflow\n    push: true\n    tags: ${{ steps.meta.outputs.tags }}\n    labels: ${{ steps.meta.outputs.labels }}\n    platforms: linux/amd64,linux/arm64\n    cache-from: type=gha\n    cache-to: type=gha,mode=max\n```\n\nKey configuration parameters:\n\n| Parameter | Value | Effect |\n|-----------|-------|--------|\n| `context` | `.` | Root directory as build context |\n| `file` | `./dockerfile.mlflow` | Explicit Dockerfile path |\n| `push` | `true` | Publish image to registry |\n| `platforms` | `linux/amd64,linux/arm64` | Multi-architecture support |\n| `cache-from` | `type=gha` | Use GitHub Actions cache for layers |\n| `cache-to` | `type=gha,mode=max` | Persist all layers to cache |\n\nThe `mode=max` cache setting caches all intermediate layers, optimizing subsequent builds.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:51-61]()\n\n#### 6. Summary Generation\n\n```yaml\n- name: Generate summary\n  run: |\n    echo \"## Docker Image Built Successfully! \" \u003e\u003e $GITHUB_STEP_SUMMARY\n    echo \"\" \u003e\u003e $GITHUB_STEP_SUMMARY\n    echo \"**Image:** \\`${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\\`\" \u003e\u003e $GITHUB_STEP_SUMMARY\n    echo \"\" \u003e\u003e $GITHUB_STEP_SUMMARY\n    echo \"**Tags:**\" \u003e\u003e $GITHUB_STEP_SUMMARY\n    echo \"\\`\\`\\`\" \u003e\u003e $GITHUB_STEP_SUMMARY\n    echo \"${{ steps.meta.outputs.tags }}\" \u003e\u003e $GITHUB_STEP_SUMMARY\n    echo \"\\`\\`\\`\" \u003e\u003e $GITHUB_STEP_SUMMARY\n```\n\nCreates a GitHub Actions job summary visible in the Actions UI, displaying the built image name, applied tags, and usage examples.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:63-80]()\n\n## Image Tagging Strategy\n\nThe metadata extraction step applies multiple tag types based on Git context:\n\n```mermaid\ngraph LR\n    subgraph \"Git Context\"\n        BRANCH[\"Branch: master\"]\n        COMMIT[\"Commit: abc1234\"]\n        PR[\"Pull Request: #42\"]\n    end\n    \n    subgraph \"Tag Generation Rules\"\n        RULE1[\"type=ref,event=branch\"]\n        RULE2[\"type=ref,event=pr\"]\n        RULE3[\"type=sha,prefix={{branch}}-\"]\n        RULE4[\"type=raw,value=latest\u003cbr/\u003eenable={{is_default_branch}}\"]\n    end\n    \n    subgraph \"Generated Tags\"\n        TAG1[\"ghcr.io/.../mlflow:master\"]\n        TAG2[\"ghcr.io/.../mlflow:pr-42\"]\n        TAG3[\"ghcr.io/.../mlflow:master-abc1234\"]\n        TAG4[\"ghcr.io/.../mlflow:latest\"]\n    end\n    \n    BRANCH --\u003e RULE1 --\u003e TAG1\n    PR --\u003e RULE2 --\u003e TAG2\n    COMMIT --\u003e RULE3 --\u003e TAG3\n    BRANCH --\u003e RULE4 --\u003e TAG4\n```\n\n**Diagram: Image Tag Generation Based on Git Context**\n\n### Tag Type Reference\n\n| Tag Type | Format | When Applied | Example |\n|----------|--------|--------------|---------|\n| Branch reference | `\u003cbranch-name\u003e` | On push to any branch | `master`, `feature-xyz` |\n| PR reference | `pr-\u003cnumber\u003e` | On pull request builds | `pr-42` |\n| SHA with prefix | `\u003cbranch\u003e-\u003cshort-sha\u003e` | Always | `master-abc1234` |\n| Latest | `latest` | Only on default branch (`master`) | `latest` |\n\nThis strategy ensures:\n- **Traceability**: SHA-based tags link images to specific commits\n- **Convenience**: `latest` tag provides easy access to production image\n- **Testing**: PR tags enable validation before merging\n- **Branch tracking**: Branch tags allow testing branch-specific changes\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:40-49]()\n\n## Multi-Platform Support\n\nThe workflow builds images for both `linux/amd64` and `linux/arm64` architectures:\n\n```yaml\nplatforms: linux/amd64,linux/arm64\n```\n\n### Architecture Support Matrix\n\n| Platform | Use Case | Supported |\n|----------|----------|-----------|\n| `linux/amd64` | x86_64 servers, cloud VMs |  |\n| `linux/arm64` | ARM servers, Apple Silicon, Raspberry Pi |  |\n\nDocker automatically selects the appropriate image variant based on the host architecture when pulling the image.\n\n### Build Cache Strategy\n\n```yaml\ncache-from: type=gha\ncache-to: type=gha,mode=max\n```\n\nThe GitHub Actions cache integration provides:\n\n1. **Layer Reuse**: Unchanged layers are fetched from cache instead of rebuilt\n2. **Cross-Build Optimization**: Cache is shared between platform builds\n3. **Persistent Storage**: Cache survives between workflow runs\n\nThe `mode=max` setting caches all intermediate layers, not just final stage layers, maximizing build speed at the cost of cache size.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:59-61]()\n\n## Docker Build Context and Exclusions\n\nThe build uses the repository root (`.`) as the context, with exclusions defined in [.dockerignore:1-51]():\n\n### Excluded Categories\n\n| Category | Patterns | Rationale |\n|----------|----------|-----------|\n| Git metadata | `.git`, `.gitignore` | Reduces image size, no runtime value |\n| Python artifacts | `__pycache__/`, `*.pyc`, `*.egg-info/` | Build-time generated, not needed in image |\n| Virtual environments | `venv/`, `env/`, `ENV/` | Local dev environments |\n| IDE files | `.vscode/`, `.idea/`, `*.swp` | Editor-specific configurations |\n| Documentation | `*.md`, `docs/` | Not needed at runtime |\n| Build artifacts | `build/`, `dist/`, `.cache/` | Intermediate build outputs |\n\nDespite using the root context, the MLflow image build only requires [dockerfile.mlflow:1-30]() itself, as it starts from the official MLflow base image and adds minimal dependencies.\n\n**Sources:** [.dockerignore:1-51](), [.github/workflows/mlflow-image-publish.yml:54-55]()\n\n## Integration with Infrastructure Stack\n\nThe published image integrates with the infrastructure stack defined in the `docker-compose.yml`:\n\n```yaml\nmlflow:\n  image: ghcr.io/\u003crepository\u003e/mlflow:latest\n  # ... service configuration\n```\n\nThe workflow summary provides copy-paste ready configuration:\n\n```\n**Usage in docker-compose.yml:**\n```yaml\nmlflow:\n  image: ghcr.io/\u003crepository\u003e/mlflow:latest\n```\n```\n\nThis automated documentation helps users quickly integrate the custom image into their deployment.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:76-80]()\n\n## Build Workflow Comparison\n\nThe MLflow image pipeline differs from the example model image pipeline ([.github/workflows/docker-publish.yml:1-51]()):\n\n| Aspect | MLflow Pipeline | Example Model Pipeline |\n|--------|----------------|------------------------|\n| Trigger paths | `dockerfile.mlflow`, workflow file | `examplemodel/` directory |\n| Multi-platform |  (amd64, arm64) |  (single platform) |\n| Build context | Repository root | `./examplemodel` |\n| Dockerfile location | `./dockerfile.mlflow` | `examplemodel/Dockerfile` |\n| Image name | `\u003crepo\u003e/mlflow` | `\u003crepo\u003e` |\n| Attestation |  |  (provenance attestation) |\n| Cache strategy | GitHub Actions cache | No caching configured |\n\nThe MLflow pipeline prioritizes platform compatibility and build performance, while the example model pipeline focuses on attestation and provenance tracking.\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:1-81](), [.github/workflows/docker-publish.yml:1-51]()\n\n## Workflow Execution and Outputs\n\nWhen the workflow completes successfully, it produces:\n\n1. **Published Docker Image**: Available at `ghcr.io/\u003cowner\u003e/\u003crepo\u003e/mlflow` with multiple tags\n2. **GitHub Actions Artifacts**: Build logs and execution history\n3. **Cache Layers**: Stored in GitHub Actions cache for subsequent builds\n4. **Job Summary**: Markdown summary visible in the Actions UI with image details\n\nThe image can be pulled using any of the generated tags:\n\n```bash\n# Pull latest from default branch\ndocker pull ghcr.io/\u003cowner\u003e/\u003crepo\u003e/mlflow:latest\n\n# Pull specific commit\ndocker pull ghcr.io/\u003cowner\u003e/\u003crepo\u003e/mlflow:master-abc1234\n\n# Pull from PR\ndocker pull ghcr.io/\u003cowner\u003e/\u003crepo\u003e/mlflow:pr-42\n```\n\n**Sources:** [.github/workflows/mlflow-image-publish.yml:1-81]()"])</script><script>self.__next_f.push([1,"2e:T589c,"])</script><script>self.__next_f.push([1,"# Deployment Guide\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/src/esri/RefugeeCampDetector.py](examplemodel/src/esri/RefugeeCampDetector.py)\n- [examplemodel/src/inference.py](examplemodel/src/inference.py)\n- [examplemodel/src/stac2esri.py](examplemodel/src/stac2esri.py)\n- [infra/setup.sh](infra/setup.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides comprehensive instructions for deploying both the infrastructure stack and trained ML models to production environments. It covers automated infrastructure provisioning, DNS/SSL configuration, service management, and multiple model deployment strategies targeting different platforms (standalone inference, ONNX runtime, ArcGIS).\n\nFor detailed infrastructure architecture, see [Infrastructure System](#4). For model training and artifact generation, see [Training Pipeline](#3.2). For local development setup, see [Local Development Setup](#7.1).\n\n---\n\n## Overview: Deployment Architecture\n\nThe deployment architecture consists of two independent but complementary layers: infrastructure services and model artifacts. The infrastructure provides MLOps capabilities, while models can be deployed to various target platforms.\n\n```mermaid\ngraph TB\n    subgraph \"Deployment Process\"\n        SETUP[\"setup.sh\u003cbr/\u003eInfrastructure Provisioner\"]\n        CONFIG[\".env Configuration\u003cbr/\u003eDomain, Credentials\"]\n        COMPOSE[\"docker-compose.yml\u003cbr/\u003eService Definitions\"]\n    end\n    \n    subgraph \"Infrastructure Services\"\n        TRAEFIK[\"Traefik\u003cbr/\u003eEdge Router + SSL\"]\n        MLFLOW[\"MLflow Server\u003cbr/\u003eExperiment Tracking\"]\n        MINIO[\"MinIO\u003cbr/\u003eArtifact Storage\"]\n        POSTGRES[\"PostgreSQL\u003cbr/\u003eMetadata Store\"]\n        HOMEPAGE[\"Homepage\u003cbr/\u003eService Monitor\"]\n        RUSTDESK[\"RustDesk\u003cbr/\u003eRemote Access\"]\n    end\n    \n    subgraph \"Model Artifacts\"\n        PTH[\"best_model.pth\u003cbr/\u003ePyTorch Checkpoint\"]\n        ONNX[\"best_model.onnx\u003cbr/\u003eONNX Model\"]\n        DLPK[\"best_model.dlpk\u003cbr/\u003eESRI Package\"]\n        STAC[\"stac_item.json\u003cbr/\u003eMetadata\"]\n    end\n    \n    subgraph \"Deployment Targets\"\n        PYTHON[\"Python Inference\u003cbr/\u003etorch.jit.load()\"]\n        ONNXRT[\"ONNX Runtime\u003cbr/\u003eCross-platform\"]\n        ARCGIS[\"ArcGIS Pro\u003cbr/\u003eRefugeeCampDetector\"]\n    end\n    \n    CONFIG --\u003e SETUP\n    SETUP --\u003e COMPOSE\n    COMPOSE --\u003e TRAEFIK\n    COMPOSE --\u003e MLFLOW\n    COMPOSE --\u003e MINIO\n    COMPOSE --\u003e POSTGRES\n    COMPOSE --\u003e HOMEPAGE\n    COMPOSE --\u003e RUSTDESK\n    \n    MLFLOW --\u003e PTH\n    MLFLOW --\u003e ONNX\n    MLFLOW --\u003e DLPK\n    MLFLOW --\u003e STAC\n    \n    PTH --\u003e PYTHON\n    ONNX --\u003e ONNXRT\n    DLPK --\u003e ARCGIS\n    \n    TRAEFIK -.-\u003e|\"routes traffic\"| MLFLOW\n    TRAEFIK -.-\u003e|\"routes traffic\"| MINIO\n    MLFLOW -.-\u003e|\"stores artifacts\"| MINIO\n    MLFLOW -.-\u003e|\"stores metadata\"| POSTGRES\n```\n\n**Diagram: Deployment Architecture Overview**\n\nSources: [infra/setup.sh:1-254](), [examplemodel/src/stac2esri.py:1-115](), [examplemodel/src/esri/RefugeeCampDetector.py:1-184]()\n\n---\n\n## Infrastructure Deployment\n\n### Prerequisites\n\nBefore deploying the infrastructure stack, ensure the following are installed on the target system:\n\n| Component | Version | Purpose | Validation Command |\n|-----------|---------|---------|-------------------|\n| Docker | 20.10+ | Container runtime | `docker --version` |\n| Docker Compose | 2.0+ | Multi-container orchestration | `docker compose version` |\n| OpenSSL | 1.1+ | Credential generation | `openssl version` |\n| Domain Name | N/A | DNS records pointing to server | `dig yourdomain.com` |\n\nThe [infra/setup.sh:25-43]() script validates these prerequisites automatically during execution.\n\nSources: [infra/setup.sh:25-43]()\n\n### DNS Configuration Requirements\n\nConfigure the following DNS records before deployment. All records should point to the server's public IP address:\n\n| Record Type | Hostname | Purpose |\n|-------------|----------|---------|\n| A | `yourdomain.com` | Homepage dashboard |\n| A | `mlflow.yourdomain.com` | MLflow tracking server |\n| A | `minio.yourdomain.com` | MinIO console UI |\n| A | `minio-api.yourdomain.com` | MinIO S3 API endpoint |\n| A | `traefik.yourdomain.com` | Traefik dashboard |\n| A | `rustdesk.yourdomain.com` | RustDesk remote access |\n| A | `postgres.yourdomain.com` | PostgreSQL database (optional) |\n\nTraefik automatically provisions Let's Encrypt SSL certificates for all configured subdomains via the ACME protocol.\n\nSources: [infra/setup.sh:214-220]()\n\n### Automated Deployment Process\n\nThe deployment process is fully automated via the `setup.sh` script, which orchestrates credential generation, service configuration, and container initialization.\n\n```mermaid\nflowchart TD\n    START[\"Execute ./setup.sh\"]\n    CHECK_DOCKER{\"Docker\u003cbr/\u003einstalled?\"}\n    CHECK_ENV{\".env\u003cbr/\u003eexists?\"}\n    CREATE_ENV[\"Create .env from\u003cbr/\u003e.env.template\"]\n    GEN_CREDS[\"Generate Credentials\u003cbr/\u003egenerate_password()\u003cbr/\u003egenerate_key()\"]\n    REPLACE_VARS[\"sed replacements\u003cbr/\u003ePOSTGRES_PASSWORD\u003cbr/\u003eAWS_ACCESS_KEY\u003cbr/\u003eTRAEFIK_HASH\"]\n    PROMPT_USER[\"Display credentials\u003cbr/\u003ePrompt for DOMAIN\"]\n    VALIDATE{\"DOMAIN !=\u003cbr/\u003eexample.com?\"}\n    CREATE_DIRS[\"mkdir volumes/\u003cbr/\u003etraefik-data/\u003cbr/\u003eminio/\u003cbr/\u003epostgres/\u003cbr/\u003erustdesk/\"]\n    SET_PERMS[\"chmod 600\u003cbr/\u003eacme.json\"]\n    PULL_IMAGES[\"docker compose pull\u003cbr/\u003eMLflow from GHCR\"]\n    START_SERVICES[\"docker compose up -d\"]\n    WAIT[\"sleep 30\"]\n    CREATE_SYSTEMD[\"Create\u003cbr/\u003e/etc/systemd/system/\u003cbr/\u003etech-infra.service\"]\n    CREATE_MANAGE[\"Create\u003cbr/\u003emanage.sh utility\"]\n    SHOW_STATUS[\"Display service URLs\u003cbr/\u003eand credentials\"]\n    END[\"Deployment Complete\"]\n    \n    START --\u003e CHECK_DOCKER\n    CHECK_DOCKER --\u003e|No| END\n    CHECK_DOCKER --\u003e|Yes| CHECK_ENV\n    CHECK_ENV --\u003e|Yes| VALIDATE\n    CHECK_ENV --\u003e|No| CREATE_ENV\n    CREATE_ENV --\u003e GEN_CREDS\n    GEN_CREDS --\u003e REPLACE_VARS\n    REPLACE_VARS --\u003e PROMPT_USER\n    PROMPT_USER --\u003e END\n    VALIDATE --\u003e|No| END\n    VALIDATE --\u003e|Yes| CREATE_DIRS\n    CREATE_DIRS --\u003e SET_PERMS\n    SET_PERMS --\u003e PULL_IMAGES\n    PULL_IMAGES --\u003e START_SERVICES\n    START_SERVICES --\u003e WAIT\n    WAIT --\u003e CREATE_SYSTEMD\n    CREATE_SYSTEMD --\u003e CREATE_MANAGE\n    CREATE_MANAGE --\u003e SHOW_STATUS\n    SHOW_STATUS --\u003e END\n```\n\n**Diagram: Infrastructure Deployment Workflow**\n\nSources: [infra/setup.sh:1-254]()\n\n### Step-by-Step Deployment\n\n#### Step 1: Initial Setup\n\nExecute the setup script from the `infra/` directory:\n\n```bash\ncd infra/\n./setup.sh\n```\n\nOn first run, the script generates `.env` from [infra/.env.template]() and creates secure credentials using `generate_password()` and `generate_key()` functions [infra/setup.sh:11-20](). It displays generated credentials and exits, prompting you to update the `DOMAIN` and `ACME_EMAIL` variables.\n\nSources: [infra/setup.sh:47-95]()\n\n#### Step 2: Configure Domain and Email\n\nEdit the `.env` file to set your production domain and Let's Encrypt email:\n\n```bash\nnano .env\n```\n\nUpdate these critical variables:\n- `DOMAIN=yourdomain.com` - Your root domain\n- `ACME_EMAIL=admin@yourdomain.com` - Email for SSL certificate notifications\n\nThe script validates these values [infra/setup.sh:107-111]() and refuses to proceed if they remain at default values.\n\nSources: [infra/setup.sh:104-111]()\n\n#### Step 3: Complete Deployment\n\nRe-run the setup script after configuration:\n\n```bash\n./setup.sh\n```\n\nThis execution:\n1. Creates volume directories [infra/setup.sh:115-117]()\n2. Sets permissions on `acme.json` for SSL certificate storage [infra/setup.sh:119-122]()\n3. Pulls Docker images including custom MLflow image from GHCR [infra/setup.sh:127-129]()\n4. Starts all services via `docker compose up -d` [infra/setup.sh:131-132]()\n5. Creates systemd service `tech-infra.service` for automatic startup [infra/setup.sh:140-162]()\n6. Generates `manage.sh` utility script [infra/setup.sh:164-210]()\n\nSources: [infra/setup.sh:115-162]()\n\n### Service Endpoints\n\nAfter successful deployment, services are accessible at the following URLs:\n\n| Service | URL | Authentication | Purpose |\n|---------|-----|----------------|---------|\n| Homepage | `https://yourdomain.com` | None | Service monitoring dashboard |\n| MLflow | `https://mlflow.yourdomain.com` | None | Experiment tracking and model registry |\n| MinIO Console | `https://minio.yourdomain.com` | Access Key / Secret Key | Object storage management |\n| MinIO API | `https://minio-api.yourdomain.com` | Access Key / Secret Key | S3-compatible API endpoint |\n| Traefik Dashboard | `https://traefik.yourdomain.com` | Basic Auth | Reverse proxy monitoring |\n| RustDesk | `https://rustdesk.yourdomain.com` | Key-based | Remote desktop access |\n| PostgreSQL | `postgres.yourdomain.com:5432` | Username / Password | Database direct access |\n\nCredentials for authenticated services are stored in `.env` and displayed during initial setup [infra/setup.sh:229-253]().\n\nSources: [infra/setup.sh:213-253]()\n\n### Service Management\n\nThe `manage.sh` utility script provides operational commands for managing the infrastructure stack:\n\n| Command | Description | Example |\n|---------|-------------|---------|\n| `./manage.sh start` | Start all services | `./manage.sh start` |\n| `./manage.sh stop` | Stop all services | `./manage.sh stop` |\n| `./manage.sh restart [service]` | Restart specific or all services | `./manage.sh restart mlflow` |\n| `./manage.sh logs [service]` | View logs (optionally follow) | `./manage.sh logs -f postgres` |\n| `./manage.sh status` | Display service status | `./manage.sh status` |\n| `./manage.sh update` | Pull latest images and restart | `./manage.sh update` |\n| `./manage.sh backup` | Create full backup of volumes and database | `./manage.sh backup` |\n\nThe backup command [infra/setup.sh:193-199]() creates timestamped snapshots in `./backups/` including volume copies and PostgreSQL dumps.\n\nSources: [infra/setup.sh:164-210]()\n\n---\n\n## Model Deployment Strategies\n\nTrained models can be deployed to multiple target platforms depending on operational requirements. The training pipeline [examplemodel/src/train.py]() generates artifacts in three formats: PyTorch checkpoints, ONNX models, and ESRI Deep Learning Packages (DLPK).\n\n```mermaid\ngraph LR\n    subgraph \"Training Artifacts\"\n        PTH[\"best_model.pth\u003cbr/\u003ePyTorch State Dict\"]\n        JITPT[\"best_model.pt\u003cbr/\u003eTorchScript JIT\"]\n        ONNX[\"best_model.onnx\u003cbr/\u003eONNX Format\"]\n        STAC[\"stac_item.json\u003cbr/\u003eSTAC-MLM Metadata\"]\n    end\n    \n    subgraph \"Conversion Pipeline\"\n        TORCH_JIT[\"torch.jit.trace()\"]\n        ONNX_EXPORT[\"torch.onnx.export()\"]\n        DLPK_GEN[\"stac2esri.py\u003cbr/\u003ecreate_dlpk()\"]\n    end\n    \n    subgraph \"Deployment Packages\"\n        JITMODEL[\"model.pt\u003cbr/\u003eJIT Model\"]\n        ONNXMODEL[\"model.onnx\u003cbr/\u003eONNX Model\"]\n        DLPKPKG[\"model.dlpk\u003cbr/\u003eZIP Archive\"]\n    end\n    \n    subgraph \"Target Platforms\"\n        PYTORCHINF[\"Python Application\u003cbr/\u003etorch.jit.load()\"]\n        ONNXINF[\"ONNX Runtime\u003cbr/\u003eC++/Python/JS\"]\n        ARCGISINF[\"ArcGIS Pro\u003cbr/\u003eRefugeeCampDetector\"]\n    end\n    \n    PTH --\u003e TORCH_JIT\n    PTH --\u003e ONNX_EXPORT\n    TORCH_JIT --\u003e JITPT\n    ONNX_EXPORT --\u003e ONNX\n    \n    JITPT --\u003e DLPK_GEN\n    STAC --\u003e DLPK_GEN\n    DLPK_GEN --\u003e DLPKPKG\n    \n    JITPT --\u003e JITMODEL\n    ONNX --\u003e ONNXMODEL\n    \n    JITMODEL --\u003e PYTORCHINF\n    ONNXMODEL --\u003e ONNXINF\n    DLPKPKG --\u003e ARCGISINF\n```\n\n**Diagram: Model Artifact Generation and Deployment Paths**\n\nSources: [examplemodel/src/stac2esri.py:1-115](), [examplemodel/src/inference.py:127-143]()\n\n### PyTorch Direct Deployment\n\nPyTorch models can be loaded directly for inference using TorchScript JIT compilation. This approach is suitable for Python-based deployment environments with PyTorch runtime available.\n\n#### Loading TorchScript Models\n\nThe [examplemodel/src/inference.py:127-143]() demonstrates loading and executing TorchScript models:\n\n```python\n# Load TorchScript model\nmodel = torch.jit.load(\"path/to/model.pt\")\n\n# Preprocess image\ninput_tensor = preprocess_image(image_path)\n\n# Execute inference\nmodel.eval()\nwith torch.no_grad():\n    output = model(input_tensor)\n```\n\nThe `preprocess_image()` function [examplemodel/src/inference.py:52-60]() applies:\n- Resize to 256256 pixels\n- RGB conversion\n- ImageNet normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n#### Inference with Metadata Generation\n\nFor production deployments requiring provenance tracking, use `predict_image_enhanced()` [examplemodel/src/inference.py:63-124](), which generates:\n- Raw prediction probabilities (`.npy` format)\n- Binary segmentation masks (`.png` format)\n- Overlay visualizations with original imagery\n- JSON metadata including inference time, model version, preprocessing parameters\n\nSources: [examplemodel/src/inference.py:52-124]()\n\n### ONNX Deployment\n\nONNX (Open Neural Network Exchange) models enable cross-platform deployment without Python or PyTorch dependencies. The training pipeline exports models to ONNX format, which can be executed by ONNX Runtime on various platforms including C++, Java, C#, and JavaScript.\n\n#### ONNX Runtime Integration\n\nDeployment with ONNX Runtime (example in Python):\n\n```python\nimport onnxruntime as ort\nimport numpy as np\n\n# Load ONNX model\nsession = ort.InferenceSession(\"best_model.onnx\")\n\n# Prepare input (CHW format, float32)\ninput_tensor = preprocess_image(image_path)\ninput_data = {session.get_inputs()[0].name: input_tensor.numpy()}\n\n# Execute inference\noutput = session.run(None, input_data)\nprediction = output[0]\n```\n\nONNX Runtime provides:\n- Hardware acceleration (CPU, CUDA, DirectML, CoreML)\n- Minimal memory footprint\n- Native code performance\n- No Python runtime required\n\nSources: [examplemodel/src/stac2esri.py:11-16]()\n\n### ESRI Deep Learning Package (DLPK) Deployment\n\nThe DLPK format packages models specifically for ArcGIS Pro and ArcGIS Enterprise deployment. It bundles the model, metadata, and inference code into a single `.dlpk` file.\n\n#### DLPK Structure\n\nThe [examplemodel/src/stac2esri.py:54-58]() `create_dlpk()` function creates a ZIP archive containing:\n\n| File | Purpose | Source |\n|------|---------|--------|\n| `model.emd` | ESRI model definition (JSON) | Generated from STAC-MLM metadata |\n| `model.pt` | TorchScript model | PyTorch checkpoint |\n| `RefugeeCampDetector.py` | Inference implementation | Custom raster function |\n\n#### ESRI Model Definition (EMD)\n\nThe [examplemodel/src/stac2esri.py:7-51]() `stacmlm_to_emd()` function converts STAC-MLM metadata to ESRI's EMD format:\n\n```json\n{\n  \"Framework\": \"PyTorch\",\n  \"ModelType\": \"ImageClassification\",\n  \"ModelFile\": \"model.pt\",\n  \"InferenceFunction\": \"RefugeeCampDetector.py\",\n  \"ImageHeight\": 256,\n  \"ImageWidth\": 256,\n  \"ExtractBands\": [0, 1, 2],\n  \"Classes\": [\n    {\"Value\": 0, \"Name\": \"Background\", \"Color\": [0, 0, 0]},\n    {\"Value\": 1, \"Name\": \"Refugee Camp\", \"Color\": [255, 0, 0]}\n  ],\n  \"ModelParameters\": {\n    \"mean\": [0.485, 0.456, 0.406],\n    \"std\": [0.229, 0.224, 0.225]\n  }\n}\n```\n\nSources: [examplemodel/src/stac2esri.py:7-51]()\n\n#### RefugeeCampDetector Class\n\nThe [examplemodel/src/esri/RefugeeCampDetector.py:25-183]() implements the ESRI raster function interface:\n\n```mermaid\nflowchart TD\n    INIT[\"initialize(**kwargs)\u003cbr/\u003eLoad JSON config\u003cbr/\u003eIdentify model path\"]\n    LOAD[\"load_model()\u003cbr/\u003etorch.jit.load()\u003cbr/\u003eSet eval mode\"]\n    GETPARAM[\"getParameterInfo()\u003cbr/\u003eReturn raster, model,\u003cbr/\u003eBatchSize, Threshold\"]\n    GETCONFIG[\"getConfiguration(**scalars)\u003cbr/\u003eExtract tile size,\u003cbr/\u003ebands, data range\"]\n    UPDATERASTER[\"updateRasterInfo(**kwargs)\u003cbr/\u003eSet output:\u003cbr/\u003ebandCount=1, pixelType=u1\"]\n    UPDATEPIXELS[\"updatePixels(tlc, shape, **pixelBlocks)\u003cbr/\u003e1. Extract raster_pixels\u003cbr/\u003e2. Normalize with mean/std\u003cbr/\u003e3. torch.jit inference\u003cbr/\u003e4. Sigmoid + threshold\u003cbr/\u003e5. Return binary mask\"]\n    \n    INIT --\u003e GETPARAM\n    GETPARAM --\u003e GETCONFIG\n    GETCONFIG --\u003e UPDATERASTER\n    UPDATERASTER --\u003e UPDATEPIXELS\n    UPDATEPIXELS --\u003e LOAD\n```\n\n**Diagram: RefugeeCampDetector Execution Flow**\n\nThe `updatePixels()` method [examplemodel/src/esri/RefugeeCampDetector.py:107-183]() processes raster tiles:\n\n1. Receives `raster_pixels` (CHW numpy array) and `raster_mask`\n2. Applies band selection [examplemodel/src/esri/RefugeeCampDetector.py:121-124]()\n3. Normalizes using ImageNet statistics [examplemodel/src/esri/RefugeeCampDetector.py:134-141]()\n4. Converts to PyTorch tensor and executes inference [examplemodel/src/esri/RefugeeCampDetector.py:143-148]()\n5. Applies sigmoid activation and threshold [examplemodel/src/esri/RefugeeCampDetector.py:172-174]()\n6. Returns binary mask (0 or 255) [examplemodel/src/esri/RefugeeCampDetector.py:180-183]()\n\nSources: [examplemodel/src/esri/RefugeeCampDetector.py:25-183]()\n\n#### Generating DLPK Packages\n\nUse the `stac2esri.py` utility to create DLPK packages from trained models:\n\n```bash\ncd examplemodel/\npython src/stac2esri.py \\\n  --stac meta/stac_item.json \\\n  --pt meta/best_model.pt \\\n  --out-dir output \\\n  --dlpk-name refugee_camp_detector.dlpk\n```\n\nThe [examplemodel/src/stac2esri.py:61-110]() `main()` function:\n1. Parses STAC-MLM metadata\n2. Locates PyTorch model artifact\n3. Generates EMD configuration\n4. Bundles all files into `.dlpk` ZIP archive\n\nSources: [examplemodel/src/stac2esri.py:61-110]()\n\n#### Deploying to ArcGIS Pro\n\n1. Open ArcGIS Pro\n2. Navigate to **Analysis**  **Raster Functions**\n3. Select **Import Deep Learning Model**\n4. Browse to `.dlpk` file\n5. Configure inference parameters (batch size, threshold)\n6. Apply to raster imagery\n\nThe `RefugeeCampDetector` class loads automatically and processes imagery tiles using the `updatePixels()` method.\n\nSources: [examplemodel/src/esri/RefugeeCampDetector.py:1-184]()\n\n---\n\n## Deployment Verification\n\n### Infrastructure Health Checks\n\nVerify all services are running correctly:\n\n```bash\ncd infra/\n./manage.sh status\n```\n\nExpected output shows all services with `Up` status. Access the Homepage dashboard at `https://yourdomain.com` for real-time monitoring.\n\n#### Service-Specific Tests\n\n| Service | Test Command | Expected Result |\n|---------|--------------|-----------------|\n| Traefik | `curl -k https://traefik.yourdomain.com` | 401 Unauthorized (auth required) |\n| MLflow | `curl https://mlflow.yourdomain.com/health` | `{\"status\": \"ok\"}` |\n| MinIO | `curl https://minio-api.yourdomain.com/minio/health/live` | `200 OK` |\n| PostgreSQL | `docker compose exec postgres pg_isready` | `accepting connections` |\n\nSources: [infra/setup.sh:134-138]()\n\n### Model Inference Tests\n\n#### Test PyTorch Model\n\n```bash\ncd examplemodel/\npython src/inference.py test_image.jpg \\\n  --model_path meta/best_model.pt \\\n  --output_dir output/test\n```\n\nVerify outputs in `output/test/`:\n- `prediction_raw.npy` - Raw probability values\n- `prediction_mask.png` - Binary segmentation\n- `prediction_overlay.png` - Visualization\n- `inference_metadata.json` - Provenance metadata\n\nSources: [examplemodel/src/inference.py:211-246]()\n\n#### Test ONNX Model\n\n```python\nimport onnxruntime as ort\nsession = ort.InferenceSession(\"meta/best_model.onnx\")\nprint(f\"Model loaded: {session.get_inputs()[0].name}\")\n# Expected: Input name displayed\n```\n\n#### Test DLPK Package\n\n1. Extract `.dlpk` to verify contents:\n```bash\nunzip -l refugee_camp_detector.dlpk\n# Expected: model.emd, model.pt, RefugeeCampDetector.py\n```\n\n2. Import into ArcGIS Pro and apply to test imagery\n\nSources: [examplemodel/src/stac2esri.py:54-58]()\n\n---\n\n## Troubleshooting\n\n### Infrastructure Issues\n\n**Problem: Services fail to start**\n- Check Docker daemon: `systemctl status docker`\n- Review logs: `./manage.sh logs [service]`\n- Verify ports not in use: `sudo netstat -tulpn | grep -E ':(80|443|5432|9000)'`\n\n**Problem: SSL certificates not provisioning**\n- Verify DNS records propagated: `dig yourdomain.com`\n- Check Traefik logs: `./manage.sh logs traefik`\n- Ensure ports 80/443 accessible from internet\n- Review ACME email in `.env` is valid\n\n**Problem: MinIO S3 access denied**\n- Verify credentials in `.env` match environment variables\n- Check bucket creation in MinIO console\n- Ensure MLflow has correct `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`\n\nSources: [infra/setup.sh:124-138]()\n\n### Model Deployment Issues\n\n**Problem: PyTorch model load fails**\n- Verify PyTorch version compatibility\n- Check model file integrity: `ls -lh meta/best_model.pt`\n- Ensure TorchScript compilation succeeded during training\n\n**Problem: ONNX Runtime errors**\n- Verify ONNX opset version compatibility\n- Check input tensor shape matches model expectations (13256256)\n- Ensure preprocessing normalization applied correctly\n\n**Problem: DLPK fails in ArcGIS Pro**\n- Check `rcd_debug.log` in DLPK extraction directory\n- Verify PyTorch installed in ArcGIS Pro environment\n- Review EMD configuration matches model architecture\n- Ensure `RefugeeCampDetector.py` implements all required methods\n\nThe [examplemodel/src/esri/RefugeeCampDetector.py:20-22]() `log()` function writes detailed diagnostic messages for debugging ESRI inference issues.\n\nSources: [examplemodel/src/esri/RefugeeCampDetector.py:17-22]()\n\n---\n\n## Security Considerations\n\n### Credential Management\n\nGenerated credentials are stored in `.env` and should be:\n- Excluded from version control (`.gitignore`)\n- Backed up securely offline\n- Rotated periodically\n\nThe [infra/setup.sh:54-59]() credential generation uses:\n- `openssl rand -base64` for passwords (160+ bits entropy)\n- `openssl rand -hex` for keys (256+ bits entropy)\n\n### Network Security\n\n- Traefik enforces HTTPS with automatic Let's Encrypt certificates\n- Basic authentication protects Traefik dashboard [infra/setup.sh:62]()\n- MinIO API requires S3 signature authentication\n- PostgreSQL accessible only via Docker network (not exposed externally)\n\n### Model Security\n\n- DLPK packages should be signed before distribution to ArcGIS users\n- Validate STAC-MLM checksums before deploying models\n- Restrict MLflow access to authorized users only\n\nSources: [infra/setup.sh:54-76]()\n\n---\n\n## Production Best Practices\n\n### High Availability\n\n- Deploy infrastructure across multiple availability zones\n- Use external PostgreSQL (RDS/CloudSQL) for database resilience\n- Configure MinIO distributed mode for object storage redundancy\n- Implement backup automation via cron: `0 2 * * * /path/to/manage.sh backup`\n\n### Monitoring\n\n- Enable Traefik access logs for audit trails\n- Configure MLflow to log deployment events\n- Monitor disk usage of volume mounts: `df -h volumes/`\n- Set up alerts for service downtime using Homepage integrations\n\n### Model Versioning\n\n- Tag model versions in MLflow registry\n- Use semantic versioning for DLPK releases\n- Document model changes in STAC-MLM metadata\n- Maintain deployment history in version control\n\nSources: [infra/setup.sh:193-199]()"])</script><script>self.__next_f.push([1,"2f:T51a6,"])</script><script>self.__next_f.push([1,"# Infrastructure Deployment\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [infra/.env.template](infra/.env.template)\n- [infra/Readme.md](infra/Readme.md)\n- [infra/setup.sh](infra/setup.sh)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document provides a comprehensive guide for deploying the production-ready infrastructure stack to a server environment. The infrastructure stack consists of seven containerized services orchestrated by Docker Compose: Traefik reverse proxy, MLflow tracking server, MinIO object storage, PostgreSQL+PostGIS database, RustDesk remote desktop server, Homepage dashboard, and supporting services.\n\nThis page covers infrastructure deployment only. For deploying trained ML models to production environments, see [Model Deployment Options](#6.2). For understanding the infrastructure architecture, see [Service Architecture](#4.1).\n\n**Sources:** [infra/Readme.md:1-83](), [infra/setup.sh:1-254]()\n\n---\n\n## Prerequisites\n\nThe infrastructure stack requires the following prerequisites on the target deployment server:\n\n| Requirement | Minimum Version | Validation Command |\n|-------------|----------------|-------------------|\n| Docker Engine | 20.10+ | `docker --version` |\n| Docker Compose | 2.0+ | `docker compose version` |\n| OpenSSL | Any recent | `openssl version` |\n| Domain Name | N/A | DNS A records configured |\n| Server OS | Linux (Ubuntu/Debian recommended) | `uname -a` |\n| Root/Sudo Access | Required for systemd setup | `sudo -v` |\n\nThe `setup.sh` script validates these prerequisites during execution at [infra/setup.sh:25-43]():\n\n```mermaid\nflowchart TD\n    START[\"setup.sh execution starts\"]\n    CHECK_DOCKER[\"Check docker command exists\"]\n    CHECK_RUNNING[\"Check docker daemon running\"]\n    CHECK_COMPOSE[\"Check docker compose available\"]\n    CHECK_OPENSSL[\"Check openssl available\"]\n    SUCCESS[\"Prerequisites validated\"]\n    FAIL[\"Exit with error message\"]\n    \n    START --\u003e CHECK_DOCKER\n    CHECK_DOCKER --\u003e|exists| CHECK_RUNNING\n    CHECK_DOCKER --\u003e|missing| FAIL\n    CHECK_RUNNING --\u003e|running| CHECK_COMPOSE\n    CHECK_RUNNING --\u003e|not running| FAIL\n    CHECK_COMPOSE --\u003e|available| CHECK_OPENSSL\n    CHECK_COMPOSE --\u003e|missing| FAIL\n    CHECK_OPENSSL --\u003e|available| SUCCESS\n    CHECK_OPENSSL --\u003e|missing| FAIL\n```\n\n**Diagram: Prerequisites Validation Flow in setup.sh**\n\n**Sources:** [infra/setup.sh:25-43]()\n\n---\n\n## DNS Configuration\n\nBefore deployment, configure DNS A records pointing to your server's public IP address. The infrastructure uses subdomain-based routing via Traefik.\n\n### Required DNS Records\n\n| Subdomain | Service | Purpose |\n|-----------|---------|---------|\n| `yourdomain.com` | Homepage | Main dashboard and service overview |\n| `mlflow.yourdomain.com` | MLflow | Experiment tracking and model registry |\n| `minio.yourdomain.com` | MinIO Console | Web UI for S3-compatible storage |\n| `minio-api.yourdomain.com` | MinIO API | S3 API endpoint for programmatic access |\n| `postgres.yourdomain.com` | PostgreSQL | Database connection endpoint |\n| `rustdesk.yourdomain.com` | RustDesk | Remote desktop server |\n| `traefik.yourdomain.com` | Traefik Dashboard | Proxy monitoring and configuration |\n\n```mermaid\ngraph LR\n    DNS[\"DNS Provider\u003cbr/\u003e(e.g., Cloudflare)\"]\n    SERVER[\"Server Public IP\u003cbr/\u003e(e.g., 203.0.113.42)\"]\n    \n    DNS --\u003e|\"A: yourdomain.com\"| SERVER\n    DNS --\u003e|\"A: mlflow.yourdomain.com\"| SERVER\n    DNS --\u003e|\"A: minio.yourdomain.com\"| SERVER\n    DNS --\u003e|\"A: minio-api.yourdomain.com\"| SERVER\n    DNS --\u003e|\"A: postgres.yourdomain.com\"| SERVER\n    DNS --\u003e|\"A: rustdesk.yourdomain.com\"| SERVER\n    DNS --\u003e|\"A: traefik.yourdomain.com\"| SERVER\n```\n\n**Diagram: DNS Record Configuration**\n\n**Sources:** [infra/Readme.md:32-42]()\n\n---\n\n## Environment Configuration\n\nThe infrastructure uses environment variables defined in `.env` for configuration. The deployment process uses [infra/.env.template:1-37]() as the base configuration template.\n\n### Configuration File Structure\n\n```mermaid\ngraph TD\n    TEMPLATE[\".env.template\u003cbr/\u003eConfiguration template\"]\n    ENV[\".env\u003cbr/\u003eActive configuration\"]\n    SETUP[\"setup.sh\u003cbr/\u003eCredential generation\"]\n    \n    TEMPLATE --\u003e|\"copied by\"| SETUP\n    SETUP --\u003e|\"generates credentials\"| ENV\n    SETUP --\u003e|\"sed replacements\"| ENV\n    ENV --\u003e|\"read by\"| COMPOSE[\"docker-compose.yml\"]\n    ENV --\u003e|\"provides variables\"| SERVICES[\"All Services\"]\n```\n\n**Diagram: Environment Configuration Flow**\n\n**Sources:** [infra/setup.sh:47-95](), [infra/.env.template:1-37]()\n\n### Critical Configuration Variables\n\nThe following variables in `.env` must be manually configured before deployment:\n\n| Variable | Purpose | Example Value | Line Reference |\n|----------|---------|---------------|----------------|\n| `DOMAIN` | Base domain for all services | `example.com` | [infra/.env.template:2]() |\n| `ACME_EMAIL` | Let's Encrypt certificate email | `admin@example.com` | [infra/.env.template:3]() |\n\n### Auto-Generated Security Credentials\n\nThe `setup.sh` script automatically generates secure credentials using OpenSSL at [infra/setup.sh:52-76](). The following credentials are generated:\n\n```mermaid\nflowchart LR\n    OPENSSL[\"OpenSSL\"]\n    \n    OPENSSL --\u003e|\"generate_password(20)\"| PG_PASS[\"POSTGRES_PASSWORD\u003cbr/\u003e20-char password\"]\n    OPENSSL --\u003e|\"generate_key(20)\"| AWS_KEY[\"AWS_ACCESS_KEY_ID\u003cbr/\u003e40-char hex\"]\n    OPENSSL --\u003e|\"generate_key(40)\"| AWS_SECRET[\"AWS_SECRET_ACCESS_KEY\u003cbr/\u003e80-char hex\"]\n    OPENSSL --\u003e|\"generate_key(16)\"| RUST_KEY[\"RUSTDESK_KEY\u003cbr/\u003e32-char hex\"]\n    OPENSSL --\u003e|\"generate_password(16)\"| TRAEFIK_PW[\"TRAEFIK_AUTH_PASSWORD\u003cbr/\u003e16-char password\"]\n    \n    TRAEFIK_PW --\u003e|\"htpasswd -nbB\"| TRAEFIK_HASH[\"TRAEFIK_AUTH_PASSWORD_HASH\u003cbr/\u003ebcrypt hash\"]\n```\n\n**Diagram: Credential Generation Process**\n\nFunctions used for generation:\n- `generate_password()` at [infra/setup.sh:11-14]() - uses `openssl rand -base64`\n- `generate_key()` at [infra/setup.sh:17-20]() - uses `openssl rand -hex`\n\n**Sources:** [infra/setup.sh:11-20](), [infra/setup.sh:52-76]()\n\n---\n\n## Deployment Process\n\n### Step 1: Clone Repository and Navigate to Infrastructure Directory\n\n```bash\ngit clone https://github.com/kshitijrajsharma/opengeoaimodelshub\ncd opengeoaimodelshub/infra\n```\n\n### Step 2: Initial Configuration\n\nThe `setup.sh` script will create `.env` from template on first run:\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n\nOn first execution, the script performs the following sequence at [infra/setup.sh:47-95]():\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant setup.sh\n    participant .env.template\n    participant .env\n    participant OpenSSL\n    \n    User-\u003e\u003esetup.sh: Execute ./setup.sh\n    setup.sh-\u003e\u003esetup.sh: Check if .env exists\n    setup.sh-\u003e\u003esetup.sh: .env not found\n    setup.sh-\u003e\u003e.env.template: Check template exists\n    setup.sh-\u003e\u003esetup.sh: cp .env.template .env\n    \n    setup.sh-\u003e\u003eOpenSSL: generate_password(20)\n    OpenSSL--\u003e\u003esetup.sh: POSTGRES_PASSWORD\n    \n    setup.sh-\u003e\u003eOpenSSL: generate_key(20)\n    OpenSSL--\u003e\u003esetup.sh: AWS_ACCESS_KEY_ID\n    \n    setup.sh-\u003e\u003eOpenSSL: generate_key(40)\n    OpenSSL--\u003e\u003esetup.sh: AWS_SECRET_ACCESS_KEY\n    \n    setup.sh-\u003e\u003eOpenSSL: generate_key(16)\n    OpenSSL--\u003e\u003esetup.sh: RUSTDESK_KEY\n    \n    setup.sh-\u003e\u003eOpenSSL: generate_password(16)\n    OpenSSL--\u003e\u003esetup.sh: TRAEFIK_PASSWORD\n    \n    setup.sh-\u003e\u003esetup.sh: htpasswd -nbB for hash\n    setup.sh-\u003e\u003esetup.sh: sed replace placeholders\n    \n    setup.sh-\u003e\u003eUser: Display generated credentials\n    setup.sh-\u003e\u003esetup.sh: Exit - manual config required\n```\n\n**Diagram: First-Run Configuration Sequence**\n\n**Sources:** [infra/setup.sh:47-95]()\n\nThe script will exit after generating credentials, requiring manual configuration of `DOMAIN` and `ACME_EMAIL`.\n\n### Step 3: Configure Domain and Email\n\nEdit the `.env` file with your domain information:\n\n```bash\nnano .env\n```\n\nUpdate the following lines (originally at [infra/.env.template:2-3]()):\n- Set `DOMAIN=yourdomain.com` (replace with your actual domain)\n- Set `ACME_EMAIL=youremail@yourdomain.com` (email for Let's Encrypt notifications)\n\nThe script validates these values at [infra/setup.sh:104-111]() and will exit if placeholders remain.\n\n### Step 4: Execute Full Deployment\n\nRun `setup.sh` again after domain configuration:\n\n```bash\n./setup.sh\n```\n\nThe script now executes the complete deployment sequence:\n\n```mermaid\nflowchart TD\n    START[\"setup.sh second run\"]\n    LOAD[\".env file loaded\u003cbr/\u003esource .env\"]\n    VALIDATE[\"Validate DOMAIN \u0026 ACME_EMAIL\u003cbr/\u003enot example.com\"]\n    DIRS[\"Create directories\u003cbr/\u003emkdir volumes/{...}\"]\n    PERMS[\"Set permissions\u003cbr/\u003echmod 600 acme.json\"]\n    DOWN[\"Stop existing services\u003cbr/\u003edocker compose down\"]\n    PULL[\"Pull container images\u003cbr/\u003edocker compose pull\"]\n    UP[\"Start services\u003cbr/\u003edocker compose up -d\"]\n    WAIT[\"Wait 30 seconds\u003cbr/\u003esleep 30\"]\n    STATUS[\"Check service status\u003cbr/\u003edocker compose ps\"]\n    SYSTEMD[\"Setup systemd service\u003cbr/\u003etech-infra.service\"]\n    MANAGE[\"Create manage.sh script\"]\n    CREDS[\"Display credentials\"]\n    END[\"Deployment complete\"]\n    \n    START --\u003e LOAD\n    LOAD --\u003e VALIDATE\n    VALIDATE --\u003e DIRS\n    DIRS --\u003e PERMS\n    PERMS --\u003e DOWN\n    DOWN --\u003e PULL\n    PULL --\u003e UP\n    UP --\u003e WAIT\n    WAIT --\u003e STATUS\n    STATUS --\u003e SYSTEMD\n    SYSTEMD --\u003e MANAGE\n    MANAGE --\u003e CREDS\n    CREDS --\u003e END\n```\n\n**Diagram: Full Deployment Sequence**\n\n**Sources:** [infra/setup.sh:104-254]()\n\nKey deployment actions:\n\n| Step | Line Reference | Action |\n|------|---------------|---------|\n| Directory creation | [infra/setup.sh:115-117]() | Creates `volumes/{traefik-data,minio,postgres,rustdesk}` |\n| Permission setup | [infra/setup.sh:119-122]() | Sets `acme.json` to mode 600 for Let's Encrypt |\n| Service cleanup | [infra/setup.sh:124-125]() | Stops any existing containers |\n| Image pull | [infra/setup.sh:127-129]() | Pulls all service images from registries |\n| Service start | [infra/setup.sh:131-132]() | Starts all services in detached mode |\n| Initialization wait | [infra/setup.sh:134-135]() | 30-second delay for service initialization |\n\n---\n\n## SSL Certificate Management\n\nSSL certificates are automatically managed by Traefik using the ACME protocol (Let's Encrypt).\n\n### Automatic Certificate Provisioning\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Traefik\n    participant LetsEncrypt as \"Let's Encrypt\u003cbr/\u003eACME CA\"\n    participant Service as \"Backend Service\u003cbr/\u003e(e.g., MLflow)\"\n    \n    User-\u003e\u003eTraefik: HTTPS request to mlflow.domain.com\n    \n    alt Certificate not exists\n        Traefik-\u003e\u003eLetsEncrypt: ACME challenge request\n        LetsEncrypt-\u003e\u003eTraefik: HTTP-01 challenge\n        Traefik-\u003e\u003eTraefik: Serve challenge response\n        LetsEncrypt-\u003e\u003eTraefik: Validate challenge\n        LetsEncrypt-\u003e\u003eTraefik: Issue certificate\n        Traefik-\u003e\u003eTraefik: Store in acme.json\n    end\n    \n    Traefik-\u003e\u003eTraefik: Terminate SSL\n    Traefik-\u003e\u003eService: Forward HTTP request\n    Service-\u003e\u003eTraefik: Response\n    Traefik-\u003e\u003eUser: HTTPS response\n```\n\n**Diagram: Automatic SSL Certificate Provisioning**\n\nThe `acme.json` file stores all certificates and is created with secure permissions (mode 600) at [infra/setup.sh:120-121](). This file persists in the `TRAEFIK_DATA_DIR` volume (configured at [infra/.env.template:6]()).\n\n**Sources:** [infra/setup.sh:120-121](), [infra/.env.template:6]()\n\n---\n\n## Systemd Integration\n\nThe deployment creates a systemd service unit for automatic startup and system integration.\n\n### Service Unit Creation\n\nThe systemd unit file is created at [infra/setup.sh:140-162]() with the following configuration:\n\n| Directive | Value | Purpose |\n|-----------|-------|---------|\n| `Description` | \"Tech Infrastructure Services\" | Service description |\n| `Requires` | `docker.service` | Dependency on Docker daemon |\n| `After` | `docker.service` | Start after Docker is ready |\n| `Type` | `oneshot` | Single execution service |\n| `RemainAfterExit` | `yes` | Keep active after start command |\n| `ExecStart` | `/usr/bin/docker compose up -d` | Start command |\n| `ExecStop` | `/usr/bin/docker compose down` | Stop command |\n| `WantedBy` | `multi-user.target` | Enable at boot |\n\n```mermaid\nstateDiagram-v2\n    [*] --\u003e Inactive\n    Inactive --\u003e Starting: systemctl start tech-infra\n    Starting --\u003e Active: docker compose up -d succeeds\n    Active --\u003e Stopping: systemctl stop tech-infra\n    Stopping --\u003e Inactive: docker compose down succeeds\n    \n    Active --\u003e Active: Server reboot triggers restart\n    \n    note right of Active\n        RemainAfterExit=yes\n        Service stays \"active\" \n        even after ExecStart completes\n    end note\n```\n\n**Diagram: Systemd Service State Lifecycle**\n\nThe service is enabled for automatic startup at [infra/setup.sh:161-162]():\n\n```bash\nsudo systemctl daemon-reload\nsudo systemctl enable tech-infra.service\n```\n\n**Sources:** [infra/setup.sh:140-162]()\n\n---\n\n## Service Management\n\nThe deployment creates `manage.sh` script at [infra/setup.sh:164-208]() providing unified service management commands.\n\n### Management Script Commands\n\n| Command | Function | Description |\n|---------|----------|-------------|\n| `./manage.sh start` | Start all services | Executes `docker compose up -d` |\n| `./manage.sh stop` | Stop all services | Executes `docker compose down` |\n| `./manage.sh restart [service]` | Restart service(s) | Restarts specified or all services |\n| `./manage.sh logs [service]` | View logs | Shows logs, `-f` follows if service specified |\n| `./manage.sh status` | Check status | Displays `docker compose ps` output |\n| `./manage.sh update` | Update images | Pulls latest images and restarts |\n| `./manage.sh backup` | Create backup | Backs up volumes and PostgreSQL dump |\n\n```mermaid\nflowchart LR\n    MANAGE[\"manage.sh\"]\n    \n    MANAGE --\u003e|\"start\"| START[\"docker compose up -d\"]\n    MANAGE --\u003e|\"stop\"| STOP[\"docker compose down\"]\n    MANAGE --\u003e|\"restart [svc]\"| RESTART[\"docker compose restart\"]\n    MANAGE --\u003e|\"logs [svc]\"| LOGS[\"docker compose logs\"]\n    MANAGE --\u003e|\"status\"| STATUS[\"docker compose ps\"]\n    MANAGE --\u003e|\"update\"| UPDATE[\"Pull images \u0026 restart\"]\n    MANAGE --\u003e|\"backup\"| BACKUP[\"Copy volumes + pg_dump\"]\n    \n    UPDATE --\u003e PULL[\"docker compose pull\"]\n    UPDATE --\u003e UPDT[\"docker compose up -d\"]\n    \n    BACKUP --\u003e MKDIR[\"mkdir backups/timestamp\"]\n    BACKUP --\u003e CP[\"cp -r volumes/\"]\n    BACKUP --\u003e PGDUMP[\"pg_dump via docker exec\"]\n```\n\n**Diagram: Management Script Command Flow**\n\n### Backup Functionality\n\nThe backup command at [infra/setup.sh:193-199]() performs:\n\n1. Creates timestamped backup directory: `./backups/YYYYMMDD_HHMMSS`\n2. Copies all Docker volumes: `volumes/`  backup directory\n3. Exports PostgreSQL database: `pg_dump` via `docker compose exec`\n\nExample backup command:\n```bash\n./manage.sh backup\n# Creates: ./backups/20240115_143022/\n#   - volumes/ (complete copy)\n#   - postgres_dump.sql\n```\n\n**Sources:** [infra/setup.sh:164-208]()\n\n---\n\n## Service Verification\n\nAfter deployment, verify services are running correctly.\n\n### Service Status Check\n\n```bash\n./manage.sh status\n# or\ndocker compose ps\n```\n\nExpected output shows all services in \"running\" state:\n\n| Service | Container Name | Expected State | Ports |\n|---------|---------------|----------------|-------|\n| `homepage` | Homepage dashboard | Up | 3000 (internal) |\n| `mlflow` | MLflow tracking | Up | 5000 (internal) |\n| `minio` | MinIO storage | Up | 9000, 9001 (internal) |\n| `postgres` | PostgreSQL+PostGIS | Up | 5432 (internal) |\n| `rustdesk` | RustDesk server | Up | Various (internal) |\n| `traefik` | Traefik proxy | Up | 80, 443, 8080 |\n\n### Service Access Verification\n\nAfter successful deployment, services are accessible at [infra/setup.sh:213-220]():\n\n```mermaid\ngraph TD\n    INTERNET[\"Public Internet\"]\n    TRAEFIK[\"Traefik\u003cbr/\u003e:80, :443\"]\n    \n    INTERNET --\u003e|\"HTTPS\"| TRAEFIK\n    \n    TRAEFIK --\u003e|\"Host: yourdomain.com\"| HOMEPAGE[\"Homepage\u003cbr/\u003eDashboard\"]\n    TRAEFIK --\u003e|\"Host: mlflow.yourdomain.com\"| MLFLOW[\"MLflow\u003cbr/\u003eTracking Server\"]\n    TRAEFIK --\u003e|\"Host: minio.yourdomain.com\"| MINIO_UI[\"MinIO\u003cbr/\u003eConsole\"]\n    TRAEFIK --\u003e|\"Host: minio-api.yourdomain.com\"| MINIO_API[\"MinIO\u003cbr/\u003eS3 API\"]\n    TRAEFIK --\u003e|\"Host: postgres.yourdomain.com\u003cbr/\u003ePort: 5432\"| POSTGRES[\"PostgreSQL\u003cbr/\u003eDatabase\"]\n    TRAEFIK --\u003e|\"Host: rustdesk.yourdomain.com\"| RUSTDESK[\"RustDesk\u003cbr/\u003eServer\"]\n    TRAEFIK --\u003e|\"Host: traefik.yourdomain.com\"| TRAEFIK_DASH[\"Traefik\u003cbr/\u003eDashboard\"]\n```\n\n**Diagram: Service Access Routing**\n\nAccess each service through your browser:\n- Homepage: `https://yourdomain.com`\n- MLflow: `https://mlflow.yourdomain.com`\n- MinIO Console: `https://minio.yourdomain.com`\n- Traefik Dashboard: `https://traefik.yourdomain.com` (requires authentication)\n\n**Sources:** [infra/setup.sh:213-220]()\n\n### Credential Access\n\nAll service credentials are displayed at deployment completion [infra/setup.sh:229-253]() and stored in `.env` file:\n\n| Service | Access Information | Environment Variables |\n|---------|-------------------|----------------------|\n| Traefik Dashboard | Username: `admin`\u003cbr/\u003ePassword: from `.env` | `TRAEFIK_AUTH_USER`, `TRAEFIK_AUTH_PASSWORD` |\n| PostgreSQL | Host: `postgres.yourdomain.com`\u003cbr/\u003ePort: 5432\u003cbr/\u003eDatabase: `mlflow` | `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB` |\n| MinIO | Console: `https://minio.yourdomain.com`\u003cbr/\u003eAPI: `https://minio-api.yourdomain.com` | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` |\n| MLflow | URL: `https://mlflow.yourdomain.com` | Uses PostgreSQL and MinIO credentials |\n\n**Important:** The `.env` file contains all credentials and must be kept secure. The script displays credentials once at [infra/setup.sh:229-253]() and recommends backing up the `.env` file.\n\n**Sources:** [infra/setup.sh:229-253](), [infra/.env.template:1-37]()\n\n---\n\n## Troubleshooting Common Issues\n\n### DNS Propagation Issues\n\nIf services are not accessible after deployment:\n\n1. Verify DNS records are correctly configured: `nslookup mlflow.yourdomain.com`\n2. DNS propagation can take up to 48 hours (typically minutes)\n3. Check Traefik logs: `./manage.sh logs traefik`\n4. Verify ACME challenges succeed in logs\n\n### Certificate Provisioning Failures\n\nIf Let's Encrypt certificate provisioning fails:\n\n1. Verify `ACME_EMAIL` is valid in `.env` at [infra/.env.template:3]()\n2. Ensure ports 80 and 443 are accessible from internet\n3. Check `acme.json` permissions: must be 600 (set at [infra/setup.sh:120-121]())\n4. Review Traefik logs for ACME errors: `./manage.sh logs traefik`\n\n### Service Start Failures\n\nIf services fail to start:\n\n1. Check Docker daemon status: `sudo systemctl status docker`\n2. Verify `.env` configuration is complete\n3. Review service-specific logs: `./manage.sh logs \u003cservice-name\u003e`\n4. Ensure sufficient disk space for volumes\n5. Check for port conflicts if services fail to bind\n\n### Image Pull Failures\n\nIf `docker compose pull` fails:\n\n1. Verify internet connectivity from server\n2. Check Docker Hub or GHCR accessibility\n3. Review `MLFLOW_IMAGE` variable at [infra/.env.template:16]()\n4. Manually pull problematic image: `docker pull \u003cimage-name\u003e`\n\n**Sources:** [infra/setup.sh:120-132](), [infra/.env.template:3-16]()\n\n---\n\n## Post-Deployment Configuration\n\n### Homepage Dashboard Customization\n\nThe Homepage dashboard configuration is stored in `homepage-config/` directory. The setup script creates `homepage-config/.env` from template at [infra/setup.sh:97-102]().\n\nUpdate `HOMEPAGE_ALLOWED_HOSTS` in `.env` to match your domain at [infra/.env.template:13]().\n\n### MLflow Backend Configuration\n\nMLflow connects to PostgreSQL and MinIO using environment variables. The connection is established automatically using:\n- PostgreSQL backend: `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`\n- MinIO artifact store: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `MINIO_BUCKET_NAME`\n\nNo additional configuration is required after deployment.\n\n### Database External Access\n\nPostgreSQL is accessible externally via Traefik at `postgres.yourdomain.com:5432`. Connection parameters from [infra/Readme.md:44-50]():\n\n- **Host:** `postgres.yourdomain.com`\n- **Port:** `5432`\n- **Database:** `mlflow`\n- **SSL Mode:** Required (Traefik terminates SSL)\n- **Username/Password:** From `.env` file\n\n**Sources:** [infra/setup.sh:97-102](), [infra/.env.template:13-28](), [infra/Readme.md:44-50]()\n\n---\n\n## Updating the Infrastructure\n\nTo update services to latest versions:\n\n```bash\n./manage.sh update\n```\n\nThis command at [infra/setup.sh:187-192]():\n1. Pulls latest images: `docker compose pull`\n2. Recreates containers with new images: `docker compose up -d`\n3. Preserves all data in volumes\n\nThe MLflow image is specified by `MLFLOW_IMAGE` variable at [infra/.env.template:16](), which references a pre-built image from GitHub Container Registry. See [MLflow Custom Image Pipeline](#5.2) for details on the custom image build process.\n\n**Sources:** [infra/setup.sh:187-192](), [infra/.env.template:16]()"])</script><script>self.__next_f.push([1,"30:T599b,"])</script><script>self.__next_f.push([1,"# Model Deployment Options\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/src/esri/RefugeeCampDetector.py](examplemodel/src/esri/RefugeeCampDetector.py)\n- [examplemodel/src/inference.py](examplemodel/src/inference.py)\n- [examplemodel/src/stac2esri.py](examplemodel/src/stac2esri.py)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis document explains the three distinct model deployment strategies supported by the OpenGeoAIModelHub training pipeline. Each deployment format serves different use cases and target platforms. The training pipeline in [examplemodel/src/train.py]() automatically generates all three formats during model training, enabling deployment flexibility without retraining.\n\nFor information about deploying the infrastructure stack itself, see [Infrastructure Deployment](#6.1). For details on the training pipeline that produces these artifacts, see [Training Pipeline](#3.2).\n\n## Deployment Format Overview\n\nThe training pipeline produces four model artifact formats that enable three primary deployment strategies:\n\n| Format | File Extension | Primary Use Case | Target Runtime |\n|--------|---------------|------------------|----------------|\n| PyTorch State Dict | `.pth` | Direct PyTorch inference, further training | Python with PyTorch |\n| TorchScript | `.pt` | Optimized PyTorch inference, mobile deployment | PyTorch C++ API, Python |\n| ONNX | `.onnx` | Cross-platform inference, production systems | ONNX Runtime (any language) |\n| ESRI DLPK | `.dlpk` | GIS integration and geospatial analysis | ArcGIS Pro, ArcGIS Server |\n\nAll formats are generated during the training process and logged to MLflow for version control and artifact management.\n\n```mermaid\ngraph TB\n    subgraph \"Training Pipeline\"\n        TRAIN[\"train_model()\u003cbr/\u003etrain.py:370\"]\n        MODEL[\"LitRefugeeCamp\u003cbr/\u003ePyTorch Lightning Model\"]\n        CHECKPOINT[\"ModelCheckpoint\u003cbr/\u003ebest_model_path\"]\n    end\n    \n    subgraph \"Artifact Generation\"\n        PTH[\"best_model.pth\u003cbr/\u003eState Dictionary\u003cbr/\u003etrain.py:438\"]\n        PT[\"best_model.pt\u003cbr/\u003eTorchScript Traced\u003cbr/\u003etrain.py:447-448\"]\n        ONNX[\"best_model.onnx\u003cbr/\u003eONNX Export\u003cbr/\u003etrain.py:451-460\"]\n        STAC[\"stac_item.json\u003cbr/\u003eSTAC-MLM Metadata\u003cbr/\u003etrain.py:470-476\"]\n    end\n    \n    subgraph \"ESRI Package Creation\"\n        EMD[\"model.emd\u003cbr/\u003eESRI Model Definition\u003cbr/\u003estac2esri.py:23-51\"]\n        DETECTOR[\"RefugeeCampDetector.py\u003cbr/\u003eInference Class\u003cbr/\u003eesri/RefugeeCampDetector.py\"]\n        DLPK[\"best_model.dlpk\u003cbr/\u003eZIP Package\u003cbr/\u003estac2esri.py:54-58\"]\n    end\n    \n    subgraph \"Deployment Targets\"\n        PYTORCH_DEPLOY[\"PyTorch Native\u003cbr/\u003ePython Environment\"]\n        ONNX_DEPLOY[\"ONNX Runtime\u003cbr/\u003eCross-Platform\"]\n        ARCGIS_DEPLOY[\"ArcGIS Pro/Server\u003cbr/\u003eGeospatial Analysis\"]\n    end\n    \n    TRAIN --\u003e MODEL\n    MODEL --\u003e CHECKPOINT\n    CHECKPOINT --\u003e PTH\n    \n    PTH --\u003e PT\n    PTH --\u003e ONNX\n    \n    PT --\u003e DLPK\n    STAC --\u003e EMD\n    EMD --\u003e DLPK\n    DETECTOR --\u003e DLPK\n    \n    PTH --\u003e PYTORCH_DEPLOY\n    PT --\u003e PYTORCH_DEPLOY\n    ONNX --\u003e ONNX_DEPLOY\n    DLPK --\u003e ARCGIS_DEPLOY\n```\n\n**Diagram: Model Artifact Generation and Deployment Flow**\n\nSources: [examplemodel/src/train.py:370-507](), [examplemodel/src/stac2esri.py:54-58]()\n\n## PyTorch Native Deployment\n\n### Format Details\n\nThe training pipeline generates two PyTorch formats optimized for different use cases:\n\n1. **State Dictionary (`.pth`)**: Raw model weights in PyTorch's native format\n2. **TorchScript (`.pt`)**: Optimized, serialized model using `torch.jit.trace`\n\nThe state dictionary is created at [examplemodel/src/train.py:438]():\n\n```python\ntorch.save(model.state_dict(), \"meta/best_model.pth\")\n```\n\nThe TorchScript model is created through tracing at [examplemodel/src/train.py:444-448]():\n\n```python\ntorch_model = clean_model.model\ntorch_model.eval()\ntraced_model = torch.jit.trace(torch_model, torch.randn(1, 3, 256, 256))\ntorch.jit.save(traced_model, \"meta/best_model.pt\")\n```\n\n### Use Cases\n\n| Scenario | Recommended Format | Rationale |\n|----------|-------------------|-----------|\n| Fine-tuning or transfer learning | `.pth` state dict | Requires access to full model architecture and optimizer state |\n| Production Python inference | `.pt` TorchScript | Optimized execution, no Python dependency on model definition |\n| Mobile/embedded deployment | `.pt` TorchScript | C++ API compatible, smaller runtime footprint |\n| Development and debugging | `.pth` state dict | Full access to model internals and gradients |\n\n### Loading and Inference\n\nThe inference system provides two methods for loading PyTorch models, as shown in [examplemodel/src/inference.py:127-133]():\n\n**State Dictionary Loading:**\n```python\nfrom model import LitRefugeeCamp\nmodel = LitRefugeeCamp()\nmodel.load_state_dict(torch.load(\"meta/best_model.pth\"))\n```\n\n**TorchScript Loading:**\n```python\nmodel = torch.jit.load(\"meta/best_model.pt\")\n```\n\nThe `predict_image()` function at [examplemodel/src/inference.py:127-142]() demonstrates the complete inference pipeline including preprocessing and postprocessing.\n\n### Deployment Requirements\n\n- **Python**: 3.8+\n- **PyTorch**: Version logged in STAC metadata (`mlm:framework_version`)\n- **Dependencies**: Listed in [examplemodel/pyproject.toml]()\n- **Input**: RGB images normalized with ImageNet statistics (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n- **Output**: Binary segmentation mask (256256 pixels)\n\nSources: [examplemodel/src/train.py:438-448](), [examplemodel/src/inference.py:127-142]()\n\n## ONNX Cross-Platform Deployment\n\n### Format Details\n\nONNX (Open Neural Network Exchange) provides a standardized format for deploying models across multiple frameworks and platforms. The model is exported at [examplemodel/src/train.py:450-460]():\n\n```python\ntorch.onnx.export(\n    torch_model,\n    dummy_input,\n    \"meta/best_model.onnx\",\n    export_params=True,\n    opset_version=11,\n    input_names=[\"input\"],\n    output_names=[\"output\"],\n    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n)\n```\n\nKey export parameters:\n- **Opset version**: 11 (ensures broad runtime compatibility)\n- **Dynamic axes**: Batch dimension is dynamic, allowing variable batch sizes\n- **Input shape**: `[batch_size, 3, 256, 256]` (NCHW format)\n- **Output shape**: `[batch_size, 1, 256, 256]` (logits before sigmoid)\n\n### Use Cases\n\n| Runtime Environment | Benefits | Implementation Notes |\n|---------------------|----------|----------------------|\n| ONNX Runtime (Python) | Fast CPU/GPU inference, no PyTorch dependency | Use `onnxruntime` or `onnxruntime-gpu` |\n| ONNX Runtime (C++) | Production services, minimal dependencies | Link against ONNX Runtime library |\n| TensorRT | NVIDIA GPU optimization | Convert ONNX to TensorRT engine |\n| OpenVINO | Intel hardware optimization | Use OpenVINO toolkit for inference |\n| Web browsers | Client-side inference | Use ONNX.js for browser deployment |\n| Mobile devices | iOS/Android apps | Use ONNX Runtime Mobile |\n\n### STAC-MLM Metadata\n\nThe ONNX artifact is documented in STAC-MLM metadata at [examplemodel/src/train.py:275-283]():\n\n```json\n{\n  \"onnx-model\": {\n    \"href\": \"meta/best_model.onnx\",\n    \"type\": \"application/octet-stream; framework=onnx\",\n    \"title\": \"ONNX Model\",\n    \"description\": \"ONNX format model for cross-platform inference and deployment\",\n    \"roles\": [\"mlm:model\", \"mlm:inference\"],\n    \"mlm:artifact_type\": \"onnx\",\n    \"mlm:compile_method\": \"aot\"\n  }\n}\n```\n\n### Inference with ONNX Runtime\n\nExample Python inference using ONNX Runtime:\n\n```python\nimport onnxruntime as ort\nimport numpy as np\n\n# Load model\nsession = ort.InferenceSession(\"meta/best_model.onnx\")\n\n# Prepare input (after preprocessing)\ninput_data = preprocess_image(image_path)  # Shape: (1, 3, 256, 256)\n\n# Run inference\noutputs = session.run([\"output\"], {\"input\": input_data.numpy()})\n\n# Apply sigmoid and threshold\nprediction = 1 / (1 + np.exp(-outputs[0]))  # Sigmoid\nbinary_mask = (prediction \u003e 0.5).astype(np.uint8)\n```\n\n### Performance Characteristics\n\n- **No PyTorch dependency**: Reduces deployment complexity\n- **Hardware optimization**: Runtime-specific optimizations for CPU/GPU\n- **Smaller memory footprint**: Optimized graph reduces memory usage\n- **Language agnostic**: Can be loaded from C++, C#, Java, JavaScript, etc.\n\nSources: [examplemodel/src/train.py:450-460](), [examplemodel/src/train.py:275-283]()\n\n## ESRI Deep Learning Package (DLPK) Deployment\n\n### Package Structure\n\nThe DLPK format is a specialized deployment package for ArcGIS integration. It is created by [examplemodel/src/stac2esri.py:54-58]() and contains three components:\n\n```mermaid\ngraph LR\n    subgraph \"best_model.dlpk (ZIP Archive)\"\n        EMD[\"model.emd\u003cbr/\u003eJSON Configuration\"]\n        PT[\"model.pt\u003cbr/\u003eTorchScript Model\"]\n        INFERENCE[\"RefugeeCampDetector.py\u003cbr/\u003eInference Class\"]\n    end\n    \n    EMD --\u003e CONFIG[\"ArcGIS Configuration\u003cbr/\u003eImage size, bands, classes\"]\n    PT --\u003e MODEL_EXEC[\"Model Execution\u003cbr/\u003etorch.jit.load()\"]\n    INFERENCE --\u003e CUSTOM_LOGIC[\"Custom Inference Logic\u003cbr/\u003eupdatePixels()\"]\n    \n    CONFIG --\u003e ARCGIS[\"ArcGIS Pro/Server\"]\n    MODEL_EXEC --\u003e ARCGIS\n    CUSTOM_LOGIC --\u003e ARCGIS\n```\n\n**Diagram: DLPK Package Structure and Components**\n\n### ESRI Model Definition (EMD)\n\nThe EMD file is generated by converting STAC-MLM metadata at [examplemodel/src/stac2esri.py:7-51]():\n\n```python\nemd = {\n    \"Framework\": \"PyTorch\",\n    \"ModelType\": \"ImageClassification\",\n    \"ModelName\": props.get(\"mlm:name\", \"RefugeeCampDetector\"),\n    \"Architecture\": props.get(\"mlm:architecture\", \"U-Net\"),\n    \"ModelFile\": \"model.pt\",\n    \"InferenceFunction\": \"RefugeeCampDetector.py\",\n    \"ImageHeight\": 256,\n    \"ImageWidth\": 256,\n    \"ImageSpaceUsed\": \"MAP_SPACE\",\n    \"ExtractBands\": [0, 1, 2],\n    \"DataRange\": [0, 1],\n    \"BatchSize\": 1,\n    \"Classes\": [\n        {\"Value\": 0, \"Name\": \"Background\", \"Color\": [0, 0, 0]},\n        {\"Value\": 1, \"Name\": \"Refugee Camp\", \"Color\": [255, 0, 0]}\n    ],\n    \"ModelParameters\": {\n        \"mean\": [0.485, 0.456, 0.406],\n        \"std\": [0.229, 0.224, 0.225]\n    },\n    \"Threshold\": 0.5\n}\n```\n\nKey configuration parameters:\n- **ImageSpaceUsed**: `MAP_SPACE` indicates the model operates on georeferenced imagery\n- **ExtractBands**: RGB bands [0, 1, 2] are extracted from input imagery\n- **DataRange**: Input pixels normalized to [0, 1] range\n- **ModelParameters**: ImageNet normalization statistics\n\n### RefugeeCampDetector Inference Class\n\nThe core inference logic is implemented in [examplemodel/src/esri/RefugeeCampDetector.py](), which provides the ArcGIS-compatible interface:\n\n```mermaid\nsequenceDiagram\n    participant ArcGIS as \"ArcGIS Raster Function\"\n    participant RCD as \"RefugeeCampDetector\"\n    participant Model as \"TorchScript Model\"\n    \n    ArcGIS-\u003e\u003eRCD: initialize(model=path_to_emd)\n    RCD-\u003e\u003eRCD: Load model.emd JSON\n    RCD-\u003e\u003eRCD: Set model_path\n    \n    ArcGIS-\u003e\u003eRCD: updatePixels(raster_pixels, raster_mask)\n    RCD-\u003e\u003eRCD: load_model() (if not loaded)\n    RCD-\u003e\u003eModel: torch.jit.load(model_path)\n    \n    RCD-\u003e\u003eRCD: Extract bands [0,1,2]\n    RCD-\u003e\u003eRCD: Normalize to [0,1]\n    RCD-\u003e\u003eRCD: Apply ImageNet normalization\n    \n    RCD-\u003e\u003eModel: model(tensor)\n    Model--\u003e\u003eRCD: output logits\n    \n    RCD-\u003e\u003eRCD: Apply sigmoid\n    RCD-\u003e\u003eRCD: Threshold \u003e 0.95\n    RCD-\u003e\u003eRCD: Convert to uint8\n    \n    RCD--\u003e\u003eArcGIS: output_pixels (binary mask)\n```\n\n**Diagram: DLPK Inference Pipeline in ArcGIS**\n\n### Implementation Details\n\nThe `RefugeeCampDetector` class implements the ArcGIS Python Raster Function API with three key methods:\n\n**1. initialize()** - [examplemodel/src/esri/RefugeeCampDetector.py:31-51]()\n- Loads EMD configuration from JSON\n- Resolves absolute path to TorchScript model\n- Defers actual model loading until first inference\n\n**2. getConfiguration()** - [examplemodel/src/esri/RefugeeCampDetector.py:85-100]()\n- Returns processing parameters to ArcGIS\n- Specifies tile size (256256)\n- Defines band extraction and data range\n- Enables input masking for NoData handling\n\n**3. updatePixels()** - [examplemodel/src/esri/RefugeeCampDetector.py:107-183]()\n- Processes each image tile\n- Applies band selection, normalization, and inference\n- Returns binary classification mask\n\nThe preprocessing pipeline at [examplemodel/src/esri/RefugeeCampDetector.py:116-141]():\n\n```python\n# Extract input pixels (C, H, W)\npix = pixelBlocks[\"raster_pixels\"]\n\n# Apply mask\npix[mask == 0] = 0\n\n# Add batch dimension (B, C, H, W)\narr = pix.astype(np.float32)\nif arr.ndim == 3:\n    arr = arr[np.newaxis]\n\n# Normalize to [0, 1]\nif arr.max() \u003e 1.0:\n    arr = arr / 255.0\n\n# Apply ImageNet normalization\nmean = np.array([0.485, 0.456, 0.406])[None, :, None, None]\nstd = np.array([0.229, 0.224, 0.225])[None, :, None, None]\narr = (arr - mean) / std\n```\n\n### Use Cases\n\n| GIS Workflow | DLPK Application | ArcGIS Component |\n|--------------|------------------|------------------|\n| Humanitarian mapping | Detect refugee camps in satellite imagery | ArcGIS Pro - Classify Pixels Using Deep Learning tool |\n| Change detection | Compare camp presence over time | Raster Function chain |\n| Automated monitoring | Scheduled inference on new imagery | ArcGIS Server - Image Service |\n| Interactive analysis | Real-time camp detection during map exploration | Dynamic raster function |\n\n### Deployment in ArcGIS Pro\n\n1. Copy `best_model.dlpk` to a location accessible by ArcGIS Pro\n2. Open the \"Classify Pixels Using Deep Learning\" tool\n3. Select the DLPK as the model definition\n4. Provide input imagery (RGB satellite imagery)\n5. Run inference - results are generated as a classified raster\n\nThe DLPK handles all preprocessing, normalization, and postprocessing automatically based on the EMD configuration.\n\nSources: [examplemodel/src/stac2esri.py:7-58](), [examplemodel/src/esri/RefugeeCampDetector.py:25-183]()\n\n## Deployment Format Comparison\n\n### Technical Specifications\n\n| Characteristic | PyTorch (.pth/.pt) | ONNX (.onnx) | DLPK (.dlpk) |\n|----------------|-------------------|--------------|--------------|\n| **Framework Dependency** | PyTorch required | Framework-agnostic | PyTorch required (bundled) |\n| **File Size** | ~50-100 MB | ~50-100 MB | ~50-100 MB + inference code |\n| **Inference Speed** | Fast (native) | Fast (optimized) | Fast (TorchScript-based) |\n| **Batch Processing** | Supported | Supported (dynamic axes) | Limited (tile-based) |\n| **GPU Acceleration** | CUDA | CUDA, TensorRT, DirectML | CUDA (if available) |\n| **Language Support** | Python (primary) | C++, Python, C#, Java, JS | Python (ArcGIS) |\n| **Mobile Deployment** | PyTorch Mobile | ONNX Runtime Mobile | Not supported |\n\n### Platform Support\n\n```mermaid\ngraph TB\n    subgraph \"PyTorch Native\"\n        PTH_LINUX[\"Linux\u003cbr/\u003eFull Support\"]\n        PTH_WIN[\"Windows\u003cbr/\u003eFull Support\"]\n        PTH_MAC[\"macOS\u003cbr/\u003eFull Support\"]\n        PTH_MOBILE[\"Mobile\u003cbr/\u003ePyTorch Mobile\"]\n    end\n    \n    subgraph \"ONNX Runtime\"\n        ONNX_LINUX[\"Linux\u003cbr/\u003eCPU/GPU\"]\n        ONNX_WIN[\"Windows\u003cbr/\u003eCPU/GPU/DirectML\"]\n        ONNX_MAC[\"macOS\u003cbr/\u003eCPU only\"]\n        ONNX_MOBILE[\"Mobile\u003cbr/\u003eiOS/Android\"]\n        ONNX_WEB[\"Web\u003cbr/\u003eONNX.js\"]\n        ONNX_EDGE[\"Edge Devices\u003cbr/\u003eARM/x86\"]\n    end\n    \n    subgraph \"ESRI DLPK\"\n        DLPK_PRO[\"ArcGIS Pro\u003cbr/\u003eWindows\"]\n        DLPK_SERVER[\"ArcGIS Server\u003cbr/\u003eWindows/Linux\"]\n        DLPK_ONLINE[\"ArcGIS Online\u003cbr/\u003eCloud\"]\n    end\n    \n    PTH_PYTORCH[\"PyTorch\u003cbr/\u003eEcosystem\"]\n    ONNX_ECOSYSTEM[\"ONNX\u003cbr/\u003eEcosystem\"]\n    ESRI_ECOSYSTEM[\"ESRI\u003cbr/\u003eEcosystem\"]\n    \n    PTH_LINUX --\u003e PTH_PYTORCH\n    PTH_WIN --\u003e PTH_PYTORCH\n    PTH_MAC --\u003e PTH_PYTORCH\n    PTH_MOBILE --\u003e PTH_PYTORCH\n    \n    ONNX_LINUX --\u003e ONNX_ECOSYSTEM\n    ONNX_WIN --\u003e ONNX_ECOSYSTEM\n    ONNX_MAC --\u003e ONNX_ECOSYSTEM\n    ONNX_MOBILE --\u003e ONNX_ECOSYSTEM\n    ONNX_WEB --\u003e ONNX_ECOSYSTEM\n    ONNX_EDGE --\u003e ONNX_ECOSYSTEM\n    \n    DLPK_PRO --\u003e ESRI_ECOSYSTEM\n    DLPK_SERVER --\u003e ESRI_ECOSYSTEM\n    DLPK_ONLINE --\u003e ESRI_ECOSYSTEM\n```\n\n**Diagram: Platform Support Matrix for Each Deployment Format**\n\n### Decision Matrix\n\nChoose your deployment format based on these criteria:\n\n**Use PyTorch Native when:**\n- Deploying in a Python-based ML pipeline\n- Need flexibility to fine-tune or retrain the model\n- Development/research environment with full PyTorch stack\n- Integrating with PyTorch Lightning or other PyTorch libraries\n\n**Use ONNX when:**\n- Deploying to production services in non-Python languages (C++, C#, Java)\n- Need maximum platform portability (Windows, Linux, macOS, mobile, web)\n- Optimizing for specific hardware (NVIDIA TensorRT, Intel OpenVINO)\n- Minimizing dependencies and runtime footprint\n- Integrating with ML serving frameworks (Triton, TF Serving)\n\n**Use DLPK when:**\n- Working within the ESRI/ArcGIS ecosystem\n- Processing georeferenced satellite imagery\n- Need GIS-specific functionality (coordinate systems, raster functions)\n- Deploying to ArcGIS Pro or ArcGIS Server\n- End-users are GIS analysts without ML expertise\n\nSources: [examplemodel/src/train.py:436-507](), [examplemodel/src/stac2esri.py:54-58]()\n\n## Artifact Generation Pipeline\n\n### Training Process Flow\n\nThe complete artifact generation pipeline is orchestrated by the `train_model()` function at [examplemodel/src/train.py:370-507]():\n\n```mermaid\nflowchart TD\n    START[\"train_model(args)\u003cbr/\u003etrain.py:370\"]\n    \n    TRAIN[\"PyTorch Lightning Trainer\u003cbr/\u003etrainer.fit(model, data_module)\u003cbr/\u003etrain.py:422\"]\n    \n    CHECKPOINT[\"ModelCheckpoint Callback\u003cbr/\u003eSaves best_model_path\u003cbr/\u003etrain.py:405-412\"]\n    \n    LOAD[\"Load Best Checkpoint\u003cbr/\u003eLitRefugeeCamp.load_from_checkpoint()\u003cbr/\u003etrain.py:433\"]\n    \n    subgraph \"Artifact Generation\"\n        STATE_DICT[\"Save State Dict\u003cbr/\u003etorch.save(state_dict)\u003cbr/\u003etrain.py:438\"]\n        \n        TORCHSCRIPT[\"Trace with torch.jit\u003cbr/\u003etorch.jit.trace()\u003cbr/\u003etrain.py:447\"]\n        SAVE_PT[\"Save TorchScript\u003cbr/\u003etorch.jit.save()\u003cbr/\u003etrain.py:448\"]\n        \n        ONNX_EXPORT[\"ONNX Export\u003cbr/\u003etorch.onnx.export()\u003cbr/\u003etrain.py:451-460\"]\n        \n        STAC_CREATE[\"Create STAC-MLM Item\u003cbr/\u003ecreate_stac_mlm_item()\u003cbr/\u003etrain.py:470-472\"]\n        \n        EMD_CREATE[\"Generate EMD from STAC\u003cbr/\u003estac2esri.stacmlm_to_emd()\u003cbr/\u003eImplied in create_dlpk\"]\n        \n        DLPK_CREATE[\"Create DLPK Package\u003cbr/\u003ecreate_dlpk()\u003cbr/\u003etrain.py:468\"]\n    end\n    \n    subgraph \"MLflow Logging\"\n        LOG_STAC[\"mlflow.log_artifact(stac_item)\u003cbr/\u003etrain.py:479\"]\n        LOG_PTH[\"mlflow.log_artifact(best_model.pth)\u003cbr/\u003etrain.py:480\"]\n        LOG_PT[\"mlflow.log_artifact(best_model.pt)\u003cbr/\u003etrain.py:481\"]\n        LOG_ONNX[\"mlflow.log_artifact(best_model.onnx)\u003cbr/\u003etrain.py:482\"]\n        LOG_DLPK[\"mlflow.log_artifact(best_model.dlpk)\u003cbr/\u003etrain.py:483\"]\n        LOG_MODEL[\"mlflow.pytorch.log_model()\u003cbr/\u003etrain.py:501-503\"]\n    end\n    \n    START --\u003e TRAIN\n    TRAIN --\u003e CHECKPOINT\n    CHECKPOINT --\u003e LOAD\n    \n    LOAD --\u003e STATE_DICT\n    LOAD --\u003e TORCHSCRIPT\n    TORCHSCRIPT --\u003e SAVE_PT\n    LOAD --\u003e ONNX_EXPORT\n    \n    STATE_DICT --\u003e STAC_CREATE\n    SAVE_PT --\u003e EMD_CREATE\n    STAC_CREATE --\u003e EMD_CREATE\n    \n    EMD_CREATE --\u003e DLPK_CREATE\n    SAVE_PT --\u003e DLPK_CREATE\n    \n    STATE_DICT --\u003e LOG_PTH\n    SAVE_PT --\u003e LOG_PT\n    ONNX_EXPORT --\u003e LOG_ONNX\n    STAC_CREATE --\u003e LOG_STAC\n    DLPK_CREATE --\u003e LOG_DLPK\n    \n    LOG_PTH --\u003e LOG_MODEL\n    LOG_PT --\u003e LOG_MODEL\n    LOG_ONNX --\u003e LOG_MODEL\n    LOG_STAC --\u003e LOG_MODEL\n    LOG_DLPK --\u003e LOG_MODEL\n```\n\n**Diagram: Complete Model Artifact Generation and Logging Pipeline**\n\n### File Locations and Naming\n\nAll artifacts are saved to the `meta/` directory with consistent naming:\n\n| Artifact | File Path | Size Range | MLflow Artifact Path |\n|----------|-----------|------------|---------------------|\n| State Dictionary | `meta/best_model.pth` | 50-100 MB | `models/best_model.pth` |\n| TorchScript | `meta/best_model.pt` | 50-100 MB | `models/best_model.pt` |\n| ONNX Model | `meta/best_model.onnx` | 50-100 MB | `models/best_model.onnx` |\n| STAC Metadata | `meta/stac_item.json` | ~10 KB | `metadata/stac_item.json` |\n| ESRI Definition | `meta/model.emd` | ~1 KB | `esri/model.emd` |\n| DLPK Package | `meta/best_model.dlpk` | 50-100 MB | `models/best_model.dlpk` |\n| Lightning Checkpoint | `checkpoints/epoch=X-step=Y.ckpt` | 100-200 MB | `checkpoints/epoch=X-step=Y.ckpt` |\n\n### STAC-MLM Metadata Integration\n\nThe STAC-MLM item at [examplemodel/src/train.py:101-367]() serves as the authoritative metadata source documenting all deployment artifacts:\n\n```python\nstac_item = {\n    \"type\": \"Feature\",\n    \"stac_version\": \"1.1.0\",\n    \"stac_extensions\": [\n        \"https://stac-extensions.github.io/mlm/v1.5.0/schema.json\",\n        \"https://stac-extensions.github.io/file/v1.0.0/schema.json\",\n        \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\"\n    ],\n    \"properties\": {\n        \"mlm:name\": \"RefugeeCampDetector\",\n        \"mlm:architecture\": \"U-Net\",\n        \"mlm:framework\": \"PyTorch\",\n        \"mlm:input\": [...],  # Input specification\n        \"mlm:output\": [...],  # Output specification\n    },\n    \"assets\": {\n        \"pytorch-state-dict-raw\": {...},  # .pth file\n        \"pytorch-state-dict\": {...},      # .pt file\n        \"onnx-model\": {...},               # .onnx file\n        \"esri-package\": {...}              # .dlpk file\n    }\n}\n```\n\nThis metadata enables:\n- **Discoverability**: Models can be catalogued and searched\n- **Reproducibility**: Complete specification of inputs, outputs, and preprocessing\n- **Traceability**: Links to source code, training data, and container images\n- **Interoperability**: Standard format for model exchange\n\nThe STAC-MLM specification is used by `stacmlm_to_emd()` at [examplemodel/src/stac2esri.py:7-51]() to automatically generate the ESRI Model Definition file, ensuring consistency between metadata and deployment configuration.\n\n### Accessing Artifacts from MLflow\n\nAfter training completes, all artifacts are available through the MLflow UI:\n\n1. Navigate to MLflow tracking server (configured in infrastructure)\n2. Select the experiment run\n3. View artifacts in the structured folder hierarchy:\n   - `models/` - All model files (.pth, .pt, .onnx, .dlpk)\n   - `metadata/` - STAC-MLM item and metadata\n   - `checkpoints/` - PyTorch Lightning checkpoints\n   - `esri/` - ESRI-specific files (EMD, inference script)\n   - `examples/` - Inference visualizations\n   - `datasets/` - Training data structure\n\nArtifacts can be downloaded individually or programmatically accessed using the MLflow API:\n\n```python\nimport mlflow\n\n# Download specific artifact\nmlflow.artifacts.download_artifacts(\n    run_id=\"\u003crun_id\u003e\",\n    artifact_path=\"models/best_model.onnx\",\n    dst_path=\"./downloaded_models\"\n)\n\n# Load PyTorch model directly\nmodel = mlflow.pytorch.load_model(\"runs:/\u003crun_id\u003e/model\")\n```\n\nSources: [examplemodel/src/train.py:370-507](), [examplemodel/src/train.py:101-367](), [examplemodel/src/stac2esri.py:7-51]()"])</script><script>self.__next_f.push([1,"31:T3285,"])</script><script>self.__next_f.push([1,"# Development Guide\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [.gitignore](.gitignore)\n- [examplemodel/.env_sample](examplemodel/.env_sample)\n- [examplemodel/pyproject.toml](examplemodel/pyproject.toml)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis guide provides technical guidance for developers working with the OpenGeoAIModelHub codebase. It covers local development environment setup, codebase organization, and common development workflows for extending or modifying the ML pipeline.\n\nFor step-by-step infrastructure deployment, see [Infrastructure Deployment](#6.1). For specific configuration of environment variables and credentials, see [Configuration Management](#4.3). For modifying individual pipeline components, refer to the subsections: [Local Development Setup](#7.1), [Working with the Training Pipeline](#7.2), and [Data Preparation and Custom Datasets](#7.3).\n\n---\n\n## Development Workflow Overview\n\nThe development workflow follows a modern Python MLOps pattern using `uv` for dependency management and MLflow for experiment orchestration. Developers work locally with the `examplemodel` package while logging experiments to either local MLflow or the remote infrastructure stack.\n\n### Development Cycle Diagram\n\n```mermaid\nflowchart TD\n    CLONE[\"git clone repository\"]\n    SETUP[\"uv sync\u003cbr/\u003e(install dependencies)\"]\n    ENV[\"Configure .env file\u003cbr/\u003e(AWS credentials, MLflow URI)\"]\n    CODE[\"Modify source code\u003cbr/\u003e(src/*.py)\"]\n    TEST[\"Local testing\u003cbr/\u003e(mlflow run)\"]\n    LOG[\"Review MLflow UI\u003cbr/\u003e(metrics \u0026 artifacts)\"]\n    COMMIT[\"git commit \u0026 push\"]\n    \n    CLONE --\u003e SETUP\n    SETUP --\u003e ENV\n    ENV --\u003e CODE\n    CODE --\u003e TEST\n    TEST --\u003e LOG\n    LOG --\u003e CODE\n    LOG --\u003e COMMIT\n    COMMIT --\u003e CODE\n    \n    style TEST fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style LOG fill:#f9f9f9,stroke:#333,stroke-width:2px\n```\n\n**Sources:** Analysis of high-level system diagrams (Diagram 5: System Integration and Data Flow)\n\n---\n\n## Code Organization\n\nThe repository is structured with clear separation between the example model implementation and infrastructure configuration. Understanding this organization is essential for efficient development.\n\n### Repository Structure\n\n```\nOpenGeoAIModelHub/\n examplemodel/                # Example ML model package\n    src/                     # Source code\n       train.py            # Training pipeline entry point\n       inference.py        # Inference pipeline entry point\n       model.py            # Model architecture definitions\n       preprocess.py       # Data preprocessing\n       validate_stac.py    # STAC metadata validation\n       esri/               # ESRI integration components\n           RefugeeCampDetector.py\n           stac2esri.py\n    MLproject               # MLflow project definition\n    pyproject.toml          # Package dependencies\n    uv.lock                 # Locked dependency versions\n    .env_sample             # Environment variable template\n docker-compose.yml          # Infrastructure stack definition\n setup.sh                    # Infrastructure automation script\n .gitignore                  # Version control exclusions\n```\n\n### Key Python Modules and Classes\n\n```mermaid\ngraph TB\n    subgraph \"Training Pipeline (train.py)\"\n        CAMPDM[\"CampDataModule\u003cbr/\u003ePyTorch Lightning DataModule\"]\n        LITCAMP[\"LitRefugeeCamp\u003cbr/\u003ePyTorch Lightning Module\"]\n        TRAINMAIN[\"train_main()\u003cbr/\u003eOrchestration function\"]\n    end\n    \n    subgraph \"Model Architecture (model.py)\"\n        UNET[\"RefugeeCampModel\u003cbr/\u003eU-Net implementation\"]\n        BLOCKS[\"ConvBlock, UpBlock\u003cbr/\u003eArchitecture components\"]\n    end\n    \n    subgraph \"Inference (inference.py)\"\n        INFERMAIN[\"inference_main()\u003cbr/\u003ePrediction orchestration\"]\n        LOADMODEL[\"load_model()\u003cbr/\u003eCheckpoint loading\"]\n    end\n    \n    subgraph \"ESRI Integration (esri/)\"\n        DETECTOR[\"RefugeeCampDetector\u003cbr/\u003eArcGISModel subclass\"]\n        STAC2ESRI[\"stac2esri()\u003cbr/\u003eDLPK packaging\"]\n    end\n    \n    TRAINMAIN --\u003e CAMPDM\n    TRAINMAIN --\u003e LITCAMP\n    LITCAMP --\u003e UNET\n    UNET --\u003e BLOCKS\n    \n    INFERMAIN --\u003e LOADMODEL\n    LOADMODEL --\u003e UNET\n    \n    STAC2ESRI --\u003e DETECTOR\n    DETECTOR --\u003e UNET\n    \n    style TRAINMAIN fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style LITCAMP fill:#f9f9f9,stroke:#333,stroke-width:2px\n    style DETECTOR fill:#f9f9f9,stroke:#333,stroke-width:2px\n```\n\n**Sources:** High-level system diagrams (Diagram 2: Example Model ML Pipeline Architecture), analysis of code structure\n\n---\n\n## Development Environment\n\n### Dependency Management with uv\n\nThe project uses `uv` as the package manager, defined in [examplemodel/pyproject.toml:1-31](). Dependencies are specified in the `[project]` section:\n\n| Dependency Category | Key Packages | Purpose |\n|---------------------|--------------|---------|\n| ML Framework | `pytorch-lightning\u003e=2.5.2`, `torch\u003e=2.7.1`, `torchvision\u003e=0.22.1` | Model training and inference |\n| MLOps | `mlflow\u003e=3.1.1` | Experiment tracking and model registry |\n| Geospatial | `geomltoolkits\u003e=0.3.9` | Geospatial data processing utilities |\n| Model Export | `onnx\u003e=1.18.0`, `onnxscript\u003e=0.3.2` | Cross-platform model deployment |\n| STAC | `stac-model\u003e=0.3.0`, `pystac\u003e=1.8.0` | ML model metadata standards |\n| Cloud Storage | `boto3\u003e=1.39.12` | S3-compatible storage access |\n\n**Sources:** [examplemodel/pyproject.toml:1-31]()\n\n### Environment Configuration\n\nEnvironment variables are required for MLflow tracking and artifact storage. The [examplemodel/.env_sample:1-5]() provides a template:\n\n```bash\nexport AWS_ACCESS_KEY_ID=key_key\nexport AWS_SECRET_ACCESS_KEY=secret_secret\nexport MLFLOW_S3_ENDPOINT_URL=https://minio-api.krschap.tech\nexport MLFLOW_TRACKING_URI=http://mlflow.krschap.tech\n```\n\nThese variables configure:\n- **AWS credentials**: For MinIO S3-compatible storage authentication\n- **MLFLOW_S3_ENDPOINT_URL**: MinIO API endpoint for artifact storage\n- **MLFLOW_TRACKING_URI**: MLflow tracking server URL for experiment logging\n\nFor local development, developers can run MLflow locally or connect to a remote infrastructure stack. See [Local Development Setup](#7.1) for detailed configuration.\n\n**Sources:** [examplemodel/.env_sample:1-5]()\n\n---\n\n## Version Control and Ignored Artifacts\n\nThe [.gitignore:1-7]() file excludes generated artifacts and temporary files from version control:\n\n| Pattern | Purpose |\n|---------|---------|\n| `examplemodel/mlruns` | Local MLflow experiment runs |\n| `examplemodel/mlartifacts` | Local MLflow artifacts directory |\n| `examplemodel/lightning_logs` | PyTorch Lightning training logs |\n| `examplemodel/src/__pycache__` | Python bytecode cache |\n| `examplemodel/checkpoints` | Model checkpoint files |\n| `examplemodel/meta` | Metadata files |\n| `*.env` | Environment variable files (secrets) |\n\nThese exclusions prevent committing large binary files, temporary artifacts, and sensitive credentials. All model artifacts should be logged to MLflow instead of committed to git.\n\n**Sources:** [.gitignore:1-7]()\n\n---\n\n## MLflow Project Entry Points\n\nDevelopment workflows primarily interact with MLflow project entry points defined in the `MLproject` file. These entry points provide standardized interfaces for different pipeline stages.\n\n### Entry Point Execution Flow\n\n```mermaid\nsequenceDiagram\n    participant Dev as \"Developer\"\n    participant UV as \"uv\u003cbr/\u003e(dependency manager)\"\n    participant MLF as \"mlflow run\u003cbr/\u003e(orchestrator)\"\n    participant TRAIN as \"train.py\u003cbr/\u003e(training script)\"\n    participant INFER as \"inference.py\u003cbr/\u003e(inference script)\"\n    participant MLFLOW as \"MLflow Server\u003cbr/\u003e(tracking)\"\n    \n    Dev-\u003e\u003eUV: \"uv sync\"\n    UV--\u003e\u003eDev: \"Dependencies installed\"\n    \n    Dev-\u003e\u003eMLF: \"mlflow run . -e preprocess\"\n    MLF-\u003e\u003eTRAIN: \"Execute preprocess_main()\"\n    TRAIN--\u003e\u003eDev: \"Data prepared in chips/ and labels/\"\n    \n    Dev-\u003e\u003eMLF: \"mlflow run . -e train\"\n    MLF-\u003e\u003eTRAIN: \"Execute train_main()\"\n    TRAIN-\u003e\u003eMLFLOW: \"Log metrics \u0026 artifacts\"\n    MLFLOW--\u003e\u003eTRAIN: \"Run ID\"\n    TRAIN--\u003e\u003eDev: \"Training complete\"\n    \n    Dev-\u003e\u003eMLF: \"mlflow run . -e inference\"\n    MLF-\u003e\u003eINFER: \"Execute inference_main()\"\n    INFER-\u003e\u003eMLFLOW: \"Log prediction results\"\n    INFER--\u003e\u003eDev: \"Inference complete\"\n```\n\n**Sources:** High-level system diagrams (Diagram 4: End-to-End Model Development and Deployment Flow)\n\n### Available Entry Points\n\nThe MLproject file defines five entry points for different pipeline stages:\n\n1. **preprocess**: Data acquisition and preparation\n2. **train**: Model training with MLflow logging\n3. **inference**: Run predictions on new imagery\n4. **validate_stac**: Validate STAC-MLM metadata compliance\n5. **stac2esri**: Convert STAC metadata to ESRI DLPK format\n\nFor detailed parameter configurations and usage examples, see [MLproject API Reference](#8.1).\n\n**Sources:** High-level system diagrams (Diagram 2: Example Model ML Pipeline Architecture)\n\n---\n\n## Common Development Tasks\n\n### Modifying Model Architecture\n\nTo modify the model architecture, work with the classes in `src/model.py`:\n- **RefugeeCampModel**: Main U-Net architecture\n- **ConvBlock**: Convolutional block component\n- **UpBlock**: Upsampling block component\n\nChanges to the architecture require corresponding updates in:\n1. `LitRefugeeCamp` class in `src/train.py` (if loss functions or metrics change)\n2. `RefugeeCampDetector` class in `src/esri/RefugeeCampDetector.py` (for ESRI deployment)\n\nSee [Working with the Training Pipeline](#7.2) for detailed guidance.\n\n### Adding Custom Metrics\n\nCustom metrics can be logged during training by modifying the training loop in `src/train.py`. Metrics are logged using `self.log()` within the `LitRefugeeCamp` class methods (`training_step`, `validation_step`).\n\n### Working with Custom Datasets\n\nThe `CampDataModule` class handles data loading. To integrate custom datasets:\n1. Prepare data in the same format (chips/ and labels/ directories)\n2. Modify preprocessing logic in `src/preprocess.py`\n3. Update data paths in `CampDataModule` initialization\n\nSee [Data Preparation and Custom Datasets](#7.3) for implementation details.\n\n**Sources:** Analysis of code structure from high-level diagrams\n\n---\n\n## Development Best Practices\n\n### Experiment Tracking\n\nAll training runs should be logged to MLflow:\n- Use `mlflow.start_run()` to create experiment runs\n- Log hyperparameters with `mlflow.log_param()`\n- Log metrics with `mlflow.log_metric()`\n- Log artifacts (models, plots) with `mlflow.log_artifact()`\n\nThis ensures reproducibility and enables comparison of different experiments.\n\n### Model Artifact Management\n\nGenerated artifacts should be:\n1. **Logged to MLflow**: Primary storage location\n2. **Excluded from git**: Via [.gitignore:1-7]()\n3. **Version-controlled**: Through MLflow's model registry\n\n### Local vs Remote Infrastructure\n\nDevelopers can work with either:\n- **Local MLflow**: Set `MLFLOW_TRACKING_URI=http://localhost:5000`\n- **Remote infrastructure**: Point to deployed stack (see [Infrastructure Deployment](#6.1))\n\nLocal development is recommended for rapid iteration, while remote infrastructure is used for team collaboration and production training runs.\n\n**Sources:** [.gitignore:1-7](), [examplemodel/.env_sample:1-5]()\n\n---\n\n## Integration Points\n\n### External Data Sources\n\nThe training pipeline integrates with:\n- **OpenAerialMap**: Satellite imagery via TMS\n- **OpenStreetMap**: Label data via Overpass API\n\nCustom data sources can be integrated by modifying `src/preprocess.py`.\n\n### MLflow Integration\n\nThe training pipeline uses MLflow for:\n- **Experiment tracking**: Metrics, parameters, and system metrics\n- **Artifact storage**: Models, plots, and metadata\n- **Model registry**: Versioned model management\n\nConfiguration is handled through environment variables in [examplemodel/.env_sample:1-5]().\n\n### ESRI ArcGIS Integration\n\nThe ESRI integration enables deployment to ArcGIS platforms:\n- **RefugeeCampDetector**: Implements `ArcGISModel` interface\n- **DLPK generation**: Packages model, metadata, and inference code\n- **STAC-MLM compliance**: Standardized metadata format\n\nSee [ESRI Integration and DLPK Generation](#3.4) for technical details.\n\n**Sources:** [examplemodel/.env_sample:1-5](), high-level system diagrams\n\n---\n\n## Next Steps\n\nFor detailed guidance on specific development tasks:\n- **Setting up local environment**: See [Local Development Setup](#7.1)\n- **Modifying training pipeline**: See [Working with the Training Pipeline](#7.2)\n- **Custom dataset integration**: See [Data Preparation and Custom Datasets](#7.3)\n- **MLflow entry point reference**: See [MLproject API Reference](#8.1)\n- **Configuration options**: See [Configuration Reference](#8.2)"])</script><script>self.__next_f.push([1,"32:T3964,"])</script><script>self.__next_f.push([1,"# Local Development Setup\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/.env_sample](examplemodel/.env_sample)\n- [examplemodel/pyproject.toml](examplemodel/pyproject.toml)\n- [examplemodel/uv.lock](examplemodel/uv.lock)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page describes how to configure a local development environment for working with the Example Model System in the OpenGeoAIModelHub repository. It covers dependency installation, environment variable configuration, and connection setup to the infrastructure services.\n\nFor information about deploying the infrastructure stack itself, see [Infrastructure Deployment](#6.1). For guidance on modifying the training pipeline after setup is complete, see [Working with the Training Pipeline](#7.2).\n\n---\n\n## Overview\n\nThe local development setup involves three primary tasks:\n\n1. **Dependency installation** using the `uv` package manager\n2. **Environment configuration** for MLflow and MinIO connectivity\n3. **Verification** that the setup can communicate with infrastructure services\n\nThe following diagram shows how these components interact during development:\n\n```mermaid\ngraph TB\n    subgraph \"Local Development Machine\"\n        DEV[\"Developer Workspace\"]\n        UV[\"uv Package Manager\"]\n        PYPROJECT[\"pyproject.toml\"]\n        UVLOCK[\"uv.lock\"]\n        ENVFILE[\".env\"]\n        VENV[\"Python Virtual Environment\"]\n        TRAIN[\"src/train.py\"]\n        INFER[\"src/inference.py\"]\n    end\n    \n    subgraph \"Remote Infrastructure\"\n        MLFLOW[\"MLflow Tracking Server\"]\n        MINIO[\"MinIO Object Storage\"]\n        POSTGRES[\"PostgreSQL Backend\"]\n    end\n    \n    DEV --\u003e|\"uv sync\"| UV\n    UV --\u003e|\"reads\"| PYPROJECT\n    UV --\u003e|\"reads\"| UVLOCK\n    UV --\u003e|\"creates\"| VENV\n    VENV --\u003e|\"contains\"| TRAIN\n    VENV --\u003e|\"contains\"| INFER\n    \n    DEV --\u003e|\"configures\"| ENVFILE\n    ENVFILE --\u003e|\"provides credentials\"| TRAIN\n    ENVFILE --\u003e|\"provides credentials\"| INFER\n    \n    TRAIN --\u003e|\"mlflow.start_run()\"| MLFLOW\n    TRAIN --\u003e|\"mlflow.log_artifact()\"| MINIO\n    MLFLOW --\u003e|\"stores metadata\"| POSTGRES\n    MLFLOW --\u003e|\"artifact_location\"| MINIO\n```\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), Diagram 5 from high-level architecture\n\n---\n\n## Prerequisites\n\nThe following software must be installed on your development machine:\n\n| Requirement | Minimum Version | Purpose |\n|-------------|----------------|---------|\n| Python | 3.10 | Runtime environment |\n| uv | Latest | Package manager and virtual environment manager |\n| Git | Any recent | Repository cloning |\n| Internet connection | N/A | Dependency download and infrastructure access |\n\n**Operating System Support:**\n- Linux (recommended for production-like development)\n- macOS (fully supported)\n- Windows (supported, but shell scripts require WSL or Git Bash)\n\n**Sources:** [examplemodel/pyproject.toml:6]()\n\n---\n\n## Installing uv Package Manager\n\nThe `uv` package manager provides fast dependency resolution and virtual environment management. Install it using one of the following methods:\n\n**Linux/macOS:**\n```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n**Windows (PowerShell):**\n```powershell\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\n**Alternative (pip):**\n```bash\npip install uv\n```\n\nVerify the installation:\n```bash\nuv --version\n```\n\n**Sources:** Diagram 5 from high-level architecture\n\n---\n\n## Cloning the Repository\n\nClone the OpenGeoAIModelHub repository:\n\n```bash\ngit clone https://github.com/kshitijrajsharma/opengeoaimodelshub.git\ncd opengeoaimodelshub/examplemodel\n```\n\nThe `examplemodel/` directory contains the ML pipeline code and its dependencies.\n\n**Sources:** Diagram 5 from high-level architecture\n\n---\n\n## Installing Project Dependencies\n\nThe project uses `pyproject.toml` for dependency specification and `uv.lock` for reproducible builds.\n\n### Dependency Structure\n\nThe following diagram maps the dependency declaration in `pyproject.toml` to specific package groups:\n\n```mermaid\ngraph LR\n    subgraph \"pyproject.toml\"\n        DEPS[\"dependencies\"]\n        DEPGRP[\"dependency-groups\"]\n    end\n    \n    subgraph \"Core Dependencies\"\n        MLFLOW[\"mlflow\u003e=3.1.1\"]\n        TORCH[\"torch\u003e=2.7.1\"]\n        PL[\"pytorch-lightning\u003e=2.5.2\"]\n        GEOM[\"geomltoolkits\u003e=0.3.9\"]\n        ONNX[\"onnx\u003e=1.18.0\"]\n        STAC[\"stac-model\u003e=0.3.0\"]\n        BOTO[\"boto3\u003e=1.39.12\"]\n    end\n    \n    subgraph \"Validation Group\"\n        JSON[\"jsonschema\u003e=4.0.0\"]\n        REQ[\"requests\u003e=2.28.0\"]\n    end\n    \n    DEPS --\u003e MLFLOW\n    DEPS --\u003e TORCH\n    DEPS --\u003e PL\n    DEPS --\u003e GEOM\n    DEPS --\u003e ONNX\n    DEPS --\u003e STAC\n    DEPS --\u003e BOTO\n    \n    DEPGRP --\u003e JSON\n    DEPGRP --\u003e REQ\n```\n\n**Sources:** [examplemodel/pyproject.toml:7-24](), [examplemodel/pyproject.toml:26-30]()\n\n### Core Dependency Roles\n\n| Package | Version Constraint | Purpose |\n|---------|-------------------|---------|\n| `mlflow` | 3.1.1 | Experiment tracking and model registry |\n| `torch` | 2.7.1 | Deep learning framework |\n| `pytorch-lightning` | 2.5.2 | Training loop abstraction |\n| `geomltoolkits` | 0.3.9 | Geospatial data processing utilities |\n| `onnx` | 1.18.0 | Model export format |\n| `stac-model` | 0.3.0 | STAC-MLM metadata generation |\n| `boto3` | 1.39.12 | S3-compatible storage client |\n\n**Sources:** [examplemodel/pyproject.toml:7-24]()\n\n### Installing Dependencies\n\nFrom the `examplemodel/` directory, run:\n\n```bash\nuv sync\n```\n\nThis command:\n1. Reads [examplemodel/pyproject.toml:1-31]()\n2. Resolves dependencies using [examplemodel/uv.lock:1-577987]()\n3. Creates a virtual environment in `.venv/`\n4. Installs all packages with exact versions from the lock file\n\nThe installation process resolves approximately 200+ transitive dependencies, including platform-specific wheels for:\n- PyTorch CUDA/CPU variants\n- NumPy optimized builds\n- Rasterio geospatial libraries\n\n**Sources:** [examplemodel/pyproject.toml:1-31](), [examplemodel/uv.lock:1-10]()\n\n### Activating the Virtual Environment\n\nAfter installation, activate the virtual environment:\n\n**Linux/macOS:**\n```bash\nsource .venv/bin/activate\n```\n\n**Windows:**\n```bash\n.venv\\Scripts\\activate\n```\n\nVerify the installation:\n```bash\npython --version  # Should show Python 3.10 or higher\npip list | grep mlflow  # Should show mlflow 3.1.1 or higher\n```\n\n**Sources:** Diagram 5 from high-level architecture\n\n---\n\n## Environment Configuration\n\nThe local development environment requires configuration to connect to the infrastructure services (MLflow, MinIO, PostgreSQL).\n\n### Environment Variable Mapping\n\n```mermaid\ngraph TB\n    subgraph \"Environment Variables\"\n        ACCESS[\"AWS_ACCESS_KEY_ID\"]\n        SECRET[\"AWS_SECRET_ACCESS_KEY\"]\n        S3EP[\"MLFLOW_S3_ENDPOINT_URL\"]\n        TRACKING[\"MLFLOW_TRACKING_URI\"]\n    end\n    \n    subgraph \"Infrastructure Services\"\n        MINIO[\"MinIO Service\u003cbr/\u003eminio-api.domain.com\"]\n        MLFLOW_SVC[\"MLflow Service\u003cbr/\u003emlflow.domain.com\"]\n        PG[\"PostgreSQL Backend\"]\n    end\n    \n    subgraph \"Python Code\"\n        BOTO3[\"boto3 client\u003cbr/\u003ein src/train.py\"]\n        MLFLOW_PY[\"mlflow.start_run()\u003cbr/\u003ein src/train.py\"]\n    end\n    \n    ACCESS --\u003e|\"S3 authentication\"| BOTO3\n    SECRET --\u003e|\"S3 authentication\"| BOTO3\n    S3EP --\u003e|\"artifact endpoint\"| BOTO3\n    TRACKING --\u003e|\"tracking server URL\"| MLFLOW_PY\n    \n    BOTO3 --\u003e|\"stores artifacts\"| MINIO\n    MLFLOW_PY --\u003e|\"logs experiments\"| MLFLOW_SVC\n    MLFLOW_SVC --\u003e|\"stores metadata\"| PG\n    MLFLOW_SVC --\u003e|\"artifact_location\"| MINIO\n```\n\n**Sources:** [examplemodel/.env_sample:1-5](), Diagram 4 from high-level architecture\n\n### Creating the Environment File\n\n1. Copy the sample environment file:\n```bash\ncp .env_sample .env\n```\n\n2. Edit `.env` with your infrastructure credentials:\n```bash\nexport AWS_ACCESS_KEY_ID=your_minio_access_key\nexport AWS_SECRET_ACCESS_KEY=your_minio_secret_key\nexport MLFLOW_S3_ENDPOINT_URL=https://minio-api.yourdomain.com\nexport MLFLOW_TRACKING_URI=http://mlflow.yourdomain.com\n```\n\n3. Load the environment variables:\n```bash\nsource .env\n```\n\n**Sources:** [examplemodel/.env_sample:1-5]()\n\n### Environment Variable Reference\n\n| Variable | Purpose | Example Value |\n|----------|---------|---------------|\n| `AWS_ACCESS_KEY_ID` | MinIO access key for artifact storage | `minioadmin` |\n| `AWS_SECRET_ACCESS_KEY` | MinIO secret key | `minioadmin` |\n| `MLFLOW_S3_ENDPOINT_URL` | MinIO API endpoint URL | `https://minio-api.yourdomain.com` |\n| `MLFLOW_TRACKING_URI` | MLflow tracking server URL | `http://mlflow.yourdomain.com` |\n\n**Note:** Despite the `AWS_` prefix, these variables are used to connect to MinIO (an S3-compatible service), not AWS itself.\n\n**Sources:** [examplemodel/.env_sample:1-5]()\n\n---\n\n## Dependency Lock File Details\n\nThe `uv.lock` file ensures reproducible builds across different machines and platforms.\n\n### Lock File Structure\n\n```mermaid\ngraph TB\n    LOCK[\"uv.lock\"]\n    \n    subgraph \"Lock File Metadata\"\n        VERSION[\"version = 1\"]\n        REVISION[\"revision = 2\"]\n        PYTHON[\"requires-python = '\u003e=3.10'\"]\n        MARKERS[\"resolution-markers\u003cbr/\u003efor Python 3.10-3.13\"]\n    end\n    \n    subgraph \"Package Entries (200+ packages)\"\n        PKG1[\"[[package]]\u003cbr/\u003ename = 'mlflow'\u003cbr/\u003eversion = '3.1.1'\"]\n        PKG2[\"[[package]]\u003cbr/\u003ename = 'torch'\u003cbr/\u003eversion = '2.7.1'\"]\n        PKG3[\"[[package]]\u003cbr/\u003ename = 'boto3'\u003cbr/\u003eversion = '1.39.12'\"]\n        PKGN[\"... 200+ more packages\"]\n    end\n    \n    subgraph \"Platform-Specific Wheels\"\n        WHEEL1[\"wheels = [\u003cbr/\u003e  manylinux2014_x86_64\u003cbr/\u003e  macosx_arm64\u003cbr/\u003e  win_amd64\u003cbr/\u003e]\"]\n    end\n    \n    LOCK --\u003e VERSION\n    LOCK --\u003e REVISION\n    LOCK --\u003e PYTHON\n    LOCK --\u003e MARKERS\n    \n    LOCK --\u003e PKG1\n    LOCK --\u003e PKG2\n    LOCK --\u003e PKG3\n    LOCK --\u003e PKGN\n    \n    PKG1 --\u003e WHEEL1\n    PKG2 --\u003e WHEEL1\n```\n\n**Sources:** [examplemodel/uv.lock:1-10]()\n\n### Example Dependency Entry\n\nThe lock file contains detailed information for each package. For example, `boto3`:\n\n- **Package name:** `boto3`\n- **Version:** `1.39.12`\n- **Dependencies:** `botocore`, `jmespath`, `s3transfer`\n- **Source:** PyPI registry\n- **Wheel URL:** Pre-built binary with cryptographic hash\n- **Installation size:** ~140 KB\n\n**Sources:** [examplemodel/uv.lock:195-206]()\n\n### Key Lock File Attributes\n\n| Attribute | Description | Example |\n|-----------|-------------|---------|\n| `version` | Lock file format version | `1` |\n| `revision` | Lock file schema revision | `2` |\n| `requires-python` | Python version constraint | `\"\u003e=3.10\"` |\n| `resolution-markers` | Platform-specific resolution markers | `\"python_full_version \u003e= '3.13'\"` |\n| `[[package]]` | Individual package specification | See `boto3` at lines 195-206 |\n| `sdist` | Source distribution with hash | For building from source |\n| `wheels` | Pre-built binaries with hashes | For quick installation |\n\n**Sources:** [examplemodel/uv.lock:1-10]()\n\n---\n\n## Verifying the Setup\n\nAfter completing the setup, verify that your environment can communicate with the infrastructure services.\n\n### Verification Checklist\n\n```mermaid\ngraph TB\n    START[\"Start Verification\"]\n    \n    CHECK1[\"Check Python version\u003cbr/\u003epython --version\"]\n    CHECK2[\"Check MLflow installation\u003cbr/\u003emlflow --version\"]\n    CHECK3[\"Check environment variables\u003cbr/\u003eecho $MLFLOW_TRACKING_URI\"]\n    CHECK4[\"Test MLflow connectivity\u003cbr/\u003emlflow experiments list\"]\n    CHECK5[\"Test MinIO connectivity\u003cbr/\u003eaws s3 ls --endpoint-url\"]\n    \n    SUCCESS[\"Setup Complete\"]\n    FAIL[\"Troubleshoot\"]\n    \n    START --\u003e CHECK1\n    CHECK1 --\u003e|\"\u003e=3.10\"| CHECK2\n    CHECK1 --\u003e|\"\u003c3.10\"| FAIL\n    CHECK2 --\u003e|\"\u003e=3.1.1\"| CHECK3\n    CHECK2 --\u003e|\"not found\"| FAIL\n    CHECK3 --\u003e|\"set\"| CHECK4\n    CHECK3 --\u003e|\"unset\"| FAIL\n    CHECK4 --\u003e|\"success\"| CHECK5\n    CHECK4 --\u003e|\"connection error\"| FAIL\n    CHECK5 --\u003e|\"success\"| SUCCESS\n    CHECK5 --\u003e|\"auth error\"| FAIL\n```\n\n### Manual Verification Commands\n\n1. **Check Python version:**\n```bash\npython --version\n# Expected: Python 3.10.x or higher\n```\n\n2. **Check MLflow installation:**\n```bash\nmlflow --version\n# Expected: mlflow, version 3.1.1 or higher\n```\n\n3. **Verify environment variables:**\n```bash\necho $MLFLOW_TRACKING_URI\necho $MLFLOW_S3_ENDPOINT_URL\necho $AWS_ACCESS_KEY_ID\n# All should return non-empty values\n```\n\n4. **Test MLflow connectivity:**\n```bash\nmlflow experiments list\n# Should return list of experiments or empty list (not connection error)\n```\n\n5. **Test MinIO connectivity (optional):**\n```bash\npip install awscli\naws s3 ls --endpoint-url $MLFLOW_S3_ENDPOINT_URL\n# Should list buckets or return access denied (not connection error)\n```\n\n**Sources:** Diagram 4 from high-level architecture\n\n---\n\n## Common Setup Issues\n\n### Issue: `uv: command not found`\n\n**Cause:** `uv` not installed or not in PATH\n\n**Solution:**\n```bash\n# Reinstall uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n# Restart terminal or reload PATH\nsource ~/.bashrc  # or ~/.zshrc\n```\n\n### Issue: Connection Refused to MLflow\n\n**Cause:** Infrastructure not running or incorrect `MLFLOW_TRACKING_URI`\n\n**Solution:**\n1. Verify infrastructure is running (see [Infrastructure Deployment](#6.1))\n2. Check `MLFLOW_TRACKING_URI` matches your deployment domain\n3. Test with `curl $MLFLOW_TRACKING_URI/api/2.0/mlflow/experiments/list`\n\n**Sources:** [examplemodel/.env_sample:4]()\n\n### Issue: S3 Authentication Failure\n\n**Cause:** Incorrect MinIO credentials\n\n**Solution:**\n1. Verify `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` match MinIO deployment\n2. Check that `MLFLOW_S3_ENDPOINT_URL` uses correct protocol (http/https)\n3. Ensure MinIO service is accessible from your network\n\n**Sources:** [examplemodel/.env_sample:1-3]()\n\n### Issue: Missing Platform-Specific Wheels\n\n**Cause:** Unsupported platform or architecture\n\n**Solution:**\n```bash\n# Force source builds (slower but compatible)\nuv sync --no-binary :all:\n```\n\n**Sources:** [examplemodel/uv.lock:14-18]()\n\n---\n\n## Next Steps\n\nWith the local development environment configured, you can:\n\n1. **Run the training pipeline** - See [Working with the Training Pipeline](#7.2)\n2. **Prepare custom datasets** - See [Data Preparation and Custom Datasets](#7.3)\n3. **Execute inference** - See [Inference System](#3.3)\n4. **Modify model architecture** - See [Model Overview and Architecture](#3.1)\n\nTo run a quick test of the setup, execute the preprocess entry point:\n\n```bash\nmlflow run . -e preprocess\n```\n\nThis will verify that:\n- MLflow can execute entry points from [MLproject]()\n- Environment variables are correctly configured\n- Dependencies are properly installed\n- Data can be downloaded from external sources\n\n**Sources:** Diagram 4 from high-level architecture, [examplemodel/pyproject.toml:1-31]()"])</script><script>self.__next_f.push([1,"33:T554b,"])</script><script>self.__next_f.push([1,"# Working with the Training Pipeline\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/MLproject](examplemodel/MLproject)\n- [examplemodel/playground.ipynb](examplemodel/playground.ipynb)\n- [examplemodel/src/model.py](examplemodel/src/model.py)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis guide covers modifying and extending the training pipeline for custom experimentation and development. It explains the architecture of the training system, how to adjust training parameters, add custom metrics, modify the model architecture, and work with MLflow experiment tracking. This guide is intended for developers who want to customize the example model for their own use cases.\n\nFor information about setting up your local environment, see [Local Development Setup](#7.1). For preparing custom datasets, see [Data Preparation and Custom Datasets](#7.3). For the complete training pipeline architecture from a system perspective, see [Training Pipeline](#3.2).\n\n---\n\n## Training Pipeline Architecture\n\nThe training pipeline is implemented in [examplemodel/src/train.py]() and orchestrated through the MLflow project entry point defined in [examplemodel/MLproject:19-32](). The pipeline follows a standard PyTorch Lightning workflow with MLflow integration for experiment tracking.\n\n### High-Level Training Flow\n\n```mermaid\nflowchart TD\n    START[\"train_model() entry point\"]\n    MLFLOW[\"mlflow.start_run()\"]\n    SYSINFO[\"get_system_info()\"]\n    DATAMOD[\"CampDataModule instantiation\"]\n    STATS[\"calculate_dataset_statistics()\"]\n    CHECKPOINT[\"ModelCheckpoint callback\"]\n    TRAINER[\"pl.Trainer instantiation\"]\n    MODEL[\"LitRefugeeCamp instantiation\"]\n    FIT[\"trainer.fit()\"]\n    LOAD[\"Load best checkpoint\"]\n    EXPORT[\"Export artifacts\u003cbr/\u003e(pth, pt, onnx)\"]\n    DLPK[\"create_dlpk()\"]\n    STAC[\"create_stac_mlm_item()\"]\n    LOG[\"Log all artifacts to MLflow\"]\n    INFER[\"log_inference_example()\"]\n    CONFMAT[\"log_confusion_matrix()\"]\n    \n    START --\u003e MLFLOW\n    MLFLOW --\u003e SYSINFO\n    SYSINFO --\u003e DATAMOD\n    DATAMOD --\u003e STATS\n    STATS --\u003e CHECKPOINT\n    CHECKPOINT --\u003e TRAINER\n    TRAINER --\u003e MODEL\n    MODEL --\u003e FIT\n    FIT --\u003e LOAD\n    LOAD --\u003e EXPORT\n    EXPORT --\u003e DLPK\n    DLPK --\u003e STAC\n    STAC --\u003e LOG\n    LOG --\u003e INFER\n    INFER --\u003e CONFMAT\n```\n\n**Sources:** [examplemodel/src/train.py:370-518]()\n\n### Key Components and Their Responsibilities\n\n| Component | Class/Function | Location | Responsibility |\n|-----------|---------------|----------|----------------|\n| **Training Orchestration** | `train_model()` | [examplemodel/src/train.py:370-518]() | Main entry point, coordinates entire pipeline |\n| **Model Architecture** | `LitRefugeeCamp` | [examplemodel/src/model.py:145-174]() | PyTorch Lightning module wrapper |\n| **Network** | `RefugeeCampDetector` | [examplemodel/src/model.py:103-138]() | U-Net semantic segmentation model |\n| **Data Management** | `CampDataModule` | [examplemodel/src/model.py:33-100]() | PyTorch Lightning data module |\n| **System Profiling** | `get_system_info()` | [examplemodel/src/train.py:25-60]() | Hardware/software environment capture |\n| **Dataset Statistics** | `calculate_dataset_statistics()` | [examplemodel/src/train.py:63-98]() | Mean, std, split sizes |\n| **Model Checkpointing** | `ModelCheckpoint` | [examplemodel/src/train.py:405-412]() | Save best models during training |\n| **STAC Metadata** | `create_stac_mlm_item()` | [examplemodel/src/train.py:101-367]() | Generate standardized model metadata |\n| **MLflow Integration** | `mlflow.*` calls | Throughout [train.py]() | Experiment tracking and artifact logging |\n\n**Sources:** [examplemodel/src/train.py](), [examplemodel/src/model.py]()\n\n---\n\n## Core Training Components\n\n### LitRefugeeCamp: PyTorch Lightning Module\n\nThe `LitRefugeeCamp` class wraps the neural network architecture with PyTorch Lightning's training abstractions. It defines the training logic, validation logic, and optimizer configuration.\n\n```mermaid\nclassDiagram\n    class LitRefugeeCamp {\n        +RefugeeCampDetector model\n        +BCELoss criterion\n        +float lr\n        +forward(x) Tensor\n        +_shared_step(batch, stage) Dict\n        +training_step(batch, batch_idx) Dict\n        +validation_step(batch, batch_idx) Dict\n        +test_step(batch, batch_idx) Dict\n        +configure_optimizers() Optimizer\n    }\n    \n    class RefugeeCampDetector {\n        +Conv2d conv1, conv2, conv3, conv4\n        +MaxPool2d pool1, pool2, pool3\n        +ConvTranspose2d up1, up2, up3\n        +Conv2d outc\n        +forward(x) Tensor\n    }\n    \n    class CampDataModule {\n        +str image_dir\n        +str label_dir\n        +int batch_size\n        +float val_ratio, test_ratio\n        +setup(stage) void\n        +train_dataloader() DataLoader\n        +val_dataloader() DataLoader\n        +test_dataloader() DataLoader\n    }\n    \n    LitRefugeeCamp *-- RefugeeCampDetector\n    LitRefugeeCamp ..\u003e CampDataModule : \"uses\"\n```\n\n**Key methods to override for customization:**\n\n- **`__init__()`** [examplemodel/src/model.py:146-150](): Change loss function, learning rate, or add model parameters\n- **`_shared_step()`** [examplemodel/src/model.py:155-162](): Modify loss computation and metric calculation\n- **`configure_optimizers()`** [examplemodel/src/model.py:173-174](): Change optimizer or add learning rate schedulers\n\n**Sources:** [examplemodel/src/model.py:145-174](), [examplemodel/src/model.py:103-138](), [examplemodel/src/model.py:33-100]()\n\n### CampDataModule: Data Pipeline\n\nThe `CampDataModule` handles all data loading, transformation, and splitting. It uses standard ImageNet normalization for transfer learning compatibility.\n\n**Transformation pipeline** [examplemodel/src/model.py:53-65]():\n- **Images**: Resize to 256256  ToTensor  Normalize with ImageNet stats\n- **Labels**: Resize to 256256 (NEAREST)  ToTensor\n\n**Split ratios** [examplemodel/src/model.py:39-41]():\n- Validation: 15% (default)\n- Test: 15% (default)\n- Training: 70% (remaining)\n\n**Sources:** [examplemodel/src/model.py:33-100]()\n\n---\n\n## Customizing Training Parameters\n\nTraining parameters can be modified at three levels: command-line arguments, MLproject defaults, or directly in code.\n\n### MLproject Entry Point Configuration\n\nThe `train` entry point in [examplemodel/MLproject:19-32]() defines default parameters:\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `epochs` | int | 1 | Number of training epochs |\n| `batch_size` | int | 32 | Batch size for training |\n| `chips_dir` | str | `data/train/sample/chips` | Image directory |\n| `labels_dir` | str | `data/train/sample/labels` | Label directory |\n| `lr` | float | 1e-3 | Learning rate |\n\n### Running with Custom Parameters\n\n```bash\n# Via MLflow\nmlflow run . -e train -P epochs=50 -P batch_size=16 -P lr=0.0001\n\n# Direct Python execution\nuv run python src/train.py \\\n  --epochs 50 \\\n  --batch_size 16 \\\n  --lr 0.0001 \\\n  --chips_dir path/to/chips \\\n  --labels_dir path/to/labels\n```\n\n### Argument Parser Configuration\n\nThe argument parser is defined in [examplemodel/src/train.py:510-516]():\n\n```python\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--epochs\", type=int, default=1)\nparser.add_argument(\"--batch_size\", type=int, default=32)\nparser.add_argument(\"--chips_dir\", type=str, default=\"data/train/sample/chips\")\nparser.add_argument(\"--labels_dir\", type=str, default=\"data/train/sample/labels\")\nparser.add_argument(\"--lr\", type=float, default=1e-3)\n```\n\n**To add new parameters:**\n1. Add argument to parser in [train.py:510-516]()\n2. Add parameter to MLproject entry point [MLproject:19-32]()\n3. Use the parameter in model/data module initialization\n\n**Sources:** [examplemodel/MLproject:19-32](), [examplemodel/src/train.py:510-516]()\n\n---\n\n## Adding Custom Metrics\n\nMetrics are logged through PyTorch Lightning's `self.log()` method and captured by MLflow automatically.\n\n### Current Metrics Implementation\n\nThe `_shared_step()` method in [examplemodel/src/model.py:155-162]() logs two metrics:\n\n```python\ndef _shared_step(self, batch, stage):\n    x, y = batch\n    y_hat = self(x)\n    loss = self.criterion(y_hat, y)\n    acc = pixel_acc(y_hat, y)  # Helper function at model.py:140-142\n    self.log(f\"{stage}_loss\", loss, prog_bar=True)\n    self.log(f\"{stage}_acc\", acc, prog_bar=True)\n    return {\"loss\": loss, \"acc\": acc}\n```\n\n### Adding Custom Metrics\n\n**Step 1:** Define metric calculation function in [model.py]():\n\n```python\ndef intersection_over_union(pred, target, threshold=0.5):\n    \"\"\"Calculate IoU for binary segmentation.\"\"\"\n    pred_binary = (pred \u003e threshold).float()\n    intersection = (pred_binary * target).sum()\n    union = pred_binary.sum() + target.sum() - intersection\n    return intersection / (union + 1e-7)  # Add epsilon to avoid division by zero\n```\n\n**Step 2:** Modify `_shared_step()` to compute and log the metric:\n\n```python\ndef _shared_step(self, batch, stage):\n    x, y = batch\n    y_hat = self(x)\n    loss = self.criterion(y_hat, y)\n    acc = pixel_acc(y_hat, y)\n    iou = intersection_over_union(y_hat, y)  # New metric\n    \n    self.log(f\"{stage}_loss\", loss, prog_bar=True)\n    self.log(f\"{stage}_acc\", acc, prog_bar=True)\n    self.log(f\"{stage}_iou\", iou, prog_bar=True)  # Log new metric\n    \n    return {\"loss\": loss, \"acc\": acc, \"iou\": iou}\n```\n\n### Post-Training Metrics\n\nAdditional metrics are logged after training completes:\n\n- **Confusion Matrix**: [examplemodel/src/train.py:491]() via `log_confusion_matrix(model, data_module, run)`\n- **Inference Example**: [examplemodel/src/train.py:490]() via `log_inference_example(model, data_module)`\n\nThese functions are defined in [utils.py]() and [inference.py]() respectively.\n\n**Sources:** [examplemodel/src/model.py:155-162](), [examplemodel/src/model.py:140-142](), [examplemodel/src/train.py:490-491]()\n\n---\n\n## Modifying Model Architecture\n\nThe model architecture consists of two layers: the neural network (`RefugeeCampDetector`) and the Lightning module wrapper (`LitRefugeeCamp`).\n\n### Current Architecture: U-Net\n\n```mermaid\ngraph LR\n    subgraph \"Encoder\"\n        INPUT[\"Input\u003cbr/\u003e3256256\"]\n        CONV1[\"conv1\u003cbr/\u003e64 channels\"]\n        POOL1[\"pool1\u003cbr/\u003eMaxPool 22\"]\n        CONV2[\"conv2\u003cbr/\u003e128 channels\"]\n        POOL2[\"pool2\u003cbr/\u003eMaxPool 22\"]\n        CONV3[\"conv3\u003cbr/\u003e256 channels\"]\n        POOL3[\"pool3\u003cbr/\u003eMaxPool 22\"]\n    end\n    \n    subgraph \"Bottleneck\"\n        CONV4[\"conv4\u003cbr/\u003e512 channels\"]\n    end\n    \n    subgraph \"Decoder\"\n        UP1[\"up1\u003cbr/\u003eConvTranspose2d\"]\n        UP2[\"up2\u003cbr/\u003eConvTranspose2d\"]\n        UP3[\"up3\u003cbr/\u003eConvTranspose2d\"]\n        OUTC[\"outc\u003cbr/\u003e1 channel\"]\n        OUTPUT[\"Output\u003cbr/\u003e1256256\"]\n    end\n    \n    INPUT --\u003e CONV1 --\u003e POOL1 --\u003e CONV2 --\u003e POOL2 --\u003e CONV3 --\u003e POOL3\n    POOL3 --\u003e CONV4 --\u003e UP1 --\u003e UP2 --\u003e UP3 --\u003e OUTC --\u003e OUTPUT\n```\n\n**Sources:** [examplemodel/src/model.py:103-138]()\n\n### Replacing the Architecture\n\n**Option 1: Modify RefugeeCampDetector directly** [examplemodel/src/model.py:103-138]()\n\nEdit the `RefugeeCampDetector` class to add skip connections, more layers, or different activation functions.\n\n**Option 2: Use a pretrained backbone**\n\nReplace the entire network with a pretrained model:\n\n```python\nimport segmentation_models_pytorch as smp\n\nclass LitRefugeeCamp(pl.LightningModule):\n    def __init__(self, lr=1e-3):\n        super().__init__()\n        self.model = smp.Unet(\n            encoder_name=\"resnet34\",        # Use ResNet34 backbone\n            encoder_weights=\"imagenet\",     # Pretrained on ImageNet\n            in_channels=3,\n            classes=1,                      # Binary segmentation\n        )\n        self.criterion = nn.BCEWithLogitsLoss()  # Note: no sigmoid in model\n        self.lr = lr\n    \n    def forward(self, x):\n        return torch.sigmoid(self.model(x))  # Apply sigmoid for inference\n```\n\n**Important:** When changing architectures:\n1. Update input/output shapes in STAC metadata [train.py:190-230]()\n2. Update ONNX export dimensions [train.py:450-460]()\n3. Update model signature for MLflow [train.py:498-503]()\n\n**Sources:** [examplemodel/src/model.py:103-138](), [examplemodel/src/model.py:145-174]()\n\n---\n\n## Working with Callbacks\n\nPyTorch Lightning callbacks provide hooks into the training lifecycle. The training pipeline currently uses `ModelCheckpoint`.\n\n### Current Callback: ModelCheckpoint\n\nConfiguration in [examplemodel/src/train.py:405-412]():\n\n```python\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"checkpoints\",\n    filename=\"epoch={epoch}-step={step}\",\n    save_top_k=1,           # Keep only best model\n    verbose=True,\n    monitor=\"val_loss\",     # Metric to monitor\n    mode=\"min\",             # Minimize validation loss\n)\n```\n\nThe best checkpoint path is accessed via [train.py:424]():\n```python\nbest_checkpoint = checkpoint_callback.best_model_path\n```\n\n### Adding Custom Callbacks\n\n**Example: Early Stopping**\n\n```python\nfrom pytorch_lightning.callbacks import EarlyStopping\n\nearly_stop_callback = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=10,\n    mode=\"min\",\n    verbose=True\n)\n\ntrainer = pl.Trainer(\n    max_epochs=args.epochs,\n    accelerator=\"auto\",\n    callbacks=[checkpoint_callback, early_stop_callback],  # Add to list\n    logger=False,\n)\n```\n\n**Example: Learning Rate Monitoring**\n\n```python\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nlr_monitor = LearningRateMonitor(logging_interval='epoch')\n\ntrainer = pl.Trainer(\n    callbacks=[checkpoint_callback, lr_monitor],\n)\n```\n\n**Example: Custom Callback for Gradient Logging**\n\n```python\nclass GradientLoggingCallback(pl.Callback):\n    def on_after_backward(self, trainer, pl_module):\n        if trainer.global_step % 100 == 0:  # Log every 100 steps\n            for name, param in pl_module.named_parameters():\n                if param.grad is not None:\n                    mlflow.log_metric(\n                        f\"grad_{name}_mean\", \n                        param.grad.mean().item(),\n                        step=trainer.global_step\n                    )\n```\n\n**Sources:** [examplemodel/src/train.py:405-412](), [examplemodel/src/train.py:414-419]()\n\n---\n\n## MLflow Integration and Experiment Tracking\n\nThe training pipeline is deeply integrated with MLflow for reproducibility and artifact management.\n\n### MLflow Logging Flow\n\n```mermaid\nsequenceDiagram\n    participant Train as \"train_model()\"\n    participant MLflow as \"mlflow\"\n    participant Postgres as \"PostgreSQL\"\n    participant MinIO as \"MinIO\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.start_run()\"\n    MLflow-\u003e\u003ePostgres: \"Create run entry\"\n    \n    Train-\u003e\u003eTrain: \"get_system_info()\"\n    Train-\u003e\u003eTrain: \"calculate_dataset_statistics()\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.log_params(system_info)\"\n    MLflow-\u003e\u003ePostgres: \"Store parameters\"\n    \n    Train-\u003e\u003eTrain: \"trainer.fit()\"\n    Note over Train: \"During each epoch\"\n    Train-\u003e\u003eMLflow: \"self.log('train_loss', ...)\"\n    Train-\u003e\u003eMLflow: \"self.log('val_loss', ...)\"\n    MLflow-\u003e\u003ePostgres: \"Store metrics\"\n    \n    Train-\u003e\u003eTrain: \"Generate artifacts\u003cbr/\u003e(pth, onnx, dlpk)\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.log_artifact('best_model.pth')\"\n    Train-\u003e\u003eMLflow: \"mlflow.log_artifact('best_model.onnx')\"\n    Train-\u003e\u003eMLflow: \"mlflow.log_artifact('best_model.dlpk')\"\n    MLflow-\u003e\u003eMinIO: \"Upload artifacts\"\n    \n    Train-\u003e\u003eMLflow: \"mlflow.pytorch.log_model()\"\n    MLflow-\u003e\u003eMinIO: \"Upload model with signature\"\n    MLflow-\u003e\u003ePostgres: \"Register model version\"\n```\n\n**Sources:** [examplemodel/src/train.py:370-507]()\n\n### Logged Parameters\n\nSystem information [train.py:374-387]() and dataset statistics [train.py:396-403]() are logged as parameters:\n\n```python\nmlflow.log_params({\n    \"train_samples\": dataset_stats[\"train_samples\"],\n    \"val_samples\": dataset_stats[\"val_samples\"],\n    \"test_samples\": dataset_stats[\"test_samples\"],\n    \"num_classes\": dataset_stats[\"num_classes\"],\n})\n```\n\n### Logged Metrics\n\nMetrics are automatically captured from PyTorch Lightning's `self.log()` calls during training:\n- `train_loss`, `val_loss` per epoch\n- `train_acc`, `val_acc` per epoch\n- Post-training: `best_val_loss`, `epochs_trained` [train.py:426-431]()\n\n### Logged Artifacts\n\nArtifacts are organized in a structured folder hierarchy [train.py:478-496]():\n\n| Artifact Path | Description | Line Reference |\n|---------------|-------------|----------------|\n| `metadata/stac_item.json` | STAC-MLM metadata | [479]() |\n| `models/best_model.pth` | PyTorch state dict | [480]() |\n| `models/best_model.pt` | TorchScript traced model | [481]() |\n| `models/best_model.onnx` | ONNX format | [482]() |\n| `models/best_model.dlpk` | ESRI Deep Learning Package | [483]() |\n| `checkpoints/epoch=*.ckpt` | PyTorch Lightning checkpoint | [484]() |\n| `datasets/train/chips/*` | Training images | [487]() |\n| `datasets/train/labels/*` | Training labels | [488]() |\n| `esri/*` | ESRI integration files | [493-496]() |\n\n### MLflow Model Registry\n\nThe final model is logged with signature and metadata [train.py:498-503]():\n\n```python\nsignature = infer_signature(\n    dummy_input.numpy(), \n    clean_model(dummy_input).detach().numpy()\n)\nmlflow.pytorch.log_model(\n    clean_model, \n    \"model\", \n    signature=signature, \n    extra_files=[stac_output_path]\n)\n```\n\nThis creates a model entry in the MLflow Model Registry that can be:\n- Deployed to various targets\n- Version controlled\n- Tagged with stages (staging, production)\n\n**Sources:** [examplemodel/src/train.py:373-503]()\n\n---\n\n## Advanced Customizations\n\n### Customizing Loss Functions\n\nThe loss function is defined in `LitRefugeeCamp.__init__()` [examplemodel/src/model.py:149]():\n\n```python\nself.criterion = nn.BCELoss()\n```\n\n**Common alternatives for segmentation:**\n\n```python\n# Focal Loss for handling class imbalance\nfrom segmentation_models_pytorch.losses import FocalLoss\nself.criterion = FocalLoss(mode='binary')\n\n# Dice Loss for better overlap optimization\nfrom segmentation_models_pytorch.losses import DiceLoss\nself.criterion = DiceLoss(mode='binary')\n\n# Combined loss\nclass CombinedLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bce = nn.BCELoss()\n        self.dice = DiceLoss(mode='binary')\n    \n    def forward(self, pred, target):\n        return self.bce(pred, target) + self.dice(pred, target)\n\nself.criterion = CombinedLoss()\n```\n\n### Customizing Optimizers and Schedulers\n\nThe optimizer is configured in `configure_optimizers()` [examplemodel/src/model.py:173-174]():\n\n```python\ndef configure_optimizers(self):\n    return torch.optim.Adam(self.parameters(), lr=self.lr)\n```\n\n**Adding learning rate schedulers:**\n\n```python\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n    scheduler = {\n        'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, \n            mode='min',\n            factor=0.5,\n            patience=5,\n            verbose=True\n        ),\n        'monitor': 'val_loss',\n    }\n    return {'optimizer': optimizer, 'lr_scheduler': scheduler}\n```\n\n**Alternative optimizers:**\n\n```python\n# AdamW with weight decay\noptimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=1e-4)\n\n# SGD with momentum\noptimizer = torch.optim.SGD(self.parameters(), lr=self.lr, momentum=0.9)\n```\n\n### Customizing STAC Metadata\n\nThe STAC-MLM metadata is generated in `create_stac_mlm_item()` [examplemodel/src/train.py:101-367](). Key sections to modify:\n\n**Model description** [train.py:124-141]():\n```python\n\"properties\": {\n    \"title\": \"Your Custom Model Title\",\n    \"description\": \"Your custom description\",\n    \"keywords\": [\"your\", \"keywords\"],\n    \"license\": \"MIT\",\n    ...\n}\n```\n\n**Input/output specifications** [train.py:190-230]():\nUpdate these when changing input dimensions, normalization, or output classes.\n\n**Asset links** [train.py:246-364]():\nAdd or modify asset entries for new artifact types.\n\n### Customizing Data Augmentation\n\nData augmentation can be added to the `CampDataModule.setup()` method [examplemodel/src/model.py:52-65]():\n\n```python\nfrom torchvision import transforms as T\n\ndef setup(self, stage=None):\n    img_tf = T.Compose([\n        T.Resize((256, 256)),\n        T.RandomHorizontalFlip(p=0.5),      # Add augmentation\n        T.RandomVerticalFlip(p=0.5),        # Add augmentation\n        T.RandomRotation(degrees=15),        # Add augmentation\n        T.ColorJitter(brightness=0.2, contrast=0.2),  # Add augmentation\n        T.ToTensor(),\n        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    # ... rest of setup\n```\n\n**Note:** For segmentation tasks, ensure augmentations are applied consistently to both images and labels using `torchvision.transforms.functional` or specialized libraries like `albumentations`.\n\n**Sources:** [examplemodel/src/model.py:145-174](), [examplemodel/src/train.py:101-367](), [examplemodel/src/model.py:52-65]()\n\n---\n\n## Summary\n\nThe training pipeline provides multiple extension points for customization:\n\n1. **Parameters**: Modify via command-line, MLproject, or argument parser\n2. **Metrics**: Add custom metrics in `_shared_step()` method\n3. **Architecture**: Replace `RefugeeCampDetector` or entire `LitRefugeeCamp` class\n4. **Callbacks**: Add PyTorch Lightning callbacks for monitoring and control\n5. **MLflow**: All metrics and artifacts automatically tracked\n6. **Loss/Optimizer**: Customize in `LitRefugeeCamp` methods\n7. **Data Augmentation**: Extend transforms in `CampDataModule.setup()`\n\nFor extending the dataset handling beyond the current implementation, see [Data Preparation and Custom Datasets](#7.3). For deploying trained models, see [Model Deployment Options](#6.2).\n\n**Sources:** [examplemodel/src/train.py](), [examplemodel/src/model.py](), [examplemodel/MLproject]()"])</script><script>self.__next_f.push([1,"34:T408c,"])</script><script>self.__next_f.push([1,"# Data Preparation and Custom Datasets\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/playground.ipynb](examplemodel/playground.ipynb)\n- [examplemodel/src/model.py](examplemodel/src/model.py)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page explains the data preparation pipeline for the refugee camp detection model, including the expected dataset structure, data loading mechanisms, preprocessing transformations, and how to adapt the system for custom datasets. For information about the training process that uses these datasets, see [Working with the Training Pipeline](#7.2). For model architecture details, see [Model Overview and Architecture](#3.1).\n\n---\n\n## Dataset Structure and Requirements\n\nThe training pipeline expects datasets organized in a paired directory structure where images and their corresponding labels are stored separately but share matching filenames.\n\n### Expected Directory Layout\n\n```\ndata/\n train/\n    sample/\n       chips/          # Satellite imagery\n          image_001.png\n          image_002.png\n          ...\n       labels/         # Segmentation masks\n           image_001.png\n           image_002.png\n           ...\n```\n\n**Requirements:**\n- Images and labels must have **identical filenames**\n- Images: RGB format (3 channels), typically PNG or JPEG\n- Labels: Grayscale format (1 channel), binary masks (0 for background, 1 for refugee camps)\n- Default input size: 256256 pixels (resized automatically if different)\n\nThe default paths are specified as command-line arguments to the training script:\n- `--chips_dir`: Path to image directory (default: `data/train/sample/chips`)\n- `--labels_dir`: Path to label directory (default: `data/train/sample/labels`)\n\n**Sources:** [examplemodel/src/train.py:513-514](), [examplemodel/src/model.py:12-30]()\n\n---\n\n## CampDataset: Core Data Loading Class\n\n### Class Architecture\n\n```mermaid\nclassDiagram\n    class CampDataset {\n        +image_dir: str\n        +label_dir: str\n        +images: List[str]\n        +transform: Callable\n        +target_transform: Callable\n        +__init__(image_dir, label_dir, transform, target_transform)\n        +__len__() int\n        +__getitem__(idx) Tuple\n    }\n    \n    class Dataset {\n        \u003c\u003cPyTorch\u003e\u003e\n    }\n    \n    Dataset \u003c|-- CampDataset\n    \n    note for CampDataset \"Loads paired images and labels\\nfrom separate directories\\nApplies transformations\"\n```\n\n### Implementation Details\n\nThe `CampDataset` class [examplemodel/src/model.py:12-30]() implements the PyTorch `Dataset` interface:\n\n| Method | Purpose | Returns |\n|--------|---------|---------|\n| `__init__()` | Initialize with directory paths and transforms | None |\n| `__len__()` | Return total number of samples | int |\n| `__getitem__(idx)` | Load and transform image/label pair at index | Tuple[Tensor, Tensor] |\n\n**Key behaviors:**\n- Automatically sorts files alphabetically to ensure consistent pairing\n- Converts images to RGB (3 channels) using PIL\n- Converts labels to grayscale (1 channel)\n- Applies optional transforms to both images and labels independently\n\n**Sources:** [examplemodel/src/model.py:12-30]()\n\n---\n\n## CampDataModule: PyTorch Lightning Data Management\n\n### Architecture Diagram\n\n```mermaid\ngraph TB\n    subgraph \"CampDataModule\"\n        INIT[\"__init__()\u003cbr/\u003eStore paths \u0026 config\"]\n        SETUP[\"setup()\u003cbr/\u003eCreate datasets \u0026 splits\"]\n        TRAIN_LOADER[\"train_dataloader()\"]\n        VAL_LOADER[\"val_dataloader()\"]\n        TEST_LOADER[\"test_dataloader()\"]\n    end\n    \n    subgraph \"Data Transformations\"\n        IMG_TF[\"Image Transform\u003cbr/\u003eResize  Tensor  Normalize\"]\n        LBL_TF[\"Label Transform\u003cbr/\u003eResize  Tensor\"]\n    end\n    \n    subgraph \"Dataset Instances\"\n        FULL_DS[\"CampDataset\u003cbr/\u003e(Full Dataset)\"]\n        TRAIN_DS[\"Train Split (70%)\"]\n        VAL_DS[\"Val Split (15%)\"]\n        TEST_DS[\"Test Split (15%)\"]\n    end\n    \n    INIT --\u003e SETUP\n    SETUP --\u003e IMG_TF\n    SETUP --\u003e LBL_TF\n    SETUP --\u003e FULL_DS\n    IMG_TF --\u003e FULL_DS\n    LBL_TF --\u003e FULL_DS\n    \n    FULL_DS --\u003e|\"random_split\"| TRAIN_DS\n    FULL_DS --\u003e|\"random_split\"| VAL_DS\n    FULL_DS --\u003e|\"random_split\"| TEST_DS\n    \n    TRAIN_DS --\u003e TRAIN_LOADER\n    VAL_DS --\u003e VAL_LOADER\n    TEST_DS --\u003e TEST_LOADER\n```\n\n### Configuration Parameters\n\nThe `CampDataModule` class [examplemodel/src/model.py:33-100]() accepts the following initialization parameters:\n\n| Parameter | Type | Default | Purpose |\n|-----------|------|---------|---------|\n| `image_dir` | str | Required | Path to directory containing images |\n| `label_dir` | str | Required | Path to directory containing labels |\n| `batch_size` | int | 32 | Number of samples per batch |\n| `val_ratio` | float | 0.15 | Fraction of data for validation (15%) |\n| `test_ratio` | float | 0.15 | Fraction of data for testing (15%) |\n| `seed` | int | 62 | Random seed for reproducible splits |\n\n### Data Splitting Strategy\n\nThe data module uses `torch.random_split()` [examplemodel/src/model.py:70-72]() to divide the dataset:\n\n```\nTotal Samples (n)\n Train: n - val_samples - test_samples  (70%)\n Val:   int(n  0.15)                   (15%)\n Test:  int(n  0.15)                   (15%)\n```\n\nThe split is **deterministic** and reproducible due to the fixed seed (default: 62).\n\n**Sources:** [examplemodel/src/model.py:33-100]()\n\n---\n\n## Data Transformations Pipeline\n\n### Image Transformations\n\n```mermaid\nflowchart LR\n    INPUT[\"Input Image\u003cbr/\u003e(Any size, RGB)\"]\n    RESIZE[\"Resize\u003cbr/\u003e256256\"]\n    TENSOR[\"ToTensor\u003cbr/\u003e[0,1] range\"]\n    NORMALIZE[\"Normalize\u003cbr/\u003eImageNet stats\"]\n    OUTPUT[\"Output Tensor\u003cbr/\u003e[3, 256, 256]\"]\n    \n    INPUT --\u003e RESIZE --\u003e TENSOR --\u003e NORMALIZE --\u003e OUTPUT\n```\n\nThe image transformation pipeline [examplemodel/src/model.py:53-59]() applies:\n\n1. **Resize**: Scale to 256256 pixels (bilinear interpolation)\n2. **ToTensor**: Convert PIL Image to PyTorch tensor, scale [0, 255]  [0, 1]\n3. **Normalize**: Apply ImageNet statistics for transfer learning compatibility\n   - Mean: `[0.485, 0.456, 0.406]` (RGB channels)\n   - Std: `[0.229, 0.224, 0.225]` (RGB channels)\n\n### Label Transformations\n\n```mermaid\nflowchart LR\n    INPUT[\"Input Mask\u003cbr/\u003e(Any size, binary)\"]\n    RESIZE[\"Resize\u003cbr/\u003e256256\u003cbr/\u003eNEAREST mode\"]\n    TENSOR[\"ToTensor\u003cbr/\u003e[0,1] range\"]\n    OUTPUT[\"Output Tensor\u003cbr/\u003e[1, 256, 256]\"]\n    \n    INPUT --\u003e RESIZE --\u003e TENSOR --\u003e OUTPUT\n```\n\nThe label transformation pipeline [examplemodel/src/model.py:60-65]() applies:\n\n1. **Resize**: Scale to 256256 pixels using **nearest-neighbor interpolation** (preserves binary values)\n2. **ToTensor**: Convert to tensor format\n\n**Critical Note:** Labels use `InterpolationMode.NEAREST` to prevent interpolation artifacts that would create non-binary values in segmentation masks.\n\n**Sources:** [examplemodel/src/model.py:53-65]()\n\n---\n\n## DataLoader Configuration\n\nEach data split is wrapped in a `DataLoader` with optimized settings:\n\n| DataLoader | Shuffle | Batch Size | Workers | Pin Memory |\n|------------|---------|------------|---------|------------|\n| Train |  Yes | 32 | 4 |  Yes |\n| Validation |  No | 32 | 4 |  Yes |\n| Test |  No | 32 | 4 |  Yes |\n\n**Performance optimizations:**\n- `num_workers=4`: Parallel data loading using 4 subprocesses\n- `pin_memory=True`: Enables faster GPU transfer (requires CUDA)\n- `shuffle=True` (train only): Randomizes batch order each epoch\n\n**Sources:** [examplemodel/src/model.py:75-100]()\n\n---\n\n## Dataset Statistics Calculation\n\nThe training pipeline calculates comprehensive dataset statistics using the `calculate_dataset_statistics()` function [examplemodel/src/train.py:63-98]().\n\n### Statistics Workflow\n\n```mermaid\nflowchart TD\n    START[\"Start Statistics Calculation\"]\n    SETUP[\"data_module.setup()\"]\n    INIT[\"Initialize accumulators\u003cbr/\u003echannel_sum = [0,0,0]\u003cbr/\u003echannel_squared_sum = [0,0,0]\"]\n    \n    LOOP_START{\"For each batch\u003cbr/\u003ein train_loader\"}\n    ACCUMULATE[\"Accumulate per-channel\u003cbr/\u003esum and squared sum\"]\n    LOOP_END{\"More batches?\"}\n    \n    CALC_STATS[\"Calculate mean and std\u003cbr/\u003emean = sum / n\u003cbr/\u003estd = sqrt(E[x] - E[x])\"]\n    COUNT_SAMPLES[\"Count samples in\u003cbr/\u003etrain/val/test loaders\"]\n    \n    RETURN[\"Return statistics dict\"]\n    \n    START --\u003e SETUP --\u003e INIT --\u003e LOOP_START\n    LOOP_START --\u003e|Yes| ACCUMULATE --\u003e LOOP_END\n    LOOP_END --\u003e|Yes| LOOP_START\n    LOOP_END --\u003e|No| CALC_STATS --\u003e COUNT_SAMPLES --\u003e RETURN\n```\n\n### Statistics Captured\n\nThe function returns a dictionary with the following metrics:\n\n```python\n{\n    \"train_samples\": int,      # Number of training samples\n    \"val_samples\": int,        # Number of validation samples\n    \"test_samples\": int,       # Number of test samples\n    \"total_samples\": int,      # Total samples processed\n    \"image_channels\": 3,       # RGB channels\n    \"image_height\": 256,       # Fixed height\n    \"image_width\": 256,        # Fixed width\n    \"pixel_mean\": [R, G, B],   # Per-channel mean values\n    \"pixel_std\": [R, G, B],    # Per-channel std deviation\n    \"num_classes\": 2           # Background + refugee_camp\n}\n```\n\nThese statistics are logged to MLflow as parameters [examplemodel/src/train.py:396-403]() and embedded in STAC-MLM metadata.\n\n**Sources:** [examplemodel/src/train.py:63-98](), [examplemodel/src/train.py:396-403]()\n\n---\n\n## Customizing for Your Dataset\n\n### Use Case 1: Different Image Dimensions\n\nTo handle images with different input sizes, modify the `Resize` transformation:\n\n```python\n# In CampDataModule.setup() method [model.py:52-65]\nimg_tf = transforms.Compose([\n    transforms.Resize((512, 512)),  # Change from (256, 256)\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n```\n\n**Note:** You must also update the model architecture [examplemodel/src/model.py:103-137]() to accept the new dimensions, particularly the pooling and upsampling layers.\n\n### Use Case 2: Different Normalization Statistics\n\nIf your imagery comes from a different domain (e.g., aerial photography instead of satellite), calculate custom normalization statistics:\n\n```python\n# Compute your dataset's statistics first\ndataset_stats = calculate_dataset_statistics(data_module)\ncustom_mean = dataset_stats[\"pixel_mean\"]  # [R_mean, G_mean, B_mean]\ncustom_std = dataset_stats[\"pixel_std\"]    # [R_std, G_std, B_std]\n\n# Use in transforms\nimg_tf = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(custom_mean, custom_std),\n])\n```\n\n### Use Case 3: Adjusting Data Split Ratios\n\nModify the split ratios when instantiating `CampDataModule`:\n\n```python\ndata_module = CampDataModule(\n    image_dir=\"path/to/images\",\n    label_dir=\"path/to/labels\",\n    batch_size=32,\n    val_ratio=0.10,    # 10% validation\n    test_ratio=0.20,   # 20% test\n    seed=42            # Different seed for different split\n)\n# Results in 70% train, 10% val, 20% test\n```\n\n### Use Case 4: Adding Data Augmentation\n\nExtend the transformation pipeline with augmentations for training data:\n\n```python\nfrom torchvision import transforms\n\ntrain_tf = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n```\n\n**Important:** Apply identical **geometric** augmentations (flips, rotations) to both images and labels, but **only** apply color augmentations to images.\n\n### Use Case 5: Multi-Class Segmentation\n\nTo adapt for multi-class segmentation (e.g., multiple camp types), modify:\n\n1. **Labels format:** Use integer labels (0, 1, 2, ...) instead of binary\n2. **Loss function:** Change from `BCELoss` to `CrossEntropyLoss` [examplemodel/src/model.py:149]()\n3. **Model output:** Change output layer from 1 channel to `num_classes` channels [examplemodel/src/model.py:116]()\n4. **Dataset statistics:** Update `num_classes` in the statistics dictionary\n\n### Use Case 6: Custom Data Module\n\nCreate a subclass for specialized loading logic:\n\n```python\nclass CustomCampDataModule(CampDataModule):\n    def setup(self, stage=None):\n        # Custom preprocessing logic\n        super().setup(stage)\n        \n        # Add custom data filtering, balancing, etc.\n        if stage == \"fit\":\n            self.train_ds = self.apply_custom_filtering(self.train_ds)\n    \n    def apply_custom_filtering(self, dataset):\n        # Your custom logic here\n        return filtered_dataset\n```\n\n**Sources:** [examplemodel/src/model.py:33-100](), [examplemodel/src/train.py:389-393]()\n\n---\n\n## Integration with Training Pipeline\n\n### Data Module Instantiation\n\nThe training script instantiates the data module with command-line arguments:\n\n```python\n# From train.py:389-393\ndata_module = CampDataModule(\n    image_dir=args.chips_dir,     # From --chips_dir argument\n    label_dir=args.labels_dir,    # From --labels_dir argument\n    batch_size=args.batch_size,   # From --batch_size argument (default: 32)\n)\n```\n\n### Usage in Training\n\n```mermaid\nsequenceDiagram\n    participant Script as train.py\n    participant DataModule as CampDataModule\n    participant Trainer as PyTorch Lightning Trainer\n    participant Model as LitRefugeeCamp\n    \n    Script-\u003e\u003eDataModule: Instantiate with paths\n    Script-\u003e\u003eDataModule: calculate_dataset_statistics()\n    DataModule-\u003e\u003eDataModule: setup() - create splits\n    DataModule--\u003e\u003eScript: Return statistics\n    Script-\u003e\u003eScript: Log statistics to MLflow\n    \n    Script-\u003e\u003eTrainer: trainer.fit(model, data_module)\n    Trainer-\u003e\u003eDataModule: train_dataloader()\n    DataModule--\u003e\u003eTrainer: Return DataLoader\n    \n    loop Training Loop\n        Trainer-\u003e\u003eDataModule: Get next batch\n        DataModule--\u003e\u003eTrainer: (images, labels)\n        Trainer-\u003e\u003eModel: Forward pass\n        Model--\u003e\u003eTrainer: Predictions\n        Trainer-\u003e\u003eTrainer: Calculate loss\n    end\n    \n    Trainer-\u003e\u003eDataModule: val_dataloader()\n    DataModule--\u003e\u003eTrainer: Return DataLoader\n    Trainer-\u003e\u003eTrainer: Validation step\n```\n\n**Sources:** [examplemodel/src/train.py:389-422]()\n\n---\n\n## Data Artifacts in MLflow\n\nThe training pipeline logs dataset artifacts to MLflow for reproducibility:\n\n| Artifact Type | Path | Purpose |\n|---------------|------|---------|\n| Training images | `datasets/train/chips/` | Full training image directory |\n| Training labels | `datasets/train/labels/` | Full training label directory |\n| Dataset statistics | Logged as MLflow parameters | Sample counts, dimensions, normalization values |\n\nThis ensures complete reproducibility - anyone can retrieve the exact training data used for a given model run.\n\n**Sources:** [examplemodel/src/train.py:486-488]()\n\n---\n\n## Best Practices\n\n### Data Quality Checklist\n\n- [ ] **Filename matching**: Verify image-label pairs have identical filenames\n- [ ] **Label format**: Ensure labels are binary (0/1) for segmentation\n- [ ] **Image format**: Use lossless formats (PNG) for labels to prevent compression artifacts\n- [ ] **Data balance**: Check class distribution in your dataset\n- [ ] **Split reproducibility**: Use a fixed seed for consistent train/val/test splits\n- [ ] **Normalization**: Use ImageNet stats for transfer learning, or calculate custom stats for your domain\n\n### Performance Optimization\n\n| Setting | Recommendation | Trade-off |\n|---------|---------------|-----------|\n| `batch_size` | 32-64 (GPU-dependent) | Larger = faster but more memory |\n| `num_workers` | 4-8 (CPU cores) | More = faster loading but more CPU |\n| `pin_memory` | True (if GPU available) | Faster transfers but more RAM |\n| Data caching | Consider for small datasets | Faster epochs but high RAM usage |\n\n### Common Pitfalls\n\n1. **Non-matching filenames**: Results in misaligned image-label pairs\n2. **Non-binary labels**: Using grayscale images [0-255] instead of binary [0-1]\n3. **Interpolation on labels**: Using bilinear resize creates non-integer values\n4. **Inconsistent splits**: Changing seed between runs breaks reproducibility\n5. **Missing normalization**: Forgetting to normalize images leads to poor convergence\n\n**Sources:** [examplemodel/src/model.py:12-100](), [examplemodel/src/train.py:389-393]()"])</script><script>self.__next_f.push([1,"35:T43a4,"])</script><script>self.__next_f.push([1,"# Reference\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [docs/research.pdf](docs/research.pdf)\n- [docs/slides.pdf](docs/slides.pdf)\n- [examplemodel/MLproject](examplemodel/MLproject)\n- [infra/.env.template](infra/.env.template)\n\n\u003c/details\u003e\n\n\n\nThis section provides technical reference documentation for the OpenGeoAIModelHub repository, including MLproject entry point specifications, configuration schemas, and contextual information. This page serves as a high-level reference overview with quick-access tables and diagrams. For detailed information, see:\n- [MLproject API Reference](#8.1) - Complete entry point specifications and parameters\n- [Configuration Reference](#8.2) - Comprehensive environment variable and configuration file documentation\n- [Research and Background](#8.3) - Project context and research information\n\n## MLproject Entry Points Quick Reference\n\nThe `MLproject` file defines five distinct entry points that orchestrate the complete ML workflow. Each entry point is a standalone command that can be executed via `mlflow run`.\n\n### Entry Point Summary\n\n| Entry Point | Purpose | Primary Outputs | Key Parameters |\n|------------|---------|-----------------|----------------|\n| `preprocess` | Data acquisition from OpenAerialMap and OpenStreetMap | Image chips, label masks | `zoom`, `bbox`, `tms`, `train_dir` |\n| `train` | Model training with PyTorch Lightning | `best_model.pth`, `best_model.onnx`, `stac_item.json` | `epochs`, `batch_size`, `chips_dir`, `labels_dir`, `lr` |\n| `inference` | Model prediction on new imagery | Prediction masks, overlay visualizations | `image_path`, `model_path`, `output_dir`, `mlflow_tracking` |\n| `validate_stac` | STAC-MLM metadata validation | Validation report | `stac_file` |\n| `stac2esri` | ESRI Deep Learning Package generation | `*.dlpk` file for ArcGIS | `stac_path`, `onnx_path`, `out_dir`, `dlpk_name` |\n\nSources: [examplemodel/MLproject:1-63]()\n\n### Entry Point Workflow Diagram\n\n```mermaid\ngraph LR\n    preprocess[\"preprocess\u003cbr/\u003e(src/preprocess.py)\"]\n    train[\"train\u003cbr/\u003e(src/train.py)\"]\n    inference[\"inference\u003cbr/\u003e(src/inference.py)\"]\n    validate_stac[\"validate_stac\u003cbr/\u003e(validate_stac_mlm.py)\"]\n    stac2esri[\"stac2esri\u003cbr/\u003e(src/stac2esri.py)\"]\n    \n    chips[\"data/train/*/chips/\"]\n    labels[\"data/train/*/labels/\"]\n    best_model_pt[\"meta/best_model.pt\"]\n    best_model_onnx[\"meta/best_model.onnx\"]\n    stac_item[\"meta/stac_item.json\"]\n    dlpk[\"meta/*.dlpk\"]\n    \n    preprocess --\u003e|produces| chips\n    preprocess --\u003e|produces| labels\n    \n    chips --\u003e|consumed by| train\n    labels --\u003e|consumed by| train\n    \n    train --\u003e|produces| best_model_pt\n    train --\u003e|produces| best_model_onnx\n    train --\u003e|produces| stac_item\n    \n    best_model_pt --\u003e|consumed by| inference\n    stac_item --\u003e|consumed by| validate_stac\n    \n    stac_item --\u003e|consumed by| stac2esri\n    best_model_onnx --\u003e|consumed by| stac2esri\n    stac2esri --\u003e|produces| dlpk\n```\n\nSources: [examplemodel/MLproject:1-63]()\n\n### Parameter Type Reference\n\n#### preprocess Entry Point\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `zoom` | int | 19 | TMS zoom level for tile retrieval |\n| `bbox` | str | \"85.51991...,27.62883...\" | Bounding box in lon_min,lat_min,lon_max,lat_max format |\n| `tms` | str | \"https://tiles.openaerialmap.org/...\" | Tile Map Service URL template with {z}/{x}/{y} placeholders |\n| `train_dir` | str | \"data/train/sample\" | Output directory for chips and labels |\n\nSources: [examplemodel/MLproject:6-17]()\n\n#### train Entry Point\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `epochs` | int | 1 | Number of training epochs |\n| `batch_size` | int | 32 | Training batch size |\n| `chips_dir` | str | \"data/train/sample/chips\" | Directory containing input image chips |\n| `labels_dir` | str | \"data/train/sample/labels\" | Directory containing label masks |\n| `lr` | float | 1e-3 | Learning rate for optimizer |\n\nSources: [examplemodel/MLproject:19-32]()\n\n#### inference Entry Point\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `image_path` | str | (required) | Path to input image for prediction |\n| `model_path` | str | \"meta/best_model.pt\" | Path to trained PyTorch model checkpoint |\n| `output_dir` | str | \"output\" | Directory for prediction outputs |\n| `mlflow_tracking` | bool | false | Enable MLflow experiment logging for inference run |\n\nSources: [examplemodel/MLproject:34-44]()\n\n#### validate_stac Entry Point\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `stac_file` | str | \"meta/stac_item.json\" | Path to STAC-MLM metadata file to validate |\n\nSources: [examplemodel/MLproject:46-49]()\n\n#### stac2esri Entry Point\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `stac_path` | str | \"meta/stac_item.json\" | Path to STAC-MLM metadata file |\n| `onnx_path` | str | \"meta/best_model.onnx\" | Path to ONNX model file |\n| `out_dir` | str | \"meta\" | Output directory for DLPK file |\n| `dlpk_name` | str | \"refugee-camp-detector.dlpk\" | Name of the generated Deep Learning Package |\n\nSources: [examplemodel/MLproject:51-62]()\n\n## Environment Configuration Quick Reference\n\nThe infrastructure stack is configured via environment variables defined in `.env.template`. These variables control domain routing, service credentials, and storage paths.\n\n### Configuration Categories\n\n```mermaid\ngraph TB\n    env_template[\".env.template\"]\n    \n    domain[\"Domain Configuration\u003cbr/\u003eDOMAIN, ACME_EMAIL\"]\n    traefik[\"Traefik Configuration\u003cbr/\u003eTRAEFIK_DATA_DIR\u003cbr/\u003eTRAEFIK_AUTH_USER\u003cbr/\u003eTRAEFIK_AUTH_PASSWORD\"]\n    homepage[\"Homepage Configuration\u003cbr/\u003eHOMEPAGE_CONFIG\u003cbr/\u003eHOMEPAGE_ALLOWED_HOSTS\"]\n    mlflow_cfg[\"MLflow Configuration\u003cbr/\u003eMLFLOW_IMAGE\"]\n    minio[\"MinIO Configuration\u003cbr/\u003eAWS_ACCESS_KEY_ID\u003cbr/\u003eAWS_SECRET_ACCESS_KEY\u003cbr/\u003eMINIO_BUCKET_NAME\u003cbr/\u003eMINIO_DATA_DIR\"]\n    postgres[\"PostgreSQL Configuration\u003cbr/\u003ePOSTGRES_USER\u003cbr/\u003ePOSTGRES_PASSWORD\u003cbr/\u003ePOSTGRES_DB\u003cbr/\u003ePOSTGRES_DATA_DIR\"]\n    rustdesk[\"RustDesk Configuration\u003cbr/\u003eRUSTDESK_DATA_DIR\u003cbr/\u003eRUSTDESK_KEY\"]\n    system[\"System Configuration\u003cbr/\u003ePUID, PGID, TZ\"]\n    \n    env_template --\u003e|lines 1-3| domain\n    env_template --\u003e|lines 5-9| traefik\n    env_template --\u003e|lines 11-13| homepage\n    env_template --\u003e|line 15-16| mlflow_cfg\n    env_template --\u003e|lines 18-22| minio\n    env_template --\u003e|lines 24-28| postgres\n    env_template --\u003e|lines 30-32| rustdesk\n    env_template --\u003e|lines 34-37| system\n```\n\nSources: [infra/.env.template:1-37]()\n\n### Critical Environment Variables\n\n| Variable | Purpose | Example Value | Security Level |\n|----------|---------|---------------|----------------|\n| `DOMAIN` | Base domain for all services | example.com | Public |\n| `ACME_EMAIL` | Let's Encrypt certificate notification email | admin@example.com | Public |\n| `TRAEFIK_AUTH_PASSWORD_HASH` | Hashed password for Traefik dashboard | (bcrypt hash) | **Secret** |\n| `AWS_ACCESS_KEY_ID` | MinIO access key (S3-compatible credentials) | (generate secure token) | **Secret** |\n| `AWS_SECRET_ACCESS_KEY` | MinIO secret key | (generate secure token) | **Secret** |\n| `POSTGRES_USER` | PostgreSQL database username | (custom username) | **Secret** |\n| `POSTGRES_PASSWORD` | PostgreSQL database password | (generate secure password) | **Secret** |\n| `RUSTDESK_KEY` | RustDesk server encryption key | (generate secure key) | **Secret** |\n| `MLFLOW_IMAGE` | MLflow container image reference | ghcr.io/.../mlflow:latest | Public |\n\nSources: [infra/.env.template:1-37]()\n\n### Service URL Pattern Reference\n\nBased on the `DOMAIN` variable, services are accessible via subdomain routing managed by Traefik:\n\n| Service | Subdomain Pattern | Example URL |\n|---------|-------------------|-------------|\n| Homepage Dashboard | `DOMAIN` | https://example.com |\n| MLflow Tracking Server | `mlflow.DOMAIN` | https://mlflow.example.com |\n| MinIO Console | `minio.DOMAIN` | https://minio.example.com |\n| MinIO API | `minio-api.DOMAIN` | https://minio-api.example.com |\n| PostgreSQL (via Traefik) | `postgres.DOMAIN` | https://postgres.example.com |\n| Traefik Dashboard | `traefik.DOMAIN` | https://traefik.example.com |\n| RustDesk Server | `rustdesk.DOMAIN` | https://rustdesk.example.com |\n\nSources: [infra/.env.template:2]()\n\n### Storage Volume Mapping\n\n```mermaid\ngraph LR\n    volumes[\"Docker Volumes\"]\n    \n    traefik_data[\"volumes/traefik-data/\u003cbr/\u003e(TRAEFIK_DATA_DIR)\"]\n    minio_data[\"volumes/minio/\u003cbr/\u003e(MINIO_DATA_DIR)\"]\n    postgres_data[\"volumes/postgres/\u003cbr/\u003e(POSTGRES_DATA_DIR)\"]\n    rustdesk_data[\"volumes/rustdesk/\u003cbr/\u003e(RUSTDESK_DATA_DIR)\"]\n    homepage_config[\"homepage-config/\u003cbr/\u003e(HOMEPAGE_CONFIG)\"]\n    \n    volumes --\u003e traefik_data\n    volumes --\u003e minio_data\n    volumes --\u003e postgres_data\n    volumes --\u003e rustdesk_data\n    volumes --\u003e homepage_config\n    \n    traefik_data --\u003e|stores| ssl_certs[\"SSL certificates\u003cbr/\u003eacme.json\"]\n    minio_data --\u003e|stores| artifacts[\"MLflow artifacts\u003cbr/\u003emodels, plots\"]\n    postgres_data --\u003e|stores| metadata[\"MLflow metadata\u003cbr/\u003eexperiments, runs\"]\n    rustdesk_data --\u003e|stores| rustdesk_state[\"RustDesk server state\"]\n    homepage_config --\u003e|stores| dashboard_cfg[\"Homepage dashboard config\"]\n```\n\nSources: [infra/.env.template:6,12,22,28,31]()\n\n## Data Directory Structure Reference\n\n### Training Data Layout\n\nThe preprocessing and training entry points expect and produce data in the following directory structure:\n\n```\ndata/train/{dataset_name}/\n chips/           # Input imagery tiles (PNG/TIFF)\n    0.png\n    1.png\n    ...\n labels/          # Corresponding label masks (PNG)\n     0.png\n     1.png\n     ...\n```\n\nSources: [examplemodel/MLproject:11,23-24]()\n\n### Model Output Artifacts\n\nThe training entry point produces the following artifacts in the `meta/` directory:\n\n```\nmeta/\n best_model.pt         # PyTorch checkpoint (LitRefugeeCamp state)\n best_model.onnx       # ONNX-exported model (interoperable)\n stac_item.json        # STAC-MLM metadata (discoverability)\n```\n\nSources: [examplemodel/MLproject:37,54]()\n\n### DLPK Package Structure\n\nThe `stac2esri` entry point produces an ESRI Deep Learning Package with internal structure:\n\n```\n{dlpk_name}.dlpk          # Zip archive containing:\n esri_model_definition.emd    # ESRI model metadata\n refugee_camp_detector.onnx   # ONNX model file\n RefugeeCampDetector.py       # Python inference class\n stac_item.json               # STAC-MLM metadata\n```\n\nSources: [examplemodel/MLproject:56]()\n\n## Command Invocation Reference\n\n### Basic MLflow Run Commands\n\nExecute entry points using the `mlflow run` command with the MLproject file:\n\n```bash\n# Preprocess data\nmlflow run examplemodel/ -e preprocess \\\n  -P zoom=19 \\\n  -P bbox=\"85.519,27.628,85.527,27.633\" \\\n  -P tms=\"https://tiles.openaerialmap.org/.../0/{z}/{x}/{y}\" \\\n  -P train_dir=\"data/train/my_dataset\"\n\n# Train model\nmlflow run examplemodel/ -e train \\\n  -P epochs=50 \\\n  -P batch_size=16 \\\n  -P chips_dir=\"data/train/my_dataset/chips\" \\\n  -P labels_dir=\"data/train/my_dataset/labels\" \\\n  -P lr=0.001\n\n# Run inference\nmlflow run examplemodel/ -e inference \\\n  -P image_path=\"test_images/camp.png\" \\\n  -P model_path=\"meta/best_model.pt\" \\\n  -P output_dir=\"predictions\" \\\n  -P mlflow_tracking=true\n\n# Validate STAC metadata\nmlflow run examplemodel/ -e validate_stac \\\n  -P stac_file=\"meta/stac_item.json\"\n\n# Generate ESRI DLPK\nmlflow run examplemodel/ -e stac2esri \\\n  -P stac_path=\"meta/stac_item.json\" \\\n  -P onnx_path=\"meta/best_model.onnx\" \\\n  -P out_dir=\"deployments\" \\\n  -P dlpk_name=\"refugee_camp_detector_v1.dlpk\"\n```\n\nSources: [examplemodel/MLproject:1-63]()\n\n### Direct Python Execution\n\nEntry points can also be invoked directly using `uv run`:\n\n```bash\n# Preprocess\nuv run python src/preprocess.py --zoom 19 --bbox \"...\" --tms \"...\" --train-dir \"data/train/sample\"\n\n# Train\nuv run python src/train.py --epochs 50 --batch_size 16 --chips_dir \"...\" --labels_dir \"...\" --lr 0.001\n\n# Inference\nuv run python src/inference.py test.png --model_path meta/best_model.pt --output_dir output\n\n# Validate STAC\nuv run python validate_stac_mlm.py meta/stac_item.json\n\n# Convert to ESRI\nuv run python src/stac2esri.py --stac meta/stac_item.json --onnx meta/best_model.onnx --out-dir meta --dlpk-name camp.dlpk\n```\n\nSources: [examplemodel/MLproject:13-17,27-32,41-44,49,58-62]()\n\n## System Requirements Reference\n\n### Dependency Management\n\nThe project uses `uv` for deterministic dependency management:\n\n- **Manifest**: `pyproject.toml` - declares dependencies and project metadata\n- **Lockfile**: `uv.lock` - pins exact versions for reproducibility\n- **Sync command**: `uv sync` - installs dependencies from lockfile\n- **Run command**: `uv run` - executes commands in the managed environment\n\n### Infrastructure Requirements\n\n| Component | Minimum | Recommended | Purpose |\n|-----------|---------|-------------|---------|\n| CPU | 2 cores | 4+ cores | Training and inference |\n| RAM | 8 GB | 16+ GB | Model training and services |\n| Disk Space | 20 GB | 50+ GB | Docker images, volumes, datasets |\n| Docker | 20.10+ | 24.0+ | Container runtime |\n| Docker Compose | 2.0+ | 2.20+ | Multi-container orchestration |\n\n### Port Allocation\n\n| Port | Service | Protocol | Access Level |\n|------|---------|----------|--------------|\n| 80 | Traefik (HTTP) | TCP | Public (redirects to 443) |\n| 443 | Traefik (HTTPS) | TCP | Public (TLS termination) |\n| 8080 | Traefik Dashboard | TCP | Internal (via reverse proxy) |\n| 5000 | MLflow Server | TCP | Internal (via reverse proxy) |\n| 9000 | MinIO API | TCP | Internal (via reverse proxy) |\n| 9001 | MinIO Console | TCP | Internal (via reverse proxy) |\n| 5432 | PostgreSQL | TCP | Internal (via reverse proxy) |\n| 21115-21119 | RustDesk | TCP/UDP | Public (direct access) |\n\n## File and Code Entity Reference\n\n### Key Python Modules\n\n| Module | Purpose | Entry Point Usage |\n|--------|---------|-------------------|\n| `src/preprocess.py` | Data acquisition and chip generation | `preprocess` entry point |\n| `src/train.py` | Model training orchestration | `train` entry point |\n| `src/inference.py` | Prediction generation | `inference` entry point |\n| `src/model.py` | PyTorch Lightning model classes | Imported by `train.py` |\n| `src/stac2esri.py` | DLPK package creation | `stac2esri` entry point |\n| `src/esri/RefugeeCampDetector.py` | ESRI inference adapter | Packaged in DLPK |\n| `validate_stac_mlm.py` | STAC-MLM schema validation | `validate_stac` entry point |\n\nSources: [examplemodel/MLproject:13,27,41,49,58]()\n\n### Configuration Files\n\n| File | Purpose | Format |\n|------|---------|--------|\n| `examplemodel/MLproject` | MLflow project definition | YAML-like DSL |\n| `examplemodel/pyproject.toml` | Python project metadata and dependencies | TOML |\n| `examplemodel/uv.lock` | Dependency version lockfile | TOML |\n| `infra/.env.template` | Environment variable template | Shell variable export |\n| `infra/docker-compose.yml` | Infrastructure service definitions | YAML |\n\nSources: [examplemodel/MLproject:1](), [infra/.env.template:1]()\n\n### Docker Images\n\n| Image | Registry | Purpose |\n|-------|----------|---------|\n| `ghcr.io/kshitijrajsharma/opengeoaimodelshub/mlflow:latest` | GitHub Container Registry | Custom MLflow server with extensions |\n| `traefik:v3.0` | Docker Hub | Reverse proxy and SSL termination |\n| `minio/minio:latest` | Docker Hub | S3-compatible object storage |\n| `postgres:16-alpine` | Docker Hub | PostgreSQL with PostGIS |\n| `rustdesk/rustdesk-server:latest` | Docker Hub | Remote desktop server |\n| `ghcr.io/gethomepage/homepage:latest` | GitHub Container Registry | Service monitoring dashboard |\n\nSources: [infra/.env.template:16]()\n\n## Default Values Summary\n\nQuick reference table for all default parameter values across entry points:\n\n| Parameter | Default Value | Entry Point |\n|-----------|---------------|-------------|\n| `zoom` | 19 | preprocess |\n| `bbox` | \"85.51991979758662,27.628837632373674,85.52736620395387,27.633394557789373\" | preprocess |\n| `tms` | \"https://tiles.openaerialmap.org/62d85d11d8499800053796c1/0/62d85d11d8499800053796c2/{z}/{x}/{y}\" | preprocess |\n| `train_dir` | \"data/train/sample\" | preprocess |\n| `epochs` | 1 | train |\n| `batch_size` | 32 | train |\n| `chips_dir` | \"data/train/sample/chips\" | train |\n| `labels_dir` | \"data/train/sample/labels\" | train |\n| `lr` | 1e-3 | train |\n| `model_path` | \"meta/best_model.pt\" | inference |\n| `output_dir` | \"output\" | inference |\n| `mlflow_tracking` | false | inference |\n| `stac_file` | \"meta/stac_item.json\" | validate_stac |\n| `stac_path` | \"meta/stac_item.json\" | stac2esri |\n| `onnx_path` | \"meta/best_model.onnx\" | stac2esri |\n| `out_dir` | \"meta\" | stac2esri |\n| `dlpk_name` | \"refugee-camp-detector.dlpk\" | stac2esri |\n\nSources: [examplemodel/MLproject:8-56]()\n\nFor comprehensive API documentation including parameter constraints, validation rules, and advanced usage patterns, see [MLproject API Reference](#8.1). For detailed environment variable descriptions and configuration schemas, see [Configuration Reference](#8.2)."])</script><script>self.__next_f.push([1,"36:T3d18,"])</script><script>self.__next_f.push([1,"# MLproject API Reference\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/MLproject](examplemodel/MLproject)\n- [examplemodel/src/train.py](examplemodel/src/train.py)\n\n\u003c/details\u003e\n\n\n\nThis document provides a complete technical reference for the MLproject file that defines the MLflow entry points for the OpenGeoAIModelHub example model system. The MLproject file serves as the orchestration interface for the refugee camp detection ML pipeline, exposing five distinct entry points that handle data preprocessing, model training, inference, metadata validation, and ESRI package generation.\n\nFor information about the training pipeline implementation details, see [Training Pipeline](#3.2). For ESRI integration specifics, see [ESRI Integration and DLPK Generation](#3.4). For broader MLflow Project structure context, see [MLflow Project Structure](#3.5).\n\n## MLproject File Structure\n\nThe MLproject file is a YAML configuration that defines the project name, optional container environment, and multiple entry points with their parameters and execution commands. It serves as the primary interface for executing ML pipeline stages via `mlflow run` commands.\n\n**MLproject Configuration Overview**\n\n```mermaid\ngraph TB\n    subgraph \"MLproject File\"\n        PROJECT[\"name: refugee-camp-detector\"]\n        DOCKER[\"docker_env (commented out)\"]\n        \n        subgraph \"Entry Points\"\n            PREPROCESS[\"preprocess\"]\n            TRAIN[\"train\"]\n            INFERENCE[\"inference\"]\n            VALIDATE[\"validate_stac\"]\n            STAC2ESRI[\"stac2esri\"]\n        end\n    end\n    \n    subgraph \"Python Scripts\"\n        PREPROCESS_PY[\"src/preprocess.py\"]\n        TRAIN_PY[\"src/train.py\"]\n        INFERENCE_PY[\"src/inference.py\"]\n        VALIDATE_PY[\"validate_stac_mlm.py\"]\n        STAC2ESRI_PY[\"src/stac2esri.py\"]\n    end\n    \n    PROJECT --\u003e PREPROCESS\n    PROJECT --\u003e TRAIN\n    PROJECT --\u003e INFERENCE\n    PROJECT --\u003e VALIDATE\n    PROJECT --\u003e STAC2ESRI\n    \n    PREPROCESS --\u003e|\"uv run python\"| PREPROCESS_PY\n    TRAIN --\u003e|\"uv run python\"| TRAIN_PY\n    INFERENCE --\u003e|\"uv run python\"| INFERENCE_PY\n    VALIDATE --\u003e|\"uv run python\"| VALIDATE_PY\n    STAC2ESRI --\u003e|\"uv run python\"| STAC2ESRI_PY\n```\n\nSources: [examplemodel/MLproject:1-63]()\n\n## Entry Point Reference\n\n### preprocess Entry Point\n\nThe `preprocess` entry point downloads satellite imagery and label data from external sources (OpenAerialMap and OpenStreetMap) and prepares training chips.\n\n**Parameters**\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `zoom` | int | 19 | Tile zoom level for imagery download |\n| `bbox` | str | \"85.51991979758662,27.628837632373674,85.52736620395387,27.633394557789373\" | Bounding box coordinates (lon_min,lat_min,lon_max,lat_max) |\n| `tms` | str | \"https://tiles.openaerialmap.org/...\" | Tile Map Service URL template with {z}/{x}/{y} placeholders |\n| `train_dir` | str | \"data/train/sample\" | Output directory for processed training data |\n\n**Command Execution**\n\n```bash\nuv run python src/preprocess.py --zoom {zoom} --bbox {bbox} --tms {tms} --train-dir {train_dir}\n```\n\n**Usage Example**\n\n```bash\nmlflow run . -e preprocess -P zoom=18 -P bbox=\"85.51,27.62,85.53,27.64\"\n```\n\nSources: [examplemodel/MLproject:6-17]()\n\n### train Entry Point\n\nThe `train` entry point executes the model training pipeline using PyTorch Lightning, logging metrics and artifacts to MLflow, and generating multiple model formats (PyTorch, ONNX, DLPK) along with STAC-MLM metadata.\n\n**Parameters**\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `epochs` | int | 1 | Number of training epochs |\n| `batch_size` | int | 32 | Training batch size |\n| `chips_dir` | str | \"data/train/sample/chips\" | Directory containing training image chips |\n| `labels_dir` | str | \"data/train/sample/labels\" | Directory containing corresponding label masks |\n| `lr` | float | 1e-3 | Learning rate for optimizer |\n\n**Command Execution**\n\n```bash\nuv run python src/train.py --epochs {epochs} --batch_size {batch_size} --chips_dir {chips_dir} --labels_dir {labels_dir} --lr {lr}\n```\n\n**Artifacts Generated**\n\nThe training entry point generates and logs the following artifacts:\n\n- `meta/best_model.pth` - PyTorch raw state dictionary\n- `meta/best_model.pt` - TorchScript traced model\n- `meta/best_model.onnx` - ONNX format model\n- `meta/best_model.dlpk` - ESRI Deep Learning Package\n- `meta/stac_item.json` - STAC-MLM metadata\n- `meta/model.emd` - ESRI model definition\n- Confusion matrix visualization\n- Example inference outputs\n\n**Usage Example**\n\n```bash\nmlflow run . -e train -P epochs=10 -P batch_size=16 -P lr=0.001\n```\n\nSources: [examplemodel/MLproject:19-32](), [examplemodel/src/train.py:370-507]()\n\n### inference Entry Point\n\nThe `inference` entry point performs prediction on a single image using a trained model, generating segmentation masks and overlay visualizations.\n\n**Parameters**\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `image_path` | str | (required) | Path to input satellite image for inference |\n| `model_path` | str | \"meta/best_model.pt\" | Path to trained model file (TorchScript format) |\n| `output_dir` | str | \"output\" | Directory for saving prediction outputs |\n| `mlflow_tracking` | bool | false | Whether to log inference results to MLflow |\n\n**Command Execution**\n\n```bash\nuv run python src/inference.py {image_path} --model_path {model_path} --output_dir {output_dir} {{--mlflow_tracking if mlflow_tracking}}\n```\n\n**Note on Conditional Parameters**\n\nThe `{{--mlflow_tracking if mlflow_tracking}}` syntax is MLflow's conditional parameter inclusion. The flag is only added to the command if the boolean parameter evaluates to true.\n\n**Usage Example**\n\n```bash\nmlflow run . -e inference -P image_path=test_image.jpg -P mlflow_tracking=true\n```\n\nSources: [examplemodel/MLproject:34-44]()\n\n### validate_stac Entry Point\n\nThe `validate_stac` entry point validates STAC-MLM metadata files against the STAC-MLM schema to ensure compliance with the standard.\n\n**Parameters**\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `stac_file` | str | \"meta/stac_item.json\" | Path to STAC-MLM JSON file to validate |\n\n**Command Execution**\n\n```bash\nuv run python validate_stac_mlm.py {stac_file}\n```\n\n**Usage Example**\n\n```bash\nmlflow run . -e validate_stac -P stac_file=custom_stac.json\n```\n\nSources: [examplemodel/MLproject:46-49]()\n\n### stac2esri Entry Point\n\nThe `stac2esri` entry point converts STAC-MLM metadata and ONNX models into ESRI Deep Learning Packages (DLPK) for deployment in ArcGIS platforms.\n\n**Parameters**\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `stac_path` | str | \"meta/stac_item.json\" | Path to STAC-MLM metadata file |\n| `onnx_path` | str | \"meta/best_model.onnx\" | Path to ONNX model file |\n| `out_dir` | str | \"meta\" | Output directory for generated DLPK |\n| `dlpk_name` | str | \"refugee-camp-detector.dlpk\" | Output filename for DLPK package |\n\n**Command Execution**\n\n```bash\nuv run python src/stac2esri.py --stac {stac_path} --onnx {onnx_path} --out-dir {out_dir} --dlpk-name {dlpk_name}\n```\n\n**Usage Example**\n\n```bash\nmlflow run . -e stac2esri -P stac_path=meta/stac_item.json -P dlpk_name=my_model.dlpk\n```\n\nSources: [examplemodel/MLproject:51-62]()\n\n## Entry Point Execution Flow\n\nThe following diagram illustrates the typical execution sequence and data dependencies between entry points:\n\n**Pipeline Execution Sequence**\n\n```mermaid\ngraph LR\n    subgraph \"Data Preparation Stage\"\n        PREPROCESS_EP[\"preprocess entry point\"]\n        CHIPS[\"data/train/sample/chips/\"]\n        LABELS[\"data/train/sample/labels/\"]\n    end\n    \n    subgraph \"Training Stage\"\n        TRAIN_EP[\"train entry point\"]\n        PTH[\"meta/best_model.pth\"]\n        PT[\"meta/best_model.pt\"]\n        ONNX[\"meta/best_model.onnx\"]\n        STAC[\"meta/stac_item.json\"]\n        DLPK[\"meta/best_model.dlpk\"]\n        EMD[\"meta/model.emd\"]\n    end\n    \n    subgraph \"Validation Stage\"\n        VALIDATE_EP[\"validate_stac entry point\"]\n    end\n    \n    subgraph \"Deployment Stage\"\n        INFERENCE_EP[\"inference entry point\"]\n        STAC2ESRI_EP[\"stac2esri entry point\"]\n    end\n    \n    PREPROCESS_EP --\u003e|\"generates\"| CHIPS\n    PREPROCESS_EP --\u003e|\"generates\"| LABELS\n    \n    CHIPS --\u003e|\"--chips_dir\"| TRAIN_EP\n    LABELS --\u003e|\"--labels_dir\"| TRAIN_EP\n    \n    TRAIN_EP --\u003e|\"creates\"| PTH\n    TRAIN_EP --\u003e|\"creates\"| PT\n    TRAIN_EP --\u003e|\"creates\"| ONNX\n    TRAIN_EP --\u003e|\"creates\"| STAC\n    TRAIN_EP --\u003e|\"creates\"| DLPK\n    TRAIN_EP --\u003e|\"creates\"| EMD\n    \n    STAC --\u003e|\"--stac_file\"| VALIDATE_EP\n    \n    PT --\u003e|\"--model_path\"| INFERENCE_EP\n    \n    STAC --\u003e|\"--stac_path\"| STAC2ESRI_EP\n    ONNX --\u003e|\"--onnx_path\"| STAC2ESRI_EP\n```\n\nSources: [examplemodel/MLproject:1-63]()\n\n## Parameter Type System\n\nThe MLproject file uses a simple type system for parameters. All parameters are passed as command-line arguments to the underlying Python scripts.\n\n**Supported Parameter Types**\n\n| MLproject Type | Python Argument Type | Example Value | Notes |\n|----------------|---------------------|---------------|-------|\n| `int` | integer | `19`, `32` | Parsed as Python int |\n| `float` | floating point | `1e-3`, `0.001` | Parsed as Python float |\n| `str` | string | `\"data/train/sample\"` | Passed as-is, quotes removed |\n| `bool` | boolean flag | `true`, `false` | Converted to flag presence/absence |\n\n**Boolean Parameter Handling**\n\nBoolean parameters use a special conditional syntax in MLproject:\n\n```yaml\n{{--flag_name if parameter_name}}\n```\n\nWhen `parameter_name` is `true`, `--flag_name` is added to the command. When `false`, the flag is omitted entirely. This is used in the inference entry point for the `mlflow_tracking` parameter.\n\nSources: [examplemodel/MLproject:7-62]()\n\n## Command Execution Pattern\n\nAll entry points follow a consistent execution pattern using the `uv` package manager:\n\n**Command Structure**\n\n```mermaid\nflowchart LR\n    UV[\"uv run python\"]\n    SCRIPT[\"script_path\"]\n    POSITIONAL[\"positional args\"]\n    NAMED[\"--named-args\"]\n    \n    UV --\u003e SCRIPT\n    SCRIPT --\u003e POSITIONAL\n    POSITIONAL --\u003e NAMED\n```\n\n**Execution Flow**\n\n1. `uv run python` - Executes Python in the uv-managed virtual environment\n2. Script path - Specifies the Python script to run\n3. Positional arguments - Required arguments (e.g., `{image_path}` in inference)\n4. Named arguments - Optional/required flags (e.g., `--epochs {epochs}`)\n\n**Parameter Substitution**\n\nMLflow performs parameter substitution using the syntax `{parameter_name}`. When executing an entry point, MLflow replaces these placeholders with actual parameter values:\n\n```yaml\ncommand: \"uv run python src/train.py --epochs {epochs}\"\n```\n\nBecomes:\n\n```bash\nuv run python src/train.py --epochs 10\n```\n\nSources: [examplemodel/MLproject:12-62]()\n\n## Entry Point to Code Entity Mapping\n\nThe following diagram maps MLproject entry points to their corresponding Python implementation details:\n\n**Entry Point Implementation Mapping**\n\n```mermaid\ngraph TB\n    subgraph \"MLproject Entry Points\"\n        PREPROCESS_EP[\"preprocess\"]\n        TRAIN_EP[\"train\"]\n        INFERENCE_EP[\"inference\"]\n        VALIDATE_EP[\"validate_stac\"]\n        STAC2ESRI_EP[\"stac2esri\"]\n    end\n    \n    subgraph \"Python Scripts\"\n        PREPROCESS_PY[\"src/preprocess.py\"]\n        TRAIN_PY[\"src/train.py\"]\n        INFERENCE_PY[\"src/inference.py\"]\n        VALIDATE_PY[\"validate_stac_mlm.py\"]\n        STAC2ESRI_PY[\"src/stac2esri.py\"]\n    end\n    \n    subgraph \"Key Functions and Classes\"\n        TRAIN_MODEL[\"train_model(args)\"]\n        GET_SYSTEM_INFO[\"get_system_info()\"]\n        CALC_STATS[\"calculate_dataset_statistics()\"]\n        CREATE_STAC[\"create_stac_mlm_item()\"]\n        \n        PREDICT[\"predict_image_enhanced()\"]\n        LOG_INFERENCE[\"log_inference_example()\"]\n        \n        CREATE_DLPK[\"create_dlpk()\"]\n        \n        CAMP_DATA[\"CampDataModule\"]\n        LIT_MODEL[\"LitRefugeeCamp\"]\n    end\n    \n    PREPROCESS_EP --\u003e|\"executes\"| PREPROCESS_PY\n    TRAIN_EP --\u003e|\"executes\"| TRAIN_PY\n    INFERENCE_EP --\u003e|\"executes\"| INFERENCE_PY\n    VALIDATE_EP --\u003e|\"executes\"| VALIDATE_PY\n    STAC2ESRI_EP --\u003e|\"executes\"| STAC2ESRI_PY\n    \n    TRAIN_PY --\u003e|\"calls\"| TRAIN_MODEL\n    TRAIN_MODEL --\u003e|\"uses\"| GET_SYSTEM_INFO\n    TRAIN_MODEL --\u003e|\"uses\"| CALC_STATS\n    TRAIN_MODEL --\u003e|\"creates\"| CREATE_STAC\n    TRAIN_MODEL --\u003e|\"uses\"| CAMP_DATA\n    TRAIN_MODEL --\u003e|\"uses\"| LIT_MODEL\n    \n    INFERENCE_PY --\u003e|\"implements\"| PREDICT\n    INFERENCE_PY --\u003e|\"implements\"| LOG_INFERENCE\n    \n    STAC2ESRI_PY --\u003e|\"implements\"| CREATE_DLPK\n    TRAIN_MODEL --\u003e|\"calls\"| CREATE_DLPK\n```\n\nSources: [examplemodel/MLproject:1-63](), [examplemodel/src/train.py:1-519]()\n\n## Common Usage Patterns\n\n### Sequential Pipeline Execution\n\n```bash\n# 1. Preprocess data\nmlflow run . -e preprocess\n\n# 2. Train model\nmlflow run . -e train -P epochs=20 -P batch_size=16\n\n# 3. Validate metadata\nmlflow run . -e validate_stac\n\n# 4. Run inference\nmlflow run . -e inference -P image_path=test.jpg -P mlflow_tracking=true\n```\n\n### Custom Parameter Configuration\n\n```bash\n# Train with custom data paths and hyperparameters\nmlflow run . -e train \\\n  -P epochs=50 \\\n  -P batch_size=64 \\\n  -P lr=0.0001 \\\n  -P chips_dir=data/custom/chips \\\n  -P labels_dir=data/custom/labels\n```\n\n### ESRI Deployment Package Generation\n\n```bash\n# Generate custom DLPK from existing artifacts\nmlflow run . -e stac2esri \\\n  -P stac_path=experiments/run_123/stac_item.json \\\n  -P onnx_path=experiments/run_123/model.onnx \\\n  -P dlpk_name=production_model_v2.dlpk\n```\n\nSources: [examplemodel/MLproject:1-63]()\n\n## Docker Environment Configuration\n\nThe MLproject file includes a commented-out Docker environment configuration:\n\n```yaml\n# docker_env:\n#   image: ghcr.io/kshitijrajsharma/opengeoaimodelshub:master\n```\n\nWhen uncommented, this configuration instructs MLflow to execute entry points inside the specified Docker container rather than the local environment. This ensures reproducible execution across different systems.\n\n**Enabling Docker Execution**\n\nTo enable Docker-based execution:\n\n1. Uncomment the `docker_env` section in MLproject\n2. Ensure Docker is installed and running\n3. Pull the specified image: `docker pull ghcr.io/kshitijrajsharma/opengeoaimodelshub:master`\n4. Run entry points normally with `mlflow run .`\n\nSources: [examplemodel/MLproject:2-3]()\n\n## Integration with MLflow Tracking\n\nAll entry points integrate with MLflow tracking when executed. The `train` entry point automatically logs:\n\n- System information (platform, GPU, PyTorch version)\n- Dataset statistics (sample counts, pixel statistics)\n- Training metrics (loss, accuracy per epoch)\n- Model artifacts (checkpoints, ONNX, DLPK, STAC metadata)\n- Visualizations (confusion matrix, example predictions)\n\nThe `inference` entry point can optionally enable tracking via the `mlflow_tracking` parameter, which logs inference results and predictions to the active MLflow run.\n\nSources: [examplemodel/src/train.py:373-506]()\n\n## Parameter Validation\n\nThe MLproject file does not perform parameter validation. Validation occurs in the Python scripts themselves through:\n\n- `argparse` in the script entry point\n- Runtime type checking in functions\n- File existence checks for path parameters\n\nInvalid parameters result in Python exceptions during script execution, which MLflow captures and reports as failed runs.\n\nSources: [examplemodel/MLproject:1-63](), [examplemodel/src/train.py:509-518]()"])</script><script>self.__next_f.push([1,"37:T5772,"])</script><script>self.__next_f.push([1,"# Configuration Reference\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [examplemodel/.env_sample](examplemodel/.env_sample)\n- [examplemodel/pyproject.toml](examplemodel/pyproject.toml)\n- [infra/.env.template](infra/.env.template)\n\n\u003c/details\u003e\n\n\n\nThis document provides a complete reference for all configuration files, environment variables, and settings used in the OpenGeoAIModelHub system. It covers both the infrastructure stack configuration and the example model configuration.\n\nFor information about setting up the infrastructure stack, see [Infrastructure Deployment](#6.1). For local development configuration, see [Local Development Setup](#7.1). For MLproject entry point parameters, see [MLproject API Reference](#8.1).\n\n## Configuration File Overview\n\nThe repository uses three primary configuration files:\n\n1. **`infra/.env.template`** - Infrastructure stack environment variables for Docker Compose services\n2. **`examplemodel/pyproject.toml`** - Python project dependencies and metadata\n3. **`examplemodel/.env_sample`** - Runtime environment variables for MLflow tracking and artifact storage\n\n```mermaid\ngraph TB\n    subgraph \"Configuration Files\"\n        ENV_TEMPLATE[\"infra/.env.template\u003cbr/\u003eInfrastructure Settings\"]\n        PYPROJECT[\"examplemodel/pyproject.toml\u003cbr/\u003ePython Dependencies\"]\n        ENV_SAMPLE[\"examplemodel/.env_sample\u003cbr/\u003eRuntime Environment\"]\n    end\n    \n    subgraph \"Docker Compose Stack\"\n        TRAEFIK[\"traefik service\"]\n        MLFLOW_SVC[\"mlflow service\"]\n        MINIO[\"minio service\"]\n        POSTGRES[\"postgresql service\"]\n        RUSTDESK[\"rustdesk service\"]\n        HOMEPAGE[\"homepage service\"]\n    end\n    \n    subgraph \"Training Pipeline\"\n        TRAIN_PY[\"train.py\"]\n        INFERENCE_PY[\"inference.py\"]\n        MODEL_PY[\"model.py\"]\n    end\n    \n    ENV_TEMPLATE --\u003e|provides env vars| TRAEFIK\n    ENV_TEMPLATE --\u003e|provides env vars| MLFLOW_SVC\n    ENV_TEMPLATE --\u003e|provides env vars| MINIO\n    ENV_TEMPLATE --\u003e|provides env vars| POSTGRES\n    ENV_TEMPLATE --\u003e|provides env vars| RUSTDESK\n    ENV_TEMPLATE --\u003e|provides env vars| HOMEPAGE\n    \n    PYPROJECT --\u003e|defines dependencies for| TRAIN_PY\n    PYPROJECT --\u003e|defines dependencies for| INFERENCE_PY\n    PYPROJECT --\u003e|defines dependencies for| MODEL_PY\n    \n    ENV_SAMPLE --\u003e|configures tracking URI| TRAIN_PY\n    ENV_SAMPLE --\u003e|configures S3 backend| TRAIN_PY\n    ENV_SAMPLE --\u003e|configures tracking URI| INFERENCE_PY\n```\n\n**Configuration File Relationships**\n\nSources: [infra/.env.template:1-37](), [examplemodel/pyproject.toml:1-31](), [examplemodel/.env_sample:1-5]()\n\n## Infrastructure Configuration (.env.template)\n\nThe `.env.template` file defines all environment variables required by the Docker Compose infrastructure stack. This file must be copied to `.env` and populated with actual values before deployment.\n\n### Domain and SSL Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `DOMAIN` | Base domain for all services | Yes | `example.com` | - |\n| `ACME_EMAIL` | Email for Let's Encrypt SSL certificate notifications | Yes | `admin@example.com` | - |\n\nThese variables configure Traefik's automatic SSL certificate management through Let's Encrypt ACME protocol. All services are exposed as subdomains of `DOMAIN`.\n\nSources: [infra/.env.template:1-3]()\n\n### Traefik Reverse Proxy Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `TRAEFIK_DATA_DIR` | Host directory for Traefik persistent data (certificates, configuration) | Yes | `./volumes/traefik-data` | `./volumes/traefik-data` |\n| `TRAEFIK_AUTH_USER` | Username for Traefik dashboard basic authentication | Yes | `admin` | `admin` |\n| `TRAEFIK_AUTH_PASSWORD` | Plain text password (used for hash generation only) | Yes | `securepassword123` | - |\n| `TRAEFIK_AUTH_PASSWORD_HASH` | bcrypt hash of password for HTTP basic auth | Yes | `$2y$05$...` | - |\n\nThe `TRAEFIK_AUTH_PASSWORD_HASH` must be a valid bcrypt hash. Generate using:\n```bash\necho $(htpasswd -nbB admin \"your-password\") | sed -e s/\\\\$/\\\\$\\\\$/g\n```\n\nThe Traefik dashboard is accessible at `traefik.${DOMAIN}` using these credentials.\n\nSources: [infra/.env.template:5-9]()\n\n### Homepage Dashboard Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `HOMEPAGE_CONFIG` | Host directory containing homepage configuration files | Yes | `./homepage-config` | `./homepage-config` |\n| `HOMEPAGE_ALLOWED_HOSTS` | Comma-separated list of allowed hostnames | Yes | `example.com,www.example.com` | - |\n\nHomepage provides a unified dashboard for monitoring all infrastructure services. It accesses the Docker socket to display service status.\n\nSources: [infra/.env.template:11-13]()\n\n### MLflow Tracking Server Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `MLFLOW_IMAGE` | Docker image for MLflow tracking server | Yes | `ghcr.io/kshitijrajsharma/opengeoaimodelshub/mlflow:latest` | See example |\n\nThis points to the custom MLflow Docker image built by the CI/CD pipeline. The image includes necessary dependencies and configuration for PostgreSQL + MinIO backend integration.\n\nMLflow server is accessible at `mlflow.${DOMAIN}`.\n\nSources: [infra/.env.template:15-16]()\n\n### MinIO Object Storage Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `AWS_ACCESS_KEY_ID` | MinIO access key (S3-compatible) | Yes | `minioadmin` | - |\n| `AWS_SECRET_ACCESS_KEY` | MinIO secret key (S3-compatible) | Yes | `minioadmin123` | - |\n| `MINIO_BUCKET_NAME` | Default bucket name for MLflow artifacts | Yes | `mlflow` | `mlflow` |\n| `MINIO_DATA_DIR` | Host directory for MinIO persistent storage | Yes | `./volumes/minio` | `./volumes/minio` |\n\nMinIO provides S3-compatible object storage for MLflow artifacts (model files, plots, datasets). The service exposes:\n- **API**: `minio-api.${DOMAIN}` - S3-compatible API endpoint\n- **Console**: `minio.${DOMAIN}` - Web-based management console\n\nThe bucket specified in `MINIO_BUCKET_NAME` must be created before MLflow can store artifacts. This is typically handled automatically by the setup script.\n\nSources: [infra/.env.template:18-22]()\n\n### PostgreSQL + PostGIS Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `POSTGRES_USER` | PostgreSQL superuser username | Yes | `mlflow` | - |\n| `POSTGRES_PASSWORD` | PostgreSQL superuser password | Yes | `secure_pg_password` | - |\n| `POSTGRES_DB` | Default database name | Yes | `mlflow` | `mlflow` |\n| `POSTGRES_DATA_DIR` | Host directory for PostgreSQL persistent storage | Yes | `./volumes/postgres` | `./volumes/postgres` |\n\nPostgreSQL stores MLflow metadata (experiments, runs, parameters, metrics, tags). The database includes PostGIS extension for geospatial data support.\n\nThe database is accessible internally at `postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgresql:5432/${POSTGRES_DB}` and externally at `postgres.${DOMAIN}:5432`.\n\nSources: [infra/.env.template:24-28]()\n\n### RustDesk Remote Desktop Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `RUSTDESK_DATA_DIR` | Host directory for RustDesk persistent storage | Yes | `./volumes/rustdesk` | `./volumes/rustdesk` |\n| `RUSTDESK_KEY` | RustDesk server encryption key | Yes | `generated-key-string` | - |\n\nRustDesk provides secure remote desktop access to the infrastructure server. The key should be generated as specified in RustDesk documentation.\n\nRustDesk service is accessible at `rustdesk.${DOMAIN}`.\n\nSources: [infra/.env.template:30-32]()\n\n### System Configuration\n\n| Variable | Purpose | Required | Example | Default |\n|----------|---------|----------|---------|---------|\n| `PUID` | User ID for file permissions | Yes | `1000` | `1000` |\n| `PGID` | Group ID for file permissions | Yes | `1000` | `1000` |\n| `TZ` | Timezone for container logs and timestamps | Yes | `UTC` | `UTC` |\n\nThese variables ensure that files created by Docker containers have correct ownership on the host system. Set `PUID` and `PGID` to match the host user running Docker Compose.\n\nSources: [infra/.env.template:34-37]()\n\n## Infrastructure Service Subdomain Mapping\n\nThe following diagram shows how environment variables in `.env.template` map to service endpoints:\n\n```mermaid\ngraph LR\n    subgraph \"Environment Variables\"\n        DOMAIN_VAR[\"DOMAIN=example.com\"]\n    end\n    \n    subgraph \"Service Endpoints\"\n        HOMEPAGE_URL[\"homepage\u003cbr/\u003eyourdomain.com\"]\n        MLFLOW_URL[\"mlflow\u003cbr/\u003emlflow.yourdomain.com\"]\n        MINIO_API_URL[\"minio-api\u003cbr/\u003eminio-api.yourdomain.com\"]\n        MINIO_CONSOLE_URL[\"minio-console\u003cbr/\u003eminio.yourdomain.com\"]\n        POSTGRES_URL[\"postgres\u003cbr/\u003epostgres.yourdomain.com:5432\"]\n        RUSTDESK_URL[\"rustdesk\u003cbr/\u003erustdesk.yourdomain.com\"]\n        TRAEFIK_URL[\"traefik-dashboard\u003cbr/\u003etraefik.yourdomain.com\"]\n    end\n    \n    DOMAIN_VAR --\u003e HOMEPAGE_URL\n    DOMAIN_VAR --\u003e MLFLOW_URL\n    DOMAIN_VAR --\u003e MINIO_API_URL\n    DOMAIN_VAR --\u003e MINIO_CONSOLE_URL\n    DOMAIN_VAR --\u003e POSTGRES_URL\n    DOMAIN_VAR --\u003e RUSTDESK_URL\n    DOMAIN_VAR --\u003e TRAEFIK_URL\n```\n\n**Service URL Resolution from Domain Variable**\n\nSources: [infra/.env.template:2]()\n\n## Example Model Configuration (pyproject.toml)\n\nThe `pyproject.toml` file defines Python project metadata and dependencies for the example model system. It follows PEP 621 standard for Python project metadata.\n\n### Project Metadata\n\n| Field | Value | Purpose |\n|-------|-------|---------|\n| `name` | `examplemodel` | Python package name |\n| `version` | `0.0.1` | Package version following semantic versioning |\n| `description` | Enhanced refugee camp detection model with production-ready STAC-MLM metadata | Project description |\n| `readme` | `README.md` | Path to README file |\n| `requires-python` | `\u003e=3.10` | Minimum Python version requirement |\n\nSources: [examplemodel/pyproject.toml:1-6]()\n\n### Core Dependencies\n\nThe following table categorizes all dependencies defined in `[project.dependencies]`:\n\n| Category | Packages | Purpose |\n|----------|----------|---------|\n| **ML Framework** | `torch\u003e=2.7.1`, `torchvision\u003e=0.22.1`, `pytorch-lightning\u003e=2.5.2` | PyTorch deep learning framework and Lightning training abstraction |\n| **MLOps** | `mlflow\u003e=3.1.1`, `pynvml\u003e=12.0.0`, `psutil\u003e=7.0.0` | Experiment tracking, GPU monitoring, system resource monitoring |\n| **Model Export** | `onnx\u003e=1.18.0`, `onnxscript\u003e=0.3.2` | ONNX model format export and scripting |\n| **GeoAI** | `geomltoolkits\u003e=0.3.9` | Geospatial ML utilities for data preparation |\n| **Metadata** | `stac-model\u003e=0.3.0`, `pystac\u003e=1.8.0`, `jsonschema\u003e=4.0.0` | STAC-MLM metadata generation and validation |\n| **Infrastructure** | `boto3\u003e=1.39.12`, `requests\u003e=2.28.0`, `dotenv\u003e=0.9.9` | AWS S3/MinIO client, HTTP requests, environment variable management |\n| **Image Processing** | `opencv-python\u003e=4.8.0` | Computer vision operations for inference |\n\nSources: [examplemodel/pyproject.toml:7-24]()\n\n### Dependency Groups\n\nThe `[dependency-groups]` section defines optional dependency sets for specific workflows:\n\n| Group | Packages | Purpose |\n|-------|----------|---------|\n| `validation` | `jsonschema\u003e=4.0.0`, `requests\u003e=2.28.0` | STAC metadata validation workflow |\n\nInstall dependency groups using:\n```bash\nuv sync --group validation\n```\n\nSources: [examplemodel/pyproject.toml:26-30]()\n\n### Dependency Resolution and Lock File\n\nDependencies are resolved and locked using `uv` package manager. The resolution is deterministic and recorded in `uv.lock` (not shown in provided files but referenced in documentation).\n\n```mermaid\ngraph TB\n    subgraph \"Dependency Management\"\n        PYPROJECT[\"pyproject.toml\u003cbr/\u003e[project.dependencies]\"]\n        UV_LOCK[\"uv.lock\u003cbr/\u003eLocked Versions\"]\n        UV_SYNC[\"uv sync command\"]\n        VENV[\".venv/\u003cbr/\u003eVirtual Environment\"]\n    end\n    \n    subgraph \"Training Pipeline\"\n        TRAIN[\"train.py\u003cbr/\u003eTraining Script\"]\n        INFERENCE[\"inference.py\u003cbr/\u003eInference Script\"]\n        MODEL[\"model.py\u003cbr/\u003eModel Architecture\"]\n    end\n    \n    PYPROJECT --\u003e|reads requirements| UV_SYNC\n    UV_SYNC --\u003e|generates/updates| UV_LOCK\n    UV_SYNC --\u003e|installs packages to| VENV\n    \n    VENV --\u003e|provides imports for| TRAIN\n    VENV --\u003e|provides imports for| INFERENCE\n    VENV --\u003e|provides imports for| MODEL\n```\n\n**Dependency Installation Flow**\n\nSources: [examplemodel/pyproject.toml:1-31]()\n\n## Runtime Configuration (.env_sample)\n\nThe `.env_sample` file provides a template for runtime environment variables required by the training and inference scripts when connecting to the infrastructure stack.\n\n### MLflow Tracking Configuration\n\n| Variable | Purpose | Required | Example |\n|----------|---------|----------|---------|\n| `MLFLOW_TRACKING_URI` | MLflow tracking server URL | Yes | `http://mlflow.krschap.tech` |\n\nThis URL points to the MLflow tracking server. It can be:\n- **Local**: `http://localhost:5000` (default MLflow port)\n- **Remote**: `http://mlflow.${DOMAIN}` (infrastructure stack)\n- **HTTPS**: `https://mlflow.${DOMAIN}` (with SSL enabled)\n\nThe training script uses this to log experiments, metrics, parameters, and artifacts.\n\nSources: [examplemodel/.env_sample:4]()\n\n### MinIO S3 Backend Configuration\n\n| Variable | Purpose | Required | Example |\n|----------|---------|----------|---------|\n| `AWS_ACCESS_KEY_ID` | MinIO access key (must match infrastructure `.env`) | Yes | `key_key` |\n| `AWS_SECRET_ACCESS_KEY` | MinIO secret key (must match infrastructure `.env`) | Yes | `secret_secret` |\n| `MLFLOW_S3_ENDPOINT_URL` | MinIO API endpoint URL | Yes | `https://minio-api.krschap.tech` |\n\nThese variables configure the S3-compatible client to store MLflow artifacts in MinIO. The credentials must match the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` set in the infrastructure `.env` file.\n\nThe `MLFLOW_S3_ENDPOINT_URL` should point to the MinIO API endpoint:\n- **Local**: `http://localhost:9000` (default MinIO port)\n- **Remote**: `https://minio-api.${DOMAIN}` (infrastructure stack)\n\nSources: [examplemodel/.env_sample:1-3]()\n\n## Configuration Flow Through System\n\nThe following diagram shows how configuration values flow from files through the system to running processes:\n\n```mermaid\nflowchart TD\n    subgraph \"Configuration Sources\"\n        ENV_TEMPLATE[\".env.template\u003cbr/\u003eInfrastructure Template\"]\n        ENV_FILE[\".env\u003cbr/\u003eActual Values\"]\n        ENV_SAMPLE[\".env_sample\u003cbr/\u003eRuntime Template\"]\n        ENV_RUNTIME[\"Runtime .env\u003cbr/\u003eActual Values\"]\n        PYPROJECT[\"pyproject.toml\u003cbr/\u003eDependencies\"]\n    end\n    \n    subgraph \"Deployment Phase\"\n        SETUP_SH[\"setup.sh script\"]\n        DOCKER_COMPOSE[\"docker-compose.yml\"]\n        SERVICES[\"Running Services:\u003cbr/\u003etraefik, mlflow, minio,\u003cbr/\u003epostgres, rustdesk, homepage\"]\n    end\n    \n    subgraph \"Development Phase\"\n        UV_SYNC[\"uv sync\"]\n        EXPORT_ENV[\"export environment vars\"]\n        ML_SCRIPTS[\"Training/Inference Scripts\"]\n    end\n    \n    subgraph \"Runtime Connections\"\n        MLFLOW_CLIENT[\"MLflow Client\u003cbr/\u003ein Training Script\"]\n        S3_CLIENT[\"Boto3 S3 Client\u003cbr/\u003ein Training Script\"]\n        MLFLOW_SERVER[\"MLflow Server\u003cbr/\u003eRunning Service\"]\n        MINIO_SERVER[\"MinIO Server\u003cbr/\u003eRunning Service\"]\n    end\n    \n    ENV_TEMPLATE --\u003e|\"copy and modify\"| ENV_FILE\n    ENV_FILE --\u003e|\"read by\"| SETUP_SH\n    ENV_FILE --\u003e|\"read by\"| DOCKER_COMPOSE\n    DOCKER_COMPOSE --\u003e|\"starts with env vars\"| SERVICES\n    \n    ENV_SAMPLE --\u003e|\"copy and modify\"| ENV_RUNTIME\n    ENV_RUNTIME --\u003e|\"export vars\"| EXPORT_ENV\n    EXPORT_ENV --\u003e|\"provides config to\"| ML_SCRIPTS\n    \n    PYPROJECT --\u003e|\"defines deps for\"| UV_SYNC\n    UV_SYNC --\u003e|\"installs packages for\"| ML_SCRIPTS\n    \n    ML_SCRIPTS --\u003e|\"creates\"| MLFLOW_CLIENT\n    ML_SCRIPTS --\u003e|\"creates\"| S3_CLIENT\n    \n    MLFLOW_CLIENT --\u003e|\"connects to\u003cbr/\u003eMLFLOW_TRACKING_URI\"| MLFLOW_SERVER\n    S3_CLIENT --\u003e|\"connects to\u003cbr/\u003eMLFLOW_S3_ENDPOINT_URL\"| MINIO_SERVER\n    \n    SERVICES --\u003e|\"includes\"| MLFLOW_SERVER\n    SERVICES --\u003e|\"includes\"| MINIO_SERVER\n```\n\n**Configuration to Runtime Flow**\n\nSources: [infra/.env.template:1-37](), [examplemodel/.env_sample:1-5](), [examplemodel/pyproject.toml:1-31]()\n\n## Environment Variable Priority\n\nWhen both infrastructure and runtime environments use similar variable names, the following priority applies:\n\n| Context | Variables Used | Scope |\n|---------|---------------|-------|\n| **Infrastructure Deployment** | `infra/.env` | Controls Docker Compose services |\n| **Training Script Execution** | `examplemodel/.env` or exported shell variables | Controls training script behavior |\n\nVariables with the same name (e.g., `AWS_ACCESS_KEY_ID`) must have matching values between `infra/.env` and `examplemodel/.env` for the training script to successfully connect to the infrastructure services.\n\n```mermaid\ngraph TB\n    subgraph \"Variable Name Collision\"\n        INFRA_AWS_KEY[\"AWS_ACCESS_KEY_ID\u003cbr/\u003ein infra/.env\"]\n        RUNTIME_AWS_KEY[\"AWS_ACCESS_KEY_ID\u003cbr/\u003ein examplemodel/.env\"]\n    end\n    \n    subgraph \"Service Usage\"\n        MINIO_SVC[\"minio service\u003cbr/\u003eUses infra/.env value\"]\n        TRAIN_SCRIPT[\"train.py\u003cbr/\u003eUses runtime .env value\"]\n    end\n    \n    subgraph \"Connection Requirement\"\n        MATCH[\"Values MUST match\u003cbr/\u003efor connection to succeed\"]\n    end\n    \n    INFRA_AWS_KEY --\u003e|configures| MINIO_SVC\n    RUNTIME_AWS_KEY --\u003e|configures| TRAIN_SCRIPT\n    \n    TRAIN_SCRIPT --\u003e|connects to| MINIO_SVC\n    MINIO_SVC -.-\u003e|validates credentials| MATCH\n    TRAIN_SCRIPT -.-\u003e|provides credentials| MATCH\n```\n\n**Environment Variable Coordination Requirement**\n\nSources: [infra/.env.template:19-20](), [examplemodel/.env_sample:1-2]()\n\n## Configuration Validation Requirements\n\n### Infrastructure Configuration\n\nBefore deploying the infrastructure stack, validate:\n\n1. **Domain DNS Resolution**: Ensure `DOMAIN` resolves to server IP address\n2. **Subdomain DNS**: Ensure wildcard DNS or individual subdomain records exist for all services\n3. **Email Validity**: `ACME_EMAIL` must be a valid email for Let's Encrypt notifications\n4. **Password Hashing**: `TRAEFIK_AUTH_PASSWORD_HASH` must be valid bcrypt hash\n5. **Directory Paths**: All `*_DATA_DIR` variables must be valid, writable paths\n6. **Credentials Strength**: Use strong, unique passwords for all `*_PASSWORD` variables\n\n### Example Model Configuration\n\nBefore running training, validate:\n\n1. **Python Version**: System Python matches `requires-python` constraint (\u003e=3.10)\n2. **Network Connectivity**: `MLFLOW_TRACKING_URI` and `MLFLOW_S3_ENDPOINT_URL` are reachable\n3. **Credential Matching**: S3 credentials match infrastructure MinIO credentials\n4. **Dependency Resolution**: Run `uv sync` successfully without conflicts\n\n## Common Configuration Patterns\n\n### Pattern 1: Local Development\n\nFor local development without infrastructure stack:\n\n**pyproject.toml**: Use as-is\n**.env_sample**: Set to local MLflow instance\n```bash\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n# Omit S3 variables to use local file storage\n```\n\n### Pattern 2: Remote Infrastructure\n\nFor production deployment with remote infrastructure:\n\n**infra/.env**: Set all variables with production values\n**examplemodel/.env**: Match credentials with infrastructure\n```bash\nexport MLFLOW_TRACKING_URI=https://mlflow.yourdomain.com\nexport AWS_ACCESS_KEY_ID=\u003cmatches-infra-value\u003e\nexport AWS_SECRET_ACCESS_KEY=\u003cmatches-infra-value\u003e\nexport MLFLOW_S3_ENDPOINT_URL=https://minio-api.yourdomain.com\n```\n\n### Pattern 3: Hybrid Development\n\nFor local training with remote artifact storage:\n\n**examplemodel/.env**: Mix local tracking with remote storage\n```bash\nexport MLFLOW_TRACKING_URI=http://localhost:5000\nexport AWS_ACCESS_KEY_ID=\u003cremote-credentials\u003e\nexport AWS_SECRET_ACCESS_KEY=\u003cremote-credentials\u003e\nexport MLFLOW_S3_ENDPOINT_URL=https://minio-api.yourdomain.com\n```\n\n## Configuration File Locations in Repository\n\n```mermaid\ngraph TB\n    subgraph \"Repository Root\"\n        INFRA_DIR[\"infra/\"]\n        EXAMPLE_DIR[\"examplemodel/\"]\n    end\n    \n    subgraph \"Infrastructure Config\"\n        ENV_TEMPLATE[\"infra/.env.template\u003cbr/\u003eTemplate File\"]\n        ENV_FILE[\"infra/.env\u003cbr/\u003eActual Configuration\u003cbr/\u003e(git-ignored)\"]\n    end\n    \n    subgraph \"Example Model Config\"\n        PYPROJECT[\"examplemodel/pyproject.toml\u003cbr/\u003eCommitted\"]\n        ENV_SAMPLE[\"examplemodel/.env_sample\u003cbr/\u003eTemplate File\"]\n        ENV_RUNTIME_FILE[\"examplemodel/.env\u003cbr/\u003eActual Configuration\u003cbr/\u003e(git-ignored)\"]\n    end\n    \n    INFRA_DIR --\u003e ENV_TEMPLATE\n    INFRA_DIR --\u003e ENV_FILE\n    EXAMPLE_DIR --\u003e PYPROJECT\n    EXAMPLE_DIR --\u003e ENV_SAMPLE\n    EXAMPLE_DIR --\u003e ENV_RUNTIME_FILE\n    \n    ENV_TEMPLATE -.-\u003e|\"copy to create\"| ENV_FILE\n    ENV_SAMPLE -.-\u003e|\"copy to create\"| ENV_RUNTIME_FILE\n```\n\n**Configuration File Organization**\n\nSources: [infra/.env.template:1](), [examplemodel/pyproject.toml:1](), [examplemodel/.env_sample:1]()\n\n## Security Considerations\n\n### Sensitive Variables\n\nThe following variables contain sensitive information and should never be committed to version control:\n\n| Variable | Location | Sensitivity | Git Status |\n|----------|----------|-------------|------------|\n| `TRAEFIK_AUTH_PASSWORD` | `infra/.env` | High | Ignored |\n| `AWS_ACCESS_KEY_ID` | `infra/.env`, `examplemodel/.env` | High | Ignored |\n| `AWS_SECRET_ACCESS_KEY` | `infra/.env`, `examplemodel/.env` | High | Ignored |\n| `POSTGRES_PASSWORD` | `infra/.env` | High | Ignored |\n| `RUSTDESK_KEY` | `infra/.env` | Medium | Ignored |\n\n### Template Files\n\nOnly `.env.template` and `.env_sample` are committed to the repository. These files contain placeholder values (e.g., `replace-with-*`) that must be replaced with actual secrets during deployment.\n\n### Credential Rotation\n\nWhen rotating credentials:\n\n1. Update `infra/.env` with new values\n2. Update `examplemodel/.env` with matching credentials\n3. Restart infrastructure services: `docker-compose down \u0026\u0026 docker-compose up -d`\n4. Restart any running training scripts to pick up new credentials\n\nSources: [infra/.env.template:8-9](), [infra/.env.template:19-20](), [infra/.env.template:25-26]()"])</script><script>self.__next_f.push([1,"38:T30a5,"])</script><script>self.__next_f.push([1,"# Research and Background\n\n\u003cdetails\u003e\n\u003csummary\u003eRelevant source files\u003c/summary\u003e\n\nThe following files were used as context for generating this wiki page:\n\n- [LICENSE](LICENSE)\n- [README.md](README.md)\n- [docs/report.pdf](docs/report.pdf)\n- [docs/research.pdf](docs/research.pdf)\n- [docs/slides.pdf](docs/slides.pdf)\n\n\u003c/details\u003e\n\n\n\n## Purpose and Scope\n\nThis page provides context on the research origins, academic background, and professional development work that led to the creation of the OpenGeoAIModelHub repository. It covers the project's connection to humanitarian geospatial modeling, standards development, and MLOps infrastructure for GeoAI applications.\n\nFor technical implementation details of the example model, see [Example Model System](#3).  \nFor infrastructure deployment specifics, see [Infrastructure System](#4).  \nFor development workflows, see [Development Guide](#7).\n\n## Project Origin and Context\n\nThe OpenGeoAIModelHub repository originated as an internship project with CDL (Caritas Development Lab) focused on developing reproducible infrastructure for humanitarian geospatial AI models. The core research question was: **How can multiple GeoAI models be deployed and managed on a single platform using metadata standards?**\n\nThe project addresses a specific gap in the humanitarian mapping ecosystem: while numerous geospatial AI models exist for tasks like building detection, refugee camp identification, and infrastructure mapping, there is no standardized approach to:\n\n1. **Model packaging and distribution** - Ensuring models can be shared across different GIS platforms\n2. **Experiment tracking** - Maintaining reproducibility of training runs and model versions\n3. **Metadata standardization** - Describing model capabilities, inputs, and outputs consistently\n4. **Deployment infrastructure** - Providing production-ready MLOps tooling for humanitarian organizations\n\nSources: [README.md:1-9]()\n\n```mermaid\ngraph TB\n    subgraph \"Problem Space\"\n        P1[\"Fragmented Model\u003cbr/\u003eDeployment\"]\n        P2[\"Lack of Reproducibility\"]\n        P3[\"Platform Lock-in\"]\n        P4[\"No Standard Metadata\"]\n    end\n    \n    subgraph \"Solution Components\"\n        S1[\"STAC-MLM\u003cbr/\u003eMetadata Standard\"]\n        S2[\"MLflow\u003cbr/\u003eExperiment Tracking\"]\n        S3[\"ESRI DLPK\u003cbr/\u003eArcGIS Integration\"]\n        S4[\"ONNX\u003cbr/\u003eCross-platform\"]\n    end\n    \n    subgraph \"Repository Implementation\"\n        R1[\"Example Model\u003cbr/\u003erefugee_camp_detection\"]\n        R2[\"MLproject\u003cbr/\u003etrain.py, inference.py\"]\n        R3[\"Infrastructure\u003cbr/\u003edocker-compose.yml\"]\n        R4[\"STAC2ESRI\u003cbr/\u003eRefugeeCampDetector.py\"]\n    end\n    \n    P1 --\u003e S3\n    P2 --\u003e S2\n    P3 --\u003e S4\n    P4 --\u003e S1\n    \n    S1 --\u003e R4\n    S2 --\u003e R3\n    S3 --\u003e R4\n    S4 --\u003e R2\n    \n    R1 -.implements.- R2\n    R2 -.orchestrated by.- R3\n    R4 -.packages.- R1\n```\n\n**Project Context Mapping**\n\nSources: [README.md:1-9](), [docs/report.pdf:1-6]()\n\n## Research Objectives\n\nThe primary research objectives that shaped this codebase were:\n\n| Objective | Implementation in Codebase | Status |\n|-----------|---------------------------|--------|\n| **Demonstrate end-to-end MLOps workflow** | Complete pipeline from data acquisition ([train.py]()) through model training to deployment ([RefugeeCampDetector.py]()) |  Complete |\n| **Implement STAC-MLM metadata standard** | STAC item generation ([train.py]()) with MLM extension for model description |  Complete |\n| **Enable multi-platform deployment** | ONNX export ([train.py]()) + ESRI DLPK packaging ([RefugeeCampDetector.py]()) |  Complete |\n| **Provide reusable infrastructure** | Docker Compose stack ([docker-compose.yml]()) with Traefik, MLflow, MinIO, PostgreSQL |  Complete |\n| **Establish reproducible experiments** | MLproject file ([MLproject]()) with parameterized entry points |  Complete |\n\nThe example model (refugee camp detection using U-Net architecture) serves as a proof-of-concept demonstrating that humanitarian geospatial models can be packaged with standardized metadata and deployed across multiple platforms (ArcGIS Pro, ONNX Runtime) from a single training pipeline.\n\nSources: [README.md:1-9](), [docs/research.pdf:1-6]()\n\n## Research Ecosystem and Related Work\n\n```mermaid\ngraph LR\n    subgraph \"External Organizations\"\n        CDL[\"CDL\u003cbr/\u003e(Caritas Development Lab)\"]\n        GEOHUM[\"GeoHUM\u003cbr/\u003eHumanitarian Mapping Network\"]\n        AGIT[\"AGIT Conference\u003cbr/\u003eSalzburg\"]\n        FOSS4G[\"FOSS4G Europe 2025\u003cbr/\u003eMostar\"]\n    end\n    \n    subgraph \"Data Sources\"\n        OAM[\"OpenAerialMap\u003cbr/\u003eSatellite Imagery\"]\n        OSM[\"OpenStreetMap\u003cbr/\u003eGround Truth Labels\"]\n    end\n    \n    subgraph \"Related Projects\"\n        COG2TILES[\"cog2tiles\u003cbr/\u003egithub.com/kshitijrajsharma/cog2tiles\"]\n        GOOGLE_MSF[\"google_buildings_msf\u003cbr/\u003egithub.com/kshitijrajsharma/google_buildings_msf\"]\n    end\n    \n    subgraph \"This Repository\"\n        TRAIN[\"train.py\u003cbr/\u003eTraining Pipeline\"]\n        INFRA[\"docker-compose.yml\u003cbr/\u003eInfrastructure\"]\n        ESRI[\"RefugeeCampDetector.py\u003cbr/\u003eESRI Integration\"]\n    end\n    \n    CDL --\u003e|\"Internship Context\"| TRAIN\n    GEOHUM --\u003e|\"Network Affiliation\"| CDL\n    AGIT --\u003e|\"Presentation Venue\"| TRAIN\n    FOSS4G --\u003e|\"Conference Presentation\"| TRAIN\n    \n    OAM --\u003e|\"Imagery Feed\"| TRAIN\n    OSM --\u003e|\"Labels Source\"| TRAIN\n    \n    COG2TILES -.-\u003e|\"Tile Processing\"| TRAIN\n    GOOGLE_MSF -.-\u003e|\"Related Work\"| TRAIN\n    \n    TRAIN --\u003e INFRA\n    TRAIN --\u003e ESRI\n```\n\n**Research Ecosystem and Connections**\n\nSources: [README.md:1-9]()\n\n### Related Projects and Tools\n\nThe repository builds on and connects to several related open-source projects:\n\n**cog2tiles** ([github.com/kshitijrajsharma/cog2tiles]()):\n- Utility for converting Cloud Optimized GeoTIFFs to tile pyramids\n- Used in preprocessing pipeline for efficient data loading\n- Complementary tool for handling large satellite imagery datasets\n\n**google_buildings_msf** ([github.com/kshitijrajsharma/google_buildings_msf]()):\n- Building footprint extraction using Google Open Buildings dataset\n- Demonstrates similar humanitarian mapping use case\n- Shares infrastructure patterns (MLflow, Docker Compose)\n\nSources: [README.md:1-9]()\n\n## Academic and Professional Context\n\n### Internship Affiliation\n\nThe work was conducted as part of an internship with **CDL (Caritas Development Lab)**, focused on developing open-source infrastructure for humanitarian AI applications. CDL is affiliated with the **GeoHUM** network, which connects researchers and practitioners working on geospatial technologies for humanitarian purposes.\n\n**Key Organizations:**\n\n- **CDL**: Research and development lab focusing on technology for humanitarian aid\n- **GeoHUM** ([geohum.eu]()): European network for humanitarian geospatial research\n- **AGIT** ([agit.at]()): Applied Geoinformatics conference in Salzburg, Austria\n- **FOSS4G Europe 2025** ([2025.europe.foss4g.org]()): Free and Open Source Software for Geospatial conference\n\nSources: [README.md:1-9](), [docs/report.pdf:1-6]()\n\n### Conference Presentations and Publications\n\nThe research and implementation have been presented at academic and professional venues:\n\n```mermaid\ngraph TD\n    subgraph \"Documentation Timeline\"\n        D1[\"Internship Report\u003cbr/\u003edocs/report.pdf\u003cbr/\u003e2025\"]\n        D2[\"Research Document\u003cbr/\u003edocs/research.pdf\u003cbr/\u003eMLOps for Humanitarian Geospatial Models\"]\n        D3[\"Presentation Slides\u003cbr/\u003egamma.app/docs/Internship-Presentation-3-min-xsuj64t3h0kyow4\"]\n        D4[\"Video Demo\u003cbr/\u003ekrschap.fra1.digitaloceanspaces.com/tosharewithleslie.webm\"]\n    end\n    \n    subgraph \"Conference Venues\"\n        C1[\"AGIT Conference\u003cbr/\u003eSalzburg\"]\n        C2[\"FOSS4G Europe 2025\u003cbr/\u003eMostra\"]\n    end\n    \n    subgraph \"Audience\"\n        A1[\"Academic Researchers\"]\n        A2[\"Humanitarian Organizations\"]\n        A3[\"GeoAI Practitioners\"]\n        A4[\"Open Source Community\"]\n    end\n    \n    D2 --\u003e C1\n    D2 --\u003e C2\n    D3 --\u003e C1\n    D3 --\u003e C2\n    \n    C1 --\u003e A1\n    C1 --\u003e A3\n    C2 --\u003e A4\n    C2 --\u003e A2\n    \n    D1 -.internal.- CDL[\"CDL Organization\"]\n    D4 -.demo for.- A2\n```\n\n**Documentation and Dissemination**\n\nSources: [README.md:1-9]()\n\n### Available Research Materials\n\n| Resource Type | Location | Description |\n|--------------|----------|-------------|\n| **Internship Report** | [docs/report.pdf]() | Complete internship documentation (2025) |\n| **Research Paper** | [docs/research.pdf]() | \"MLOps for Humanitarian Geospatial Models\" - technical paper |\n| **Presentation Slides** | [gamma.app/docs/Internship-Presentation-3-min-xsuj64t3h0kyow4]() | 3-minute presentation deck |\n| **Video Demonstration** | [krschap.fra1.digitaloceanspaces.com/tosharewithleslie.webm]() | System walkthrough and demo |\n| **Live Deployment** | [mlflow.krschap.tech]() | Production MLflow instance |\n\nSources: [README.md:1-9]()\n\n## Technical Foundations and Standards\n\nThe research implementation leverages several key technical standards and frameworks:\n\n### Metadata Standards\n\n**STAC-MLM (SpatioTemporal Asset Catalog - Machine Learning Model Extension)**:\n- Provides standardized JSON schema for describing ML model metadata\n- Implemented in [train.py]() during model export\n- Enables model discovery and cataloging\n- Stores model parameters, training configuration, and performance metrics in `stac_item.json`\n\n### Model Packaging Standards\n\n**ONNX (Open Neural Network Exchange)**:\n- Cross-platform model format for interoperability\n- Exported from PyTorch checkpoints in [train.py]()\n- Enables deployment without framework dependencies\n- Stored as `best_model.onnx` artifact\n\n**ESRI Deep Learning Package (DLPK)**:\n- ArcGIS Pro-compatible model package format\n- Generated by [RefugeeCampDetector.py]()\n- Bundles ONNX model + STAC metadata + Python inference script\n- Enables direct deployment in ArcGIS Pro\n\nSources: [src/train.py](), [src/esri/RefugeeCampDetector.py]()\n\n```mermaid\ngraph LR\n    subgraph \"Standards Layer\"\n        STAC[\"STAC-MLM\u003cbr/\u003eMetadata Schema\"]\n        ONNX_STD[\"ONNX\u003cbr/\u003eModel Format\"]\n        DLPK_STD[\"ESRI DLPK\u003cbr/\u003ePackage Format\"]\n    end\n    \n    subgraph \"Implementation Layer\"\n        TRAIN_FUNC[\"create_stac_item()\u003cbr/\u003etrain.py:400-500\"]\n        ONNX_EXPORT[\"torch.onnx.export()\u003cbr/\u003etrain.py:350-380\"]\n        DLPK_CREATE[\"create_dlpk()\u003cbr/\u003eRefugeeCampDetector.py:50-150\"]\n    end\n    \n    subgraph \"Artifact Layer\"\n        STAC_FILE[\"stac_item.json\u003cbr/\u003eModel Metadata\"]\n        ONNX_FILE[\"best_model.onnx\u003cbr/\u003eModel Weights\"]\n        DLPK_FILE[\"best_model.dlpk\u003cbr/\u003eArcGIS Package\"]\n    end\n    \n    STAC --\u003e TRAIN_FUNC\n    ONNX_STD --\u003e ONNX_EXPORT\n    DLPK_STD --\u003e DLPK_CREATE\n    \n    TRAIN_FUNC --\u003e STAC_FILE\n    ONNX_EXPORT --\u003e ONNX_FILE\n    DLPK_CREATE --\u003e DLPK_FILE\n    \n    STAC_FILE --\u003e DLPK_CREATE\n    ONNX_FILE --\u003e DLPK_CREATE\n```\n\n**Standards Implementation Pipeline**\n\nSources: [src/train.py](), [src/esri/RefugeeCampDetector.py]()\n\n### MLOps Framework\n\n**MLflow**:\n- Experiment tracking and model registry platform\n- Configured in [docker-compose.yml]() as `mlflow` service\n- Tracks metrics, parameters, and artifacts for all training runs\n- Provides REST API for model deployment\n- Backed by PostgreSQL for metadata storage and MinIO for artifact storage\n\n**Infrastructure Components** (defined in [docker-compose.yml]()):\n- `mlflow` service: Experiment tracking server (port 5000)\n- `postgres` service: PostgreSQL + PostGIS for metadata storage\n- `minio` service: S3-compatible object storage for artifacts\n- `traefik` service: Reverse proxy with automatic SSL\n\nSources: [docker-compose.yml:1-200](), [setup.sh:1-100]()\n\n## Repository Documentation Structure\n\nThe repository provides comprehensive documentation across multiple formats:\n\n| Documentation Type | Primary Audience | File Reference |\n|-------------------|-----------------|----------------|\n| Technical README | Developers, DevOps engineers | [README.md:1-9]() |\n| Internship Report | Academic supervisors, CDL | [docs/report.pdf]() |\n| Research Paper | Academic researchers, conference attendees | [docs/research.pdf]() |\n| API Reference | ML engineers, model developers | This wiki page 8.1 |\n| Configuration Guide | System administrators | This wiki page 8.2 |\n| Development Guide | Contributing developers | This wiki page 7 |\n\nThis multi-layered documentation approach ensures that different stakeholders (academic, technical, operational) can access information appropriate to their needs.\n\nSources: [README.md:1-9](), [docs/report.pdf](), [docs/research.pdf]()"])</script><script>self.__next_f.push([1,"5:[\"$\",\"$L15\",null,{\"repoName\":\"kshitijrajsharma/opengeoaimodelshub\",\"hasConfig\":false,\"canSteer\":true,\"children\":[\"$\",\"$L16\",null,{\"wiki\":{\"metadata\":{\"repo_name\":\"kshitijrajsharma/opengeoaimodelshub\",\"commit_hash\":\"393e3158\",\"generated_at\":\"2026-01-13T11:23:12.622956\",\"config\":null,\"config_source\":\"none\"},\"pages\":[{\"page_plan\":{\"id\":\"1\",\"title\":\"Overview\"},\"content\":\"$17\"},{\"page_plan\":{\"id\":\"2\",\"title\":\"Getting Started\"},\"content\":\"$18\"},{\"page_plan\":{\"id\":\"2.1\",\"title\":\"Prerequisites and Installation\"},\"content\":\"$19\"},{\"page_plan\":{\"id\":\"2.2\",\"title\":\"Quick Start Guide\"},\"content\":\"$1a\"},{\"page_plan\":{\"id\":\"3\",\"title\":\"Example Model System\"},\"content\":\"$1b\"},{\"page_plan\":{\"id\":\"3.1\",\"title\":\"Model Overview and Architecture\"},\"content\":\"$1c\"},{\"page_plan\":{\"id\":\"3.2\",\"title\":\"Training Pipeline\"},\"content\":\"$1d\"},{\"page_plan\":{\"id\":\"3.3\",\"title\":\"Inference System\"},\"content\":\"$1e\"},{\"page_plan\":{\"id\":\"3.4\",\"title\":\"ESRI Integration and DLPK Generation\"},\"content\":\"$1f\"},{\"page_plan\":{\"id\":\"3.5\",\"title\":\"MLflow Project Structure\"},\"content\":\"$20\"},{\"page_plan\":{\"id\":\"3.6\",\"title\":\"Dependencies and Configuration\"},\"content\":\"$21\"},{\"page_plan\":{\"id\":\"3.7\",\"title\":\"Utilities and Helper Functions\"},\"content\":\"$22\"},{\"page_plan\":{\"id\":\"4\",\"title\":\"Infrastructure System\"},\"content\":\"$23\"},{\"page_plan\":{\"id\":\"4.1\",\"title\":\"Service Architecture\"},\"content\":\"$24\"},{\"page_plan\":{\"id\":\"4.2\",\"title\":\"Setup and Deployment\"},\"content\":\"$25\"},{\"page_plan\":{\"id\":\"4.3\",\"title\":\"Configuration Management\"},\"content\":\"$26\"},{\"page_plan\":{\"id\":\"4.4\",\"title\":\"Traefik Reverse Proxy\"},\"content\":\"$27\"},{\"page_plan\":{\"id\":\"4.5\",\"title\":\"MLflow Tracking Server\"},\"content\":\"$28\"},{\"page_plan\":{\"id\":\"4.6\",\"title\":\"Storage Services (MinIO and PostgreSQL)\"},\"content\":\"$29\"},{\"page_plan\":{\"id\":\"4.7\",\"title\":\"Additional Services\"},\"content\":\"$2a\"},{\"page_plan\":{\"id\":\"5\",\"title\":\"CI/CD Pipelines\"},\"content\":\"$2b\"},{\"page_plan\":{\"id\":\"5.1\",\"title\":\"Docker Image Publishing Workflows\"},\"content\":\"$2c\"},{\"page_plan\":{\"id\":\"5.2\",\"title\":\"MLflow Custom Image Pipeline\"},\"content\":\"$2d\"},{\"page_plan\":{\"id\":\"6\",\"title\":\"Deployment Guide\"},\"content\":\"$2e\"},{\"page_plan\":{\"id\":\"6.1\",\"title\":\"Infrastructure Deployment\"},\"content\":\"$2f\"},{\"page_plan\":{\"id\":\"6.2\",\"title\":\"Model Deployment Options\"},\"content\":\"$30\"},{\"page_plan\":{\"id\":\"7\",\"title\":\"Development Guide\"},\"content\":\"$31\"},{\"page_plan\":{\"id\":\"7.1\",\"title\":\"Local Development Setup\"},\"content\":\"$32\"},{\"page_plan\":{\"id\":\"7.2\",\"title\":\"Working with the Training Pipeline\"},\"content\":\"$33\"},{\"page_plan\":{\"id\":\"7.3\",\"title\":\"Data Preparation and Custom Datasets\"},\"content\":\"$34\"},{\"page_plan\":{\"id\":\"8\",\"title\":\"Reference\"},\"content\":\"$35\"},{\"page_plan\":{\"id\":\"8.1\",\"title\":\"MLproject API Reference\"},\"content\":\"$36\"},{\"page_plan\":{\"id\":\"8.2\",\"title\":\"Configuration Reference\"},\"content\":\"$37\"},{\"page_plan\":{\"id\":\"8.3\",\"title\":\"Research and Background\"},\"content\":\"$38\"}]},\"children\":\"$L39\"}]}]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"TechArticle\\\",\\\"headline\\\":\\\"Overview\\\",\\\"description\\\":\\\"This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr\\\",\\\"image\\\":\\\"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub/og-image.png\\\",\\\"datePublished\\\":\\\"2026-01-13T11:23:12.622956\\\",\\\"dateModified\\\":\\\"2026-01-13T11:23:12.622956\\\",\\\"author\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"url\\\":\\\"https://deepwiki.com\\\"},\\\"publisher\\\":{\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"DeepWiki\\\",\\\"logo\\\":{\\\"@type\\\":\\\"ImageObject\\\",\\\"url\\\":\\\"https://deepwiki.com/icon.png\\\"}},\\\"mainEntityOfPage\\\":{\\\"@type\\\":\\\"WebPage\\\",\\\"@id\\\":\\\"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub\\\"}}\"}}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]\n"])</script><script>self.__next_f.push([1,"e:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"kshitijrajsharma/opengeoaimodelshub | DeepWiki\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr\"}],[\"$\",\"meta\",\"2\",{\"name\":\"keywords\",\"content\":\"kshitijrajsharma/opengeoaimodelshub,kshitijrajsharma,opengeoaimodelshub,documentation,wiki,codebase,AI documentation,Devin,Overview\"}],[\"$\",\"link\",\"3\",{\"rel\":\"canonical\",\"href\":\"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"kshitijrajsharma/opengeoaimodelshub | DeepWiki\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:site_name\",\"content\":\"DeepWiki\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:image\",\"content\":\"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub/og-image.png?page=1\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:site\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:creator\",\"content\":\"@cognition\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"kshitijrajsharma/opengeoaimodelshub | DeepWiki\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"This document provides a high-level introduction to the OpenGeoAIModelHub repository, explaining its architecture, core components, and dual purpose as both an example ML model implementation and a pr\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:image\",\"content\":\"https://deepwiki.com/kshitijrajsharma/opengeoaimodelshub/og-image.png?page=1\"}],[\"$\",\"link\",\"16\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"17\",{\"rel\":\"icon\",\"href\":\"/icon.png?1ee4c6a68a73a205\",\"type\":\"image/png\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"18\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?a4f658907db0ab87\",\"type\":\"image/png\",\"sizes\":\"180x180\"}],[\"$\",\"$L3a\",\"19\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"13:\"$e:metadata\"\n"])</script><script>self.__next_f.push([1,"39:[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]\n"])</script></body></html>