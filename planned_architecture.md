# Planned Architecture

The following is a text-based description of the planned
new architecture - most likely to have architecture diagrams
generated via an open tool or simply DeepWiki.

## The Proposed Stack (Summary)

- Metadata database: Postgres, via CloudNativePG in cluster
- User facing UI: fAIr frontend.
- User facing API: fAIr backend.
- ML model workflow orchstrator: Flyte
- ML model experimentation and tracking: MLFlow
- ML model inference: ONNX (k8s and browser-based)
- ML model artifact storage: AWS S3
- ML model discoverability: STAC extension, possibly
  attached to existing STAC served for OpenAerialMap
  / eoAPI?

## Geo-AI Metadata

- See a nice writeup by Kshitij on the current state of Geo-AI
  architecture [here](./geo_ai_stac.md).
- We lean heavily on STAC at HOT, in particular for our open
  imagery (such as drone imagery).
- We plan to use a STAC extension to document our open AI models
  that are generated by fAIr and making them accessible to end
  users (+ easily discoverable by code too).
- A crucial piece of metadata will be the runtime requirements
  for model artifacts: e.g. if running via browser onnxruntime
  if they need WASM, WebGPU, WebNN enabled to run.

## Add MLFlow Into The Workflow

- MLFlow isn't a workflow orchestrator, but is more of an ML workflow
  experimentor and lineage tracker.
- We can add MLFlow linkage into our Python scripts, allowing for
  ML workflow artifacts and metadata to be recorded in MLFlow.
- In the end, we can reference the MLFlow metadta in the STAC
  metadata for tracking back the ML model training and viewing
  entire data / model lineage.

### Model Registry

- Models are interated and each version is stored in
  the MLFlow registry, plus metadata.
- It's possible to promote models from dev --> staging
  --> production from the registry, depending on how
  good they are.
- The model registry is the source of truth for ML model
  iteration (referencing a run ID, model name, version).

E.g. as part of STAC metadata

```json
{
  "rel": "mlflow-run",
  "href": "mlflow://runs/abc123"
}
```

## Replace Celery --> Flyte

- Sam did an assessment of available MLOps tools and determined
  that Flyte is probably best for our needs.
- ZenML was a strong contender, but it doesn't seem as production
  ready guaranteed and isn't backed by big players.
- Old school tools like Kubeflow work well, but are not particularly
  friendly to use from both an Ops and Dev perspective.
- Generic workflow orchestrators such as Airflow, Prefect, Dagster,
  Luigi, Argo, etc don't seem as suitable / tailored for ML workflows
  (despite me having a love for Argo and favouring that for generic
  workflows).
- They also recently released v2, which looks like a nice upgrade.

### How Flytes Fits In

- Flyte is installed inside the Kubernetes cluster, with a 
  GPU-enabled node at it's disposal.
- Experimentation is done by MLFlow.
- Whenever training is required, the info is passed to the
  Flyte API.
- Flyte will pick up the info and run any training inside the
  k8s cluster, tracking the metadata and temp artifacts with
  [MLFlow integration](https://docs-legacy.flyte.org/en/latest/flytesnacks/examples/mlflow_plugin/index.html).
- The workloads will be distributed across nodes in the k8s cluster,
  in particular with GPU-enabled nodes when needed.
- Final data and params are passed into Flyte for training.
- The final artifacts are passed to KServe for serving and inference.
  (Seldon Core could also be a good options)
- Flyte also has plugins to generate ONNX models from Tensorflow,
  PyTorch, Scikit-Learn outputs,
  e.g. https://github.com/flyteorg/flytekit/tree/master/plugins/flytekit-onnx-tensorflow

### How DO Flyte & ML Flow Compare?

MLflow
- Experiment tracking
- Metrics, parameters, artifacts
- Model registry

Flyte
- Workflow orchestration
- Task scheduling (CPU/GPU)
- Data passing between steps
- Retry, caching, versioned workflows

## Model Inference

- Ideally we generate ONNX models for inference.
- We can run inference within the cluster if needed, serving
  the final output to users (via KServe?).
- Else we can use onnxruntime in the browser (WASM) to do the
  inference entirely in the users browser.
- Or we use both, one of the key feature of fAIr is to be able push its generated predictions for validation via already integrated tool like Mapswipe and JOSM , meanwhile inbrowser prediction remains useful to test and visulize the predictions in the browser directly 

**Batch inference**: Flyte
**Online API inference**: KServe via fAIr backend
**Client-side inference**: ONNX WASM

## The Kubernetes Cluster

- Underpins the usage of all these tools - hosted there.
- Node autoscaling (GPU-enabled in particular) is done via
  Karpenter on AWS, with scaling down to 0 if possible.
- Autoscaling of Flyte workflows is done via KEDA most
  likely, increasing the number of instances of the Flyte
  components with demand.
- Resource usage will be tracked via Prometheus, and
  visualised via a Grafana dashboard: observability.

### Workflow Observability - Logging

- OpenTelemetry is the gold-standard open-source way
  to track Python data pipelines.
- Flyte has support to configure OTEL as part of it's
  helm chart deployment, via a configmap, see
  [here](https://github.com/flyteorg/flyte/pull/6543).
- Python ML training workflows should also ideally
  be intrumented using OTEL, so at the very least stdlib
  logging can be recorded, and at best we can have traces
  from the fAIr frontend --> fAIr API call --> postgres
  queries --> Flyte workflow details --> ML workflow traces
  and back.
